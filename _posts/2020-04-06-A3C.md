---
layout:     post
title:      Asynchronous Methods for Deep Reinforcement Learning
subtitle:   A3C from Deepmind
date:       2020-04-06 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - DeepMind
    - Reinforcement Learning
    - A3C

---

 

## Asynchronous Methods for Deep Reinforcement Learning

### Abstract

- 提出了一个轻量级的DRL框架，利用异步GD(**asynchronous gradient descent**)对DNN controller 进行优化。
- 提出了 4个标准RL算法 的异步版本，并表明并行的actor-learners对训练有稳定的作用
- 性能最好是 actor-critic的异步版本(A3C)，在多核CPU训练的时间只需GPU上的训练时间的一半
- asynchronous actor-critic 在各种连续马达控制问题以及使用视觉输入随机3D迷宫的导航任务上都成功。



### Introduction

> - On-line : step-by-step
> - Off-line : episode-by-episode



- DNN 可以给 RL 提供 rich representations.   但是认为 **online RL + DNN**，是unstable.   因为:
  - sequence of online RL agent is **non-stationary**, 
  - online RL updates are strongly **correlated**.  

- 通过 **experience replay** memory, reduces non-stationarity and decorrelates updates,  但该方法只能 off-policy.  
- experience replay 缺点:   
  - more **memory** and **computation** per real interaction;    
  - requires off-policy ,  data 可能来自 older policy.

本文提出一个与之前很不同的RL范例paradigm:   

1. 异步并行 multi agent 代替 experience replay ;  
2. parallelism 也能 **decorrelates data** into more stationary process, 因为agent会面对更多样的state
3. 该思想使得  on-poliy(Sarsa, n-step methods,  AC), off-policy(Q-learning) 方法 + DNN 更稳定高效
4. 不用GPU或者其他分布式架构,  单机多核CPU就可以;  更少资源更好结果.
5. 表现最好的: **asynchronous advantage actor-critic (A3C)** , 支持离散和连续问题, 训练feedforward and recurrent agent. 



### Related Work

- (Nair et al., 2015) General Reinforcement Learning Architecture (Gorila)    
  Gorila使用100个独立的actor-learner进程和30个参数服务器，总共130台机器.  
  Gorila能够在49个Atari游戏中明显优于DQN。在许多游戏中，Gorila达到了DQN的分数，比DQN快20倍以上。
- (Chavez et al., 2015) parallelizing DQN
- (Li & Schuurmans, 2011), MapReduce 被加速矩阵操作. batch RL with linear function approximation.
- (Grounds & Kudenko, 2008)  parallel Sarsa
- (Tsitsiklis, 1994)   研究 异步Q-learning的收敛性 : 只要过期信息最终总是被丢弃，并且满足其他几个假设，那么在部分信息过期的情况下, Q-learning收敛.
- (Bertsekas, 1982)  distributed dynamic programming
- 其他 evolutionary methods, 很方便并行.



### Reinforcement Learning Background

##### value-based model-free RL

one-step Q-learning  $i$ th loss func:

$$
L_{i}\left(\theta_{i}\right)=\mathbb{E}\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i-1}\right)-Q\left(s, a ; \theta_{i}\right)\right)^{2}
$$

n-step Q-learning  
target:   $n$ step return :  $$r_{t}+\gamma r_{t+1}+\cdots+\gamma^{n-1} r_{t+n-1}+ \max _{a} \gamma^{n} Q\left(s_{t+n}, a\right)$$

这使得单个r直接影响到n个之前的(s,a)的值。这使得将reward传播到相关(s,a)的过程可能更有效率。


##### policy-based model-free RL

Standard **REINFORCE** : $\nabla_{\theta} \log \pi\left(a_{t} | s_{t} ; \theta\right) R_{t}$   
**unbiased** estimate of $\nabla_{\theta} \mathbb{E}\left[R_{t}\right] .$   

**baseline** (Williams, 1992)   $\nabla_{\theta} \log \pi\left(a_{t} \vert s_{t} ; \theta\right)\left(R_{t}-b_{t}\left(s_{t}\right)\right)$ : reduce the **variance** while unbiased   

A learned estimate of the **value function** is commonly used as the baseline $b_{t}\left(s_{t}\right) \approx V^{\pi}\left(s_{t}\right)$ , leading to a much lower variance estimate of the policy gradient. 

When an approximate value function is used as the baseline, the quantity $R_{t}-b_{t}$ used to scale the policy gradient can be seen as an estimate of the **advantage** of action $a_{t}$ in state $s_{t},$ or $A\left(a_{t}, s_{t}\right)=Q\left(a_{t}, s_{t}\right)-V\left(s_{t}\right)$,  because $R_{t}$ is an estimate of $Q^{\pi}\left(a_{t}, s_{t}\right)$ and $b_{t}$ is an estimate of $V^{\pi}\left(s_{t}\right) .$ This approach can be viewed as an **actor-critic** architecture where the policy $\pi$ is the **actor** and the **baseline** $b_{t}$ is the **critic** (Sutton  & Barto, $1998 ;$ Degris et al., 2012 ).



### Asynchronous RL Framework

现在提出 多线程异步版本的RL: multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic.   
目标: 可靠地 train DNN policy , without large resource requirements  
虽然底层的RL方法有很大的不同: actor-critic, on-policy policy search method，Q-learning, off-policy value-based method，基于两个主要的想法，使这四种算法都能符合我们的目标。

- **asynchronous actor-learners**, 类似 Gorila.  多核单机版.  省去网络通信消耗, 并可以在train的时候使用Hogwild! (Recht et al., 2011) style updates
- explicitly use different exploration policies in each actor-learner to **maximize this diversity**.  
  通过在不同的线程中运行不同的探索策略，多个actor-learninger并行做online 更新, 参数的整体变化在时间上的相关性会变小。因此，我们**不使用replay memory，而是依靠并行的agent采用不同的探索策略 来起稳定作用。**

另外, 除了学习的稳定性, 使用多个并行actor-learner也有很多实际的好处:

- 训练时间的减少，和并行actor-learner的数量上大致呈线性关系。
- 由于不再依experience replay来稳定学习，因此可以使用Sarsa和actor-critic等**on-policy**方法来稳定地训练

下面介绍4个变体:

##### Asynchronous one-step Q-learning

<img src="/img/2020-04-01-DQN.assets/image-20200412034126275.png" alt="image-20200412034126275" style="zoom:50%;" />

- 每个线程与自己的env副本交互，并在每一步计算Q-learning loss的gradient。
- 在计算loss时，我们使用一个共享的、缓慢变化的 **target networ**，就像DQN。
- **累积多个timesteps的gradient**, 然后再apply到model, 类似minibatches。这就减少了多个actor learners互相覆盖对方的更新的几率。多个timesteps的累积更新也提供了权衡 computational efficiency与data efficiency的能力。
- 我们发现给予每个线程不同的探索策略有助于提高鲁棒性。以这种方式增加探索的多样性，通常也提高性能。使得探索策略不同的方式很多, 这里使用的是 ε-greedy, 每个线程周期性地从某个分布中sample ε. 



##### Asynchronous one-step Sarsa

- 与异步Q-learning算法基本相同, 只是target不一样: $r+\gamma Q\left(s^{\prime}, a^{\prime} ; \theta^{-}\right)$ 
- 仍然使用了 target network , accumulated over multiple timestep



##### Asynchronous n-step Q-learning

![image-20200412151455953](/img/2020-04-01-DQN.assets/image-20200412151455953.png)

- 跟之前不同的是, 使用了 forward view , 而不是像 eligibility traces 常用的backward view. 
- 我们发现，当使用momentum-based方法训练神经网络, backpropagation through time时，使用forward view更容易
- 为了计算单次更新，算法首先使用其探索策略选择 $t_{\max }$ 步 action. 得到的结果是，agent从env中接收到自上次更新以来 $t_{\max }$ 个的reward。  
  然后, 该算法为上次更新后遇到的每个(s,a)对计算出n-step Q-learning updates的gradient。每一个n-stepupdate使用尽可能长的n-step return，导致最后一个状态的one-step update、倒数第二个状态的two-step update，以此类推，总的更新次数最多为$t_{\max }$。累积的更新在一个gradient step中applied。



##### Asynchronous advantage actor-critic A3C

![image-20200412162252747](/img/2020-04-01-DQN.assets/image-20200412162252747.png)

- A3C ,  maintains   $\pi\left(a_{t} \vert s_{t} ; \theta\right)$ and  $V\left(s_{t} ; \theta_{v}\right)$ 
- **forward view**,  uses mix of n-step returns to update  policy and value-function.   
  policy and value function are updated after every $t_{\max }$ actions or terminal. 
- The update performed : $\nabla_{\theta^{\prime}} \log \pi\left(a_{t} | s_{t} ; \theta^{\prime}\right) A\left(s_{t}, a_{t} ; \theta, \theta_{v}\right)$    
  advantage function  $A\left(s_{t}, a_{t} ; \theta, \theta_{v}\right)$:   $\sum_{i=0}^{k-1} \gamma^{i} r_{t+i}+\gamma^{k} V\left(s_{t+k} ; \theta_{v}\right)-V\left(s_{t} ; \theta_{v}\right)$  
  where $k$  <= $t_{\max }$ , 根据s 而定. 
- 注意, 虽然将两个网络参数 policy $θ$ 和  value $θ_v$ 分开显示, 但实践中, 会共享一些网络参数.  
  共享使用一个CNN, 非输出层都共享;   后面接了两个输出:  softmax for policy , linear for value.

- 我们发现，在目标函数中加入策略π的熵entropy，可以阻止过早地收敛到suboptimal deterministic策略，改善了探索的效果。  
  该技术最初是由(Williams & Peng, 1991)提出，他发现该技术对需要hierarchical behavior的任务特别有帮助。  
- 包括熵正则化项的 loss 公式:  $\nabla_{\theta^{\prime}} \log \pi\left(a_{t} \vert s_{t} ; \theta^{\prime}\right)\left(R_{t}-V\left(s_{t} ; \theta_{v}\right)\right)+ \beta \nabla_{\theta^{\prime}} H\left(\pi\left(s_{t} ; \theta^{\prime}\right)\right)$



##### Optimization

异步框架中,对优化算法的选择, 研究了三种:  SGD with momentum, RMSProp (Tieleman \& Hinton, 2012) without shared statistics, and RMSProp with shared statistics. 

使用了标准的non-centered RMSProp:

$$
g=\alpha g+(1-\alpha) \Delta \theta^{2} \text { and } \theta \leftarrow \theta-\eta \frac{\Delta \theta}{\sqrt{g+\epsilon}}
$$

结果表明, RMSProp where statistics $g$ are shared across threads is considerably more robust 



### Experiments

##### Atari 2600 Games

图1比较了在Nvidia K40 GPU上训练的DQN算法和在5款Atari 2600游戏上使用16个CPU内核训练的异步方法的学习速度。  
结果显示，我们提出的四种异步方法都能在Atari上成功训练神经网络controllers。  
异步方法的学习速度往往比DQN更快，在一些游戏上的学习速度明显更快，而训练时只用16个CPU内核。  
此外，研究结果表明，n步方法在某些游戏上的学习速度比单步方法快。  
总的来说，A3C明显优于所有三种Valuebased的方法。

![image-20200412171450825](/img/2020-04-01-DQN.assets/image-20200412171450825.png)

然后，我们在57款雅达利游戏上评估了A3C。   
使用对六款雅达利游戏（Beamrider、Breakout、Pong、Q*bert、Seaquest和Space Invaders）tuned hyperparameters (learning rate and amount of gradient norm clipping)，然后对所有57款游戏 固定。  
我们既训练了feedforward agent，也训练了一个在最后隐藏层后增加256个LSTM单元的recurrent agent。 

我们使用16个CPU内核对agent进行了4天的训练，而其他agent在Nvidia K40 GPU上训练了8到10天。  
表1显示了我们通过（A3C）训练的agent所获得的平均分和中位数的human-normalized，以及目前最先进的技术。  
A3C在只使用16个CPU内核和不使用GPU的情况下，在57场比赛中的平均得分比最先进的方法显著提高了一半的训练时间。  
此外，经过短短一天的训练后，A3C与Dueling Double DQN的human-normalized平均分相匹配，几乎达到了Gorila的human-normalized中位数。  
我们注意到，在Double DQN 和Dueling Double DQN 中提出的许多改进，都可以被纳入到我们提出的1-stepQ和n-stepQ方法中，并有类似的潜在改进。

$$
\begin{array}{|l|c|c|c|}
\hline \text { Method } & \text { Training Time } & \text { Mean } & \text { Median } \\
\hline \hline \text { DQN } & 8 \text { days on GPU } & 121.9 \% & 47.5 \% \\
\text { Gorila } & \text { 4 days, 100 machines } & 215.2 \% & 71.3 \% \\
\text { D-DQN } & \text { 8 days on GPU } & 332.9 \% & 110.9 \% \\
\text { Dueling D-DQN } & \text { 8 days on GPU } & 343.8 \% & 117.1 \% \\
\text { Prioritized DQN } & \text { 8 days on GPU } & 463.6 \% & 127.6 \% \\
\text { A3C, FF } & \text { 1 day on CPU } & 344.1 \% & 68.2 \% \\
\text { A3C, FF } & \text { 4 days on CPU } & 496.8 \% & 116.6 \% \\
\text { A3C, LSTM } & \text { 4 days on CPU } & 623.0 \% & 112.6 \% \\
\hline
\end{array}
$$



##### TORCS Car Racing Simulator

赛车游戏, 在大约12小时的训练中，A3C在所有四种游戏场景中的得分都达到了人类测试者的75%到90%之间。



##### Continuous Action Control Using the MuJoCo Physics Simulator

连续动作空间任务.  只评估了A3C，因为与基于值的方法不同，它很容易扩展到连续动作。在所有问题中，使用物理状态或像素作为input，A3C在不到24小时的训练中就找到了很好的解决方案，通常在几个小时内就能找到。



##### Labyrinth

3D环境, 让agent学习在随机生成的迷宫中寻找奖励。 因为agent在每次都要面对一个新的迷宫，必须学习一个探索随机迷宫的通用策略。  
我们在这个任务上训练了一个A3C LSTM agent，只使用84×84 RGB图像作为输入。最终的平均得分约为50分，表明该agent只使用视觉输入就能学会合理的探索随机3D 迷宫的策略。



#### Scalability and Data Efficiency

我们研究了训练时间和data efficiency如何随着并行actor-learners数量的变化而变化，分析了我们提出的框架的有效性。  
当并行使用多个worker更新共享模型时，人们会觉得在理想的情况下，对于给定的任务和算法，在不同的worker数量下，取得一定分数的training steps将保持不变。因此，优势将仅仅来自于系统在相同时间内消耗更多的数据，并且可能提高了探索。

表2显示了在7个Atari游戏中平均使用越来越多的并行actor-learners的数量所实现的训练加速。这些结果表明，所有四种方法都通过使用多个worker线程实现了大幅提速，其中16个线程至少带来了一个数量级的提速。这证实了我们提出的框架能够很好地随并行worker的数量扩展，有效地利用资源。

$$
\begin{array}{|l|c|c|c|c|c|}
\hline  {\text { Number of threads }} \\
\hline \text { Method } & 1 & 2 & 4 & 8 & 16 \\
\hline \text { 1-step Q } & 1.0 & \mathbf{3 . 0} & \mathbf{6 . 3} & \mathbf{1 3 . 3} & \mathbf{2 4 . 1} \\
\hline \text { 1-step SARSA } & 1.0 & \mathbf{2 . 8} & \mathbf{5 . 9} & \mathbf{1 3 . 1} & \mathbf{2 2 . 1} \\
\hline \text { n-step Q } & 1.0 & \mathbf{2 . 7} & \mathbf{5 . 9} & \mathbf{1 0 . 7} & \mathbf{1 7 . 2} \\
\hline \text { A3C } & 1.0 & 2.1 & 3.7 & 6.9 & 12.5 \\
\hline
\end{array}
$$

表2,  每个线程的训练速度平均值。  

有点令人惊讶的是，异步的one-step Q-learning和Sarsa算法表现出了超线性加速，这不能用纯粹的计算收益来解释。  
我们观察到，one-step方法, 当使用更多的并行actor-learner时,  通常需要更少的数据来。  
我们认为这是由于多线程减少了one-step方法中的偏差。  
这些效应在图3中显示得更清楚，图3显示了不同数量的actor-learner和训练方法在5个Atari游戏上的平均得分与训练帧总数的对比图，图4显示了平均得分与现实时间的对比图。

![image-20200412200116321](/img/2020-04-01-DQN.assets/image-20200412200116321.png)

不同数量的actor-learner的data efficiency 比较, X轴显示的是总的训练epochs数，一个epoch对应400万帧（跨所有线程）。y轴显示的是平均得分 .  结论, single step方法显示了并行worker数越多, 数据效率提升越多。



![image-20200412200153624](/img/2020-04-01-DQN.assets/image-20200412200153624.png)

不同数量的actor-learners在五款Atari游戏中的训练速度比较。x轴是训练时间，y轴显示的是平均分。每个曲线显示的是三种最佳learning rate的平均数。所有的异步方法都显示出使用更多的并行actor-learner的训练速度显著提高。



#### Robustness and Stability

![image-20200412232136438](/img/2020-04-01-DQN.assets/image-20200412232136438.png)

最后，我们分析了四种异步算法的稳定性和鲁棒性。对于这四种算法中的每一种，在五种游戏中训练了模型使用50种不同的learning rate和random initializations。图显示的是A3C的结果分数散点图。通常，每种方法和game组合的学习率都在一定范围内，可以得到很好的分数，这表明所有的方法对学习率和随机初始化的选择都很稳健。事实上，在学习率较好的区域几乎没有得分为0的点，这表明这些方法都是稳定的，在学习开始后崩不会溃或发散。



### Conclusions and Discussion

提出了四种标准RL算法的异步版本，并表明它们能够稳定地训练各种域的神经网络controllers。  
当使用16个CPU内核训练时，比在Nvidia K40 GPU上训练的DQN更快，A3C在训练时间上只需一半。

主要发现之一，使用并行actor-learners更新共享模型对value-based方法的学习过程具有稳定的效果。虽然这表明在没有experience replay的情况下，稳定的online Q-learning是可以实现的，但这并不意味着experience replay没用。将experience replay纳入异步RL框架中，可以通过重用旧数据，大幅提高这些方法的data efficiency。这反过来可能会导致像TORCS这样的领域的训练时间大大加快，因为在这些领域中，与环境交互比我们所使用的架构更新模型的成本更高。

将现有的其他RL方法或最近DRL方面的进展与我们的异步框架结合起来，可能改进我们的方法。  
虽然我们的n-step方法使用的是forward view, 直接使用修正后的n-step returns作为target.  但更常见的是, 使用backward view,  通过eligibility traces  隐含地结合不同的return。  
A3C方法可以通过使用其他Advantage函数的方法改进，如广义优势估计GAE(Schulman et al., 2015b)   
所有value-based方法都可以从减少Q值的overestimation bias的不同方法中受益 (Van Hasselt et al., 2015; Bellemare et al., 2016)。  
另一个更具推测性的方向是尝试将最近关于true online temporal difference方法的工作 (van Seijen et al., 2015) 与非线性函数近似相结合。

除算法改进外，NN架构的一些改进也是可能的: dueling architecture(Wang et al., 2015) ;  
spatial softmax(Levine et al., 2015)



#### Optimization Details

我们的异步框架对优化算法的实现不使用任何锁，以便在使用大量线程时最大限度地提高吞吐量。

为了在异步优化环境中应用RMSProp，要决定是moving average of elementwise squared gradients $g$ 是共享的还是per-thread。我们实验了两个版本。1. RMSProp，每个线程维护自己的g， 2. Shared RMSProp，向量g在线程之间共享，并且在没有锁定的情况下异步更新。在线程之间共享也减少了内存需求，每个线程少用一个参数向量的拷贝。

实验结果, Shared RMSProp 最稳定.



#### Experimental Setup

- 16 actor-learner threads running on a single machine and no GPUs

- All methods performed updates after every 5 actions （$t_\text{max}=5，I_\text{Update}=5$） 

- optimization : shared RMSProp , decay factor of α = 0.99

- three asynchronous value-based methods used a shared target network updated every 40000 frames.

- network 结构同(Mnih et al., 2013)

-  discount of γ = 0.99 

- value based methods sampled the exploration rate $\epsilon$ from a distribution taking three values $\epsilon_{1}, \epsilon_{2}, \epsilon_{3}$ with probabilities $0.4,0.3,0.3 .$ The values of $\epsilon_{1}, \epsilon_{2}, \epsilon_{3}$ were annealed from 1 to 0.1,0.01,0.5 respectively over the first four million frames.

- A3C  used entropy regularization with a weight $\beta=0.01$ 

- initial leaming rate was sampled from a LogUniform($10^{-4} , 1 0 ^ { - 2 }$ )  distribution and annealed to 0 over the course of training. 

  

#### Continuous Action Control Using the MuJoCo Physics Simulator

下面是连续动作的一些设置差异. 

- low dimensional physical state case,  inputs are mapped to a hidden state using one hidden layer with 200 ReLU units;  pixels , input was passed through two layers of spatial convolutions without any non-linearity or pooling.

- output of the encoder layers were fed to a single layer of 128 LSTM cells.

- the output layer of the policy network,  two outputs of the policy network are two real number vectors which we treat as the mean vector $μ$ and scalar variance $σ^2$ of a multidimensional normal distribution with a spherical covariance.

- To act, the input is passed through the model to the output layer where we sample from the normal distribution determined by $μ$ and  $σ^2$ . In practice,  $μ$ is modeled by a linear layer and $σ^2$ by a SoftPlus operation, log(1 + exp(x)), as the activation computed as a function of the output of a linear layer.

- policy network and value network do not share any parameters, 

- since the episodes were typically at most several hundred time steps long, we did not use any bootstrapping in the policy or value function updates and batched each episode into a single update.

- As in the discrete action case, we included an entropy cost which encouraged exploration.

- used a cost on the differential entropy of the normal distribution defined by the output of the actor network, $-\frac{1}{2}\left(\log \left(2 \pi \sigma^{2}\right)+1\right),$ we used a constant multiplier of $10^{-4}$ for this cost across all of the tasks examined. 

  



 


## Reference

Asynchronous Methods for Deep Reinforcement Learning  https://arxiv.org/abs/1602.01783


















