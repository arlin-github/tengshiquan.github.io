---
layout:     post
title:      UCL Course on RL,  Planning by Dynamic Programming
subtitle:   David Silver 的课程笔记2
date:       2019-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---



# Planning by Dynamic Programming

利用 bellman 期望公式 DP来  PE,  再greedy improve,  PI 

利用 bellman 最佳公式 DP 来 VI . 

Prediction and Control





1. Introduction
2. Policy Evaluation
3. Policy Iteration
4. Value Iteration
5. Extensions to Dynamic Programming
6. Contraction Mapping





### What is Dynamic Programming?

1. **Dynamic** sequential or temporal component to the problem   
   “动态”指的是该问题的时间序贯部分
2. **Programming** optimising a “program”, i.e. a policy   
   “规划”指的是去优化一个策略



DP :  a method for solving complex problems  By breaking them down into **subproblems** 

1. Solve the subproblems

2. Combine solutions to subproblems 

- 动态规划， 大问题划分为子问题，然后反向递推解决
- 有点像UCS寻路算法，只不过起点终点交换身份
- $v(s_{t+n}) \to v(s_{t+n-1}) \dots   v(s_{t+1}) \to v(s_t)$   从终点递推到起点， 没有用到启发



#### Requirements for Dynamic Programming

- **Optimal substructure**  **最优子结构**
  - ***Principle of optimality*** applies   **最优性原理** 
  - Optimal solution can be decomposed into subproblems 

- Overlapping subproblems
  - Subproblems recur many times  子问题重复出现多次
  - Solutions can be cached and reused   子问题的解可以复用

> **最优性原理**（多阶段决策过程的最优决策序列具有这样的性质：不论初始状态和初始决策如何，对于前面决策所造成的某一状态而言，其后各阶段的决策序列必须构成最优策略），从而问题的最优解可以分解为子问题最优解； 马尔科夫性
>
> 简单的说, 就是,  当前局面的最优就是,不管过去如何, 从今之后的每一步都得是最优的.

MDP satisfy both properties 

- **Bellman equation** gives recursive decomposition 
- **Value function** stores and reuses solutions 



### Planning by Dynamic Programming

- DP assumes full knowledge of the MDP  ,  不能是POMDP
- DP used for **planning** in an MDP 

> “规划”指的是在了解整个MDP的基础上求解最优策略，也就是在模型的基础上：包括状态行为空间、转换矩阵、奖励等。这类问题不是典型的强化学习问题，可以用规划来进行预测和控制。基于model，不用与环境进行交互。  
>

- For **Prediction**:   求某个策略的V
  - Input:  **MDP** $⟨\cal S, \cal A,\cal  P,\cal  R, \gamma⟩$ and **policy** $π$      
           or **MRP** $⟨S,\mathcal P^π\mathcal ,R^π,γ⟩ $

  - Output: value function $v_π$ 

- For **Control**:   求最优V以及最优策略
  - Input: MDP $⟨\cal S, \cal A,\cal  P,\cal  R, \gamma⟩$
  - Output: optimal value function $v_∗$  and optimal policy $π_∗$



##### Other Applications of Dynamic Programming

- Scheduling algorithms 调度算法
- String algorithms 字符串算法（e.g. sequence alignment）
- Graph algorithms 图算法（e.g. shortest path algorithms）  最短路径, A*
- Graphical models 图形模型（e.g. Viterbi algorithm）



### Policy Evaluation

#### Iterative Policy Evaluation

- Problem: evaluate a given policy $π$  
- Solution: iterative application of **Bellman expectation** backup   
- 迭代逼近收敛 $v_1 →v_2 →...→v_π$     
  这里的下标是iteration的次数, 迭代版本的含义

- Using **synchronous** **backups**: 
  - At each iteration k+1,
  - For all states $s ∈ \mathcal S$, 
  - Update $v_{k+1}(s)$ from $v_k(s')$  



通过bellman公式**反向迭代** ，用下一个状态的v值来更新当前状态的，直到该策略的在所有状态s下的v收敛

<img src="/img/2019-03-02-Silver.assets/image-20200630230148586.png" alt="image-20200630230148586" style="zoom:33%;" />
$$
v_{k+1}(s) = \sum_{a \in \mathcal A} \pi(a|s) \Big(\mathcal R_s^a + \gamma\sum_{s' \in \mathcal S}\mathcal P_{ss'}^a v_k(s') \Big)
\\ \mathbf v_{k+1} = \mathcal R^\pi + \gamma \mathcal P^\pi \mathbf v_k
$$

显然对于树的分支特别多的情况下，这种的计算量是巨大的

下面这个例子, 说明两个点:

1. 通过PE算的Value函数可以用作新的policy
2. Value 函数不需要迭代到收敛有时就可以得到最优策略. 

下图里面的数值有位数没显示全. 

<img src="/img/2019-03-02-Silver.assets/image-20200630230601335.png" alt="image-20200630230601335" style="zoom:33%;" />



<img src="/img/2019-03-02-Silver.assets/image-20200630230631915.png" alt="image-20200630230631915" style="zoom:33%;" />









### How to Improve a Policy

1. **Evaluate** the policy $π$

2. **Improve** the policy by acting greedily with respect to $v_π$
  
   $$
   π′ = greedy(v_π)
   $$

- In general, need more iterations of improvement / evaluation 
- policy iteration always **converges** to $π_∗$



### Policy Iteration

<img src="/img/2019-03-02-Silver.assets/image-20190215165532474.png"  style="zoom: 33%;" />

一个 iteration就是把所有的 s 都反向迭代一遍

- **Policy evaluation** :  Estimate $v_π$  Iterative policy evaluation 
- **Policy improvement** :  Generate $π′ ≥ π$  **Greedy** policy improvement 



#### Policy Improvement

- Consider a deterministic policy, $a = π(s)$   **关键点是只改一个特定状态s下的a,策略其他部分不变**
- improve the policy by acting greedily $π′ = \arg \max_{a \in \mathcal A} q_π(s,a)$ 
- improves the value from any state s over one step  $q_\pi(s, \pi'(s)) = \max_{a \in \mathcal A} q_\pi(s,a) \ge q_\pi(s,\pi(s)) = v_\pi(s)$

- therefore improves the value function, $v_{π′}(s) ≥ v_π(s)$   

$$
\begin{aligned}
v_\pi(s) \leq q_\pi(s,\pi'(s))& = \mathbb E_{\pi'}[R_{t+1} + \gamma v_\pi(S_{t+1})| S_t =s]
\\& \leq \mathbb E_{\pi'}[R_{t+1} + \gamma q_\pi(S_{t+1},\pi'(S_{t+1}))| S_t =s]
\\& \leq \mathbb E_{\pi'}[R_{t+1} + \gamma R_{t+2}+ \gamma^2 q_\pi(S_{t+2},\pi'(S_{t+2}))| S_t =s]
\\& \leq \mathbb E_{\pi'}[R_{t+1} + \gamma R_{t+2}+\dots| S_t =s] = v_{\pi'}(s) 
\end{aligned}
$$

- If improvements stop, 即找到不动点 ,  $q_\pi(s,\pi'(s)) = \max_{a \in \mathcal A} q_\pi(s,a) = q_\pi(s,\pi(s)) = v_\pi(s) $ 
- Then , Bellman optimality equation has been satisfied  $v_\pi(s) = \max_{a \in \mathcal A} q_\pi(s,a) $
- 因此, 收敛到了最优策略   $v_\pi(s) = v_*(s)$



 

#### Modified Policy Iteration

- Does policy evaluation need to converge to $v_π$   改进，PE 不需要收敛到$v_π$就可以improve
- Or should we introduce a stopping condition  
  - e.g. ε-convergence of value function  利用收敛下限
- Or simply stop after k iterations of iterative policy evaluation?  规定迭代步数
- Why not update policy every iteration?   
  - i.e. stop after k = 1   **只迭代一遍就是 Value Iteration**



### Generalised Policy Iteration  GPI

![image-20190215165532474](/img/2019-03-02-Silver.assets/image-20190215165532474.png)

- **广义策略迭代（Generalized Policy iteration，GPI）**指PE和PI交互的一般概念，而不依赖于两个过程的粒度（granularity）和其他细节。
- 几乎所有强化学习方法都可以被很好地描述为GPI。策略 $\pi$ 总是基于值函数 V 被改善（the policy always being improved with respect to the value function），并且PE的值函数 V 总是趋向值函数 $V^{\pi}$ （the value function always being driven toward the value function for the policy）。
- 如果评价过程和提升过程都稳定下来，即不再发生变化，那么值函数和策略必须都是最优的。



### Value Iteration

##### Principle of Optimality 最优原理

Any optimal policy can be subdivided into two components:

- An optimal first action $A_∗$    在s下走最强招
- Followed by an optimal policy from successor state $S′$     之后状态s'的也都走的好



##### Theorem (Principle of Optimality)

A policy $π(a \vert s)$ achieves the optimal value from state s, $v_π(s) = v_∗(s)$, if and only if 

- For any state s′ reachable from s
- $π$ achieves the optimal value from state s′,     $v_π(s′) = v_∗(s′)$

即策略在某一个状态是最优的时候，等价于在后继的所有状态 都是最佳策略



#### Deterministic Value Iteration

- If we know the solution to subproblems $v_∗(s′)$    后继状态已经最优
- Then solution $v_∗(s)$ can be found by one-step lookahead,  backup一步, 则得到当前最优  
  

$$
v_*(s)  \leftarrow  \max_{a \in \mathcal A} \mathcal R_s^a + \gamma\sum_{s' \in \mathcal S} \mathcal P_{ss'}^a v_*(s')
$$

- Intuition: start with final rewards and work backwards   从最末尾的reward开始反向迭代
- Still works with loopy, stochastic MDPs     对循环的MDP, 随机 MDP 也适用



### Value Iteration

- Problem: find optimal policy $π$
- Solution: iterative application of Bellman optimality backup  基于bellman最佳方程来迭代
-  $v_1 →v_2 →...→v_∗$

- Using synchronous backups 
  - At each iteration k + 1 
  - For all states s ∈ S   
  - Update $v_{k+1}(s)$ from $v_k (s')$ 
- Convergence to $v_∗$ 
- Unlike policy iteration, there is **no explicit policy**
- Intermediate value functions may not correspond to any policy

<img src="/img/2019-03-02-Silver.assets/image-20190219155457357.png"  style="zoom:50%;" />

与策略迭代不同，在值迭代过程中，算法不会给出明确的策略，需要之后再从根节点出发，查找各个子节点的v，来得出策略；迭代过程中得到的价值函数，对应的是该V局面下的greedy策略。价值迭代虽然不需要策略参与，但仍然需要知道状态之间的转移概率，也就是需要知道模型。

下图有个 弧线, 表示 选最佳的, max

<img src="/img/2019-03-02-Silver.assets/image-20200701002853014.png" alt="image-20200701002853014" style="zoom: 50%;" />

$$
v_{k+1}(s) = \max_{a \in \mathcal A}  \Big(\mathcal R_s^a + \gamma\sum_{s' \in \mathcal S}\mathcal P_{ss'}^a v_k(s') \Big)
\\ \mathbf v_{k+1} = \max_{a \in \mathcal A} \mathcal R^a + \gamma \mathcal P^a \mathbf v_k
$$

与bellman期望方程相比，贝尔曼最优方程**将期望操作变为最大操作，也即隐式地实现了策略改进这一步**，不过是固定为greedy的，而不是其他策略改进方法，我们通过greedy得到的策略通常为**确定性策略**。






### Synchronous Dynamic Programming Algorithms

| Problem    | Bellman Equation                                         | Algorithm                   |
| ---------- | -------------------------------------------------------- | --------------------------- |
| Prediction | Bellman Expectation Equation                             | Iterative Policy Evaluation |
| Control    | Bellman Expectation Equation + Greedy Policy Improvement | Policy Iteration            |
| Control    | Bellman Optimality Equation                              | Value Iteration             |

- m个action，n个状态
- Algorithms are based on state-value function $v_π(s)$ or $v_∗(s)$
- Complexity $O(mn^2)$ per iteration, for m actions and n states
- Could also apply to action-value function $q_π(s, a)$ or $q_∗(s, a)$
- Complexity $O(m^2n^2)$ per iteration



### Extensions to Dynamic Programming

#### Asynchronous Dynamic Programming  异步DP, ADP

- DP methods described so far used **synchronous** backups 
- i.e. all states are backed up in parallel  
  按顺序一个个状态访问过去，浪费了很多不必要的访问 
- **Asynchronous DP** backs up states individually, in any order   异步DP不按顺序去 backup 状态
- For each selected state, apply the appropriate backup 
- Can significantly reduce computation 
- Guaranteed to **converge** if **all states** continue to be selected  若所有状态都遍历到，则**收敛**



Three simple ideas for **asynchronous** dynamic programming: 

- **In-place** dynamic programming 
- **Prioritised sweeping**
- **Real-time** dynamic programming 



##### In-Place Dynamic Programming

- 像是 program trick, 实现方便,  只存一份，立刻用新值，收敛更快
- Synchronous value iteration stores two copies of value function 

$$
{\color{red}{v_{new}(s)}} \leftarrow \max_{a \in \mathcal A}  \Big(\mathcal R_s^a + \gamma\sum_{s' \in \mathcal S}\mathcal P_{ss'}^a {\color{red}{v_{old}(s')}} \Big)
\\ v_{old} ← v_{new}
$$

- In-place value iteration only stores one copy of value function

$$
{\color{red}{v(s)}} \leftarrow \max_{a \in \mathcal A}  \Big(\mathcal R_s^a + \gamma\sum_{s' \in \mathcal S}\mathcal P_{ss'}^a {\color{red}{v(s')}} \Big)
$$

##### Prioritised sweeping

- **state 有了各自的重要性，不再统一对待， 预测误差越大，则越要优先对待**
- Use magnitude of **Bellman error** to guide state selection, e.g.

$$
\bigg | \max_{a \in \mathcal A} \bigg(\mathcal R_s^a + \gamma\sum_{s' \in \mathcal S}\mathcal P_{ss'}^a v(s') \bigg) - v(s) \bigg |
$$

- Backup the state with the **largest** remaining Bellman error
- Update Bellman error of affected states after each backup 
- Requires knowledge of reverse dynamics (predecessor states) , 需要知道dynamics
- Can be implemented efficiently by maintaining a priority queue    可用优先级队列实现



##### Real-Time Dynamic Programming

- 更新那些仅与个体关系密切的状态，同时使用个体的经验来知道更新状态的选择。
- 有些状态虽然理论上存在，但在现实中几乎不会出现。利用已有现实经验。
- 该方法会限制探索的广度，很多情况访问不到

- Idea: only states that are relevant to agent
- Use agent’s experience to guide the selection of states
- After each time-step $S_t,A_t,R_{t+1}$
- Backup the state $S_t$

$$
v(S_t) \leftarrow \max_{a \in \mathcal A}  \Big(\mathcal R_{S_t}^a + \gamma\sum_{s' \in \mathcal S}\mathcal P_{S_ts'}^a v(s') \Big)
$$



### Full-width and sample backups

#### Full-Width Backups

<img src="/img/2019-03-02-Silver.assets/image-20200701002853014.png" alt="image-20200701002853014" style="zoom: 50%;" />



- DP uses **full-width backups**   , DP 全部子状态backup

- For each backup (sync or async) 
  - Every successor state and action is considered
  - Using knowledge of the MDP transitions and reward function 

- DP is effective for medium-sized problems (millions of states)
- For large problems DP suffers Bellman’s  **curse of dimensionality**  维度灾难,  指数级
- Even one backup can be too expensive



#### Sample Backups

- Using **sample** rewards and **sample** transitions $⟨S, A, R, S′⟩$ instead of reward function R and transition dynamics P
-  跟上面相对，不用全部s’的反向迭代，只是随机选一个子状态来backup；  MC, TD

Advantages: 

- **Model-free**: no advance knowledge of MDP required     采样， 无需模型
- Breaks the curse of dimensionality through sampling 
- Cost of backup is constant, independent of $n = \vert S \vert$ 

 

### Approximate Dynamic Programming 

- Approximate the value function   DP也可以使用近似函数
- Using a function approximator $\hat v(s, \mathbf w)$ 
- Apply dynamic programming to $\hat v(·, \mathbf w)$    
  这里DP是指将bellman最佳方程的迭代应用到 近似函数值，然后监督学习
- e.g. **Fitted Value Iteration** repeats at each iteration k
  - Sample states $\tilde S  ⊆ S$
  
  - For each state $s ∈ \tilde S$ , estimate target value using Bellman optimality equation  
    $$
    \tilde v_k(s) = \max_{a \in \mathcal A} \Big(\mathcal R_s^a + \gamma \sum_{s' \in \mathcal S}\mathcal P_{ss'}^a \hat v(s',\mathbf w_k) \Big)
    $$
    
  - Train next value function $$\hat{v}\left(\cdot, \mathbf{w}_{\mathbf{k}+1}\right)$$ using targets $$\left\lbrace\left\langle s, \tilde{v}_{k}(s)\right\rangle\right\rbrace$$





### Contracting Mapping

压缩映射,   证明 收敛性以及 收敛速度的理论

> Contracting mapping theorem
> 对于完备的metric space $\langle M, d\rangle$ ，如果 $f: M\mapsto M$ 是它的一个压缩映射，那么
>
> * 在该metric space中，存在唯一的不动点 $x_*$ 满足 $f(x_*) = x_*$ 。
> * 并且，对于任意的 $x\in M$ , 定义序列 $f^2(x) = f(f(x)) , f^3(x) = f(f^2(x)) , \cdots,  f^n(x)=f(f^{n-1}(x))$, 该序列会收敛于 $$x_*$$ 即 $$\lim_{n\rightarrow \infty} f^n(x) = x_*$$



#### Value Function Space

- vector space $\mathcal V$ over value functions , $\vert \mathcal S \vert$ dimensions 
- Each point in this space fully specifies a value function v(s)  这个空间中的一个点就是一个估值函数
- Bellman backup do to points in this space?

- it brings value functions closer ,  backup 操作使得这些函数在空间离得更近 , backup使得 v(s) 逐渐逼近 $v_\pi(s)$
- therefore the backups must **converge** on a unique solution 所以必定会收敛到一个解 ,不动点

- measure **distance** between state-value functions u and v by the $∞$-norm ,  使用无穷范数norm来测距
- i.e. the largest difference between state values,  无穷范数

$$
\| u - v\|_\infty = \max_{s \in \mathcal S} |u(s) - v(s) |
$$

- Define the Bellman expectation **backup operator** $T^π$  ,  定义压缩算子

$$
T^\pi(v) = \mathcal R^\pi + \gamma \mathcal P^\pi v
$$

- This operator is a $γ$-**contraction**, i.e. it makes value functions closer by at least $γ$ ,  所以折扣系数在这里是压缩的系数

$$
\begin{aligned}
\left\|T^{\pi}(u)-T^{\pi}(v)\right\|_{\infty} &=\left\|\left(\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} u\right)-\left(\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} v\right)\right\|_{\infty} \\
&=\left\|\gamma \mathcal{P}^{\pi}(u-v)\right\|_{\infty} \\
& \leq\left\|\gamma \mathcal{P}^{\pi}\right\| u-v\left\|_{\infty}\right\|_{\infty} \\
& \leq \gamma\|u-v\|_{\infty}
\end{aligned}
$$

下面是利用 Contraction Mapping Theorem 以及 不动点 证明收敛性

##### Theorem (Contraction Mapping Theorem)

For any metric space $\mathcal{V}$ that is complete (i.e. closed) under an operator $T(v),$ where $T$ is a $\gamma$ -contraction

- T **converges** to a **unique fixed point**
- At a **linear convergence rate** of $\gamma$