---
layout:     post
title:      CS 285. Distributed RL
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

 

##  Distributed RL

Richard Liaw

本课主要是分布式RL的框架.   视频后面讲了运行环境的仿真. 



### Common Computational Patterns for RL

How can we **better utilize** our computational resources **to accelerate** RL progress?

<img src="/img/CS285.assets/image-20200327233446827.png" alt="image-20200327233446827" style="zoom:33%;" />

### History of large scale distributed RL

<img src="/img/CS285.assets/image-20200327233608202.png" alt="image-20200327233608202" style="zoom:50%;" />



- **2013**  **DQN** Playing Atari with Deep Reinforcement Learning (Mnih 2013)

- **2015**  **GORILA**  Massively Parallel Methods for Deep Reinforcement Learning (Nair 2015)
- **2016** **A3C** Asynchronous Methods for Deep Reinforcement Learning (Mnih 2016)
- **2018** **IMPALA**  Scalable Distributed Deep-RL with **Importance Weighted Actor-Learner Architectures** (Espeholt 2018) 
- **2018**  **Ape-X**  Distributed Prioritized Experience Replay (Horgan 2018) 
- **2019**  **R2D3** Making Efficient Use of Demonstrations to Solve Hard Exploration Problems  (Le Paine 2019)



#### 2013/2015: DQN

<img src="/img/CS285.assets/image-20200327233855450.png" alt="image-20200327233855450" style="zoom:50%;" />



```python
for i in range(T):
    s, a, s_1, r = evaluate()
    replay.store((s, a, s_1, r))
    minibatch = replay.sample()
    q_network.update(mini_batch)
    if should_update_target():
        q_network.sync_with(target_net)
```



#### 2015: General Reinforcement Learning Architecture (GORILA)



<img src="/img/CS285.assets/image-20200327234057625.png" alt="image-20200327234057625" style="zoom:50%;" />



##### GORILA Performance

<img src="/img/CS285.assets/image-20200327234653339.png" alt="image-20200327234653339" style="zoom:50%;" />



### History of large scale distributed RL

#### 2016: Asynchronous Advantage Actor Critic (A3C)

<img src="/img/CS285.assets/image-20200327234854301.png" alt="image-20200327234854301" style="zoom: 50%;" />



```python
# Each worker:
while True:
    sync_weights_from_master()
    for i in range(5):
        collect sample from env
    grad = compute_grad(samples)
    async_send_grad_to_master()
```



##### A3C Performance 

Changes to GORILA:

1. **Faster updates**
2. **Removes** the replay buffer , 但用了其他的采样技术, 思路类似, 去相关性
3. **Moves** to Actor-Critic (from Q learning)



<img src="/img/CS285.assets/image-20200327235242465.png" alt="image-20200327235242465" style="zoom:50%;" />

`A3C FF` , A3C with feed-forward net 



#### Importance Weighted Actor-Learner Architectures (IMPALA)

**Motivated by progress in distributed deep learning!**  DeepMind



<img src="/img/CS285.assets/image-20200327235432721.png" alt="image-20200327235432721" style="zoom: 33%;" />



因为采样的actor 与 learner 解耦, 所以actor的policy 会滞后. 一般learning很快. 

##### How to correct for Policy Lag? Importance Sampling!

<img src="/img/CS285.assets/image-20200327235516043.png" alt="image-20200327235516043" style="zoom:50%;" />

V-trace , 使学习更加平稳.

Given an actor-critic model:

1. Apply importance-sampling to policy gradient

   $$
   \mathbb{E}_{a_{s} \sim \mu\left(\cdot | x_{s}\right)}\left[\frac{\pi_{\bar{\rho}}\left(a_{s} | x_{s}\right)}{\mu\left(a_{s} | x_{s}\right)} \nabla \log \pi_{\bar{\rho}}\left(a_{s} | x_{s}\right) q_{s} | x_{s}\right]
   $$
   
2. Apply importance sampling to critic update
  V-trace target
  Consider a trajectory $$\left(x_{t}, a_{t}, r_{t}\right)_{t=s}^{t=s+n}$$ generated by the actor following some policy $\mu$ . We define the $n$ -steps V-trace target for $V\left(x_{s}\right)$, our value approximation at state $x_{s}$, as:
  $$
  v_{s} \stackrel{\text { def }}{=} V\left(x_{s}\right)+\sum_{t=s}^{s+n-1} \gamma^{t-s}\left(\prod_{i=s}^{t-1} c_{i}\right) \delta_{t} V
  $$
  

#### Ape-X/R2D2 (2018)

Scaling Off-Policy learning...

**Ape-X:**

1. Distributed DQN/DDPG/R2D2
2. Reintroduces replay
3. **Distributed Prioritization:** Unlike Prioritized DQN, initial priorities are not set to “max TD”



<img src="/img/CS285.assets/image-20200328000803235.png" alt="image-20200328000803235" style="zoom:50%;" />



##### Ape-X Performance

<img src="/img/CS285.assets/image-20200328000852337.png" alt="image-20200328000852337" style="zoom:50%;" />



#### With Demonstrations: R2D3 (2019)

<img src="/img/CS285.assets/image-20200328001010392.png" alt="image-20200328001010392" style="zoom:50%;" />



### Other interesting distributed architectures

#### QT-Opt

https://arxiv.org/pdf/1806.10293.pdf

<img src="/img/CS285.assets/image-20200328001113678.png" alt="image-20200328001113678" style="zoom:50%;" />



#### AlphaZero

<img src="/img/CS285.assets/image-20200328001149128.png" alt="image-20200328001149128" style="zoom:50%;" />



#### Evolution Strategies



<img src="/img/CS285.assets/image-20200328001231734.png" alt="image-20200328001231734" style="zoom:33%;" />

![image-20200328001245656](/img/CS285.assets/image-20200328001245656.png)

<img src="/img/CS285.assets/image-20200328001324125.png" alt="image-20200328001324125" style="zoom:33%;" />



#### Beyond RL: Population-based Training

https://deepmind.com/blog/article/population-based-training-neural-networks

<img src="/img/CS285.assets/image-20200328001410964.png" alt="image-20200328001410964" style="zoom:50%;" />

##### Benefits of PBT

![image-20200328001643188](/img/CS285.assets/image-20200328001643188.png)





#### RLlib: Abstractions for Distributed Reinforcement Learning (ICML'18)

http://rllib.io/



##### RL research scales with compute

<img src="/img/CS285.assets/image-20200328001956908.png" alt="image-20200328001956908" style="zoom:50%;" />

<img src="/img/CS285.assets/image-20200328002047915.png" alt="image-20200328002047915" style="zoom:50%;" />

##### How do we leverage this hardware?

<img src="/img/CS285.assets/image-20200328002231836.png" alt="image-20200328002231836" style="zoom:50%;" />



#### Systems for RL today

- Many implementations (16000+ repos on GitHub!) 
- how general are they (and do they scale)?
  - PPO: multiprocessing, MPI 
  - AlphaZero: custom systems 
  - Evolution Strategies: Redis 
  - IMPALA: Distributed TensorFlow 
  - A3C: shared memory, multiprocessing, TF
- Huge variety of algorithms and distributed systems used to implement, but little reuse of components



#### Challenges to reuse

1. Wide range of physical execution strategies for one "algorithm"

   <img src="/img/CS285.assets/image-20200328002616457.png" alt="image-20200328002616457" style="zoom:50%;" />

2. Tight coupling with deep learning frameworks

   Different parallelism paradigms:
    – Distributed TensorFlow vs TensorFlow + MPI?

3. Large variety of algorithms with different structures

   <img src="/img/CS285.assets/image-20200328002713534.png" alt="image-20200328002713534" style="zoom:50%;" />



#### We need abstractions for RL

*Good abstractions decompose RL algorithms into reusable components.*

Goals:

- **Code reuse** across deep learning frameworks 
- **Scalable** execution of algorithms
- Easily **compare** and **reproduce** algorithms



#### Structure of RL computations

<img src="/img/CS285.assets/image-20200328003008160.png" alt="image-20200328003008160" style="zoom:50%;" />

<img src="/img/CS285.assets/image-20200328003035143.png" alt="image-20200328003035143" style="zoom:50%;" />



#### Many RL loop decompositions

<img src="/img/CS285.assets/image-20200328003147136.png" alt="image-20200328003147136" style="zoom: 33%;" />



<img src="/img/CS285.assets/image-20200328003218435.png" alt="image-20200328003218435" style="zoom:33%;" />



##### Common components

<img src="/img/CS285.assets/image-20200328003350404.png" alt="image-20200328003350404" style="zoom:33%;" />

<img src="/img/CS285.assets/image-20200328003410772.png" alt="image-20200328003410772" style="zoom:33%;" />



#### Structural differences

Async DQN (Mnih et al; 2016) 

- Asynchronous optimization 
- Replicated workers
- Single machine

Ape-X DQN (Horgan et al; 2018)

- Central learner
- Data queues between components 
- Large replay buffers
- Scales to clusters

\+ Population-Based Training (Jaderberg et al; 2017)

- Nested parallel computations
- Control decisions based on intermediate results

**...and this is just one family!**

➝ **No existing system can effectively meet all the varied demands of RL workloads.**  目前没有万能的



#### Requirements for a new system

Goal: Capture a broad range of RL workloads with high performance and substantial code reuse

1. Support **stateful** computations
   - e.g., simulators, neural nets, replay buffers
   - big data frameworks, e.g., Spark, are typically stateless
2. Support **asynchrony**
   - difficult to express in MPI, esp. nested parallelism
3. Allow easy composition of (distributed) components



#### Ray System Substrate

- RLlib builds on Ray to provide higher-level RL abstractions
- Hierarchical parallel task model with stateful workers
  - flexible enough to capture a broad range of RL workloads (vs specialized sys.)

<img src="/img/CS285.assets/image-20200328004416872.png" alt="image-20200328004416872" style="zoom:50%;" />



#### Hierarchical Parallel Task Model

1. Create Python class instances in the cluster (stateful workers) 
2. Schedule short-running tasks onto workers
   - Challenge: High performance: 1e6+ tasks/s, ~200us task overhead

<img src="/img/CS285.assets/image-20200328004544329.png" alt="image-20200328004544329" style="zoom:50%;" />



#### Unifying system enables RL Abstractions

<img src="/img/CS285.assets/image-20200328004919851.png" alt="image-20200328004919851" style="zoom: 67%;" />



<img src="/img/CS285.assets/image-20200328005043592.png" alt="image-20200328005043592" style="zoom:50%;" />



#### RLlib Reference Algorithms

- **High-throughput architectures**
  - –  Distributed Prioritized Experience Replay (Ape-X)
  - –  Importance Weighted Actor-Learner Architecture (IMPALA)

- **Gradient-based**
  - –  Advantage Actor-Critic (A2C, A3C)
  - –  Deep Deterministic Policy Gradients (DDPG)
  - –  Deep Q Networks (DQN, Rainbow)
  - –  Policy Gradients
  - –  Proximal Policy Optimization (PPO)
- **Derivative-free**
  - –  **Augmented Random Search (ARS)**
  - –  Evolution Strategies



#### Scale your algorithms with RLlib

- Beyond a "collection of algorithms",
- RLlib's abstractions let you easily implement and scale new algorithms (multi-agent, novel losses, architectures, etc)

<img src="/img/CS285.assets/image-20200328005321739.png" alt="image-20200328005321739" style="zoom:50%;" />



##### Code example: training PPO

<img src="/img/CS285.assets/image-20200328005651172.png" alt="image-20200328005651172" style="zoom: 40%;" />



**Summary:** Ray and RLlib addresses challenges in providing scalable abstractions for reinforcement learning.









#### Ray distributed execution engine

- Ray provides **Task parallel** and **Actor** APIs built on **dynamic task graphs**

  <img src="/img/CS285.assets/image-20200328005844861.png" alt="image-20200328005844861" style="zoom:50%;" />

- These APIs are used to build distributed **applications**, **libraries** and **systems**



#### Ray distributed scheduler

- Faster than Python multi- processing on a single node
- Competitive with MPI in many workloads

<img src="/img/CS285.assets/image-20200328005925719.png" alt="image-20200328005925719" style="zoom:50%;" />




