---
layout:     post
title:      UCL Course on RL,  Classic Games
subtitle:   David Silver 的课程笔记9
date:       2019-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---



# Classic Games

核心内容,   

- Minimax(αβ)  + Value 拟合
- self-play + Value 拟合 ;  TD学习各种node的value
- 博弈论 , imperfect,    CFR, smooth UCT





1. State of the Art
2. Game Theory
3. Minimax Search
4. Self-Play Reinforcement Learning
5. Combining Reinforcement Learning and Minimax Search
6. Reinforcement Learning in Imperfect-Information Games
7. Conclusions



## Game Theory

### Optimality in Games

- What is the **optimal policy** $π^i$ for ith player?
- If all other players **fix** their policies $π^{−i}$
- **Best response** $π_∗^i (π^{−i} )$ is optimal policy against those policies  最佳对应
- **Nash equilibrium** is a **joint policy** for all players   纳什均衡,  联合策略

$$
\pi^i = π_∗^i (π^{−i} )
$$

- such that every player’s policy is a best response
- i.e. no player would choose to deviate from Nash



#### Single-Agent and Self-Play Reinforcement Learning

- 把对手看成环境的一部分,  求BR的问题可以建模为 MDP RL问题
- **Best response** is **solution** to single-agent RL problem 
  - Other players become part of the environment 
  - Game is reduced to an MDP
  - Best response is **optimal policy** for this **MDP** 
- Nash equilibrium is **fixed-point** of **self-play** RL  纳什均衡是selfplay的不动点
  - Experience is generated by playing games between agents  $a_1 \sim \pi^1 , a_2 \sim \pi^2, ...$ , 每个agent轮流执行策略
    - Each agent learns **best response** to other players    每个agent学习针对其他人的BR
  - One player’s policy determines another player’s environment  ; 当前玩家的策略决定了其他人的env
  - All players are adapting to each other	; 所有人在适应其他人



#### Two-Player Zero-Sum Games   零和博弈

- We will focus on a special class of games:
  - A two-player game has two (alternating) players 
    - We will name player 1 white and player 2 black 
- A **zero sum game** has equal and opposite rewards for black and white 

$$
R^1 + R^2 = 0
$$

- We consider methods for finding **Nash equilibria** in these games,  两种方法
  - **Game tree search** (i.e. **planning**)  ;   通过game tree 搜索出最优解
  - Self-play reinforcement learning ;  迭代出来





## Perfect and Imperfect Information Games  信息完全

- A **perfect information** or Markov game is fully observed

- An **imperfect information** game is partially observed



### Minimax

- A **value function** defines the expected total reward given **joint policies** $\pi=\left\langle\pi^{1}, \pi^{2}\right\rangle$

$$
v_\pi(s) = \mathbb E_\pi[G_t|S_t=s]
$$

- A **minimax** value function **maximizes** **white**’s expected return while **minimizing** **black**’s expected return 

$$
v_*(s) = \max_{\pi_1}\min_{\pi_2} v_\pi(s)
$$

- A **minimax policy** is a joint policy $\pi=\left\langle\pi^{1}, \pi^{2}\right\rangle$ that achieves the minimax values
- There is a **unique** minimax value function
- A minimax policy is a **Nash equilibrium**



- p1本身有自己的策略， p2也有自己的策略， 两个合起来称为联合策略。 该策略下， 对每个状态肯定有个v期望；
- 对v， 如果是非zero-sum，可以理解为双人合作在游戏中最终能获取到多少的reward
- 对 zero-sum game,   得分总和加起来为0；  比如对A而言，A赢 得1分, r=1，输了得-1分；这时，v就是其中一方的得分期望。
- 联合策略下， v表示player1的得分， 那么minimax值，就是在p1的时候取v最大，p2动作的时候取v最小； 最终达到 minimax 最优策略，纳什均衡， 即双方都走最强招



#### Minimax Search

- Minimax values can be found by **depth-first game-tree search** 



<img src="/img/2019-03-02-Silver.assets/image-20200704212053328.png" alt="image-20200704212053328" style="zoom:50%;" />



### Value Function in Minimax Search

- Search tree grows **exponentially**  指数扩大
- Impractical to search to the end of the game 走到底不现实，所以往后看几步，就开始评估
- Instead use **value function** approximator $v(s,\mathbf w) ≈ v_∗(s)$  可以使用Value函数, 来评估, 不走到底
  - aka evaluation function, heuristic function  可以用 神经网络，或者启发式 value评估函数
- Use **value function** to estimate minimax value at leaf nodes  在叶子节点使用评估函数
- Minimax search run to **fixed depth** with respect to leaf values  遍历固定的深度，不能太深



#### Binary-Linear Value Function

![image-20200704212257454](/img/2019-03-02-Silver.assets/image-20200704212257454.png)

- Binary feature vector $\mathbf x(s)$: e.g. one feature per piece
- Weight vector $\mathbf w$ : e.g. value of each piece
- Position is evaluated by **summing weights of active features**  

-  是否有某个特征，每个特征一个权重的线性拟合算法

##### Deep Blue 深蓝

- Knowledge
  - 8000 handcrafted chess features 
  - **Binary-linear value function**
  - Weights largely **hand-tuned** by human experts   参数都是人工调的

- Search
  - High performance parallel alpha-beta search 
  - 480 special-purpose VLSI chess processors  主要是硬件
  - Searched 200 million positions/second 
  - Looked ahead 16-40 ply 



##### Chinook

- Knowledge
  - **Binary-linear value function** 
  - 21 **knowledge-based features** (position, mobility, ...)  特征还是要基于规则来
  - x4 phases of the game 

- Search 
  - High performance **alpha-beta search** 
  - Retrograde analysis  最后几手是最强手，遍历出来的
    - Search backward from won positions
    - Store all winning positions in lookup tables 
    - Plays **perfectly** from last n checkers 





## Self-Play Reinforcement Learnings

#### Self-Play Temporal-Difference Learning

- 目标, **value 函数去学习 G**

- Apply **value-based** RL algorithms to games of **self-play**  左右互搏
- MC: update value function towards the return $G_t$ 

$$
∆ \mathbf w=α(G_t −v(S_t,\mathbf w))∇_\mathbf w v(S_t,\mathbf w)
$$

- TD(0): update value function towards successor value $v(S_{t+1})$ 

$$
∆\mathbf w = α(v(S_{t+1},\mathbf w)−v(S_t,\mathbf w))∇_\mathbf wv(S_t,\mathbf w) 
$$

- TD(λ): update value function towards the λ-return $G_t^λ$ 

$$
∆ \mathbf w=α(G_t^λ −v(S_t,\mathbf w))∇_\mathbf wv(S_t,\mathbf w) 
$$



#### Policy Improvement with Afterstates

- 即 player1 在 s下 进行a**之后**， 对player1看到的局面的v估值 ，本质上就是 s,a 的q值
  对游戏， 每个用户只能看到自己，player1 进行a之后， 游戏会进入 player2动作的状态
  也可以用 (a_player1,s_player2) 这种组合对作为  player2的该状态的表示
- For **deterministic** games it is sufficient to estimate $v_∗(s)$   因为没有随机状态转移，所以 afterstate就相当于 s,a 
- This is because we can efficiently evaluate the **afterstate**  下一状态s'前面一点点

$$
q_*(s,a) = v_*(succ(s,a))
$$

- Rules of the game define the successor state succ(s,a)  

- 对于很多棋类， 中间r=0， 所以有 $q_*(s) = v_*(s') $ 

- Actions are selected e.g. by min/maximising afterstate value  

$$
A_t = \mathop{\text{argmax}}_a v_∗(succ(S_t,a)) \quad \text{for white}
\\ A_t = \mathop{\text{argmin}}_a v_∗(succ(S_t,a)) \quad \text{for black}
$$

- This improves joint policy for both players 两方都选最强



#### Self-Play TD in Othello: Logistello

<img src="/img/2019-03-02-Silver.assets/image-20200704213247024.png" alt="image-20200704213247024" style="zoom:50%;" />



- Logistello created its own features  先是位置的，然后做了很多图形上的特征
- Start with raw input features, e.g. “black stone at C1?” 
- Construct new features by conjunction/disjunction 
- Created 1.5 million features in different configurations  1.5m 特征...
- Binary-linear value function using these features 
- 强化学习就是学习这些特征的权重



##### Reinforcement Learning in Logistello

- Logistello used **generalised policy iteration**  **GPI**
  - Generate batch of self-play games from current policy  先按当前策略随机打
  - **Evaluate** policies using Monte-Carlo (regress to outcomes)  MC统计结果评估
  - **Greedy** policy **improvement** to generate new players  ; **greedy 改善**



### TD Gammon: Non-Linear Value Function Approximation

<img src="/img/2019-03-02-Silver.assets/image-20200704213312565.png" alt="image-20200704213312565" style="zoom:50%;" />

- Initialised with random weights
- Trained by games of **self-play**
- Using **non-linear TD learning** 

$$
δ_t =v(S_{t+1},\mathbf w)−v(S_t,\mathbf w)
\\∆\mathbf w = αδ_t∇_{\mathbf w}v(S_t,\mathbf w)
$$

- 最后一步， 如果赢了，值为1， 输了为0

- Greedy policy improvement (**no exploration**)    课程里面说，因为信息对称， 所以可以看到所有的情况 
- Algorithm always **converged** in practice
- Not true for other games 对其他游戏不收敛

- 一个具体实现，对于这种信息对称的情况， 把两个玩家的信息全部放到输入特征里面，然后特征，玩家1操作特征加[1,0],玩家2加[0,1]；输出1是玩家1赢， 输出0是玩家1输
- 所以对于玩家2，要求模型尽可能输出0 ，即 Agent 要选择 让v往最小方向走的招数； 为的是尽可能的造成玩家1的麻烦，如果玩家2越厉害，则会推进玩家1变强



![image-20200704213449185](/img/2019-03-02-Silver.assets/image-20200704213449185.png)





### Combining Reinforcement Learning and Minimax Search

#### Simple TD

- 每个节点是一个状态， 子节点是后继状态
- **minimax + TD**
- TD: update value towards successor value
- Value function approximator v(s,w) with parameters w
- Value function backedup from **raw value at next state** 

$$
v(S_t,\mathbf w) ← v(S_{t+1},\mathbf w)
$$

- First **learn** value function by TD learning
- Then use value function in **minimax search** (**no learning**)      **minimax 在TD中search**

- 在minimax算法中使用之前的拟合评估v函数； $v_+$ 就是minimax的**search value**, 这个状态下minimax之后的得分

$$
v_+(S_t,\mathbf w) = \mathop{\text{minimax}}_{s \in leaves(S_t)} v(s,\mathbf w)
$$

- simple TD下，  v(s)本身代表的是什么含义，每步player0的最终得分， 在某一策略下，player0的赢分；则TD的确可以迭代到收敛值；然后通过minimax来改进策略？



##### Simple TD: Results

- Othello: superhuman performance in Logistello
- Backgammon: superhuman performance in TD-Gammon
- Chess: poor performance
- Checkers: poor performance
- In chess tactics seem necessary to find signal in position
- e.g. hard to find checkmates **without search**
- Can we **learn** directly from **minimax search values**?



#### TD Root

- TD root: update **value** towards **successor search value**  用下一个状态的minimax值来迭代当前状态值

<img src="/img/2019-03-02-Silver.assets/image-20190213182648443.png" style="zoom:50%;" />

- 上图中， 粉色$S_{t+1}$  是 S_t 的一个子节点，  也是策略实际走到的状态s'
- 直接用下个状态的minimax值来更新当前v，直接迭代minimax值
- **Search value** is computed at root position $S_t$

$$
v_+(S_t,\mathbf w) = \mathop{\text{minimax}}_{s \in leaves(S_t)} v(s,\mathbf w)
$$

- Value function **backed up** from search value at next state

$$
v(S_t,\mathbf w) ← v_+(S_{t+1},\mathbf w) = v(l_+(S_{t+1}),\mathbf w)
$$

- Where $l_+(s)$ is the leaf node achieving minimax value from s  取得最值的那个叶子节点



##### TD Root in Checkers: Samuel’s Player

- First ever TD learning algorithm (Samuel 1959) 
- Applied to a Checkers program that learned by self-play 
- Defeated an amateur human player
- Also used other ideas we might now consider strange



#### TD Leaf

- TD leaf: update **search value** towards **successor search value**
- 用下个状态最值的叶子minimax值更新现在的最值对应叶子的v， 他们是上下层关系，很可能是父子关系，用新状态的值来迭代旧的；  直接改相应的s_t时刻的叶子值 

<img src="/img/2019-03-02-Silver.assets/image-20190213213707732.png" style="zoom:50%;" />

- Search value computed at current and next step

$$
v_+(S_t,\mathbf w) = \mathop{\text{minimax}}_{s \in leaves(S_t)} v(s,\mathbf w),v_+(S_{t+1},\mathbf w) = \mathop{\text{minimax}}_{s \in leaves(S_{t+1})} v(s,\mathbf w)
$$

- Search value at step t backed up from search value at t + 1

$$
v(S_t,\mathbf w) ← v_+(S_{t+1},\mathbf w) 
\\ \implies v(l_+(S_{t}),\mathbf w) ←  v(l_+(S_{t+1}),\mathbf w)
$$

##### TD leaf in Chess: Knightcap

- Learning
  - Knightcap trained against expert opponent 
  - Starting from standard piece values only 
  - Learnt weights using TD leaf 

- Search
  - Alpha-beta search with standard enhancements 
- Results
  - Achieved master level play after a small number of games 
  - Was not effective in self-play
  - Was not effective without starting from good weights 



##### TD leaf in Checkers: Chinook

- Original Chinook used hand-tuned weights 
- Later version was trained by self-play 
- Using TD leaf to adjust weights 
  - Except material weights which were kept fixed 
- Self-play weights performed ≥ hand-tuned weights 
- i.e. learning to play at superhuman level 



#### TreeStrap

- 子节点往上迭代

- TreeStrap: update **search values** towards **deeper search values**

<img src="/img/2019-03-02-Silver.assets/image-20190213221423784.png" style="zoom:50%;" />

- Minimax search value computed at all nodes $s ∈ nodes(S_t)$
- Value backed up from search value, at same step, for all nodes

$$
v(s,\mathbf w) ← v_+(s,\mathbf w) 
\\ \implies  v(s,\mathbf w) ← v(l_+(s),\mathbf w)
$$

##### Treestrap in Chess: Meep

- Binary linear value function with 2000 features
- Starting from random initial weights (no prior knowledge)
- Weights adjusted by TreeStrap
- Won 13/15 vs. international masters
- Effective in self-play
- Effective from random initial weights





https://www.chessprogramming.org/Meep

http://www0.cs.ucl.ac.uk/staff/d.silver/web/Applications_files/bootstrapping.pdf



![image-20190214103218614](/img/2019-03-02-Silver.assets/image-20190214103218614.png)

<img src="/img/2019-03-02-Silver.assets/image-20190214104306136.png" style="zoom:50%;" />

![TDRootAndLeaf.jpg](/img/2019-03-02-Silver.assets/TDRootAndLeaf.jpg)

![MeepStraps.jpg](/img/2019-03-02-Silver.assets/MeepStraps.jpg)

| Algorithm     | Elo       |
| ------------- | --------- |
| Untrained     | 250 ± 63  |
| TD-Leaf       | 1068 ± 36 |
| RootStrap(αβ) | 1362 ± 59 |
| TreeStrap(mm) | 1807 ± 32 |
| TreeStrap(αβ) | 2157 ± 31 |



#### Simulation-Based Search

- **Self-play** reinforcement learning can replace **search** 
- Simulate games of self-play from root state $S_t$ 
- Apply RL to simulated experience 
  - Monte-Carlo Control $\implies$ **Monte-Carlo Tree Search** 
  - Most effective variant is **UCT algorithm** 
    - Balance exploration/exploitation in each node using UCB 
  - Self-play UCT **converges** on minimax values 
  - **Perfect information**, zero-sum, 2-player games   **只适用于 完全信息**
  - Imperfect information: see next section 



##### Performance of MCTS in Games

- **MCTS** is best performing method in many challenging games 
  - Go (last lecture)
  - Hex
  - Lines of Action 
  - Amazons 
- In many games simple **Monte-Carlo search** is enough 
  - Scrabble 
  - Backgammon 



##### Simple Monte-Carlo Search in Maven

- Learning
  - Maven evaluates moves by score + v(rack) 
  - Binary-linear value function of rack
  - Using one, two and three letter features
  - Q??????, QU?????, III????
  - Learnt by Monte-Carlo policy iteration (cf. Logistello) 
- Search
  - **Roll-out** moves by imagining n steps of self-play 
  - **Evaluate** resulting position by **score + v(rack)** 
  - **Score** move by **average** evaluation in rollouts 
  - **Select** and play **highest** scoring move 
  - Specialised endgame search using B* 



## Game-Tree Search  in Imperfect Information Games

![image-20200704215044054](/img/2019-03-02-Silver.assets/image-20200704215044054.png)



![image-20200704215104794](/img/2019-03-02-Silver.assets/image-20200704215104794.png)



- Players have **different** information states and therefore **separate** search trees  两个玩家将搜索树分开
- There is one **node** for each **information state**  ,  infoset
  - summarising what a player knows
  - e.g. the cards they have seen 

- Many **real states** may **share** the **same information state** 
- May also **aggregate** states e.g. with similar value 



### Solution Methods for Imperfect Information Games

**Information-state game tree** may be solved by: 

- Iterative forward-search methods 
  - e.g. **Counterfactual regret minimization**  **CFR** 
  - “Perfect” play in Poker (heads-up limit Hold’em) 
- **Self-play** reinforcement learning 
- e.g. **Smooth UCT** 
   - 3 silver medals in two- and three-player Poker (limit Hold’em) 
   - Outperformed massive-scale forward-search agents 





##### Smooth UCT Search

- Selfplay不再只对付当前的策略的对手， 而是把所有的对手的策略求平均
- Apply **MCTS** to **information-state** game tree
- Variant of UCT, inspired by game-theoretic **Fictitious Play** 
  - Agents learn against and respond to opponents’ **average behaviour** 
- Extract **average strategy** from nodes’ action counts, 

$$
\pi_{avg}(a|s) = \frac{N(s,a)}{N(s)}
$$

- At each node, pick actions according to

$$
A \sim \begin{cases} UCT(S), &\text{with probability η} 
\\ \pi_{avg}(\cdot|S),&\text{with probability 1-η}  \end{cases}
$$

- Empirically, in variants of Poker: 
  - Naive MCTS **diverged**   MCTS 发散!!
  - <u>Smooth UCT **converged** to **Nash equilibrium**</u> 



##### RL in Games: A Successful Recipe

| Program                                               | Input features                                             | Value Fn       | RL        | Training           | Search    |
| ----------------------------------------------------- | ---------------------------------------------------------- | -------------- | --------- | ------------------ | --------- |
| Chess  								Meep           | Binary  								Pieces, pawns, ... | Linear         | TreeStrap | Self-Play / Expert | αβ        |
| Checkers  								Chinook     | Binary  								Pieces, ...        | Linear         | TD leaf   | Self-Play          | αβ        |
| Othello  								Logistello   | Binary  								Disc configs       | Linear         | MC        | Self-Play          | αβ        |
| Backgammon  								TD Gammon | Binary  								Num checkers       | Neural network | TD(λ)     | Self-Play          | αβ / MC   |
| Go  								MoGo              | Binary  								Stone patterns     | Linear         | TD        | Self-Play          | MCTS      |
| Scrabble  								Maven       | Binary  								Letters on rack    | Linear         | MC        | Self-Play          | MC search |
| Limit Hold’em  								SmooCT | Binary  								Card abstraction   | Linear         | MCTS      | Self-Play          | -         |









