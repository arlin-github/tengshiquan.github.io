---
layout:     post
title:      二阶优化算法
subtitle:   牛顿法, 拟牛顿法
date:       2019-11-27 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-newton.jpg"
catalog: true
tags:
    - gradient descent
    - mathine learning
    - neural network
    - deep learning
    - Optimiztion
---



# 二阶优化算法

核心是主要围绕着怎么简化 求Hessian矩阵的逆矩阵 , 没有仔细研究



## 牛顿法 Newton's method

牛顿法是一种**迭代求解**方法，使用函数*f* (*x*)的泰勒级数的前面几项来寻找方程*f* (*x*) = 0的根。 

由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/NewtonIteration_Ani.gif/600px-NewtonIteration_Ani.gif" width="60%">

一阶泰勒公式展开, 这里虽然有等号, 其实是个近似:

$$
f(x) \approx f(x_0) + f'(x_0)(x-x_0)
$$

下面要求 f(x) =0 的时候, x的值, 令 f(x) = 0, 则有

$$
x \approx x_0 - \frac{f(x_0)}{f'(x_0)}
$$

因为是个近似, 所以求出的x的值肯定不是解, 但会比$x_0$ 更靠近目标, 再把x代入迭代,得到

$$
x_{n+1}=x_{n}-{\frac {f(x_{n})}{f'(x_{n})}}
$$



> 收敛的充分条件：若f(x)二阶可导(f'(x)是连续函数)，那么在待求的零点的周围存在一个区域，只要起始点 位于这个邻近区域内，那么牛顿-拉弗森方法必定收敛。



### Newton's method in optimization

优化问题可以转化为求f(x)驻点，即找出 f'(x) = 0 的解 



牛顿法的基本思想是利用迭代点 $x_k$处的**一阶导数(梯度)**和**二阶导数(Hessen矩阵)**对目标函数进行二次函数近似，然后把二次模型的极小点作为新的迭代点，并不断重复这一过程，直至求得满足精度的近似极小值。





先看一维的情形，要求$f'(x)=0$。对函数$f'(x)$进行泰勒展开近似，得到

$$
f'(x) \approx f'(x_0) + f''(x_0)(x-x_0)
$$

令$f'(x)=0$,  即得到

$$
x \approx x_0 -  \frac{f'(x_0) }{f''(x_0) }
$$

则迭代公式

$$
x_{n+1}=x_n-\frac{f'(x_n)}{f''(x_n)}
$$

<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Newton_optimization_vs_grad_descent.svg/440px-Newton_optimization_vs_grad_descent.svg.png" width="20%">

A comparison of [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent)(green) and Newton's method (red) for minimizing a function (with small step sizes). Newton's method uses curvature information to take a more direct route.看图可以看出来, 绿色是走的梯度下降, 红色利用了二阶导数信息, 更加直接地奔向目标点



##### 关于牛顿法和梯度下降法的效率对比：

对于梯度下降
$$
x \leftarrow x + \alpha  f'
$$

牛顿法是利用二阶以及一阶导数的信息去迭代，梯度下降只是利用一阶导数，所以牛顿法就更快。类似于滑下一个坡, 如果能看到前方快到底了,直接跳下去则会快不少. 

<img src="/img/Newton.assets/image-20191126234041654.png" alt="image-20191126234041654" style="zoom:50%;" />

　

##### 牛顿法的优缺点总结：

优点：二阶收敛，收敛速度快

缺点：

- 牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
- 初始值的选取非常重要，初始值选得不好有可能会直接导致算法不收敛。





多维变量的情况

$$
f(\mathbf{x}) \approx f(\mathbf{a}) + Df(\mathbf{a}) (\mathbf{x}-\mathbf{a})
   +  \frac{1}{2} (\mathbf{x}-\mathbf{a})^T Hf(\mathbf{a}) (\mathbf{x}-\mathbf{a}).
$$

$$
\mathbf x_{n + 1} = \mathbf{x}_n -  [Hf({\mathbf x}_n )]^{-1} \nabla f(\mathbf x_n)
$$

其中， $\nabla f( \mathbf x_n )$  为梯度向量, 也有表示为$\mathbf g$,  $Hf(  \mathbf  x  _n )$为 **海森矩阵 Hessian matrix**

$$
{\mathbf  H}={\begin{bmatrix}{\dfrac  {\partial ^{2}f}{\partial x_{1}^{2}}}&{\dfrac  {\partial ^{2}f}{\partial x_{1}\,\partial x_{2}}}&\cdots &{\dfrac  {\partial ^{2}f}{\partial x_{1}\,\partial x_{n}}}\\[2.2ex]{\dfrac  {\partial ^{2}f}{\partial x_{2}\,\partial x_{1}}}&{\dfrac  {\partial ^{2}f}{\partial x_{2}^{2}}}&\cdots &{\dfrac  {\partial ^{2}f}{\partial x_{2}\,\partial x_{n}}}\\[2.2ex]\vdots &\vdots &\ddots &\vdots \\[2.2ex]{\dfrac  {\partial ^{2}f}{\partial x_{n}\,\partial x_{1}}}&{\dfrac  {\partial ^{2}f}{\partial x_{n}\,\partial x_{2}}}&\cdots &{\dfrac  {\partial ^{2}f}{\partial x_{n}^{2}}}\end{bmatrix}}.
$$

$$
{\mathbf  H}_{i,j}={\frac  {\partial ^{2}f}{\partial x_{i}\partial x_{j}}}.
$$

复杂性

**▪ Hessian 矩阵非正定（非凸）导致无法收敛；**

**▪ Hessian 矩阵维度过大带来巨大的计算量。**



> 对于 f(x) 输出是向量的情况，  $\nabla f(\mathbf x_n)$  则为 *雅克比矩阵*(Jacobian Matrix)

$$
{\displaystyle \mathbf {J} ={\begin{bmatrix}{\dfrac {\partial \mathbf {f} }{\partial x_{1}}}&\cdots &{\dfrac {\partial \mathbf {f} }{\partial x_{n}}}\end{bmatrix}}={\begin{bmatrix}{\dfrac {\partial f_{1}}{\partial x_{1}}}&\cdots &{\dfrac {\partial f_{1}}{\partial x_{n}}}\\\vdots &\ddots &\vdots \\{\dfrac {\partial f_{m}}{\partial x_{1}}}&\cdots &{\dfrac {\partial f_{m}}{\partial x_{n}}}\end{bmatrix}}}
$$

$$
{\displaystyle \mathbf {J} _{ij}={\frac {\partial f_{i}}{\partial x_{j}}}.}
$$



原始牛顿法，由于迭代公式没有步长因子，相当于定长迭代， 有时可能不稳定。

引入

### 阻尼牛顿法 Damped Newton Method







## 拟牛顿法 Quasi-Newton Methods

拟牛顿法 都是用来解决 牛顿法 本身的 复杂计算、难以收敛、局部最小值等问题。

拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用**正定矩阵**来近似Hessian矩阵的逆，从而简化了运算的复杂度。

拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

用一个近似矩阵B替代逆Hessian矩阵H。不同算法的矩阵B的计算有差异，但大多算法都是采用迭代更新的思想在tranning更新矩阵B。



#### 拟牛顿条件

拟牛顿方程， 割线条件 (Secant  condition)

令 $ B \approx H, D \approx H^{-1}$

设经过k次迭代后，在此处泰勒二阶展开

$$
f(\mathbf x)\approx f(\mathbf x_{k})+\nabla f(\mathbf x_{k}) (\mathbf x -\mathbf x_{k} )+{\frac {1}{2}} (\mathbf x -\mathbf x_{k} )^{\mathrm {T} }\  \nabla^2f(\mathbf x_{k})  (\mathbf x -\mathbf x_{k} )
$$

两边同时做梯度运算

$$
\nabla f(\mathbf x)\approx \nabla f(\mathbf x_{k}) +H_{k} (\mathbf x -\mathbf x_{k} )
$$

令 $x = x_{k+1}$ ，得到  **拟牛顿方程**

$$
\mathbf g_{k+1} - \mathbf g_{k} \approx  H_k ( \mathbf x_{k+1} - \mathbf x_{k} )
$$

令 $\mathbf s_k = \mathbf x_{k+1} - \mathbf x_{k} ,\mathbf   y_k = \mathbf g_{k+1} - \mathbf g_{k} $ 

$$
\mathbf   y_k = B_{k+1}  \cdot \mathbf s_{k} \\
\mathbf   s_k = D_{k+1}  \cdot \mathbf y_{k}
$$

### DFP 算法

核心迭代公式

$$
D_{k+1} = D_k + \Delta D_k
$$

其中 $D_0 $ 一般取  $I$ , 关键是 如何构造 校正矩阵 $\Delta D$
采用 待定法，  形式比较 tricky.  保证了 $\Delta D $ 的对称性

$$
\Delta D = \alpha \mathbf u \mathbf u^T +  \beta \mathbf v \mathbf v^T
$$

代入 拟牛顿公式 ，得

$$
\mathbf   s_k = D_{k+1}  \cdot \mathbf y_{k}  + \alpha \mathbf u \mathbf u^T\mathbf y_{k} +  \beta \mathbf v \mathbf v^T\mathbf y_{k}  \\
\mathbf   s_k   = D_{k+1}  \cdot \mathbf y_{k}  + (\alpha \mathbf u^T\mathbf y_{k}) \mathbf u +  (\beta \mathbf v^T\mathbf y_{k})\mathbf v
$$

猜测，不妨令  $ \alpha \mathbf u^T\mathbf y_{k} = 1 ,   \beta \mathbf v^T\mathbf y_{k} = -1$

代入上面 ，

$$
\mathbf   s_k  =  D_{k+1}  \cdot \mathbf y_{k}  +  \mathbf u -  \mathbf v
$$

不妨令 

$$
\mathbf  u  =   \mathbf s_k  \ , \  \mathbf v =   D_{k}  \cdot \mathbf y_{k}
$$

再代入，得(D是对称矩阵) 

$$
\alpha  = \frac{1}{ \mathbf s^T  \mathbf y_k} , \ \  \beta = - \frac{1}{ (\mathbf D_k\mathbf y_k  )^T \mathbf y_k} = - \frac{1}{ \mathbf y_k  ^T \mathbf D_k \mathbf y_k}
$$

最终得迭代公式

$$
\Delta D_k = \frac{ \mathbf s_k  \mathbf s_k^T}{ \mathbf s_k^T  \mathbf y_k} - \frac{\mathbf D_k  \mathbf y_k\mathbf y_k ^T \mathbf D_k}{\mathbf y_k  ^T \mathbf D_k \mathbf y_k}
$$

$$
H_{k+1}=B_{k+1}^{-1}=  {\displaystyle H_{k}+{\frac {\Delta x_{k}\Delta x_{k}^{\mathrm {T} }}{\Delta x_{k}^{\mathrm {T} }\,y_{k}}}-{\frac {H_{k}y_{k}y_{k}^{\mathrm {T} }H_{k}}{y_{k}^{\mathrm {T} }H_{k}y_{k}}}}
$$

### BFGS 算法

性能佳， 求解无约束**非线性优化**问题的最常用方法。

BFGS 直	逼近 H

$$
B_{k+1} = B_k + \Delta B_k
$$

$$
\Delta B_k = \alpha \mathbf u \mathbf u^T +  \beta \mathbf v \mathbf v^T
$$

$$
\mathbf y_k = B_k s_k + (\alpha u^Ts_k)u + (\beta v^T s_k)v
$$

令  $ \alpha u^Ts_k = 1, \ \beta v^T s_k=-1, \mathbf  u  =   \mathbf y_k  \ , \  \mathbf v =   B_{k}  \cdot \mathbf s_{k} $ , 

$$
B_{k+1} = B_{k}+{\frac {y_{k}y_{k}^{\mathrm {T} }}{y_{k}^{\mathrm {T} }\Delta x_{k}}}-{\frac {B_{k}\Delta x_{k}(B_{k}\Delta x_{k})^{\mathrm {T} }}{\Delta x_{k}^{\mathrm {T} }B_{k}\,\Delta x_{k}}}
$$

### L-BFGS 算法

在 BFGS算法中需要用到一个 $N \times N$ 的矩阵 D， 当N 很大时候， 例如10万， 需要内存约 74GB

基本思想：不在存储完整的 矩阵$D_k$, 而是存储向量 $s_i, y_i$, 同时， 只存固定m个最新的  $s_i, y_i$，

这样储存由 $O(N^2)$ 降为 $O(mN)$





## References

https://www.cnblogs.com/shixiangwan/p/7532830.html

https://blog.csdn.net/itplus/article/details/21896453



