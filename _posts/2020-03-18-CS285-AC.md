---
layout:     post
title:      CS 285. Actor-Critic
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Berkeley
    - CS285
    - Actor-Critic
    - Reinforcement Learning

---




## Actor-Critic Algorithms

### Lecture

1. Improving the policy gradient with a critic
2. The policy evaluation problem
3. Discount factors
4. The actor-critic algorithm

##### Goals:

- Understand how policy evaluation fits into policy gradients 
- Understand how actor-critic algorithms work



本课的思路, 从PG推导出 Q,V,  再推导出 bootstrap TD learning, 再到AC.  
trajectory 是以t为维度分析的, 适合连续图像state.

Eligibility traces & n-step returns  , GAE

off-policy AC : Q-Prop



#### Review

- Actor-critic algorithms:
  - **Actor: the policy**
  - **Critic: value function**
  - **Reduce variance of policy gradient**

- Policy evaluation
  - Fitting value function to policy
- **Discount factors**
  - Carpe diem Mr. Robot  , Dead State
  - ...but also a variance reduction trick
- Actor-critic algorithm design
  - **One network** (with two heads) or **two networks**
  - **Batch-mode**, or **online (+ parallel)**
- State-dependent baselines
  - Another way to use the critic
  - Can combine: **n-step returns** or **GAE**





#### Improving the policy gradient

$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})\hat{Q}_{i,t}\right]
$$



- PG 利用causality,  将 total reward 换成了 $\hat Q_{i,t}$
- $\hat Q_{i,t}$ : estimate of expected reward   taken $a_{i,t}$ in $s_{i,t}$ ,  是个统计值, 因为采样的随机性, 方差很大, 很可能不准 ;   



下面正式引入Q, V函数.  这门课的角度跟其他的不一样.   而这两个值函数就是充当critic的作用. 只要有一个就行.

- $$Q(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'}) \vert \mathbf{s}_t,\mathbf{a}_t]$$  :  true  expected  **reward-to-go** , Q的真值, 代入梯度公式效果肯定更好,  是个 **lower variance PG**  算法
- 再考虑 **baseline**,  $$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \vert \mathbf{s}_{i, t}\right)\left(Q\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)-b\right)$$. 
  -  从 average reward  =>average Q, $$b_t=\frac{1}{N}\sum_iQ^\pi(\mathbf{s}_{i,t},\mathbf{a}_{i,t})$$  , 从baseline 这个角度引入V
-  $$V (\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim\pi_\theta(\mathbf{a}_t\vert \mathbf{s}_t)}[Q (\mathbf{s}_t,\mathbf{a}_t)]$$ ,   直观解释, 比平均好的action有更多的可能性. 
  - 这里,有个homework, 是证明baseline 是可以 depend on  state. 
  - 代入, 得到Advantage.  下面公式里面都带上标$\pi$
- $$Q^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})\vert\mathbf{s}_t,\mathbf{a}_t]$$.
- $$V^\pi(\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim\pi_\theta(\mathbf{a}_t\vert\mathbf{s}_t)}[Q^\pi(\mathbf{s}_t,\mathbf{a}_t)]$$.
- $A^\pi(\mathbf{s}_t,\mathbf{a}_t)=Q^\pi(\mathbf{s}_t,\mathbf{a}_t)-V^\pi(\mathbf{s}_t)$  how much better $\mathcal a_t$ is 

用Advantage改进目标公式

- $$\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}\vert\mathbf{s}_{i,t})A^\pi(\mathbf{s}_t,\mathbf{a}_t)\right]$$ 
  - A越准确, 方差越低
  - 然而 Advantage  function 也是未知的, 所以只能approximation
- $$\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}\vert\mathbf{s}_{i,t}) \bigg(\sum_{t'=1}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'}) -b \bigg) \right]$$  
  - 之前的baseline公式, 是一种Advantage, 但不是best.  括号中的部分 unbiased, but high variance single-sample estimate 
  - 显然 从一个个sample中直接平均求 V, Q, A 效果不好, 方差大



### Value function fitting



<img src="/img/CS285.assets/image-20200403003855608.png" alt="image-20200403003855608" style="zoom:50%;" />



下面寻找更好的 estimate方法 ,  fit  what  to  what ,   V, Q, A 三个选哪个作为拟合目标,因为Q,A都需要(s,a)

- **一般AC算法都是on-policy, 拟合V ,   off-policy的情况会拟合Q** 
- Q的bootstrap . 

$$
\begin{aligned}
Q^\pi(\mathbf{s}_t,\mathbf{a}_t)&=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})\vert\mathbf{s}_t,\mathbf{a}_t] \\

& 拆分Q, 当前的r(s_t,a_t)不依赖于任何\theta, 是个准确的函数值, 再加上未来V的期望 \\ 

&=r(\mathbf{s}_t,\mathbf{a}_t)+ \underline{ \sum_{t'=t+1}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_t,\mathbf{a}_t]}  \longrightarrow   V^\pi(\mathbf{s}_{t+1})  \\ 

& 因为具体的s_{t+1} 是什么状态并不知道, 所以还是要用期望 \\

&=r(\mathbf{s}_t,\mathbf{a}_t)+\mathbf{E}_{\mathbf{s}_{t+1}\sim p(\mathbf{s_{t+1}}|\mathbf{s}_t,\mathbf{a}_t)}[V^\pi(\mathbf{s}_{t+1})] \\

&如果 V^\pi 的估值比较准确,那做个近似是ok的,就是只用\text{single sample}的V(s_{t+1}),是个无偏估计 \\
& \approx  r(\mathbf{s}_t,\mathbf{a}_t)+ V^\pi(\mathbf{s}_{t+1})\\ 

\end{aligned}
$$

因为$s_{t+1}$有各种可能,这里只取一个具体sample值, 所以是个近似. 但这是个decent  approximation ,损失一些精度, 引入了一些方差, 但是很方便使用. 

- 下面 A 可以方便的用V 表示
  $$
  A^\pi(\mathbf{s}_t,\mathbf{a}_t)\approx r(\mathbf{s}_t,\mathbf{a}_t)+V^\pi(\mathbf{s}_{t+1})-V^\pi(\mathbf{s}_t)
  $$



#### Policy evaluation

下面就考虑怎么准确的 fit  V, 也就是 评估当前策略. 

#### 方法之一:   Monte Carlo policy evaluation

- $$V^\pi(\mathbf{s}_t)\approx\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$  single sample estimation

- $$V^\pi(\mathbf{s}_t)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$   reset the simulator

  - 需要在$s_t$这个状态很多采样, 需要在终点后回到$s_t$,  很多env无法做到, 如自动驾驶.

  <img src="/img/CS285.assets/image-20200318235808468.png" alt="image-20200318235808468" style="zoom: 33%;" />

下面假设env无法reset,  只能一个一个trajectory 的sample.  **这里是从时间t维度看的, 而不是具体的s.** 所以下面图中的多个trajectory, 在t时刻, 可能遇到差不多类似的$s_t$(欧拉距离非常接近).  

<img src="/img/CS285.assets/image-20200319000233746.png" alt="image-20200319000233746" style="zoom:33%;" />

> 感觉这种的考虑的方式适合连续情况, 比如自动驾驶, 如果用图像做state, 基本无法重现同一个state, 或者state本身不包含太多逻辑结构里面的env;  对于围棋之类的, 两个state,只差一个落子, 结果天壤之别的,不是很适用. 还一个是情况有些MDP的 $s_t$ , t 与 具体 s 的关联程度可能很低.  
> 用时间维度的t来分析问题, 对很多问题不是很适合. 不是不能做, 纯粹是基于t的一种统计了. 感觉RL想用统一的框架去解决所有问题, 忽视了问题本身的特性, 目前来说还是不靠谱.

- $V(s_t)$方差很大.   value  function的value 会changes drastically as time step changes,  即v(last t) = last step reward,   v(firtst  step) = sum T 个reward
- $$V^\pi(\mathbf{s}_t)\approx\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$  ,  not as good  as  $$V^\pi(\mathbf{s}_t)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$ ,  but  pretty good.  是不如同一个$s_t$出发, 但还不错, 但如果有很多类似的$s_t$, 也会减小方差 ; 神经网络也有泛化能力,会去平均比较相似的一类state的reward
- 神经网络会有一些偏倚。因为可能我们从比较接近的两个状态出发做出的两条轨迹，结果上可能会有很大的偏倚；而对于神经网络来说，相似的输入基本上也就对应了相似的输出。毕竟神经网络只是一个函数的逼近器而已，对于确定性模型，它的输出是良定的 (**well-defined**)，**意味着对于同一个输入对应同一个输出**。神经网络拟合很多样本，将其综合起来得到一个低方差的估计：从相似的出发点，一条轨迹极好，另一条极不好，神经网络会认为这是不合理的，并将其平均处理。如果样本越多，函数将近似得越好，结果就越好。但是譬如在两个相似状态之间存在一个断崖，那么答案就会出现问题，但是不管怎么说估计的方差总是小的。
- PG的方差指的是variance of the entire gradient, not the variance in a particular state. 整个的方差才是需要care的, 下面网络开始学习的时候反向传播BP, 应用到网络参数中. 从初始状态开始生成很多trajectory是ok的.



> sutton的书上认为: 尽管带baseline的REINFORCE算法同时使用了策略函数和状态值函数，但是我们不认为它是一个actor-critic算法因为它的值函数只作为一个baseline而没有作为一个critic。That is, it is not used for bootstrapping, but only as a baseline for the state.  
> 我感觉没所谓. 只要多拟合了一个值函数, 并用该函数代入到PG就可以认为是critic了,  拟合V,Q都行.   直接把Gain代入PG那个不算 



##### Monte Carlo evaluation with function approximation

下面使用 单个sample 的Gain来训练 V(s)函数.   注意, 神经网络仍然拟合的是具体的s, 按s的维度来的.  上面的分析是按照t维度来分析.

<img src="/img/CS285.assets/image-20200331171445107.png" alt="image-20200331171445107" style="zoom:67%;" />

- training  data: $$\left\{\left(\mathbf{s}_{i,t},y_{i,t}:=\sum_{t'=t}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)\right\}$$
- Supervised regression:   神经网络监督学习最小化loss  $\mathcal{L}(\phi)=\frac{1}{2}\sum_i\left\Vert\hat{V}_\phi^\pi(\mathbf{s}_i)-y_i\right\Vert^2$



#### 方法之二:  TD   Can we do better?



- 引入 TD,  **bootstrap** estimate.  

  > 感觉 RL是基于统计的, 函数拟合的,  这个算法只是为了拟合value的时候尽可能的方差小, 拟合起来更容易. 不会去考虑value是怎么来的. 但对一定规模问题已经可以求解.

- ideal target :  这里有i, 代表一个sample;  跟之前 Q值的近似一样
  $$
  \begin{aligned}
  y_{i,t} &=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_{i,t}] \\
  &\approx r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+ \sum_{t'={t+1}}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_{i,{t+1}}]  \\
  &\approx r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+V^\pi(\mathbf{s}_{i,t+1}) \quad 这个真值不知道 \\
  &\approx r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+\hat{V}_\phi^\pi(\mathbf{s}_{i,t+1}) \quad 这个目标是有偏的, 但是方差小, 因为更稳
  \end{aligned}
  $$

- variance 与 biases  两者的关系类似于**信噪比 signal-to-noise ratio** , 两个信号, 一个方差很高, 淹没了真值,  一个方差很低, 稍微有点偏差, **样本数量比较少的时候**, 当然后者好.  不过这个也看具体问题, 需要 tradeoff.   **权衡,  方差 ,准确率 以及 采样数.** 

- 之前的 Monte Carlo target :  $$y_{i,t} = \sum_{t'=t}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})$$

training  data: $$\left\{\left(\mathbf{s}_{i, t}, r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)\right)\right\}$$    ;		biased, low variance

- **一个点是, 如果用 bootstrap,  一开始的网络的输出都是垃圾值**,  所以网络的参数初始化方面可以做点文章, 比如用蒙特卡洛的return来初始化.



##### Policy evaluation examples

从大框架说, TD-Gammon  与 AlphaGo 差不多.

<img src="/img/CS285.assets/image-20200319024553029.png" alt="image-20200319024553029" style="zoom:50%;" />



### Actor-critic algorithm

**batch actor-critic algorithm**:

1. sample $$\left\{\mathbf{s}_{i}, \mathbf{a}_{i}\right\}$$ from $$\pi_{\theta}(\mathbf{a} \vert \mathbf{s})$$ (run it on the robot)
2. fit $\hat{V}_{\phi}^{\pi}$ (s) to sampled reward sums  
3. evaluate $$\hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}^{\prime}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)$$
4.  $$\nabla_{\theta} J(\theta) \approx \sum_{i} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i} \vert \mathbf{s}_{i}\right) \hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)$$ . 
5.    $\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$

显然这里有两个网络要去train.   1.  V fit Gain .  2.PG 用的 A = r + Vs' - Vs



#### Discount factors

1. **episodic task**
2. **continuous/cyclical tasks**  

discount 主要是为了 无限长问题的reward的收敛. 然后在逻辑上也能说得通, 未来回报对现在打折.

- what if $T$ (episode length) is $\infty ?$ $\hat{V}_{\phi}^{\pi}$ can get infinitely large in many cases

- simple trick: **better to get rewards sooner than later**

$$
y_{i, t} \approx r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)
$$
discount factor $\gamma \in[0,1]$ (0.99 works well)



<img src="/img/CS285.assets/image-20200319033806252.png" alt="image-20200319033806252" style="zoom: 33%;" />

为了更好的理解 $\gamma$,  假设原来有个reward不含discount的MDP, 可以在这个MDP中构建一个新增一个死亡状态 (death state), 原来的状态转移概率都乘以$\gamma$ ;  本质上是一个吸收状态，且此后收益一直为0（或者理解为游戏立即结束).  其实这样理解更麻烦. 原来的意义很直观. 



##### discount factors for policy gradients

$$
\begin{aligned}
&y_{i, t} \approx r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)\\
&\mathcal{L}(\phi)=\frac{1}{2} \sum_{i}\left\|\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)-y_{i}\right\|^{2} \quad \begin{array}{l}
\text { with critic: } \\
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)(\overbrace{r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t}\right)}^{\hat A^\pi(\mathbf s_{i,t}, \mathbf a_{i,t})})
\end{array}
\end{aligned}
$$



what about **(Monte Carlo) policy gradients**? 这里比较重要, 关于$\gamma$ 的解释

- option 1:  对$s_t$后面每个r discount 一下 ,  不是准确的PG,  但基于PG算法喜欢用这个
  $$
  \nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\sum_{t^{\prime}=t}^{T} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right)
  $$

- option 2:  对所有r,  都discount , 一般简单问题, 代码实现都是这个. 
  $$
  \begin{aligned}
  \nabla_{\theta} J(\theta) & \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} \gamma^{t-1} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right) \\
  & \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\sum_{t'=t}^{T} \gamma^{t^{\prime}-1} r \left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right) \\
  & \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \gamma^{t-1} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\sum_{t'=t}^{T} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right)
  \end{aligned}
  $$

- Option2  看起来更像 **short-term** 策略, 因为更关注最近的r , 后期的r因为权重原因, 不太重要了. 对于上面那个自己构建的MDP, 有死亡状态的,  有几率直接结束游戏的, 所以必须更加关注前面步骤的优化 , 显然option2更合适. 

- Option1 是实践中经常用到的,    主要是解决 **continuous/cyclical tasks**,  比如想让机器人走路尽可能时间长.  用option2会在前期过度优化.  **option1 实际上并不是准确的policy gradient**. option1 可以被解释为approximation to the gradient of average reward without a discount. 参考 Philip Thomas 论文. 实际上是手动设计了reward .  

  我的理解 , Option1 这样想象一下就好了, 如果一个机器人执行online, 运行了很长了时间了, 比如t, 则当前得到的reward会被$\gamma^t$ 缩小的特别小, 反馈就不起啥作用了. 所以对连续的问题, 重要的就是活在当下,任何时刻都可以看做新的起点. 

- Option 1的直觉解释, 数列的收敛性.  $\gamma \to 1$ , 可以得到 total average reward, 让其收敛.  但未来的reward是高度不确定的, 越往后方差越大, 后期超级不稳定, 基本会让任务失败.  所以通过$\gamma$ 来切断很遥远未来的reward, 将他们弄的很小, 减小方差的手段.  然后关注于当下时刻的new  reward, 因为这些都是更准确的reward, 可以作为 biased但方差更小的average reward.  还是权衡 方差与准确

- 所以还是看具体问题来选择参数.  游戏类的不太关注这个,  搞机器人control的讲究无限长周期的平均回报,  主要考虑的是后期要稳, 收敛.  只有reward无限大的情况, 才必须使用$\gamma$  



#### Actor-critic algorithms (with discount)

**batch** actor-critic algorithm:

1. sample $$\left\{\mathbf{s}_{i}, \mathbf{a}_{i}\right\}$$ from $$\pi_{\theta}(\mathbf{a} \vert \mathbf{s})$$ (run it on the robot)
2. fit $$\hat{V}_{\phi}^{\pi}(\mathbf{s})$$ to sampled reward sums
3. evaluate $$\hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}^{\prime}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)$$
4. $$\nabla_{\theta} J(\theta) \approx \sum_{i} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i} \vert \mathbf{s}_{i}\right) \hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)$$ .
5. $$\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$$ .

V fit Gain,  PG A



**online** actor-critic algorithm:

1. take action a $$\sim \pi_{\theta}(\mathbf{a} \vert \mathbf{s}),$$ get $$\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}, r\right)$$
2. update $$\hat{V}_{\phi}^{\pi}$$ using target $$r+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}^{\prime}\right)$$ 
3. evaluate $$\hat{A}^{\pi}(\mathbf{s}, \mathbf{a})=r(\mathbf{s}, \mathbf{a})+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}^{\prime}\right)-\hat{V}_{\phi}^{\pi}(\mathbf{s})$$
4. $$\nabla_{\theta} J(\theta) \approx \nabla_{\theta} \log \pi_{\theta}(\mathbf{a} \vert \mathbf{s}) \hat{A}^{\pi}(\mathbf{s}, \mathbf{a}) $$.
5. $$\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta) $$.

V fit r+V' ,  PG A.  
这里可以看出  **online 的特性,  就是一个action 一个update.** 

online的也可以改进为 batch 版本,  两个网络的拟合可以batch化 ,works best with a batch (e.g., parallel workers). 这样的话, 工作线程有可能用一个稍微旧一点的policy,或者旧一点的value函数, 但却会更稳定, 对online来说, 一般都是并行运行, 稳定性很重要, batch 更大.

<img src="/img/CS285.assets/image-20200319112756604.png" alt="image-20200319112756604" style="zoom:50%;" />



##### Architecture design

AC 分开用两个神经网络, 比较简单稳定.   也可以用一个网络, 可以共享特征, 使用起来也更高效, 但实践中设计超参比较困难, 因为有两个梯度而且大小完全不一样.

两个网络的, no shared features between actor & critic  
simple & stable

<img src="/img/CS285.assets/image-20200401011518509.png" alt="image-20200401011518509" style="zoom:50%;" />   

<img src="/img/CS285.assets/image-20200401011537702.png" alt="image-20200401011537702" style="zoom:50%;" />



#### Use Critics as better baselines

PG 里面, 可以用 G, V, Q ,A

##### state-dependent baselines,  基于state , 即V(s)

- Actor-Critic :  有偏, 方差小 , Advantage

  $$
  \nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_\theta\log\pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})\left(r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+\gamma\hat{V}^\pi_\phi(\mathbf{s}_{i,t+1})-\hat{V}^\pi_\phi(\mathbf{s}_{i,t})\right)
  $$
  
  - \+ lower variance (due to critic)
  - \- not unbiased (if the critic is not perfect)
  - Critics不像MC那样直接平均,  方差更小.  问题是, critic估值不准的时候,  r+V' 是 biased. 
  
- Policy-Gradient:  无偏 高方差 , Gain
  
  $$
  \nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_\theta\log\pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})\left(\left(\sum_{t'=t}^T\gamma^{t'-t}r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)-b\right)
  $$
  
  - \+ no bias
  - \- higher variance (because single-sample estimate)

  

- 结合两者的优点.    后面的差其实是一种 Advantage:  Gain - V
  
  $$
  \nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_\theta\log\pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})\left(\left(\sum_{t'=t}^T\gamma^{t'-t}r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)-\hat{V}^\pi_\phi(\mathbf{s}_{i,t})\right)
  $$
  
  - \+ no bias
  - \+ lower variance (baseline is closer to rewards)
  - sutton 书上的 REINFORCE with Baseline, baseline 是 V(s)



##### action-dependent baselines

这里介绍很少见的,  off-policy的,  拟合Q的 AC算法:  Q-prop

baseline 与action 相关, 则肯定是 biased. 这个方式很少见. baseline用Q, 要修正. 

-    $$Q^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{T} E_{\pi_{\theta}}\left[r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right) \vert \mathbf{s}_{t}, \mathbf{a}_{t}\right]$$  .
-    $$V^{\pi}\left(\mathbf{s}_{t}\right)=E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \vert  \mathbf{s}_{t}\right)}\left[Q^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$$  .
-    $$A^{\pi}\left(\mathrm{s}_{t}, \mathrm{a}_{t}\right)=Q^{\pi}\left(\mathrm{s}_{t}, \mathrm{a}_{t}\right)-V^{\pi}\left(\mathrm{s}_{t}\right)$$  .

下面看两种A

-  $$\hat{A}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{\infty} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-V_{\phi}^{\pi}\left(\mathbf{s}_{t}\right) \quad$$ 无偏高方差
-  $$\hat{A}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{\infty} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-Q_{\phi}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$  如果critic准, 趋向0 ;  not correct 不准确

$$
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\hat{Q}_{i, t}-Q_{\phi}^{\pi}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)+\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} E_{\mathbf{a} \sim \pi_{\theta\left(\mathbf{a}_{i} | \mathbf{s}, t\right)}}\left[Q_{\phi}^{\pi}\left(\mathbf{s}_{i, t}, \mathbf{a}_{t}\right)\right]
$$

这里把$Q^\pi$不再称为baseline, 叫control variant.  上式前面部分把$Q^\pi$ 减去了, 期望为0, 但是有误差, 后面又加回来的是 expected value , 因为不是0, 所以需要加回来补偿. 这么做主要是因为$Q^\pi$ 是个近似函数, 所以其期望可能是有解析解的(根据构造的形式而异, 如策略是高斯的，Q函数是二次的，那么这个结构就是可以在没有样本的情况下得到评估的，但是是否有这样的结构还是要看具体问题是不是可以这样来近似), 就不需要基于sample一个个算, 这个解析解就是个稳定值也容易求解, 所以上式第二项就比第一项容易求解.  
一个重要思路, 就是用解析解来解决 data efficient 问题, 将问题构造为近似函数.  不过现在流行NN了...

use a critic *without* the bias (still unbiased), provided second term can be evaluated Gu et al. 2016 (**Q-Prop**) 

基于state的baseline是无偏的，基于action的baseline是有偏的，但是偏差有可能可以通过一个校正项补救回来。



### Eligibility traces & n-step returns

第一次引入 Eligibility traces,   **MC 与 TD 的统一**.  权衡 方差与误差 .

- $$\hat{A}_\text{C}^\pi(\mathbf{s}_t,\mathbf{a}_t)=r(\mathbf{s}_t,\mathbf{a}_t)+\gamma\hat{V}^\pi_\phi(\mathbf{s}_{t+1})-\hat{V}^\pi_\phi(\mathbf{s}_t)$$.
  - lower variance
  - higher bias if value is wrong (it always is)
- $$\hat{A}_\text{MC}^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^\infty \gamma^{t'-t}r(\mathbf{s}_{t'},\mathbf{a}_{t'})-\hat{V}^\pi_\phi(\mathbf{s}_t)$$.
  - no bias
  - higher variance (because single-sample estimate)

所以还是 bias 与 variance 的tradeoff.  合并两者  combine these two

<img src="/img/CS285.assets/image-20200319154245060.png" alt="image-20200319154245060" style="zoom:33%;" />

图示, 未来的reward 方差很大, 20年后干啥, 与15分钟后干啥.  可以在方差很大前就切断. 把未来之后的很多推断直接删除直觉上也不会影响太大. 

<img src="/img/CS285.assets/image-20200319160334960.png" alt="image-20200319160334960" style="zoom: 33%;" />

如图, 红线表示, 已经有discount  factor \gamma$ 将未来的reward降权了.  绿色则是在某个时间点直接截断了.   这算是对 discount这个模式的一个改进. 不过还是比较粗糙的.  

**n-step returns**:
$$
\hat{A}_n^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^{t+n}\gamma^{t'-t}r(\mathbf{s}_{t'},\mathbf{a}_{t'})+\gamma^n\hat{V}^\pi_\phi(\mathbf{s}_{t+n})-\hat{V}^\pi_\phi(\mathbf{s}_t)
$$

一般选 n >1 .  n=1, 就是 $A_c$, n无穷大就是 $A_{MC}$ .  调整这个n来获得最佳信噪比. 



#### Generalized advantage estimation  GAE

- Schulman et al. (2016) 提出了**广义优势估计** (**Generalized Advantage Estimation, GAE**)。本质上是 n步收益的推广。

- $$\hat{A}_\text{GAE}^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{n=1}^\infty w_n\hat{A}_n^\pi(\mathbf{s}_t,\mathbf{a}_t)$$ .   Weighted combination of n-step returns
- **如何选择权重,  Mostly prefer cutting earlier (less variance), 希望前期的权重大**, 所以 $w_n\propto\lambda^{n-1}$ 
- 展开代入得到  $$\hat{A}_\text{GAE}^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^\infty (\gamma\lambda)^{t'-t}\delta_{t'}$$ , 其中 $$\delta_{t'}=r(\mathbf{s}_{t'},\mathbf{a}_{t'})+\gamma\hat{V}_\phi^\pi(\mathbf{s}_{t'+1})-\hat{V}_\phi^\pi(\mathbf{s}_{t'})$$
- 这里看出, $\gamma$ 与$\lambda$ 有些类似, 都起到了早期截断,减小方差的作用; 所以说明了discount的确可以减少方差.



#### Actor-critic examples

<img src="/img/CS285.assets/image-20200319165107156.png" alt="image-20200319165107156" style="zoom:33%;" />

- High dimensional continuous control with generalized advantage estimation (Schulman, Moritz, L., Jordan, Abbeel ‘16)
- Batch-mode actor-critic
- **Blends Monte Carlo and function approximator estimators (GAE)**

##### FPS 射击游戏

- Asynchronous methods for deep reinforcement learning (Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu ‘16)
- **Online actor-critic, parallelized batch**
- **N-step returns** with N = 4
- **Single network for actor and critic**



### Actor-critic suggested readings

- Classic papers
  - Sutton, McAllester, Singh, Mansour (1999). Policy gradient methods for reinforcement learning with function approximation: actor-critic algorithms with value function approximation

- Deep reinforcement learning actor-critic papers
  - Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu (2016). Asynchronous methods for deep reinforcement learning: A3C -- parallel online actor-critic
  - Schulman, Moritz, L., Jordan, Abbeel (2016). High-dimensional continuous control using generalized advantage estimation: batch-mode actor-critic with blended Monte Carlo and function approximator returns
  - Gu, Lillicrap, Ghahramani, Turner, L. (2017). Q-Prop: sample-efficient policy- gradient with an off-policy critic: policy gradient with Q-function control variate



 









































































