---
layout:     post
title:      CS 285. Actor-Critic
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---




## Actor-Critic Algorithms

#### Improving the policy gradient

- PG 利用causality,  将 total reward 换成了 Q

- $\hat Q_{i,t}$ : estimate of expected reward   taken $a_{i,t}$ in $s_{i,t}$ ,  是个统计值, 因为采样的随机性, 方差很大, 很可能不准

- $$Q(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'}) \vert \mathbf{s}_t,\mathbf{a}_t]$$  :  true  expected  reward-to-go , Q的真值, 代入梯度公式效果肯定更好,  是个 **lower variance PG**  算法

- 再考虑 baseline,  从 average reward  =>average Q, $$b_t=\frac{1}{N}\sum_iQ^\pi(\mathbf{s}_{i,t},\mathbf{a}_{i,t})$$ 

- 而  $$V (\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim\pi_\theta(\mathbf{a}_t\vert \mathbf{s}_t)}[Q (\mathbf{s}_t,\mathbf{a}_t)]$$ ,   直观解释, 比平均好的action有更多的可能性. 

- 这里,有个homework, 是证明baseline 是可以 depend on  state. 

- 代入, 得到Advantage.  下面公式里面都带上标$\pi$

- $$Q^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})\vert\mathbf{s}_t,\mathbf{a}_t]$$

- $$V^\pi(\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim\pi_\theta(\mathbf{a}_t\vert\mathbf{s}_t)}[Q^\pi(\mathbf{s}_t,\mathbf{a}_t)]$$

- $A^\pi(\mathbf{s}_t,\mathbf{a}_t)=Q^\pi(\mathbf{s}_t,\mathbf{a}_t)-V^\pi(\mathbf{s}_t)$  how much better $\mathcal a_t$ is 

- $$\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}\vert\mathbf{s}_{i,t})A^\pi(\mathbf{s}_t,\mathbf{a}_t)\right]$$ ,   A越准确, 方差越低

- 然而 Advantage  function 也是未知的, 所以只能approximation

- $$\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}\vert\mathbf{s}_{i,t}) \bigg(\sum_{t'=1}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'}) -b \bigg) \right]$$  , 之前的baseline公式, 是一种Advantage,但不是best.  括号中的部分 unbiased, but high variance single-sample estimate 

- 显然 从一个个sample中直接平均求 V, Q, A 效果不好, 方差大

- 下面寻找更好的 estimate方法 ,  fit  what  to  what ,   V, Q, A 三个选哪个作为拟合目标

- 一般AC算法都是拟合V ,   off-policy的情况会拟合Q

- $$
  \begin{aligned}
  Q^\pi(\mathbf{s}_t,\mathbf{a}_t)&=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})\vert\mathbf{s}_t,\mathbf{a}_t] \\
  
  & 拆分Q, 当前的r(s_t,a_t)不依赖于任何\theta, 是个准确的值, 再加上未来V的期望 \\ 
  
  &=r(\mathbf{s}_t,\mathbf{a}_t)+\sum_{t'=t+1}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_t,\mathbf{a}_t] \\ 
  
  
  &=r(\mathbf{s}_t,\mathbf{a}_t)+ V^\pi(\mathbf{s}_{t+1})\quad 因为具体的s_{t+1} 是什么状态并不知道, 所以还是要用期望 \\
  
  &=r(\mathbf{s}_t,\mathbf{a}_t)+\mathbf{E}_{\mathbf{s}_{t+1}\sim p(\mathbf{s_{t+1}}|\mathbf{s}_t,\mathbf{a}_t)}[V^\pi(\mathbf{s}_{t+1})] \\
  
  &如果 V^\pi 的估值比较准确,那可能做个近似是ok的,利用一个无偏估计的近似,就是只用single\  sample的下个状态的V \\
  & \approx  r(\mathbf{s}_t,\mathbf{a}_t)+ V^\pi(\mathbf{s}_{t+1})\\ 
  &因为s_{t+1}有各种可能,这里只取一个具体sample.但这是个decent\  approximation ,损失一些精度, 引入了一些方差, 但是很方便使用\\
  \end{aligned}
  $$

  

- 下面 A 可以方便的用V 表示
  $$
  A^\pi(\mathbf{s}_t,\mathbf{a}_t)\approx r(\mathbf{s}_t,\mathbf{a}_t)+V^\pi(\mathbf{s}_{t+1})-V^\pi(\mathbf{s}_t)
  $$



#### Policy evaluation

- 下面就考虑怎么准确的 fit  V, 也就是 评估当前策略. 

- 方法之一: Monte Carlo policy evaluation  (PG也是蒙特卡洛)

- $$V^\pi(\mathbf{s}_t)\approx\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$  single sample estimation

- $$V^\pi(\mathbf{s}_t)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$  需要在$s_t$这个状态很多采样, 需要在终点后回到$s_t$, reset the simulator, 很多env无法做到

  <img src="/img/CS285.assets/image-20200318235808468.png" alt="image-20200318235808468" style="zoom: 33%;" />

- 下面假设env无法reset,  只能一个一个trajectory 的sample.  这里是从时间t维度看的, 而不是具体的s. 所以下面图中的多个trajectory, 在t时刻, 可能遇到差不多类似的$s_t$(欧拉距离非常接近).  

- 感觉这种的考虑的方式适合连续情况, 比如自动驾驶, 如果用图像做state, 基本无法重现同一个state, 或者state本身不包含太多逻辑性在里面的env;  对于围棋之类的, 两个state,只差一个落子, 结果天壤之别的,不是很适用. 还一个是情况比如是迷宫, $s_t$ 中, t 与 s 的关联程度可能很低.

- 感觉RL想用统一的框架去解决所有问题, 忽视了问题本身的特性, 目前来说还是不靠谱.

- value  function的value 会changes drastically as time step changes,  即v(last t) = last step reward,   v(firtst  step) = sum T 个reward

- $$V^\pi(\mathbf{s}_t)\approx\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$  ,  not as good  as  $$V^\pi(\mathbf{s}_t)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$ ,  but  pretty good.  是不如同一个$s_t$出发, 但还不错, 但如果有很多类似的$s_t$, 也会减小方差 ; 神经网络也有泛化能力,会去平均比较相似的一类state的reward

- PG的方差指的是variance of the entire gradient, not the variance in a particular state. 整个的方差才是需要care的, 下面学习的时候BP, 应用到网络参数中. 从初始状态开始生成很多trajectory是ok的.

- <img src="/img/CS285.assets/image-20200319000233746.png" alt="image-20200319000233746" style="zoom:33%;" />

- training  data: $$\left\{\left(\mathbf{s}_{i,t},y_{i,t}:=\sum_{t'=t}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)\right\}$$

- Supervised regression:   神经网络监督学习最小化loss  $\mathcal{L}(\phi)=\frac{1}{2}\sum_i\left\Vert\hat{V}_\phi^\pi(\mathbf{s}_i)-y_i\right\Vert^2$

##### Can we do better?

- 引入 TD,  **bootstrap** estimate.  感觉 RL是基于统计的, 函数拟合的,  这个算法只是为了拟合value的时候尽可能的方差小, 拟合起来更容易. 不会去考虑value是怎么来的. 但对一定规模问题已经可以求解.

- ideal target :  这里有i, 代表一个sample;  跟之前 Q值的近似一样
  $$
  \begin{aligned}
  y_{i,t} &=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_{i,t}] \\
  &\approx r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+ \sum_{t'={t+1}}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_{i,{t+1}}]  \\
  &\approx r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+V^\pi(\mathbf{s}_{i,t+1}) \quad 这个真值不知道 \\
  &\approx r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+\hat{V}_\phi^\pi(\mathbf{s}_{i,t+1}) \quad 这个目标是有偏的, 但是方差小, 因为更稳
  \end{aligned}
  $$

- variance 与 biases  两者的关系类似于信噪比 signal-to-noise ratio , 两个信号, 一个方差很高, 淹没了真值,  一个方差很低, 稍微有点偏差, 样本数量比较少的时候, 当然后者好.  不过这个也看具体问题, 需要 tradeoff.

- 之前的 Monte Carlo target :  $$y_{i,t} = \sum_{t'=t}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})$$

- training  data: $$\left\{\left(\mathbf{s}_{i, t}, r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)\right)\right\}$$   biased, low variance

- 一个点是, 如果用 bootstrap,  一开始的网络的输出都是垃圾值,  所以网络的参数初始化方面可以做点文章, 比如用蒙特卡洛的return来初始化.



##### Policy evaluation examples

从大框架说, TD-Gammon  与 AlphaGo 差不多.

<img src="/img/CS285.assets/image-20200319024553029.png" alt="image-20200319024553029" style="zoom:50%;" />



### Actor-critic algorithm

batch actor-critic algorithm:
1. sample $$\left\{\mathbf{s}_{i}, \mathbf{a}_{i}\right\}$$ from $$\pi_{\theta}(\mathbf{a} \vert \mathbf{s})$$ (run it on the robot)
2. fit $\hat{V}_{\phi}^{\pi}$ (s) to sampled reward sums
3. evaluate $$\hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}^{\prime}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)$$
4.  $$\nabla_{\theta} J(\theta) \approx \sum_{i} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i} \vert \mathbf{s}_{i}\right) \hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)$$
5.    $\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$

显然这里有两个网络要去train. 



#### Discount factors

1. episodic task
2. continuous/cyclical tasks

- what if $T$ (episode length) is $\infty ?$ $\hat{V}_{\phi}^{\pi}$ can get infinitely large in many cases

- simple trick: **better to get rewards sooner than later**
- ​    $$y_{i, t} \approx r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)$$
- discount factor $\gamma \in[0,1]$ (0.99 works well)



<img src="/img/CS285.assets/image-20200319033806252.png" alt="image-20200319033806252" style="zoom: 33%;" />

 为了更好的理解 $\gamma$, 可以构建一个不含discount的MDP, 但是该MDP的RL公式是带discount的.  在状态集合中新增一个死亡状态 (death state)，本质上是一个吸收状态，且此后收益一直为0（或者理解为游戏立即结束). 



##### discount factors for policy gradients

$$
\begin{aligned}
&y_{i, t} \approx r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)\\
&\mathcal{L}(\phi)=\frac{1}{2} \sum_{i}\left\|\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)-y_{i}\right\|^{2} \quad \begin{array}{l}
\text { with critic: } \\
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)(\overbrace{r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t}\right)}^{\hat A^\pi(\mathbf s_{i,t}, \mathbf a_{i,t})})
\end{array}
\end{aligned}
$$



what about (Monte Carlo) policy gradients? 这里比较重要, 关于$\gamma$ 的解释

- option 1:  对$s_t$后面每个r discount 一下
  $$
  \nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\sum_{t^{\prime}=t}^{T} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right)
  $$

- option 2:  对所有r 都discount 
  $$
  \begin{aligned}
  \nabla_{\theta} J(\theta) & \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} \gamma^{t-1} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right) \\
  & \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\sum_{t'=t}^{T} \gamma^{t^{\prime}-1} r \left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right) \\
  & \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \gamma^{t-1} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\sum_{t'=t}^{T} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right)
  \end{aligned}
  $$

- Option2  看起来更像 short-term 策略, 因为更关注最近的r , 后期的r因为权重原因, 不太重要了. 对于上面那个自己构建的MDP, 有死亡状态的,  有几率直接结束游戏的, 所以必须更加关注前面步骤的优化 , 显然option2更合适.  
- Option1 是实践中经常用到的,    主要是解决 continuous/cyclical tasks,  比如想让机器人走路尽可能时间长.  用option2会在前期过度优化.  option1 实际上并不是准确的policy gradient. option1 可以被解释为approximation to the gradient of average reward without a discount. 参考 Philip Thomas 论文.
- Option 1的直觉解释, 数列的收敛性.  $\gamma \to 1$ , 可以得到 total average reward, 让其收敛.  但未来的reward是高度不确定的, 越往后方差越大, 后期超级不稳定, 基本会让任务失败.  所以通过$\gamma$ 来切断很遥远未来的reward, 将他们弄的很小, 减小方差的手段.  然后关注于当下时刻的new   reward, 因为这些都是更准确的reward, 可以作为 biased但方差更小的average reward.
- 所以还是看具体问题来选择参数.  游戏类的不太关注这个,  搞机器人control的讲究无限长周期的平均回报,  主要考虑的是后期要稳, 收敛.  只有reward无限大的情况, 才必须使用$\gamma$



#### Actor-critic algorithms (with discount)

**batch** actor-critic algorithm:

1. sample $$\left\{\mathbf{s}_{i}, \mathbf{a}_{i}\right\}$$ from $$\pi_{\theta}(\mathbf{a} \vert \mathbf{s})$$ (run it on the robot)
2. fit $$\hat{V}_{\phi}^{\pi}(\mathbf{s})$$ to sampled reward sums
3. evaluate $$\hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}^{\prime}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)$$
4. $$\nabla_{\theta} J(\theta) \approx \sum_{i} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i} \vert \mathbf{s}_{i}\right) \hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)$$ .
5. $$\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$$ .

**online** actor-critic algorithm:

1. take action a $$\sim \pi_{\theta}(\mathbf{a} \vert \mathbf{s}),$$ get $$\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}, r\right)$$
2. update $$\hat{V}_{\phi}^{\pi}$$ using target $$r+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}^{\prime}\right)$$ 
3. evaluate $$\hat{A}^{\pi}(\mathbf{s}, \mathbf{a})=r(\mathbf{s}, \mathbf{a})+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}^{\prime}\right)-\hat{V}_{\phi}^{\pi}(\mathbf{s})$$
4. $$\nabla_{\theta} J(\theta) \approx \nabla_{\theta} \log \pi_{\theta}(\mathbf{a} \vert \mathbf{s}) \hat{A}^{\pi}(\mathbf{s}, \mathbf{a}) $$.
5. $$\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta) $$.



online的也可以改进为 batch 版本,  两个网络的拟合可以batch化 ,works best with a batch (e.g., parallel workers). 这样的话, 工作线程有可能用一个稍微旧一点的policy,或者旧一点的value函数, 但却会更稳定, 对online来说, 一般都是并行运行, 稳定性很重要, batch 更大.

<img src="/img/CS285.assets/image-20200319112756604.png" alt="image-20200319112756604" style="zoom:50%;" />



##### Architecture design

AC 分开用两个神经网络.   也可以用一个网络, 可以共享特征, 使用起来也更高效, 但实践中设计超参比较困难, 因为有两个梯度而且大小完全不一样.



#### Use Critics as better baselines

##### state-dependent baselines

- Critics不像MC那样直接平均,  方差更小.  问题是, critic估值不准的时候,  r+V' 是 biased. 
  $$
  \nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_\theta\log\pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})\left(r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+\gamma\hat{V}^\pi_\phi(\mathbf{s}_{i,t+1})-\hat{V}^\pi_\phi(\mathbf{s}_{i,t})\right)
  $$
  
- PG 是 无偏 高方差(because single-sample estimate)
  $$
  \nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_\theta\log\pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})\left(\left(\sum_{t'=t}^T\gamma^{t'-t}r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)-b\right)
  $$
  
- 结合两者的优点.    后面的差其实是一种 Advantage
  $$
  \nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\nabla_\theta\log\pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})\left(\left(\sum_{t'=t}^T\gamma^{t'-t}r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)-\hat{V}^\pi_\phi(\mathbf{s}_{i,t})\right)
  $$
  

##### action-dependent baselines

baseline 与action 相关, 则肯定是 biased

-    $$Q^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{T} E_{\pi_{\theta}}\left[r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right) \vert \mathbf{s}_{t}, \mathbf{a}_{t}\right]$$  .
-    $$V^{\pi}\left(\mathbf{s}_{t}\right)=E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} \vert  \mathbf{s}_{t}\right)}\left[Q^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$$  .
-    $$A^{\pi}\left(\mathrm{s}_{t}, \mathrm{a}_{t}\right)=Q^{\pi}\left(\mathrm{s}_{t}, \mathrm{a}_{t}\right)-V^{\pi}\left(\mathrm{s}_{t}\right)$$  .
-  $$\hat{A}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{\infty} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-V_{\phi}^{\pi}\left(\mathbf{s}_{t}\right) \quad$$ 无偏高方差
-  $$\hat{A}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\sum_{t^{\prime}=t}^{\infty} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)-Q_{\phi}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$  如果critic准, 趋向0 , 不准确
-  $$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\hat{Q}_{i, t}-Q_{\phi}^{\pi}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)+\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} E_{\mathbf{a} \sim \pi_{\theta\left(\mathbf{a}_{i} | \mathbf{s}, t\right)}}\left[Q_{\phi}^{\pi}\left(\mathbf{s}_{i, t}, \mathbf{a}_{t}\right)\right]$$

这里把$Q^\pi$不再称为baseline, 叫control variant.  上式前面部分把$Q^\pi$ 减去了, 期望为0, 有了误差, 后面又加回来的是 expected value , 因为不是0, 所以需要加回来补偿. 这么做主要是因为$Q^\pi$ 是个近似函数, 所以其期望可能是有解析解的(根据构造的形式而异), 就不需要基于sample一个个算, 这个解析解就是个稳定值也容易求解, 所以上式第二项就比第一项容易求解.

use a critic *without* the bias (still unbiased), provided second term can be evaluated Gu et al. 2016 (**Q-Prop**)

基于state的baseline是无偏的，基于action的baseline是有偏的，但是偏差有可能可以通过一个校正项补救回来。



### Eligibility traces & n-step returns

- $$\hat{A}_\text{C}^\pi(\mathbf{s}_t,\mathbf{a}_t)=r(\mathbf{s}_t,\mathbf{a}_t)+\gamma\hat{V}^\pi_\phi(\mathbf{s}_{t+1})-\hat{V}^\pi_\phi(\mathbf{s}_t)$$.
  - lower variance
  - higher bias if value is wrong (it always is)
- $$\hat{A}_\text{MC}^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^\infty \gamma^{t'-t}r(\mathbf{s}_{t'},\mathbf{a}_{t'})-\hat{V}^\pi_\phi(\mathbf{s}_t)$$.
  - no bias
  - higher variance (because single-sample estimate)

所以还是 bias 与 variance 的tradeoff.  合并两者  combine these two

<img src="/img/CS285.assets/image-20200319154245060.png" alt="image-20200319154245060" style="zoom:33%;" />

图示, 未来的reward 方差很大, 20年后干啥, 与15分钟后干啥.  可以在方差很大前就切断. 把未来之后的很多推断直接删除直觉上也不会影响太大. 

<img src="/img/CS285.assets/image-20200319160334960.png" alt="image-20200319160334960" style="zoom: 33%;" />

如图, 红线表示, 已经有discount  factor \gamma$ 将未来的reward降权了.  绿色则是在某个时间点直接截断了.   这算是对 discount这个模式的一个改进. 不过还是比较粗糙的.  

n-step returns:

$$
\hat{A}_n^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^{t+n}\gamma^{t'-t}r(\mathbf{s}_{t'},\mathbf{a}_{t'})+\gamma^n\hat{V}^\pi_\phi(\mathbf{s}_{t+n})-\hat{V}^\pi_\phi(\mathbf{s}_t)
$$

一般选 n >1 .  n=1, 就是 $A_c$, n无穷大就是 $A_{MC}$ .  调整这个n来获得最佳信噪比. 



#### Generalized advantage estimation  GAE

- Schulman et al. (2016) 提出了**广义优势估计** (Generalized Advantage Estimation, GAE)。本质上是 n步收益的推广。

- $$\hat{A}_\text{GAE}^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{n=1}^\infty w_n\hat{A}_n^\pi(\mathbf{s}_t,\mathbf{a}_t)$$ .
- 如何选择权重,  Mostly prefer cutting earlier (less variance), 希望前期的权重大, 所以 $w_n\propto\lambda^{n-1}$ 
- 展开代入得到  $$\hat{A}_\text{GAE}^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^\infty (\gamma\lambda)^{t'-t}\delta_{t'}$$ , 其中 $$\delta_{t'}=r(\mathbf{s}_{t'},\mathbf{a}_{t'})+\gamma\hat{V}_\phi^\pi(\mathbf{s}_{t'+1})-\hat{V}_\phi^\pi(\mathbf{s}_{t'})$$
- 这里看出, $\gamma$ 与$\lambda$ 有些类似, 都起到了早期截断,减小方差的作用; 所以说明了discount的确可以减少方差.



#### Review

- Actor-critic algorithms:
  - **Actor:the policy**
  - **Critic:value function**
  - **Reduce variance of policy gradient**

- Policy evaluation
  - Fitting value function to policy
- **Discount factors**
  - Carpe diem Mr. Robot  , Dead State
  - ...but also a variance reduction trick
- Actor-critic algorithm design
  - **One network** (with two heads) or **two networks**
  - **Batch-mode**, or **online (+ parallel)**
- State-dependent baselines
  - Another way to use the critic
  - Can combine: **n-step returns** or **GAE**



#### Actor-critic examples

<img src="/img/CS285.assets/image-20200319165107156.png" alt="image-20200319165107156" style="zoom:33%;" />

- High dimensional continuous control with generalized advantage estimation (Schulman, Moritz, L., Jordan, Abbeel ‘16)
- Batch-mode actor-critic
- **Blends Monte Carlo and function approximator estimators (GAE)**

##### FPS 射击游戏

- Asynchronous methods for deep reinforcement learning (Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu ‘16)
- **Online actor-critic, parallelized batch**
- **N-step returns** with N = 4
- **Single network for actor and critic**



### Actor-critic suggested readings

- Classic papers
  - Sutton, McAllester, Singh, Mansour (1999). Policy gradient methods for reinforcement learning with function approximation: actor-critic algorithms with value function approximation

- Deep reinforcement learning actor-critic papers
  - Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, Kavukcuoglu (2016). Asynchronous methods for deep reinforcement learning: A3C -- parallel online actor-critic
  - Schulman, Moritz, L., Jordan, Abbeel (2016). High-dimensional continuous control using generalized advantage estimation: batch-mode actor-critic with blended Monte Carlo and function approximator returns
  - Gu, Lillicrap, Ghahramani, Turner, L. (2017). Q-Prop: sample-efficient policy- gradient with an off-policy critic: policy gradient with Q-function control variate



 









































































