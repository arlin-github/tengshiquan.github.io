---
layout:     post
title:      AlphaGo
subtitle:   AlphaGo from Deepmind
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-AlphaGo.jpg"
catalog: true
tags:
    - AI
    - DeepMind
    - Reinforcement Learning
    - AlphaGo

---

# AlphaGo



## Mastering the game of Go with deep neural networks and tree search

长期以来，围棋一直被认为是人工智能的经典游戏中最具挑战性的，因为它的搜索空间巨大，而且评估**棋盘局势(board position)**和**落子(select moves)**的难度很大。在这里，我们介绍一种新的方法，使用 "**价值网络(value networks)**"来评估棋盘局势和 "**策略网络(policy networks)**"来落子。这些深度神经网络是由人类专家棋谱的监督学习和自我对弈(self-play)的强化学习相结合的新方法训练出来的。在不进行任何前向搜索(lookahead search)的情况下，神经网络能以最先进的**蒙特卡洛树搜索(Monte Carlo tree search)**程序的水平下围棋。我们还介绍了一种新的搜索算法，将**蒙特卡洛模拟(Monte Carlo simulation)**与价值网络和策略网络相结合。利用这种搜索算法，我们的程序AlphaGo以99.8%的胜率战胜了其他围棋程序，并以5局0胜的战绩击败了人类欧洲围棋冠军，这是计算机程序首次在全尺寸围棋比赛中击败人类职业棋手，这在以前看来至少要等十年后才能完成的壮举。



下面只记要点



所有的**完美信息(perfect information)**博弈都有一个**最优值函数(optimal value function)** : $v^*(s)$.    
可通过在**搜索树(search tree)**上递归调用$$v^*(s)$$ 求解, 包含约 $b^d$ 个走棋序列,  b: breadth(合法落子数目) , d: depth(game length)  
国际象棋（b≈35,d≈80），围棋（b≈250,d≈150），穷举搜索(exhaustive search)不可行  

搜索空间search space可以通过两个原则减小:   estimate  sample

1. depth 通过棋局评估降低, 通过 $v(s)≈v^∗(s)$  剪枝 s 下面的子树，v(s)预测s之后的结果.  
   这种方法已经在国际象棋，国际跳棋，黑白棋中得到了超越人类，但认为对围棋不可行
2. breadth 通过 $p(a\vert s)$ action采样来降低, 如**蒙特卡洛走子(Monte Carlo rollouts)**,  
   平均 rollouts 得到有效的局势评估.  该法在backgammon, Scrabble超出人类，围棋达到弱业余水平。

**Monte Carlo tree search (MCTS)** 用 **Monte Carlo rollouts** estimate  V(s).   
模拟执行的次数越多，搜索树也就越大，相关的值也就越准。policy也会随着时间的推移，通过选择具有更高估值的子树来改进。渐渐地，policy**收敛**到optimal play，evaluation收敛到optimal value function。   
目前最厉害的围棋程序是基于MCTS, 再配合专家棋谱训练出来的policy加强。这些policy被用来缩小高概率action的搜索范围，并在rollout中对动作进行采样。这个方法已经达到了业余高手的级别。  
但之前的policy 以及 value func 基于**linear combination** of input features, 能力弱

19 × 19 image => CNN => representation of the position  
evaluating positions, value network ;  sampling actions,  policy network  

我们使用流水线pipeline分阶段训练network.   文章里, 用p,RL里一般用$\pi$

- **SL policy network** $p_\sigma$ ,  专家棋谱训练 ,  梯度质量高, 学得快
- **fast policy** $p_\pi$  ,  快速落子,   rapidly sample actions during rollouts
-  **(RL) policy network** $p_\rho$ , improves SL policy network by optimizing final outcome of **self-play**.  
  这个policy network的目标是goal最大, 而不是像SL那样maximizing predictive accuracy.
- **value network** $v_\theta$ , **predicts the winner** of RL policy network against itself.  

**AlphaGo efficiently combines the policy and value networks with MCTS.**  
AlphaGo = policy network + value network + MCTS



![image-20200413034628288](/img/2020-04-07-AlphaGo.assets/image-20200413034628288.png)



下面是卷积，上面柱子是各个落子几率的高低

<img src="/img/2020-04-07-AlphaGo.assets/image-20200413034706028.png" alt="image-20200413034706028" style="zoom: 33%;" />



### Supervised learning of policy networks

SL policy network 的输入:
$$
\begin{array}{lrl}
\hline \text { Feature } & \text { # of planes } & \text { Description } \\
\hline \text { Stone colour } & 3 & \text { Player stone / opponent stone / empty } \\
\text { Ones } & 1 & \text { A constant plane filled with 1 } \\
\text { Turns since } & 8 & \text { How many turns since a move was played } \\
\text { Liberties  气} & 8 & \text { Number of liberties (empty adjacent points) } \\
\text { Capture size } & 8 & \text { How many opponent stones would be captured } \\
\text { Self-atari size } & 8 & \text { How many of own stones would be captured } \\
\text { Liberties after move} & 8 & \text { Number of liberties after this move is played } \\
\text { Ladder capture } & 1 & \text { Whether a move at this point is a successful ladder capture } \\
\text { Ladder escape } & 1 & \text { Whether a move at this point is a successful ladder escape } \\
\text { Sensibleness } & 1 & \text { Whether a move is legal and does not fill its own eyes } \\
\text { Zeros } & 1 & \text { A constant plane filled with 0 } \\
\hline \text { Player color } & 1 & \text { Whether current player is black } \\
\hline
\end{array}
$$
policy network (all but last feature) and value network (all features).



SL policy networks trained on randomly sampled (*s*, *a*), using SGD to maximize the **likelihood** of human 

$$
\Delta \sigma \propto \frac{\partial \log p_{\sigma}(a | s)}{\partial \sigma}
$$

SL policy network 13-layer,  from 30 million positions.   3000万日志  
棋谱拟合准确率: 57.0% 使用所有特征,  55.7% 只使用盘面以及历史记录; 其他团队最好 44.4% 

下图, 拟合准确率略微的提高能使棋力提高不少；更大的network可以获得更高的准确率但是evaluate更慢.  
384的卷积核反而不如256的

<img src="/img/2020-04-07-AlphaGo.assets/image-20200413184811327.png" alt="image-20200413184811327" style="zoom:33%;" />

因为准确率高的policy network在search比较慢,  $3 \mathrm{ms}$ 选一个action, 所以训练了一个快速网络. rollout policy $p_{\pi}(a | s),$ linear softmax of small pattern features (下表) with weights $\pi ;$  accuracy of $24.2 \%,$  $2 \mu \mathrm{s}$ 
$$
\begin{array}{lrl}
\hline \text { Feature } & \text { # of patterns } & \text { Description } \\
\hline \text { Response } & 1 & \text { Whether move matches one or more response pattern features } \\
\text { Save atari } & 1 & \text { Move saves stone(s) from capture } \\
\text { Neighbour } & 8 & \text { Move is 8-connected to previous move } \\
\text { Nakade } & 8192 & \text { Move matches a nakade pattern at captured stone } \\
\text { Response pattern } & 32207 & \text { Move matches 12-point diamond pattern near previous move } \\
\text { Non-response pattern } & 69338 & \text { Move matches 3 } \times 3 \text { pattern around move } \\
\hline \text { Self-atari } & 1 & \text { Move allows stones to be captured } \\
\text { Last move distance } & 34 & \text { Marhattan distance to previous two moves } \\
\text { Non-response pattern } & 32207 & \text { Move matches 12-point diamond pattern centred around move } \\
\hline
\end{array}
$$
rollout policy (first set) , tree policy (first and second set). Patterns are based on stone colour (black/white/empty) and liberties (1, 2, ≥3) at each intersection of the pattern.



### Reinforcement learning of policy networks

RL policy network $p_\rho$ 网络结构与SL policy network 一样. 参数初始化为$\rho = \sigma$.    
用policy network $p_\rho$ 和随机选的前几个版本的policy network 下棋.  随机从一个池里面选择对手, 可以增强学习稳定性, 防止当前策略过拟合.   
reward function r(s), 中间全0, t < T.    
结果outcome $z_t = ± r(s_T)$ , terminal reward ,  +1 for winning and −1 for losing.   
SGD, maximizes expected outcome:

$$
\Delta \rho \propto \frac{\partial \log p_{\rho}\left(a_{t} | s_{t}\right)}{\partial \rho} z_{t}
$$

evaluate RL policy performance in game play, sampling move $a_{t} \sim p_{\rho}\left(\cdot | s_{t}\right)$   
RL policy won $80 \%$  SL.    
Using no search, RL policy won $85 \%$  Pachi(base on MC search)  



### Reinforcement learning of value networks

局势评估 position evaluation  
value函数估计, estimating value function $v^{p}(s)$ , predicts position $s$  outcome, 定义式

$$
v^{p}(s)=\mathbb{E}\left[z_{t} | s_{t}=s, a_{t \ldots T} \sim p\right]
$$

显然无法知道 $v^{*}(s)$ ; 实践中, 使用最强的 RL policy network $p_{\rho}$去估计 value func $v^{p_\rho}$ .  
approximate value function :  $$v_{\theta}(s) \approx v^{p_\rho}(s) \approx v^{*}(s)$$   
Value network 结构跟policy的差不多, 输出层变为一个标量.   
train: regression on state-outcome pairs $(s, z),$ SGD minimize MSE

$$
\Delta \theta \propto \frac{\partial v_{\theta}(s)}{\partial \theta}\left(z-v_{\theta}(s)\right)
$$

该方法会**过拟合**, 因为数据之间强相关correlated, 一局游戏, 前后连续的s变化很小, regression target就一个.  
用这种方式在KGS data上训练, value network记住了游戏的结果, 而不是泛化到新的positions.    
MSE  0.37 on test set,  0.19 on training set.   

为了减轻这个问题，使用self-play 数据，从不同棋局中采样不同位置生成3000万个新的训练数据。其中每一局棋都是RL policy network自我对弈直到游戏结束为止。  
在该数据上训练, MSEs :  training 0.226 , test 0.234 ; 表明只有极小的过拟合。

<img src="/img/2020-04-07-AlphaGo.assets/image-20200413184941057.png" alt="image-20200413184941057" style="zoom: 33%;" />

上图显示了value network对于positions的evaluation 准确度，相比MC rollout 使用的fast rollout policy，valuefunction 更加准确。  
$v_{\theta}(s)$ 使用RL policy network $p_\rho$ 到达 MC rollout的准确度, 但计算量只有15000分之一



### Searching with policy and value networks

![image-20200413195845345](/img/2020-04-07-AlphaGo.assets/image-20200413195845345.png)

- a, 每次simulation选择最大Q的边遍历
- b, 叶子节点可能展开, 如展开, 计算P值
- c,  模拟结束后，评估叶子节点，1用V网络 2用快速走子走到底来评估输赢 
- d,  将子节点的所有结果平均 迭代递推updateQ



AlphaGo combines  policy and value networks in MCTS algorithm, selects actions by lookahead search.   

Search tree的每条边$(s, a)$, 都存有**action value** $Q(s, a)$, **visit count** $N(s, a)$, **prior probability** $P(s, a)$.   
该tree 从根节点开始通过对局模拟来遍历 (自顶向下完成一局游戏, no backup).   

每次模拟对局, 在 $t$ , 选 $a_t$ 

$$
a_{t}=\operatorname{argmax}\left(Q\left(s_{t}, a\right)+u\left(s_{t}, a\right)\right) \\
u(s, a) \propto \frac{P(s, a)}{1+N(s, a)}
$$

$u$与先验概率成正比，随着重复访问增加而衰减以鼓励exploration.   
当遍历在$L$步到达一个叶子节点$s_L$，有可能被展开. 叶子节点的$s_{L}$ 只被SL policy network $p_{\sigma}$执行一次, 输出存到$P$ , 对每个合法 action $a$ $P(s, a)=p_{\sigma}(a | s) .$   
叶子节点被两种方式评估：1. value network $v_{\theta}\left(s_{L}\right)$  	2. fast rollout policy $p_{\pi}$ 随机玩到底的结果$z_{L}$   
两个合并为叶子节点评估 $V\left(s_{L}\right)=(1-\lambda) v_{\theta}\left(s_{L}\right)+\lambda z_{L}$ . 

在模拟的最后，所有遍历到的edge的Q值和visit count被更新:

$$
N(s, a)=\sum_{i=1}^{n} 1(s, a, i) \\
Q(s, a)=\frac{1}{N(s, a)} \sum_{i=1}^{n} 1(s, a, i) V\left(s_{L}^{i}\right)
$$

$s_{L}^{i}$ 是第 $i$-th次模拟的叶子节点 , $1(s, a, i)$ 表示(s,a)在第$i$次模拟中被是否遍历.   
一旦搜索完成, 算法从根节点开始选择访问次数最多的action。

注意,对AlphaGo, SL policy network $p_{\sigma}$ 比 stronger RL policy network $p_{\rho}$ 表现的好, 可能因为人类的对落子的选择更宽泛,而RL 优化一个最佳move.   
然而, 对于value function,  从RL中得到的 $v_{\theta}(s) \approx v^{p_{\rho}}(s)$ 强于来自 SL policy network $v_{\theta}(s) \approx v^{p_{\sigma}}(s)$.

相对于传统的启发式搜索而言,评估policy and value networks需要多几个数量级的计算量。为了有效的结合MCTS和DNN，AlphaGo在CPU上执行simulations使用了异步的多线程搜索，在GPU上并行计算policy and value networks。  
最终版本的AlphaGo使用40个搜索线程，48个CPU和8个GPU。我们也实现了一个分布式版本的AlphaGo，40个搜索线程，1202个CPU和176个GPU。



### Evaluating the playing strength of AlphaGo

现在最强的商业程序CrazyStone、Zen，最强的开源程序Pachi、Fuego, 都基于高性能MCTS算法。我们也对比了开源程序GnuGo(用搜索算法的最高水平，超过了MCTS)。所有的程序都要求每5秒内完成一次下子。

<img src="/img/2020-04-07-AlphaGo.assets/image-20200414040942709.png" alt="image-20200414040942709" style="zoom:33%;" />

结果显示出单机版的AlphaGo比前面的所有的围棋程序更强，在495次游戏中赢得了494次(99.8%). 为了加大挑战难度我们让四个子，AlphaGo在和Crazy Stone,Zen 和Pachi的比赛中, 分别赢得77%，86%，99%的游戏. 分布式版本的AlphaGo异常的强大，对比单机版的AlphaGo赢得的77%的棋局。

<img src="/img/2020-04-07-AlphaGo.assets/image-20200414041622670.png" alt="image-20200414041622670" style="zoom: 33%;" />

评估了不同的AlphaGo版本. 只使用value network(λ = 0)或者快速走子策略(λ = 1)评估位置。即使没有快速走子策略 AlphaGo仍然超过了其他所有的围棋程序，这表明value network 可能代替 MC evaluation。然而混合(λ = 0.5)的AlphaGo表现最佳，赢了95%其他不同的AlphaGo。  

说明两个state估计的机制是互补的：value network用 strong, slow $p_{\rho}$ ; rollouts , weak , faster, $p_{\pi}$

<img src="/img/2020-04-07-AlphaGo.assets/image-20200414043004921.png" alt="image-20200414043004921" style="zoom:33%;" />



最后，我们评估了分布式版本的AlphaGo对抗樊辉（职业2段）的比赛。AlphaGo赢得了五盘所有的比赛。这是第一次计算机打败人类职业棋手，没有任何的让子, 以前被认为要至少10年以上的时间。



### Discussion

在这个工作中，基于一个DNN和tree search的结合开发了一个围棋程序。  
开发了 move selection and position evaluation functions,  base on DNN,  trained by SL, RL  
引入了新的搜索算法，它成功的把NN evaluation和Monte Carlo rollouts结合在一起。  
AlphaGo把这些组成部分按照比例集成在一起，成为了一个高性能的tree search引擎。

比赛时计算量比国际象棋要少，说明机器学习是提前学到了知识，然后再后面直接使用，而不是及时演算.   
AlphaGo评估次数数千倍少于深蓝，通过更智能地选择局面，利用策略网络，用价值网络更精确地评估这些局面来弥补，这种方法或许更接近于人类的下棋方式。此外，深蓝使用的是人类手工调参数的估值函数.

围棋在人工智能所面临的许多困难方面具有典范意义：具有挑战性的决策任务，难以解决的搜索空间，以及复杂到无法用策略或值函数直接近似的最优解。之前在计算机围棋领域的重大突破，即MCTS的引入，导致了许多其他领域的相应进步；例如，一般博弈、经典规划、部分观察规划、调度和约束满足等。通过将树形搜索与策略和价值网络相结合，AlphaGo终于在围棋中达到了专业的水平，为人类级的性能提供了希望，现在可以在其他看似难缠的人工智能领域实现人类级的性能。






## Reference

Mastering the game of Go without human knowledge  http://augmentingcognition.com/assets/Silver2017a.pdf







