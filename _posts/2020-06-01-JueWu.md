---
layout:     post
title:      腾讯绝悟
subtitle:   Mastering Complex Control in MOBA Games with Deep Reinforcement Learning
date:       /img/2020-06-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-tx-ai.jpg"
catalog: true
tags:
    - AI
    - Imperfect Information
    - PPO
    - MOBA

---



# 腾讯绝悟

## Mastering Complex Control in MOBA Games with Deep Reinforcement Learning

论文主要聚焦在 1vs1时, MOBA游戏中复杂的技能控制.  即 微操.

提出了 off-policy dual-clip PPO , 相比之前的 clip-PPO , 两头都有截断. 



### Introduction

复杂度:

![image-20200615160617180](/img/2020-06-01-JueWu.assets/image-20200615160617180.png)



MOBA 1v1的复杂性还来自于游戏机制。为了赢得比赛，在部分可观察的环境中，代理必须学会计划、攻击、去防御、控制技能组合、诱导和欺骗对手。除了玩家和对手之外，还存在更多的游戏单位，例如：小兵和炮塔。这就给目标的选择带来了挑战，它需要精妙的决策序列和相应的动作控制。此外，MOBA游戏中不同的英雄有着截然不同的打法。不同英雄的动作控制可以完全改变，这就需要强大而统一的建模。最后，MOBA 1v1缺乏高质量的人类游戏数据，这使得监督学习变得不可行，因为玩家一般使用1v1模式来练习英雄.

解决方案:

1. Large-scale system for exploration
2. Unified modeling
3. Self-play



### Method

##### Deep reinforcement learning system

- Large-scale
- Off-policy

##### Algorithm

- Multi-modal feature design
- Actor-critic neural network
- Multiple action control strategies
- Dual-clip PPO





![image-20200615164505449](/img/2020-06-01-JueWu.assets/image-20200615164505449.png)



#### System Design

整体设计架构

1. Al Server
   - Actor, where self-play happens
   - Interact with GameCore , 就是Env
2. Dispatch Server
   - Data collect, compress & transmit    采集
3. Memory Pool
   - For data storage	, FIFO	 , 数量级在百万
   - Feed data to RL Learner
4. RL Learner
   - For training reinforcement learning model
   - Model sysn to Al Server via P2P



特点:

- Large-scale
  - Support up to 1000+ GPU cards, 500,000+ CPUs tested with our Beta Environment
- Off-policy
  - Actor highly decoupled from Learner  ,  Actor 与 Learner 解耦, 定期同步.





#### Algorithm Design

![image-20200615165427109](/img/2020-06-01-JueWu.assets/image-20200615165427109.png)



##### Input

- Observable game unit attributes
  - Heroes, creeps, turrets, etc.
- Observable game states   主要是经济, 人头, 当前塔的状态之类
- Local-view image-like channels       玩家周围的图像转为 卷积层的channel的输入

除了图像,后面都是直接FC-Relu,   然后把这三种输入 连接起来



##### 网络的中间部分

- LSTM
- Action mask
  - For pruning RL exploration   用于剪枝
- Target Attention      用于在攻击中选择攻击目标
- Actor-critic network
  - Policy & value share parameter





##### Output

- Hierarchical, multi-label   有层级, 多个输出 ,    其他的输出受button的控制
  - First, predict **which action** to take, i.e., Button E.g, move   操作的大方向
  - Second, predict **how to execute** that action E.g, the direction to move        细节
  - What about label correlations?       Control dependency decoupling
    - Action labels have **correlations**, but are treated **independently** 简单的认为是不相关的. 
    To simplify episode -sampling & objective optimization 


$$
\operatorname{maximize}_{\theta} \sum_{l a b e l_{-} i} E_{s_{t}, a_{t} \sim \pi_{\theta_{o l d}}}\left[\frac{\pi_{\theta}\left(a_{t}^{l a b e l_{-} i} | s_{t}\right)}{\pi_{\theta_{o l d}}\left(a_{t}^{l a b e l_{-} i} | s_{t}\right)}\left(R-V_{\theta_{o l d}}\left(s_{t}\right)\right)\right]-\frac{1}{2} E_{s_{t} \sim \pi_{\theta_{o l d}}}\left[\left(R-V_{\theta}\left(s_{t}\right)\right)^{2}\right]
$$

![image-20200615181741850](/img/2020-06-01-JueWu.assets/image-20200615181741850.png)



因为将层次的label视为不相关的, 所以前面部分直接相加.    中间部分Policy Loss 是PPO的一个变体.



标准的PPO: 带截断.

$$
 L^{c l i p}(\theta)=E_{t}\left[\min \left(\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} | s_{t}\right)} (R-V) , \operatorname{clip}\left(\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} | s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right)(R-V)\right)\right]
$$


作者认为原始的PPO在 large-scale & off-policy setting > **policy deviations** , 策略会偏的很大.  

$$
\begin{array}{ccc}
\begin{array}{c}
\text { when } \pi_{\theta}\left(a_{t}^{(i)} | s_{t}\right) \gg \pi_{\theta_{\text {old }}}\left(a_{t}^{(i)} | s_{t}\right) \\
\text { and } \  \hat{A}_{t}<0
\end{array} &  \to \frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{\text {old }}}\left(a_{t} | s_{t}\right)} \hat{A}_{t} \ll 0
\end{array}
$$

<img src="/img/2020-06-01-JueWu.assets/image-20200615185235883.png" alt="image-20200615185235883" style="zoom:50%;" />

上图右边的c, 就是$\eta$ . 

 **dual-clip PPO**:

$$
L^{\mathrm{clip}}(\theta)=E_{t}\left[\max \left(\min \left(\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{o l d}}\left(a_{t} | s_{t}\right)}(R-V), \operatorname{clip}\left(\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{o l d}}\left(a_{t} | s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right)\right), \eta(R-V)\right)\right]
$$




### Experiments

##### System

- 40+ GPU cards & 15000+ CPU cores used to train one hero
- 80,000 samples per second per GPU   
- FP16 for data transmission , 节省带宽

##### Algorithm

- LSTM
  - time step 16, unit size 1024
  - 也用了GRU, 效果差不多, GRU更轻量点.
- Discount factor 0.998

- Generalized advantage estimation (GAE)  算Advantage
  - Lambda  0.95
- Dual-clip PPO
  - Two clip parameters are 0.2 and 3, respectively





![image-20200615192659968](/img/2020-06-01-JueWu.assets/image-20200615192659968.png)

上表显示, 左边AI的数据都是比右边的强. 



<img src="/img/2020-06-01-JueWu.assets/image-20200615192726911.png" alt="image-20200615192726911" style="zoom:50%;" />

2100场只输了4场 ,  孙悟空输了两场, 可能是因为运气不好, 没出暴击.





与之前的MCTS的变种方法比较, 比较的谁打同一个对手, 谁赢的快.

<img src="/img/2020-06-01-JueWu.assets/image-20200615192831379.png" alt="image-20200615192831379" style="zoom:50%;" />

MCTS, planning比较耗时,不太适合realtime的情况.





大概训练12小时达到星耀水平, 30小时到王者水平.

<img src="/img/2020-06-01-JueWu.assets/image-20200615192852556.png" alt="image-20200615192852556" style="zoom:50%;" />



##### Ablation

- AM: action mask    加速收敛
- **TA: target attention**  对胜率影响最大
- LSTM
- Base: Full w/o AM TA LSTM

<img src="/img/2020-06-01-JueWu.assets/image-20200615192738098.png" alt="image-20200615192738098" style="zoom:50%;" />



#### reward shaping

启发式的, 没有用到reverse-learning

<img src="/img/2020-06-01-JueWu.assets/image-20200615192918327.png" alt="image-20200615192918327" style="zoom:50%;" />

这里的kill杀一个人reward是负数, 因为拿个人头可以得到其他很多经验金钱, 所以这个kill的负数是个修正.



可以在线分析reward的变化曲线, 用于调整英雄的策略.

![image-20200615192942561](/img/2020-06-01-JueWu.assets/image-20200615192942561.png)





#### Conclusion and Future Work

- **Action control** of different MOBA heroes
  - Complex, a big challenging to Al research
- We develop a **super-human** Al agent which has mastered the complex action control in MOBA **1v1** games  解决了1v1
- Our deep reinforcement learning framework
  - System design
  - Algorithm design
    - Multi-modal feature design
    - Actor-critic neural network
    - Multiple action control strategies
    - Dual-clip PPO





视频里, 作者补充了, 之所以选PPO, 是因为选了DQN以及A3C不work.






## Reference

Mastering Complex Control in MOBA Games with Deep Reinforcement Learning

https://arxiv.org/abs/1912.09729

腾讯AI Lab：用深度强化学习在王者荣耀虚拟环境中构建「绝悟」AI

https://v.qq.com/x/page/z30479qhpdi.html









