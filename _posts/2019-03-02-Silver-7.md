---
layout:     post
title:      UCL Course on RL, Integrating Learning and Planning
subtitle:   David Silver 的课程笔记7
date:       2019-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---



# Integrating Learning and Planning

之前都是model-free的, 本课 learn model 

主要内容 MTCS



1. Introduction
2. Model-Based Reinforcement Learning
3. Integrated Architectures
4. Simulation-Based Search





- Last lecture: learn **policy** directly from **experience**
- Previous lectures: learn **value** function directly from **experience**
- This lecture: learn **model** directly from **experience**
- and use **planning** to construct a value function or policy
- Integrate **learning** and **planning** into a single architecture



##### Model-Based and Model-Free RL

- Model-Free RL 
  - No model 
  - **Learn** value function (and/or policy) from experience 
- Model-Based RL 
  - Learn a model from experience
  - **Plan** value function (and/or policy) from model 



### Model-Based Reinforcement Learning

<img src="/img/2019-03-02-Silver.assets/image-20190209012522192.png"  style="zoom: 50%;" />



#### Advantages of Model-Based RL

- Advantages:
  - Can efficiently **learn model by supervised learning** methods
  - Can **reason** about model **uncertainty** 

- Disadvantages: 
  - First learn a model, then construct a value function ⇒ two sources of approximation error 



### Learning a Model

#### What is a Model?

- model $\mathcal M$ is a **representation** of an MDP $\langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}\rangle$, parametrized by $η$

- **model** $\mathcal M = ⟨\mathcal P_η,\mathcal R_η⟩$ represents state **transitions** $Pη ≈ P$ and **rewards** $\mathcal R_η ≈ \mathcal R$ 
  
  $$
  S_{t+1} \sim \mathcal{P}_{\eta}\left(S_{t+1} \mid S_{t}, A_{t}\right) \\
  R_{t+1}=\mathcal{R}_{\eta}\left(R_{t+1} \mid S_{t}, A_{t}\right)
  $$
  
- Typically assume conditional independence between state transitions and rewards , $\mathbb{P}\left[S_{t+1}, R_{t+1} \mid S_{t}, A_{t}\right]=\mathbb{P}\left[S_{t+1} \mid S_{t}, A_{t}\right] \mathbb{P}\left[R_{t+1} \mid S_{t}, A_{t}\right]$

 


#### Model Learning

- Goal: estimate model $\mathcal M_η$ from experience ${S_1, A_1, R_2, ..., S_T }$

- This is a supervised learning problem

$$
S_1,A_1 →R_2,S_2
\\ S_2,A_2 →R_3,S_3
\\...
\\S_{T−1},A_{T−1} →R_T,S_T
$$

- Learning s,a → r is a **regression** problem
- Learning s,a → s′ is a **density estimation** problem
- Pick loss function, e.g. **mean-squared error**, **KL divergence**, ...
- Find parameters η that minimise empirical loss



#### Examples of Models

- **Table Lookup Model**
- Linear Expectation Model 
- Linear Gaussian Model 
- Gaussian Process Model 
- **Deep Belief Network Model**



#### Table Lookup Model

- 统计出现概率
- Model is an explicit MDP, $\hat {\mathcal P}$, $\hat {\mathcal R}$   显式表达

$$
\hat{ \mathcal P}_{s,s'}^a = \frac{1}{N(s,a)} \sum_{t=1}^T \mathbf 1(S_t,A_t,S_{t+1} = s,a,s')
\\ \hat{ \mathcal R}_{s}^a = \frac{1}{N(s,a)} \sum_{t=1}^T \mathbf 1(S_t,A_t = s,a)R_t
$$

- Alternatively
  - At each time-step t, record experience tuple  $⟨S_t , A_t , R_{t+1}, S_{t+1}⟩$

  - To sample model, randomly pick tuple matching $⟨s,a,·,·⟩$ 



### Planning with a Model

- 这个时候, 不再直接与环境交互, 还是与model交互, 然后得出决策输出给env
- Given a model $\mathcal M_η = ⟨\mathcal P_η, \mathcal R_η⟩$ 
- Solve the MDP $⟨\mathcal S, \mathcal A, \mathcal P_η, \mathcal R_η⟩$ 
- Using favourite **planning algorithm** 
  - Value iteration 
  - Policy iteration 
  - **Tree search**
     ... 



#### Sample-Based Planning

- 怎么与 model 交互, model 负责产出 sample
- A simple but powerful approach to planning 
- Use the model **only** to **generate samples** 
- **Sample** experience from model 

$$
S_{t+1} ∼ \mathcal P_η(S_{t+1} | S_t,A_t) 
\\ R_{t+1} = \mathcal R_η(R_{t+1} | S_t,A_t)
$$

- Apply model-free RL to samples, e.g.: 
  - Monte-Carlo control 
  - Sarsa 
  - Q-learning 

- Sample-based planning methods are often more efficient 
  - Real experience
  - Sampled experience



##### Planning with an Inaccurate Model

基于模型的RL最好也只能逼近到 该模型的最佳解；如果模型本身不准，则无法求到真实最佳解

Given an imperfect model $⟨\mathcal P_η, \mathcal R_η⟩ ≠ ⟨\mathcal P, \mathcal R⟩$
Performance of model-based RL is limited to optimal policy for approximate MDP $⟨\mathcal S,\mathcal A,\mathcal P_η,\mathcal R_η⟩$
 i.e. Model-based RL is only as good as the estimated model 

When the model is inaccurate, planning process will compute a suboptimal policy 

Solution 1: when model is wrong, use model-free RL 
Solution 2: reason explicitly about model uncertainty 





### Integrated Architectures

#### Dyna

##### Real and Simulated Experience

- Real experience     Sampled from environment (true MDP)

$$
S'\sim \mathcal P_{s,s'}^a 
\\ R = \mathcal R_s^a
$$

- **Simulated** experience  Sampled from model (approximate MDP)

$$
S' \sim \mathcal P_\eta(S'|S,A)
\\ R = \mathcal R_\eta(R|S,A)
$$



##### Integrating Learning and Planning

- Model-Free RL 
  - No model 
  - Learn value function (and/or policy) from **real experience** 
- Model-Based RL 
  - Learn a model from real experience
  - Plan value function (and/or policy) from **simulated experience** 

- **Dyna** 
  - Learn a model from real experience
  - **Learn and plan** value function (and/or policy) from real and
    simulated experience

##### Dyna Architecture

<img src="/img/2019-03-02-Silver.assets/image-20190209211731980.png" style="zoom: 60%;" />

##### Dyna-Q Algorithm

![image-20190209221759140](/img/2019-03-02-Silver.assets/image-20190209221759140.png)



##### Dyna-Q with an Inaccurate Model



### Forward Search

前向搜索, 不过是搜索整个子树

<img src="/img/2019-03-02-Silver.assets/image-20200701231841050.png" alt="image-20200701231841050" style="zoom:50%;" />

- Forward search algorithms select the best action by **lookahead**
- They build a **search tree** with the current state st at the root
- Using a model of the MDP to look ahead
- No need to solve whole MDP, just **sub-MDP** starting from now 



### Simulation-Based Search

<img src="/img/2019-03-02-Silver.assets/image-20200701231900775.png" alt="image-20200701231900775" style="zoom:50%;" />

- Forward search paradigm using **sample-based planning**
- Simulate episodes of experience from now with the model
- Apply **model-free RL** to simulated episodes   基于model采样episode再RL
- Simulate episodes of experience from now with the model

$$
\{\color{red}{s_t^k},A_t^k,R_{t+1}^k ,...,S^k_T \}^K_{k=1} ∼ \mathcal M_v
$$

- Apply model-free RL to simulated episodes 
  - Monte-Carlo control → Monte-Carlo search 
  - Sarsa → TD search 



#### Monte-Carlo Search

##### Simple Monte-Carlo Search

使用MC来评估当前node 在simulation policy下的每个action的Q, 每个action采样K个episode

- Given a model $\mathcal M_ν$ and a **simulation policy** $π$

- For each action $a ∈ \mathcal A$
  - Simulate K episodes from current (real) state $s_t$

  $$
  \{\color{red}{s_t,a},R_{t+1}^k,S_{t+1}^k,A_{t+1}^k ,...,S^k_T \}^K_{k=1} ∼ \mathcal M_{v},\pi
  $$

  - Evaluate actions by mean return (**Monte-Carlo evaluation**)

  $$
  Q(\color{red}{s_t,a}) = \frac{1}{K}\sum_{k=1}^K G_t  \overset{P}{\to} q_\pi(s_t,a)
  $$

  - Select current (real) action with **maximum value**

$$
a_t = \mathop{\textrm {argmax}}_{a \in \cal{A}}Q(s_t,a)
$$



##### Monte-Carlo Tree Search (Evaluation)

从某个node开始,评估各个action的Q, 按照policy的分布来采样action的episode, 显然这个对比上面, 对某些常见的action估的更准一些, 上面是均匀的采样.

- Given a model $\mathcal M_ν$

- Simulate K episodes from current state $s_t$ using current **simulation policy** $π$

$$
\{\color{red}{s_t},A_t^k,R_{t+1}^k,S_{t+1}^k,...,S^k_T \}^K_{k=1} ∼ \mathcal M_{v},\pi
$$

- Build a search tree containing visited states and actions
- **Evaluate** states Q(s, a) by mean return of episodes from s, a

$$
Q(\color{red}{s,a}) =\frac{1}{N(s,a)}\sum_{k=1}^k\sum_{u=t}^T \mathbf 1(S_u,A_u=s,a)G_u  \overset{P}{\to} q_\pi(s_t,a)
$$

- After search is finished, select current (real) action with maximum value in search tree 

$$
a_t = \mathop{\text {argmax}}_{a \in \cal{A}}Q(s_t,a)
$$



##### Monte-Carlo Tree Search (Simulation)

MCTS 收到到最优解

- In MCTS, the simulation policy $π$ **improves**
- Each simulation consists of two phases (in-tree, out-of-tree) 
  - **Tree policy** (**improves**): pick actions to **maximise** Q(S,A) 
  - **Default policy** (**fixed**): pick actions **randomly** 

- Repeat (each simulation)
  - **Evaluate** states Q(S,A) by Monte-Carlo evaluation 
  - **Improve** tree policy, e.g. by ε − greedy(Q) 

- **Monte-Carlo control** applied to **simulated experience** 
- **Converges** on the **optimal search tree**, $Q(S,A) → q_∗(S,A)$ 



#### Advantages of MC Tree Search

- Highly selective best-first search
- Evaluates states **dynamically** (unlike e.g. DP)
- Uses sampling to break **curse of dimensionality**
- Works for “black-box” models (**only requires samples**)
- **Computationally efficient**, anytime, parallelisable



### Temporal-Difference Search

基本没见到

- Simulation-based search
- Using TD instead of MC (bootstrapping)
- **MC tree search** applies **MC control** to **sub-MDP** from now
- **TD search** applies **Sarsa** to **sub-MDP** from now



##### MC vs. TD search

- For model-free reinforcement learning, bootstrapping is helpful 
  - TD learning reduces variance but increases bias 
  - TD learning is usually more efficient than MC 
  - TD(λ) can be much more efficient than MC 

- For simulation-based search, bootstrapping is also helpful 
  - TD search reduces variance but increases bias
  - TD search is usually more efficient than MC search 
  - TD(λ) search can be much more efficient than MC search 



##### TD Search

- Simulate episodes from the current (real) state $s_t$
- Estimate action-value function Q(s,a)
- For each step of simulation, update action-values by Sarsa

$$
\Delta Q(S,A) = \alpha(R+\gamma Q(S',A') - Q(S,A))
$$
- Select actions based on action-values Q(s,a)    e.g. ε-greedy

- May also use function approximation for Q



### Dyna-2

- In Dyna-2, the agent stores two sets of feature weights 
  - Long-term memory 
  - Short-term (working) memory 

- Long-term memory is updated from **real** experience using TD learning 
  - General domain knowledge that applies to any episode 

- Short-term memory is updated from **simulated** experience using TD search 
  - Specific local knowledge about the current situation

- Over value function is sum of long and short-term memories 



<img src="/img/2019-03-02-Silver.assets/image-20200702001110634.png" alt="image-20200702001110634" style="zoom:50%;" />

对围棋,  TD  leaning + TD search  比 UCT 效果要好.