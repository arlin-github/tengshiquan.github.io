---
layout:     post
title:      UCL Course on RL,  Function Approximation
subtitle:   David Silver 的课程笔记5
date:       2019-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---



# Value Function Approximation

本章核心是DQN 及两个技巧

1. Introduction
2. **Incremental** Methods
3. **Batch** Methods





### Large-Scale Reinforcement Learning

- Backgammon: $10^{20}$ states
- Computer Go: $10^{170}$ states
- Helicopter: continuous state space

scale up  to 大规模MDP问题



### Value Function Approximation   函数逼近

- So far we have represented value function by a **lookup table**

  - Every state s has an entry V(s) Or every state-action pair s,a has an entry Q(s,a)

- Problem with large MDPs:

  - There are **too many** states and/or actions to **store** in memory
  - It is **too slow** to **learn** the value of each state individually

- Solution for large MDPs:

  - Estimate value function with **function approximation**
    $$
    \hat v(s, \mathbf w) \approx v_\pi(s) \quad or \quad \hat q(s, a,\mathbf w) \approx q_\pi(s,a)
    $$
  
- **Generalise** from seen states to unseen states   泛化
  
- **Update parameter w using MC or TD learning**  不是去更新table，而且是更新拟合函数的参数





##### Types of Value Function Approximation

<img src="/img/2019-03-02-Silver.assets/image-20190203224215926.png" style="zoom: 60%;" />

1. 针对状态本身，输出这个状态的近似价值； 

2. 针对状态行为对，输出状态行为对的近似价值； **action in**

3. 针对状态本身，输出一个向量，向量中的每一个元素是该状态下采取一种可能行为的价值。 **action out**

显然第三种在使用的效率上是很高的，但应该比前面的难训练



#### function approximators

- **Linear** combinations of features
- **Neural network**
- Decision tree
- Nearest neighbour 
- Fourier / wavelet bases 
- ... 

1. **differentiable** function approximators  可导的, 可以使用梯度
2. Furthermore, we require a training method that is suitable for **non-stationary, non-iid** data  
   MDP过程的数据，非静态,  非独立同分布，前后状态之间关联性很高，监督学习不完全适合；监督学习的数据可以视为iid





## Incremental Methods 

每走一步获得新数据，然后online更新value fuction

#### Gradient Descent

- 梯度下降算法
- Let $J(\mathbf  w)$ be a differentiable function of parameter vector $\mathbf w$
- **gradient**

$$
\nabla_{\mathbf w}J(\mathbf w) =  \begin{pmatrix}
 \frac{\partial J(\mathbf w)}{\partial  \mathbf w_1}\\
\vdots\\
\frac{\partial J(\mathbf w)}{\partial  \mathbf w_n}\\
\end{pmatrix}
$$

- To find a local minimum of $J(\mathbf w)$ , Adjust $\mathbf w$ in direction of -ve gradient

$$
\Delta \mathbf w = - \frac{1}{2} \alpha \nabla_{\mathbf w}J(\mathbf w)
$$

##### Stochastic Gradient Descent  SGD

目标函数以及梯度, 监督学习

- Goal: find parameter vector $\mathbf w$ minimising **mean-squared error**  ,  mse

$$
J(\mathbf w) = \mathbb E_\pi[(v_\pi(S) - \hat v(S,\mathbf w))^2]
$$

- Gradient descent finds a **local minimum**

$$
\Delta \mathbf w = - \frac{1}{2} \alpha \nabla_{\mathbf w}J(\mathbf w) 
= \alpha \mathbb E_\pi[(v_\pi(S) - \hat v(S,\mathbf w)) \nabla_{\mathbf w}\hat v(S,\mathbf w)]
$$

- **Stochastic gradient descent** **samples** the gradient

$$
\Delta \mathbf w =   \alpha  (v_\pi(S) - \hat v(S,\mathbf w)) \nabla_{\mathbf w}\hat v(S,\mathbf w)
$$

- **Expected update is equal to full gradient update**,  full 意味着在全部数据上做一次梯度 



### Linear Function Approximation

监督学习中, 线性拟合, 可以收敛到全局最优, 就是找到最佳的线性拟合函数. 

- Represent state by  **Feature Vectors**

$$
\mathbf x(S)  =  \begin{pmatrix}
\mathbf x_1 (S)\\
\vdots\\
\mathbf x_n (S)\\
\end{pmatrix}
$$

- Represent value function by a linear combination of features 

$$
\hat v(S,\mathbf w) = \mathbf x(S)^\top \mathbf w = \sum_{j=1}^n \mathbf x_j(S)\mathbf w_j
$$

- Stochastic gradient descent **converges** on **global optimum** 

$$
\Delta \mathbf w =   \alpha  (v_\pi(S) - \hat v(S,\mathbf w)) \mathbf x(S)
$$

- Update = step-size × prediction error × feature value
- 线性V拟合,  是监督学习, 训练target是已经是evaluate完成的 $v_\pi(s)$ 



##### ~~Table Lookup Features~~

- One-hot 表示state，每个state对应一个index,  对状态超级多的情况，会超级长, 不实用
- Table lookup is a special case of linear value function approximation
- Using table lookup features

$$
\mathbf x^{table}(S)  =  \begin{pmatrix}
\mathbf 1 (S = s_1)\\
\vdots\\
\mathbf 1 (S = s_n)\\
\end{pmatrix}
$$

- Parameter vector $\mathbf  w$ gives value of each individual state

$$
\hat v(S,\mathbf w)  =  \begin{pmatrix}
\mathbf 1 (S = s_1)\\
\vdots\\
\mathbf 1 (S = s_n)\\
\end{pmatrix} \cdot 
\begin{pmatrix}
\mathbf w_1\\
\vdots\\
\mathbf w_n\\
\end{pmatrix}
$$



### Incremental Prediction Algorithms

- **对prediction问题，全局最优只是指的是 找到最好的线性拟合函数，全体数据上误差最小的， 只需GD，调整好学习率，肯定可以在全体样本上找到误差最小的**  这个还好，难的是下面的control



- Have assumed **true** value function $v_π(s)$ given by supervisor ; 之前假设已经有了Oracle真值v ,不现实
- But in RL there is no supervisor, only rewards  ;    但RL里面的v要去算
- In practice, we substitute a **target** for $v_π(s)$  ;   没有真值，**先强化学习再代入GD公式**
  - MC : $\Delta \mathbf w = \alpha ({\color{red}{G_t}} - \hat v(S_t,\mathbf w) )\nabla_{\mathbf w} \hat v(S_t,\mathbf w)$ 
  - TD(0) : $\Delta \mathbf w = \alpha ({\color{red}{R_{t+1}+\gamma \hat v(S_{t+1},\mathbf w)}} - \hat v(S_t,\mathbf w) )\nabla_{\mathbf w} \hat v(S_t,\mathbf w)$ 
  - TD($\lambda$) : $\Delta \mathbf w = \alpha ( {\color{red}{G^\lambda_t}} - \hat v(S_t,\mathbf w) )\nabla_{\mathbf w} \hat v(S_t,\mathbf w)$



#### Monte-Carlo with Value Function Approximation

- Return $G_t$ is an **unbiased**, **noisy** sample of true value $v_π(S_t)$   
  G是V的一个采样  无偏有噪，即无偏有方差的
- Can therefore apply supervised learning to “training data”: $⟨S_1, G_1⟩, ⟨S_2, G_2⟩, ..., ⟨S_T , G_T ⟩$
- For example, using linear Monte-Carlo policy evaluation

$$
\begin{align}
\Delta \mathbf w &= \alpha ({\color{red}{G_t}} - \hat v(S_t,\mathbf w) )\nabla_{\mathbf w} \hat v(S_t,\mathbf w) \\
& = \alpha(G_t-\hat{v}(S_t, \mathbf{w})) \mathbf x(S_t)\\
\end{align}
$$

- 线性收敛到全局最优
- Monte-Carlo evaluation **converges** to a **local optimum** Even when using non-linear value function approximation   非线性 MC evaluation 也能**收敛**到一个局部最优



#### TD Learning with Value Function Approximation

通过TD来收敛到value的真值，但中间bootstrap的时候，用到值函数的近似值,   这个近似值是有偏的; 模型本身的训练, 若每次都训练到收敛, 则起到了近似qtable的作用；每bootstrap一次, target 就会跟准一些. 



- The **TD-target** $R_{t+1} + γ\hat v(S_{t+1}, \mathbf w)$ is a **biased** sample of true value $v_π(S_t)$  ,  TDtarget的值 有偏
- apply supervised learning to “training data”: $⟨S_1, R_2+\gamma \hat v(S_2,\mathbf w)⟩, ⟨S_2, R_3+\gamma \hat v(S_3,\mathbf w)⟩, ..., ⟨S_{T-1} , R_T ⟩$

- using linear TD(0) : $\Delta \mathbf w = α({\color{red}{R_{t+1} + γ\hat v(S_{t+1}, \mathbf w)}} − \hat v(S, \mathbf w))∇_{\mathbf w} \hat v(S, \mathbf w) = αδ \mathbf x(S)$ ;   
  TD err 乘 linear gradient

- Linear TD(0) **converges** (close) to **global optimum**   
  收敛到全局最优, 尽管target是有偏估计， 即肯定能拟合到最终的全局最优点

- 对于TD ，最后一步的sample肯定是准的， 然后中间有偏的sample， R的部分是准的； 但对于中间R=0的env，只有最后一步是准，中间都是有偏



#### TD(λ) with Value Function Approximation

- The λ-return $G_t^λ$ is also a **biased** sample of true value $v_π(s)$
- “training data”: $⟨S_1, G_1^\lambda⟩, ⟨S_2, G_2^\lambda⟩, ..., ⟨S_{T-1} , G_{T-1}^\lambda ⟩$

- Forward view linear TD(λ)

$$
\Delta \mathbf w = α( {\color{red}{G_{t}^\lambda}} − \hat v(S, \mathbf w))∇_{\mathbf w} \hat v(S, \mathbf w) = α( {\color{red}{G_{t}^\lambda}} − \hat v(S, \mathbf w))\mathbf x(S)
$$

- Backward view linear TD(λ)

$$
\begin{align}
\delta_t &= R_{t+1} + \gamma \hat v(S_{t+1},\mathbf w) - \hat v(S_t,\mathbf w)
\\ E_t &= \gamma \lambda E_{t-1} + \mathbf x(S_t) 
\\ \Delta \mathbf w &= \alpha \delta_t E_t
\end{align}
$$

- Forward view and backward view linear TD(λ) are equivalent



### Incremental Control Algorithms

#### Control with Value Function Approximation

<img src="/img/2019-03-02-Silver.assets/image-20190220000024445.png" style="zoom: 50%;" />

- Policy evaluation **Approximate** policy evaluation, $q(·, ·,\mathbf w) ≈ q_π$
- Policy improvement **ε-greedy policy improvement**



#### Action-Value Function Approximation

- 监督学习， 还是先要得到 true value q ，即 oracle
- Approximate the action-value function $\hat q(S, A, \mathbf w) ≈ q_π(S, A)$
- Minimise **MSE** between approximate action-value fn $\hat q(S,A,\mathbf w)$ and true action-value fn $q_π(S,A)$

$$
J(\mathbf w) = \mathbb E_\pi[(q_\pi(S,A) - \hat q(S,A,\mathbf w))^2]
$$

- Use **SGD** to find a **local minimum**

$$
-\frac{1}{2}\nabla_\mathbf w J(\mathbf{w})=(q_\pi(S, A)-\hat{q}(S, A, \mathbf{w}))\nabla_\mathbf w \hat{q}(S, A, \mathbf{w})\\
\Delta\mathbf{w}=\alpha (q_\pi(S, A)-\hat{q}(S, A, \mathbf{w}))\nabla_\mathbf w\hat{q}(S, A, \mathbf{w})
$$



##### Linear Action-Value Function Approximation

- feature vector

$$
\mathbf x(S,A)  =  \begin{pmatrix}
\mathbf x_1 (S,A)\\
\vdots\\
\mathbf x_n (S,A)\\
\end{pmatrix}
$$

- Represent action-value fn by linear combination of features

$$
\hat q(S,A,\mathbf w) = \mathbf x(S,A)^\top \mathbf w = \sum_{j=1}^n \mathbf x_j(S,A)\mathbf w_j
$$

- **SGD** update

$$
\nabla_\mathbf w \hat q(S,A,\mathbf w) = \mathbf x(S,A)
\\ \Delta \mathbf w =   \alpha  (q_\pi(S,A) - \hat q(S,A,\mathbf w)) \mathbf x(S,A)
$$



### Incremental Control Algorithms

- 没有真值，使用 TD 来bootstrap
- Like prediction, we must substitute a target for $q_π(S,A)$
  - MC : $\Delta \mathbf w = \alpha ({\color{red}{G_t}} - \hat q(S_t,A_t,\mathbf w) )\nabla_{\mathbf w} \hat q(S_t,A_t,\mathbf w)$
  - TD(0) : $\Delta \mathbf w = \alpha ({\color{red}{R_{t+1}+\gamma \hat q(S_{t+1},A_{t+1},\mathbf w)}} - \hat q(S_t,A_t,\mathbf w) )\nabla_{\mathbf w} \hat q(S_t,A_t,\mathbf w)$
  - TD($\lambda$) : $\Delta \mathbf w = \alpha ( {\color{red}{q^\lambda_t}} - \hat v(S_t,A_t,\mathbf w) )\nabla_{\mathbf w} \hat q(S_t,A_t,\mathbf w)$
  - Backward view linear TD(λ)

$$
\begin{align}
\delta_t &= R_{t+1} + \gamma \hat q(S_{t+1},A_{t+1},\mathbf w) - \hat q(S_t,A_t,\mathbf w)
\\ E_t &= \gamma \lambda E_{t-1} + \nabla_\mathbf w \hat q (S_t,A_t,\mathbf w) 
\\ \Delta \mathbf w &= \alpha \delta_t E_t
\end{align}
$$



##### Linear Sarsa with Coarse Coding in Mountain Car

Tile Coding是Coarse Coding的一种，特别适合用于多维连续空间, 可以见sutton的书

##### Linear Sarsa with Radial Basis Functions in Mountain Car



##### Study of λ: Should We Bootstrap?

<img src="/img/2019-03-02-Silver.assets/image-20200701151127227.png" alt="image-20200701151127227" style="zoom:50%;" />

- 总的来说**λ=1的时候通常算法表现是很差**的, 即MC方法的确收敛的慢 
- TD(0)是比MC好得多的方法，这说明了Bootstrap的重要性
- 不同的任务对应的最优λ值是不太容易确定的。



### Convergence

Bootstrap 是很好， 但是不是收敛呢

##### Baird’s Counterexample

例子说明TD 不保证收敛 , 详细见 sutton 书



#### Convergence of Prediction Algorithms

MC使用的是实际价值的有噪声无偏估计，虽然很多时候表现很差，但总能收敛至局部或全局最优解。TD性能通常更加优秀，但TD 不收敛.  这还只是 prediction, 一般只看control. 

| On/Off-Policy | Algorithm                | Table Lookup    | Linear          | Non-Linear      |
| ------------- | ------------------------ | --------------- | --------------- | --------------- |
| On-Policy     | MC<br/>TD(0)<br/>TD(λ)   | Y<br />Y<br />Y | Y<br />Y<br />Y | Y<br />N<br />N |
| Off-Policy    | MC<br />TD(0)<br />TD(λ) | Y<br />Y<br />Y | Y<br />N<br />N | Y<br />N<br />N |



##### Gradient Temporal-Difference Learning

在prediction问题中，引入新的 **Gradient TD**算法，可以收敛 ; 但讨论不多 , 目前看来该方向不重要

- **TD does not follow the gradient of any objective function**  , TD 没有遵循着目标函数的梯度
- This is why TD can diverge when off-policy or using  non-linear function approximation
- **Gradient TD** follows true gradient of projected Bellman error 

| On/Off-Policy | Algorithm                   | Table Lookup    | Linear          | Non-Linear      |
| ------------- | --------------------------- | --------------- | --------------- | --------------- |
| On-Policy     | MC<br />TD<br />Gradient TD | Y<br />Y<br />Y | Y<br />Y<br />Y | Y<br />N<br />Y |
| Off-Policy    | MC<br />TD<br />Gradient TD | Y<br />Y<br />Y | Y<br />N<br />Y | Y<br />N<br />Y |



#### Convergence of Control Algorithms

| Algorithm                   | Table Lookup    | Linear          | Non-Linear      |
| ------------- | --------------------------- | --------------- | --------------- |
| Monte-Carlo Control | Y            | (Y)    | N |
| Sarsa               | Y            | (Y) | N |
| Q-Learning | Y | N | N |
| Gradient Q-Learning | Y | Y | N |

(Y) = chatters around near-optimal value function 表示在接近最优价值函数附近震荡

- Control 问题，大多数都能得到较好的策略，但是理论上只要存在**函数近似**，就都不是严格收敛的，比较常见的是在最优策略上下震荡，逐渐逼近然后突然来一次发散，再逐渐逼近, 但最终很难收敛于一点。



## Batch Methods

- **Gradient descent** is simple and appealing

- But it is not sample efficient   之前都是sample一个数据就一次更新， 效率太低

- Batch methods seek to find the best fitting value function

- Given the agent’s experience (“training data”)

  



### Least Squares Prediction

- 在一部分数据里面**方差之和**最小的， 而不是遇到一步就更新一步
- Given value function approximation $\hat v(s,\mathbf w) ≈ v_π(s)$  
- **experience** $\mathcal D$ consisting of ⟨state,value⟩ pairs  
   $\mathcal D = {⟨s_1, v_1^π⟩, ⟨s_2, v_2^π⟩, ..., ⟨s_T , v_T^π ⟩}$ 
- Which parameters $\mathbf w$ give the best fitting value fn $\hat v(s,\mathbf w)$ ?
- **Least squares** algorithms find parameter vector w minimising **sum-squared error** between v̂ (st,𝕨)  and target values $v^π_t$,

$$
\begin{align}
LS(\mathbf{w}) & = \sum^T_{t=1}(v_t^\pi-\hat{v}(s_t, \mathbf{w}))^2 \\
& = \mathbb{E}_\mathcal{D}[(v^\pi-\hat{v}(s, \mathbf{w}))^2] \\
\end{align}
$$





####  Stochastic Gradient Descent with Experience Replay

- Given **experience** consisting of ⟨state, value⟩ pairs  $\mathcal D = {⟨s_1, v_1^π⟩, ⟨s_2, v_2^π⟩, ..., ⟨s_T , v_T^π ⟩}$ 

- Repeat: 
  1. **Sample** state, value from experience :  $⟨s,v^π⟩ ∼ \mathcal D $
  2. Apply **SGD** update : $\Delta \mathbf w=α(v^π − \hat v(s,\mathbf w))∇_{\mathbf w} \hat v(s,\mathbf w)$

- **Converges** to **least squares** solution $\mathbf w^\pi= \text{argmin}_\mathbf w LS(\mathbf w)$ 

随机的从 数据集里面取，打破了前后状态之间的**关联性**； 监督学习里面比较强调数据的随机性，更容易收敛到好的结果



#### Experience Replay in Deep Q-Networks (DQN)

- TD方法结合DNN可能不会收敛，但DQN使用 **experience replay**和**fixed Q-targets** 能做到效果很好。
- 对于modelfree的情况， replay一个很大的作用就是学习model，那些<s,a,r,s'>的采样是很有价值的；
- DQN uses **experience replay** and **fixed Q-targets**  
- Take action $a_t$ according to **ε-greedy** policy    采样是探索性策略
- Store **transition** $(s_t,a_t,r_{t+1},s_{t+1})$ in **replay memory** $\mathcal D$       
  存到buffer，**注意这个时候没存Q** ，因为Q是算出来的，越来越准, 也没有存下一个a ，因为a是通过qmax算出来的
- Sample random **mini-batch** of transitions $(s,a,r,s′)$ from $\mathcal D$   随机采样打破了状态之间的联系
- Compute **Q-learning targets** w.r.t. old, **fixed parameters $w^−$**  
- Optimise **MSE** between Q-network(for predicting) and Q-learning targets    

$$
\mathcal L_i(w_i) = \mathbb E_{s,a,r,s'\sim\mathcal D_i} \left[\left( r+ \gamma \max_{a'}Q(s',a';w_i^-) -Q(s,a;w_i)  \right)^2 \right]
$$

- Using variant of **SGD**

- 维护两个神经网络DQN1，DQN2,一个网络固定参数专门用来产生目标值，目标值相当于标签数据。另一个网络专门用来更新参数。
- 第二个神经网络会暂时冻结参数，我们从冻结参数的网络而不是从正在更新参数的网络中获取目标值，这样增加了算法的稳定性。经过一次批计算后，把冻结参数的网络换成更新的参数再次冻结产生新一次迭代时要用的目标值。
- 在传统Qlearning中，根据公式， 更新一次Q value 的时候，会同时更新 target 的Qvalue , 因为都是更新的函数的参数，然后去算出来的； 而DQN实际上是走了若干步，才更新一次 target DQN的参数

 

##### python 里面实现 DQN 

1. Experience replay  每采样一次，就往  replay memory 里面加，然后把整个replay memory再sample出来训练
2. 对model来说， 一个episode会train很多次， target model 是每个episode 结束才更新一次；target不太会乱跑
3. 因为训练是针对 s,a，用的target新值，  Q(s,a) <= r + Q(s',a') 才来自 target model，Q(s)的其他动作值还是用model本身的
4. 对replay ， 存入的就是 s,a,r,s'  ; 所以每次 experience replay的时候，就是抽样出来一些再用最新的model算一遍 q 值；有价值的就是这些  dynamics
5. 当然这些 dynamics 是目前策略以及一些随机, 采样来的, 一直学也不一定能找到最优, 如果策略没有推着agent往最优的方向走的话,  光靠随机是很难走出最优的.



#### DDQN = Double DQN

a' 可以来自 model ， Q(s',a')来自 target model ,  即选择a'与算Q(s',a')放了个网络, 解耦.



#### DQN in Atari

- End-to-end learning of values $Q(s,a)$ from pixels s
- Input state s is stack of raw pixels from last 4 frames
- Output is $Q(s,a)$ for 18 joystick/button positions
- Reward is change in score for that step 
- Network architecture and hyperparameters fixed across all games

![image-20190220200811646](/img/2019-03-02-Silver.assets/image-20190220200811646.png)





### Convergence of Control Algorithms

| Algorithm           | Table Lookup | Linear | Non-Linear |
| ------------------- | ------------ | ------ | ---------- |
| Monte-Carlo Control | Y            | (Y)    | N          |
| Sarsa               | Y            | (Y)    | N          |
| Q-Learning          | Y            | N      | N          |
| LSPI                | Y            | (Y)    | --         |

(Y) = chatters around near-optimal value function 表示在最优价值函数附近震荡





### ~~Linear Least Squares Prediction~~

- 本方法只适用于线性拟合，直接求解析解，局限性比较大,  可以不太看

- **Experience replay** finds least squares solution
- But it may take many iterations
- Using linear value function approximation $\hat v(s, \mathbf w) = \mathbf x(s)^⊤ \mathbf w$
- We can solve the least squares solution directly   直接求解析解 
- At minimum of $LS(\mathbf w)$, the expected update must be zero

$$
\begin{align}
\mathbb E_{\mathcal D}[\Delta \mathbf w] &= 0
\\ \alpha\sum_{t=1}^T \mathbf x(s_t)(v_t^\pi - \mathbf x(s_t)^\top\mathbf w) & =0
\\ \sum_{t=1}^T \mathbf x(s_t)v_t^\pi & = \sum_{t=1}^T \mathbf x(s_t) \mathbf x(s_t)^\top\mathbf w 
\\ \mathbf w &= \left( \sum_{t=1}^T \mathbf x(s_t) \mathbf x(s_t)^\top  \right)^{-1} \sum_{t=1}^T \mathbf x(s_t)v_t^\pi
\end{align}
$$

- For N features, direct solution time is $O(N^3)$
- Incremental solution time is $O(N^2)$ using Shermann-Morrison



#### Linear Least Squares Prediction Algorithms

- We do not know true values $v_t^π$
- In practice, our “training data” must use noisy or biased samples of $v_t^π$
  - **LSMC** Least Squares Monte-Carlo uses return 
    ​   $v_t^π ≈ G_t$
  - **LSTD** Least Squares Temporal-Difference uses TD target 
    ​   $v_t^π ≈R_{t+1}+γ \hat v(S_{t+1}, \mathbf w) $
  - **LSTD(λ)** Least Squares TD(λ) uses λ-return 
    ​   $v_t^π ≈ G_t^λ$
- In each case solve directly for **fixed point** of MC / TD / TD(λ) 
- LSMC

$$
\begin{align} 0 & = \sum_{t=1}^T\alpha \left(G_t - \hat v(S_t,\mathbf w) \right)\mathbf x(S_t)
\\ \mathbf w &= \left(\sum_{t=1}^T \mathbf x(S_t)\mathbf x(S_t)^\top\right)^{-1} \sum_{t=1}^T \mathbf x(S_t)G_t
\end{align}
$$

- LSTD 

$$
\begin{align} 0 & = \sum_{t=1}^T\alpha \left(R_{t+1}+ \gamma \hat v(S_{t+1},\mathbf w) - \hat v(S_t,\mathbf w) \right)\mathbf x(S_t)
\\ \mathbf w &= \left(\sum_{t=1}^T \mathbf x(S_t)(\mathbf x(S_t)- \gamma \mathbf x(S_{t+1}))^\top\right)^{-1} \sum_{t=1}^T \mathbf w(S_t)R_{t+1}
\end{align}
$$

- LSTD(λ)

$$
\begin{align} 0 & = \sum_{t=1}^T\alpha \delta_t E_t
\\ \mathbf w &= \left(\sum_{t=1}^T  E_t (\mathbf x(S_t)- \gamma \mathbf x(S_{t+1}))^\top\right)^{-1} \sum_{t=1}^T   E_t R_{t+1}
\end{align}
$$




##### Convergence of Linear Least Squares Prediction Algorithms

| On/Off-Policy | Algorithm                      | Table Lookup           | Linear                 | Non-Linear               |
| ------------- | ------------------------------ | ---------------------- | ---------------------- | ------------------------ |
| On-Policy     | MC<br />LSMC<br />TD<br />LSTD | Y<br />Y<br />Y<br />Y | Y<br />Y<br />Y<br />Y | Y<br />--<br />N<br />-- |
| Off-Policy    | MC<br />LSMC<br />TD<br />LSTD | Y<br />Y<br />Y<br />Y | Y<br />Y<br />N<br />Y | Y<br />--<br />N<br />-- |





### Least Squares Control

#### Least Squares Policy Iteration   LSPI

- Policy evaluation: Policy evaluation by **least squares Q-learning**
- Policy improvement: Greedy policy improvement



#### Least Squares Action-Value Function Approximation

- Approximate action-value function $q_π(s,a)$ 
- using linear combination of features $\mathbf x(s,a)$ :  $\hat q(s, a,\mathbf  w) =\mathbf x(s, a)^⊤\mathbf w ≈ q_π(s, a) $
- Minimise least squares error between $\hat q(s, a,\mathbf w)$ and $q_π(s, a)$ 
- from experience generated using policy π
- consisting of ⟨(state, action), value⟩ pairs  $\mathcal D = {⟨(s_1, a_1), v_1^π⟩, ⟨(s_2, a_2), v_2^π⟩, ..., ⟨(s_T , a_T ), v_T^π ⟩}  $  



#### Least Squares Control

- For policy evaluation, we want to efficiently use all experience 
- For control, we also want to improve the policy
- This experience is generated from many policies
- So to evaluate $q_π(S,A)$ we must learn **off-policy** 

**这里说明， off-policy 的两个方案，一个模仿Q-learning 一个是importa sampling**

- We use the same idea as Q-learning:   **从target policy 里面取 a'**
  - Use experience generated by old policy $S_t,A_t,R_{t+1},S_{t+1} ∼π_{old}$
  - Consider alternative successor action $A′ = π_{new} (S_{t+1})$ 
  - Update $\hat q(S_t , A_t ,\mathbf w)$ towards value of alternative action $R_{t+1} + γ\hat q(S_{t+1}, A′,\mathbf w))$ 



##### Least Squares Q-Learning

- Consider the following linear Q-learning update

$$
\delta = R_{t+1} + \gamma \hat q(S_{t+1},\pi(S_{t+1}),\mathbf w) - \hat q(S_t,A_t,\mathbf w) 
\\ \Delta \mathbf w= \alpha \delta \mathbf x(S_t,A_t)
$$

- LSTDQ algorithm: solve for total update = zero

$$
0 = \sum_{t=1}^T \alpha(R{t+1}+\gamma \hat q(S_{t+1},\pi(S_{t+1},\mathbf w) - \hat q(S_t,A_t,\mathbf w)) \mathbf x(S_t,A_t)
\\ \mathbf w = \left(\sum_{t=1}^T \mathbf x(S_t,A_t)\Big(\mathbf x(S_t,A_t)-\gamma \mathbf x(S_{t+1},\pi(S_{t+1})) \Big)^\top \right) \sum_{t=1}^T \mathbf x(S_t,A_t)R_{t+1}
$$



##### Least Squares Policy Iteration Algorithm

- The following pseudocode uses LSTDQ for policy evaluation
- It repeatedly re-evaluates experience $\mathcal D$ with different policies

> function LSPI-TD($\mathcal D, π_0$)   
> ​ $π′ ← π_0$   
> ​ repeat   
> ​   $π ← π′$  
>     Q ← LSTDQ($π,\mathcal D$)   
> ​   for all $s ∈\mathcal S$ do   
> ​     $π′(s) ← \text{argmax}_{a∈A} Q(s, a)$   
> ​   end for   
> ​ until $(π ≈ π′)$   
> ​ return π   
> end function      
















