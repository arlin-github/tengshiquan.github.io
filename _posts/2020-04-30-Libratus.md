---
layout:     post
title:      Libratus 冷扑
subtitle:   Superhuman AI for heads-up no-limit poker, Libratus beats top professionals
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-poker.jpg"
catalog: true
tags:
    - AI
    - Imperfect Information
    - CFR

---



# Libratus

## Superhuman AI for heads-up no-limit poker: Libratus beats top professionals

2017  Noam Brown from CMU

论文以及视频的笔记.



看完了原理,  博弈论玩的是概率的游戏!  大家拼谁的概率估的更准, 怎么样走到各个subgame的纳什均衡.



Blueprint = Abstraction(card , bet) + MCCFR

三个模块:  blueprint ,  safe subgame solving , self-improver 

最核心的技术就是  **reach subgame solving**





信息完全,可以用minimax, ab-prunning

博弈论的方法, 考虑对手所有可能的策略.  不在乎别人是否知道自己的策略, 只要自己的策略近似纳什均衡.  与RL不同.

基于 博弈论的方法,  不在乎别人是否知道自己的策略, 只要自己的策略近似纳什均衡.  与RL不同.

RL的问题是, 可能会有漏洞, 普通人也许也能发现漏洞, 就是说离纳什均衡还是有距离. 不能保证不会输.

德州是那种, 求解难验证简单的. 

信息非对称, 是数学期望的游戏; 也是重复的游戏, 随机的游戏

对手建模, 这个在短时间交手就能找到漏洞, 对计算机来说还太难, 对人类高手来说可以, 是个值的探索的领域.

斗地主, 根据玩家id, 建模,  实际上成本比较大.





**Why are imperfect-information games hard?**
**Because an optimal strategy for a subgame cannot be determined from that subgame alone**

- 不完全信息最大的难点, 是一个subgame的最优策略, 不是由该subgame单独能决定的.

- 另外一个难点, 是 state 没有 well-defined value.



<img src="/img/2020-04-30-Libratus.assets/image-20200601160356651.png" alt="image-20200601160356651" style="zoom: 33%;" />



<img src="/img/2020-04-30-Libratus.assets/image-20200601161040529.png" alt="image-20200601161040529" style="zoom: 33%;" />



<img src="/img/2020-04-30-Libratus.assets/image-20200601161609048.png" alt="image-20200601161609048" style="zoom: 50%;" />



<img src="/img/2020-04-30-Libratus.assets/image-20200601161718245.png" alt="image-20200601161718245" style="zoom:50%;" />



例子, P1的 sell action 看起来是不影响 play action 的subgame的.  但随着 sell 的EV的改变,  P2的最优策略也随之改变.  

所以, 核心就是, 不完全信息需要对整个gametree都比较了解, 才能找到均衡解. 显然, 我们可能没有整个游戏的均衡策略, 但我们可以estimate 某个subgame的value, 从而使得其他subgame可以求解. 对上面的例子, 只要能估计准P1在sell时候的EV, 就可以得到subgame的最优策略.

**Theorem**: If **estimates** of opponent values are off by at most $$\delta$$, then safe subgame solving has at most $$\delta$$ exploitability.    只要EV估计的准, 就可以使用 safe subgame solving. 

 



<img src="/img/2020-04-30-Libratus.assets/image-20200601164415018.png" alt="image-20200601164415018" style="zoom: 33%;" />

![image-20200601164506592](/img/2020-04-30-Libratus.assets/image-20200601164506592.png)

<img src="/img/2020-04-30-Libratus.assets/image-20200601164513861.png" alt="image-20200601164513861" style="zoom:33%;" />



<img src="/img/2020-04-30-Libratus.assets/image-20200601164548997.png" alt="image-20200601164548997" style="zoom:33%;" />



Libratus只对游戏早期部分计算近似最优策略.  对之后的部分该采取什么策略, 只是给出大概的蓝图.  

然后在实际的比赛中, 会遇到特定的subgame,  这时针对该subgame , 要给出更好的详细策略, 同时要使该详细策略匹配蓝图策略,  **使用其他subgame的 expected value 来计算该subgame的最优策略**.  

之后不断重复此过程 , 遇到新的subgame, 计算详细策略, fit in 蓝图策略. 



### Libratus 的核心模块



<img src="/img/2020-04-30-Libratus.assets/image-20200601165716891.png" alt="image-20200601165716891" style="zoom: 50%;" />



1. Abstraction ,求纳什均衡, 是off-line, 在game开始前就预计算好的.   MCCFR
2. subgame 详细策略计算,  fit in blueprint ,  online
3. 自我改进, 根据前一天的比赛, 找到blueprint里面不太准确的估计, 并对那些不准的特殊情况进行重新计算修正;  相当于及时的修补漏洞. 





#### Solving the Whole Game

不用抽象, 已经解决了前两个

- Rhode Island Hold'em: $$10^{9}$$ decisions
  - Solved with LP [Gilpin \& sandholm 2005]
- limit Texas Hold'em: $$10^{13}$$ decisions 
  - Essentially solved with Counterfactual Regret Minimization+ 
- Required 262 TB compressed to $$11 \mathrm{TB}$$
  - No-Limit Texas Holdem: 10 $$^{161}$$ decisions Way too big 





与信息完全的博弈解决方法不一样, **信息完全更多的使用 online 方法, 注重实时计算, 为游戏当前的情况找到最优策略**: alphaGo  MCTS,  Deepblue, ab剪枝; 如果没有实时planning, 只用网络, alphazero不如人类 ;       信息不完全更多的使用offline方法, 提前计算好策略, 尝试找到一个策略, 能覆盖到游戏的整体, 然后到了实际play的时候, 根据所处的情况, 查看预计算好的策略是怎么对应的. 

1. Linear Programming
2. CFR+  

这两个方法都是预计算的, 没有实时计算. 



<img src="/img/2020-04-30-Libratus.assets/image-20200602022606343.png" alt="image-20200602022606343" style="zoom:50%;" />





空间太大, 遍历一遍都不可能, 需要Abstraction.   Abstraction是人工启发式的一种形式.    简化的时候, 要尽可能的保留原始游戏的策略特征. 然后计算出简化版本的纳什均衡. 然后在原始游戏里play.   因此,我们在原始游戏中遇到一个特定的情况, 映射到简化游戏, 找到最佳策略, 再映射回去, 同时希望得到一个不错的近似策略.  

 

#### Abstraction in Libratus

- Abstracting chance's action (cards in poker)
  - Same algorithm that we used in **Tartanian8** [Brown, Ganzfried \& Sandholm AAMAS-15]
  - But much **finer abstraction**
    -  $$1^{\text {ss }}$$ and $$2^{\text {nd }}$$ betting round: no abstraction
    -  $$3^{\text {sd }}$$ betting round: 55M card histories $$\rightarrow 2.5 \mathrm{M}$$ buckets
    -  $$4^{\text {th }}$$ betting round: 2.4B card histories -> 1.25M buckets

- Abstracting player's actions (bet sizes in poker)

  - Largely based on what top humans and Als do  基于过去的AI跟人类经验
  - Added radical bet sizes  一些不太会出现的bet
  - Optimized some of the bet sizes in the early parts of the tree [Brown \& Sandholm AAAl-14]    

  对前两轮, 有纳什均衡, 所以找到了最佳的bet 组合, 虽然是简化过的, 伸缩性稍微差点.



### 两种Abstraction

**Action Abstraction**,  特别的, 将bet 501 视为500, 叫做  Action Translation.    
这可能成为一个很大的弱点, 因为人类玩家会使用 不常见的bet size 来扰乱AI, 造成很多 rounding error. AI会对potsize 很困惑. 

<img src="/img/2020-04-30-Libratus.assets/image-20200603025802298.png" alt="image-20200603025802298" style="zoom:50%;" />

<img src="/img/2020-04-30-Libratus.assets/image-20200531161114632.png" alt="image-20200531161114632" style="zoom:50%;" />



**Information Abstraction** ,  Card Abstraction,  Chance Abstraction

<img src="/img/2020-04-30-Libratus.assets/image-20200603030114560.png" alt="image-20200603030114560" style="zoom:50%;" />

上图, 这个bucket是不合适的, 对高手来说, 是有潜在区别的, 作者认为这个是 Claudico(作者之前的AI) 的主要弱点. 



### Libratus Abstraction

- #### $$1^{\text {st }}$$ and $$2^{\text {nd }}$$ round: **no card abstraction, dense action abstraction**

- #### $$3^{3^{\circ}}$$ and $$4^{\text {th }}$$ round: **card abstraction, sparse action abstraction**

  - Helps reduce exponential blowup

- Total size of abstract strategy: **50TB**









如何计算早期的纳什均衡策略: 

- Improvement on Monte-Carlo Counterfactual Regret Minimization [Lanctot et al. NIPS-09]
- Starts visiting less often paths where our own actions don't look promising (similar to Brown \& Sandholm NIPS-15 paper and AAAl-17 workshop paper)
  =>  Speedup =>  can solve larger abstractions  ??
- Also, the imperfect-recall abstraction, in effect, becomes finer grained
  => Better solution quality
- Distributed across $$1+195$$ compute nodes
  - Distribution along game tree, not "embarrassingly parallel"







### MCCFR

Abstraction建立好了以后, 开始计算 蓝图策略;  然后在实际play中, 改进.

使用MCCFR 预计算蓝图策略. 

每次迭代中, 一个玩家遍历gametree, update其regret, 另外一个玩家只是sample 自己的action.

下图, 玩家1遍历, 玩家2 sample.  P1根据当前regret的比例来选择action.



![image-20200531163834256](/img/2020-04-30-Libratus.assets/image-20200531163834256.png)

Libratus 改进了MCCFR.   **regret-based prunning**  , 例如, 有一些起手牌特别差, 像27, 人类肯定会fold, 就不用去扩展了, 还有一些好牌AA, 肯定不会去fold, 还有如一些node的action的EV是很大的负数 ;  如果一些action总是给比较差的reward, 那么就该减少探索的频率 , 不用每次迭代都去遍历. 



### Subgame Solving



#### New ideas in subgame solver

- Provably **safe** subgame solving taking into account opponent's mistakes in the hand so far
- Off-tree actions & nested subgame solving
- Subgame solving starts much earlier
- No card abstraction in the subgame
- Changed our action abstraction between hands







既然有了blueprint策略, 那么假设对手会按照blueprint策略,  会有什么问题. 

<img src="/img/2020-04-30-Libratus.assets/image-20200603034336002.png" alt="image-20200603034336002" style="zoom:50%;" />

根据blueprint策略, P1 遇到head, 80%play, 遇到Tails, 30%play.  

当P1选择了play之后,  P2可以推测当前所处的infoset里面具体哪个node的几率, 即belief. 



<img src="/img/2020-04-30-Libratus.assets/image-20200603034403165.png" alt="image-20200603034403165" style="zoom:50%;" />

下面假设这些几率都是对的,  下面计算P2的BR, 针对该几率.   显然P2这个时候认在左边的node的几率更高, 在左边的情况下, 显然选Heads, 让P1得1分. 但这个纯策略会很容易被P1 改变策略来针对. 

<img src="/img/2020-04-30-Libratus.assets/image-20200603035433714.png" alt="image-20200603035433714" style="zoom:50%;" />



所以, maintain一个belief概率, 来表示当前所处的node,  然后找到BR针对该belief, 并不是一个好方法. 因为该方法, 对对手的玩法有很强的假设,  假定别人按照blueprint , 但其实别人不会.  blueprint只是一个近似均衡策略, 不是唯一的, 如果对手发现P1按照blueprint来玩,则有很多策略针对.  

也就是说, 如果别人的策略是固定并且按照blueprint来玩, 那么求贝叶斯belief是可行的.  但人类的顶尖高手, 善于发现AI的问题, 并且能立刻调整策略, 所以还是坚持纳什均衡策略比较safe.



详细点的讲解:

<img src="/img/2020-04-30-Libratus.assets/image-20200603171104249.png" alt="image-20200603171104249" style="zoom:50%;" />

这里左边的策略是blueprint, 不是纳什均衡策略, 但还可以; 

P2是Libratus, 下面要算出一个更好的P2的策略. 这里, p2不会使用更精细的Abstraction, 为了简化说明, 只是使用一样的Abstraction. 

unsafe的方案是使用 贝叶斯法则.  然后算出了,针对的纯策略.   有些领域可以这么用. 

第一个safe的解决方案: 

<img src="/img/2020-04-30-Libratus.assets/image-20200603172159548.png" alt="image-20200603172159548" style="zoom:50%;" />

上图,紫色的是blueprint, P2根据blueprint玩的EV分别是0, 0.5; 然后P1可以选择 enter subgame 或者 选择 blueprint的 alt action, EV.   P2要修改策略,保证P1的收益不比blueprint的好.

算出来的策略, 碰巧跟blueprint 一样. 该策略并没有改进, 比如变成 never forfeit. 因为这个的收益对P2来说永远不好.

<img src="/img/2020-04-30-Libratus.assets/image-20200603174304503.png" alt="image-20200603174304503" style="zoom:50%;" />



下面是一个改进:

maxmargin refinement.  惩罚P1选择 enter.   对各种类型的P1 ,定义margin. 下面就是maximize P1选择enter之后的lose. 相当于逼P1继续选择Abstraction的blueprint策略. 

![image-20200603175352397](/img/2020-04-30-Libratus.assets/image-20200603175352397.png)

算出来的策略如下.

<img src="/img/2020-04-30-Libratus.assets/image-20200603180343680.png" alt="image-20200603180343680" style="zoom:50%;" />



策略经过更新, P1在head的时候, 选enter, ev是-0.25 , 那么可以选择alt , 获得0.5的ev. 这点也可以改进.

<img src="/img/2020-04-30-Libratus.assets/image-20200603183734857.png" alt="image-20200603183734857" style="zoom:50%;" />



Safe subgame resolving 的概念是2014年提出的,但是实践中没好的结果, 因为没有好的估计.  

**现在不再假设对手的策略, 而是去估计对手各个action在不同subgame中的EV.** 然后用这些信息, 去重新计算自己的subgame的策略. 显然只要把对手的EV估计的准, 该方法就可行.

利用blueprint 估计对手的其他subgame的value, 然后决定自己在这个subgame里面的策略.



#### Reach subgame resolving

这个改进是有理论保证的.   下面的deviate 主要指P1 脱离了之前计算的Abstraction的game tree.



<img src="/img/2020-04-30-Libratus.assets/image-20200604033906836.png" alt="image-20200604033906836" style="zoom:50%;" />

现在紫色是P2对P1的估计, 不管P1在什么状态, EV=-1;  P2最好策略就是两边0.5, 使得P1选play的EV=0;

<img src="/img/2020-04-30-Libratus.assets/image-20200604034232881.png" alt="image-20200604034232881" style="zoom:50%;" />



如果P2观察到P1之前的action,选过sell, 并且EV=0.5 ; 



<img src="/img/2020-04-30-Libratus.assets/image-20200604034341866.png" alt="image-20200604034341866" style="zoom:50%;" />



这意味着P1 会选择圆圈里的action作为策略. 总EV=0.25;   这个时候, 我们的P2可以改进自己的策略. 

<img src="/img/2020-04-30-Libratus.assets/image-20200604035421970.png" alt="image-20200604035421970" style="zoom: 33%;" />

使其得到一个均衡策略.   新的策略, 是的P1在Tail的时候选play的时候,EV=-0.5 ;    在Head的时候, play的EV=0.5, 但这个无所谓, 因为上面有个deviate 的action的ev已经是0.5.   这样P1的总EV=0.

但这样有点问题, 看下面的例子:

<img src="/img/2020-04-30-Libratus.assets/image-20200604035936312.png" alt="image-20200604035936312" style="zoom:50%;" />

 P1有个deviate action, 在两个node.   如果P1不选Deviate action , 选择continue,  则进入chance  node. 第二层的chance node , 所有人都可以看到 outcome, 是公开的. 对P2来说, 这两个chance node无法区分.

<img src="/img/2020-04-30-Libratus.assets/image-20200604040616178.png" alt="image-20200604040616178" style="zoom:50%;" />

假设作为P2观察到 P1选择play,  意味着我们在这两种情况里的一种.  假设第二层的chance node 选left. 

<img src="/img/2020-04-30-Libratus.assets/image-20200604041516819.png" alt="image-20200604041516819" style="zoom:50%;" />

因为P2观察到P1在Head的时候选Eviate action 的EV是0.5, 右边是EV=-0.5;  则P2可以改进策略. 但问题是, 如果第二层的chance 走右边. 按照上面的逻辑, 最终得到下面

<img src="/img/2020-04-30-Libratus.assets/image-20200604041902207.png" alt="image-20200604041902207" style="zoom:50%;" />

则造成P1选play的EV改变. P1就有了漏洞可以钻.

到这里就可以理解为什么这个策略叫 reach subgame solving . 就是要考虑 **reach prob**.

<img src="/img/2020-04-30-Libratus.assets/image-20200604042237473.png" alt="image-20200604042237473" style="zoom:50%;" />

考虑到chance的prob,  将最下面EV乘以0.5.   

主要是考虑这个node之后有多少个subgame 以及相应的 reach prob 以及最终EV.

<img src="/img/2020-04-30-Libratus.assets/image-20200604045252277.png" alt="image-20200604045252277" style="zoom:50%;" />



safe的思想就是, 我们已经预先计算了一套策略,近似均衡;   然后对遇到的实际state, 我们实时plan, 但不希望在之前的近似均衡的基础上增加 可利用性.   实际上subgame solving 是降低可利/用性的.









下面一个例子是, 对手使用了一个 没什么道理的action, 策略认为几率是0.  如下面, 对手上来就 all-in.

<img src="/img/2020-04-30-Libratus.assets/image-20200603040525289.png" alt="image-20200603040525289" style="zoom:50%;" />

 上来就allin的action, 在任何刚开局的情况下,都应该是0概率.



<img src="/img/2020-04-30-Libratus.assets/image-20200603044050649.png" alt="image-20200603044050649" style="zoom:50%;" />

Libratus 的做法是,  safe Subgame Solving ,  思想是, 不再对对手的策略做假设,  我们假设对手针对我方均衡策略的BR能得到的reward.  然后我方实时找到一个策略, 使得对手的收益变差.

例如, 对手如果是2,7,那么均衡策略的话,应该会fold; 4,8, 应该call; AA应该raise small.

如果我们能确认, 对手选择该action, 比均衡策略更差,  我们就能保证, 对手得到一个更小的EV, 那么我方就会最后赢.

那么可以用blueprint 策略来估计这些EV.



**Theorem:** If subgame **estimated values** *are* close to the **true values**, then subgame solving plays close to a Nash equilibrium strategy.



##### How good are our estimates?

Test game of Flop Texas Hold’em using an abstraction that is 0.02% of the full game size:

![image-20200603045643015](/img/2020-04-30-Libratus.assets/image-20200603045643015.png)

使用Abstraction 计算出来的策略,能很好的估计一个action在纳什均衡策略里的值, 很接近.

上图是说, 在一个flop版本的德州里面,  通过Abstraction得到的策略, 分别与完美打法打, 输赢在两端,都很大. 但如果Abstraction的值估计, 还是比较准. 

即, Abstraction给出的是bad策略, 但作为estimate还是很好的. 

可以类比于, Random rollout in MCTS,  作为策略肯定不好, 但作为估值, 还行



之前的Action Abstraction的 rounding error 问题的解决:



<img src="/img/2020-04-30-Libratus.assets/image-20200603110531055.png" alt="image-20200603110531055" style="zoom:50%;" />

P1选了一个不在Abstraction的bet, 叫作 off-tree action.  那么就创建一个subgame, 实时解决, 得到一个解, 使得对手选择该action 不会比选 Abstraction 的action更好,  即EV<=max(x,y,z).  这样就可以再成为一个纳什均衡.   下面流程中再遇到这样的情况,都可以这么解决 



##### Experiments on medium-sized games

- Our best reach subgame solving technique has $$3 x$$ less exploitability than the best prior safe subgame-solving technique
- Nested reach subgame solving is $$12 x$$ less exploitable than best action-mapping technique



<img src="/img/2020-04-30-Libratus.assets/image-20200603120311519.png" alt="image-20200603120311519" style="zoom:50%;" />







### Self-improver

<img src="/img/2020-04-30-Libratus.assets/image-20200603143038752.png" alt="image-20200603143038752" style="zoom:50%;" />



当对手使用不在Abstraction里面的action时,   晚上更新.  

这个相当于,让高手找AI的漏洞, 然后AI去完善.





### Observations About Libratus’s Play

- Strengths:
  - Many different bet sizes
  - “Donk betting”
  - Huge overbets
  - Near-perfect balance
- Weaknesses:
  - “No” opponent exploitation



### What about 3+ players?

- Theoretically problematic
  - With 3+ players, it is still possible to lose in expectation when playing Nash
  - Calculating Nash becomes PPAD-complete
  - Unclear if other solution concepts (e.g., Extensive-Form Correlated Equilibrium) are appropriate

- In pratice, same techniques do well in poker anyway
  - Most players fold, so most games become 2-player very quickly 
  - Little opportunity for collaboration (and trying is against the rules) 
  - There are now superhuman Als for essentially all popular variants of poker

- 3+ players poses an interesting challenge, but there are better domains for evaluation than poker
  - Not much player interaction in poker













### Abstract

heads-up no-limit Texas hold’em , 12万局比赛，击败了四位顶尖的人类高手. 

组成模块:  

- 整体策略, 蓝图 blueprint
- 子博弈的 细分策略
- 蓝图自我改进算法,   针对 蓝图策略里面可能暴露的弱点



### Game-solving approach in Libratus

3个主要模块:

1. abstraction ,  将德扑简化,  计算出来的策略作为蓝图策略.  针对前面回合, 可以作为详细策略, 但对于后面回合, 则相当于近似策略. 

2. 对子博弈更精细的abstraction ,  针对后面回合, 实时求解.  与完美信息中的子博弈求解技术不同，Libratus并不孤立地解决子博弈的抽象，而要确保子博弈的细粒度求解符合蓝图策略。  
   每当对手下出一个抽象中没有的动作时，子博弈解法就会包含这个动作。我们称之为**嵌套子博弈解法nested subgame solving**。这种技术带有一个可证明的安全保证。

3. self-improver ,  它填补了蓝图策略抽象中缺失的分支，并为这些分支计算了博弈论策略。原则上，可以预先进行所有此类计算，但是游戏树太大而无法实现。 为了解决这种复杂性，Libratus使用对手的实际行动来对game tree 里面没有展开的节点进行扩充。

   这个概念比较容易理解, game tree 里面没走到的分支, 就没有计算到均衡策略, 相当于弱点. 如果被人发现了, 则立刻补上. 那么随着玩的对手越来越多, 则game tree也会越来越大, 越来越没漏洞. 



#### Abstraction and equilibrium finding: Building a blueprint strategy

两种Abstraction:  bet , card

对imperfect information问题的一个解决方案是简单地将整个博弈作为一个整体来推理，而不是其中的部分。在这种方法中，可能使用linear program或迭代算法 对整个博弈进行预计算。例如， CFR+ 被用来近乎最优地解决了heads-up limit Texas hold’em，这是一个相对简单的版本，它有大约$$10^{13}$$个 决策点.

然而, HUNL 有$$10^{161}$$ 个决策点,  遍历一次game tree 是不现实的, 所以预计算不可行.  但许多决策点都很相似, 例如押注100与101区别不大.   100与20000之间, 可以只考虑100的递增, 这就是 **action abstraction**.  一个简化的博弈, 尽可能的保留原博弈的策略, 大大降低了解决的复杂度. Libratus的动作抽象是由之前的AI在比赛中的常见押注分析出来的.  但是, 在game tree的早期，特定的下注大小是由一个与应用无关的参数优化算法决定的，该算法收敛到一个局部最优的下注大小集.

抽象的另一种形式是对**Chance**发生的行动进行抽象，也就是牌的抽象 **card abstraction**。类似的手牌被归类到一起，并进行相同的处理。从直觉上来说，king-high flush和queen-high flush之间没有什么区别。把这些手牌视为相同的牌，降低了游戏的复杂性，从而使计算起来更容易。然而，其实是有区别的, 在高级别的游戏中，这些区别可能就是输赢之间的关键。Libratus在第一轮和第二轮下注时不使用任何牌的抽象。最后两个下注回合，有相当多的状态，只在蓝图策略中进行抽象。第三轮的5500万种不同的手牌可能性被算法归纳为250万个抽象bucket，第四轮的24亿种不同的可能性被算法归纳为125万个抽象bucket。然而，在这些回合中，AI并没有遵循蓝图策略，而是应用了嵌套子博弈求解法，不使用任何牌的抽象。因此，在实际游戏过程中，每张扑克牌都是单独考虑的。  冷扑没有使用card抽象.

Abstraction 规则定义好之后, 开始通过 self-play 计算 blueprint 策略.  使用 **MCCFR** . MCCFR 保证一个玩家的平均regret会收敛到0. 

对于每一场模拟游戏，MCCFR选择一个玩家（称之为traverser 遍历者），该玩家将探索每一个可能的行动，并更新他的regret，而对手只需按照当前的regret决定的策略下棋。该算法在每一手牌后，两个玩家的角色切换。每当模拟游戏中的任何一个玩家面临一个决策点时，该玩家都会根据这些行动的regret选择一个概率分布的行动（这些行动是由他在之前的游戏中所学到的知识决定的）。对于第一个游戏，AI还没有学到任何东西，因此在行动上使用统一的随机分布。在traverser决策点，MCCFR以深度优先的方式探索每一个动作。在对手决策点，MCCFR根据概率分布对一个动作进行采样。这个过程在每一个决策点重复进行，直到游戏结束，得到一个奖励，然后向上传递。当traverser决策点的每一个动作都得到一个奖励时，MCCFR根据动作的概率分布计算出该决策点的加权平均奖励。然后，通过将该行动返回的值相加，再减去该决策点的加权平均奖励，来更新每个行动的regret。然后将加权平均奖励向上传递给前一个决策点，以此类推。  MCCFR的流程.

**改进后的MCCFR**版本在每次迭代时都会遍历游戏树的一小部分。直观地讲，在游戏中，有很多明显的次优操作，反复探索这些操作会浪费计算资源，而这些资源可以更好地用于改进策略。我们的算法不是探索每一个假设的替代行动，看看它的回报是什么，而是在博弈过程中，我们的算法从概率上跳过**skip**了那些不可取的行动，当它深入到树的深处时，这些行动会有非常负面的regret。在实践中，这使MCCFR的速度提高了三倍，并使我们能够解决更大的抽象问题。

这种跳过也减轻了因 **imperfect recall**而产生的问题。该领域最先进的实用抽象，包括我们的抽象，都是 imperfect recall 的抽象。因为在一个抽象卡牌bucket中的所有决策点都有相同的策略，更新其中一个决策点的策略会导致更新所有决策点的策略。如果所有的决策点在达成的解上共享相同的最优策略，这不是问题，但是，在实践中，它们的最优策略之间存在着差异，它们有效地 "战斗"，将bucket的策略推向自己的最优策略。跳过负regret动作，意味着在实际博弈中永远不会达成的决策点将不再有策略更新，从而使实际博弈过程中会发生的决策点向自己的最优策略靠拢，从而使bucket的策略向自己的最优策略靠拢。

我们在HUNL的前两轮的抽象上运行了我们的算法，在前两轮的HUNL中，我们的算法是非常详细的，但在最后两轮中，我们的算法相对粗糙。然而，Libratus在最后两轮中从未按照抽象策略。相反，它只是在这两轮中使用抽象的蓝图策略来估计玩家在子博弈中, 得到一手牌后应该得到什么奖励。这个估计是用来在实际比赛中确定一个更精确的策略，这将在下一节中描述。





#### Nested safe subgame solving

抽象化方法已经产生了强大的扑克AI，但不足以在HUNL中达到超人的性能。除了抽象化之外，Libratus基于之前对子博弈解法的研究, 对游戏进行中实际遇到的情况, 实时生成更详细的策略.

Libratus的特点是**在子博弈解法方面取得了许多进展**，这些进展被证明是**实现超强性能的关键**。

Libratus只在HUNL的早期部分按照抽象的蓝图策略，因为早期，可能的状态数量相对较少，我们可以承受的抽象是非常详细的 (即只是稍微简化一下)。在到达第三轮bet时，或者在博弈中剩余的博弈树足够小的任何一个早期点，Libratus会构造一个新的、更详细的抽象，对剩余的子博弈进行**实时**求解。





然而，在不完全信息博弈中，subgame求解有一个重大的挑战。一个subgame不能孤立地求解，因为它的最优策略可能取决于其他的、未达到的subgame。之前使用实时subgame求解的AI通过假设对手按照蓝图策略下棋来解决这个问题。然而，对手可以通过简单地切换到不同的策略来利用这一假设。由于这个原因，这种技术可能会产生比蓝图策略差得多的策略，被称为**unsafe subgame solving**。另一方面，**safe subgame solving**技术则保证subgame的新策略无论对手可能使用什么策略，都不会使对手更好过。他们通过确保子游戏的新策略适合原始抽象的总体蓝图策略来实现这一目标。相对于蓝图策略，确保对手没有更好的状况是可行的，因为我们可以重复使用蓝图策略。然而，现在，既然抽象在subgame中更加细化了，而且我们可以更好地分辨出subgame的策略细微之处，那么我们或许可以找到一个比之前的战略更优于对手的改进，让对手无论拿什么牌都不差。

下面的描述, 看起来比较混乱... 看ppt以及讲解才好理解一些.

现在，描述Libratus确定子游戏中改进策略的核心技术。 为了说明问题，我们假设玩家2（P2）正在确定针对玩家1（P1）的改进策略。 假设P2在子游戏之外的策略为$$\sigma_2$$，则存在一些P2可以在子游戏中玩的最佳策略$$\sigma_2^\prime$$。 我们想实时找到或近似 $$\sigma_2^*$$。 我们假设，即使我们不知道$$\sigma_2^*$$本身，但对于P1可能拿到的每个手牌，我们都可以通过与$$\sigma_2^*$$进行最佳对抗, 来很好地估计P1在该子游戏中用该手牌的价值。 尽管我们不完全知道这些值，但可以使用蓝图策略在子游戏中P1接收到的值来近似它们。 稍后我们证明，如果这些估计近似准确，则可以近似$$\sigma_2^*$$。

为了仅使用蓝图中的值在子游戏中找到接近$$\sigma_2^*$$的策略，我们创建了包含子游戏和其他结构的增强子游戏（图1）。在增强子游戏开始时，P1被私下发了一把随机扑克牌。假设P2在子游戏之前按照$$\sigma_2$$进行游戏，并且考虑到P1的手牌，则在这种情况下，P2可能拥有的手牌存在特定的概率分布。根据该概率分布，P2被发牌。然后，P1可以选择进入子游戏（现在比蓝图策略更为详细），或者选择立即结束增强子游戏的替代收益。根据蓝图策略，替代收益的价值是我们对该子游戏中该手牌的P1值的估计。如果P1选择进入子游戏，则游戏将正常进行直到到达游戏结束。我们可以像解决蓝图策略一样解决这种增强子游戏. 





<img src="/img/2020-04-30-Libratus.assets/image-20200530190052033.png" alt="image-20200530190052033" style="zoom:50%;" />

Fig. 1. Subgame solving.   最上面 , 一个具体的subgame 是红色三角形 ,   红色以及蓝色表示 blueprint 策略. 白色的路径表示到达子博弈之前的行动顺序。 中间,  一个子博弈更精细的策略,通过解决一个扩大的子博弈来确定, 在每次迭代中, 对手会被随机发一手牌,并选择旧的abstraction(红) 或新的更精细的abstraction(绿) , 双方的策略都可以改变, 使得Libratus在对付每个对手手牌时,更精细的策略至少要与原始的abstraction一样好   ;  最下面, 一个新的策略取代了老的.

























 








## Reference

Superhuman AI for heads-up no-limit poker: Libratus beats top professionals


















