---
layout:     post
title:      PYTorch Notes
subtitle:   
date:       2019-12-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-py.jpg"
catalog: true
tags:
    - pytorch
---



# PYTorch Notes

pytorch的笔记





#### Init layer parameters

*CLASS*   `torch.nn.Linear`(*in_features*, *out_features*, *bias=True*)

Applies a linear transformation to the incoming data: $y = xA^T + b$

- Shape:

  Input: $(N, *, H_{in})$ where star means any number of additional dimensions and $H_{in} = \text{in_features}$

  Output: $(N, *, H_{out})$

- Variables

  **~Linear.weight** –  $(\text{out_features}, \text{in_features})$ . The values are initialized from $\mathcal{U}(-\sqrt{k}, \sqrt{k})$, where $k = \frac{1}{\text{in_features}}$

  **~Linear.bias** –$(\text{out_features})$



##### Single layer

To initialize the weights of a single layer, use a function from [`torch.nn.init`](http://pytorch.org/docs/master/nn.html#torch-nn-init). For instance:

```python
conv1 = torch.nn.Conv2d(...)
torch.nn.init.xavier_uniform(conv1.weight)
```

Alternatively, you can modify the parameters by writing to `conv1.weight.data` (which is a [`torch.Tensor`](http://pytorch.org/docs/master/tensors.html#torch.Tensor)). Example:

```python
conv1.weight.data.fill_(0.01)
```

The same applies for biases:

```python
conv1.bias.data.fill_(0.01)
```

##### `nn.Sequential` or custom `nn.Module`

Pass an initialization function to [`torch.nn.Module.apply`](http://pytorch.org/docs/master/nn.html#torch.nn.Module.apply). It will initialize the weights in the entire `nn.Module` recursively.

> **apply(*fn*):** Applies `fn` recursively to every submodule (as returned by `.children()`) as well as self. Typical use includes initializing the parameters of a model (see also torch-nn-init).

Example:

```python
def init_weights(m):
    if type(m) == nn.Linear:
        torch.nn.init.xavier_uniform(m.weight)
        m.bias.data.fill_(0.01)

net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
net.apply(init_weights)
```

from https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch





# WHAT IS PYTORCH?

It’s a Python-based scientific computing package targeted at two sets of audiences:

- A replacement for **NumPy** to use the power of **GPUs**
- a deep learning research platform that provides maximum flexibility and speed



PyTorch provides two main features:

- An n-dimensional Tensor, similar to numpy but can run on GPUs
- **Automatic differentiation** for building and training neural networks



## Getting Started

### Tensors

Tensors : similar to NumPy’s ndarrays,  can be used on GPU

```python
from __future__ import print_function
import torch
```

Construct a 5x3 matrix, uninitialized:

```python
x = torch.empty(5, 3)
print(x)
```

声明了一段内存空间,但未初始化, 所以里面的值是不确定的, 所以print的时候也可能报错!

```python
x = torch.rand(5, 3)
print(x)
```

```python
tensor([[0.4707, 0.7056, 0.8309],
        [0.3842, 0.7964, 0.8027],
        [0.1584, 0.0038, 0.0977],
        [0.3735, 0.5157, 0.2004],
        [0.3184, 0.5997, 0.1331]])
```

```python
x = torch.zeros(5, 3, dtype=torch.long)
x = torch.tensor([5.5, 3])
```

```python
x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes
x = torch.randn_like(x, dtype=torch.float)    # override dtype!
print(x)                                      # result has the same size
```

```python
print(x.size())
```

```python
torch.Size([5, 3])
```



### Operations

##### Addition:

```python
y = torch.rand(5, 3)
print(x + y)
```

```python
print(torch.add(x, y))
```

```python
result = torch.empty(5, 3)
torch.add(x, y, out=result)
```

```python
# adds x to y
y.add_(x)
print(y)
```

Any operation that mutates a tensor **in-place is post-fixed with an `_`**. For example: `x.copy_(y)`, `x.t_()`, will change `x`.



You can use standard NumPy-like **indexing** with all bells and whistles!

```python
print(x[:, 1])
```

**Resizing**:  resize/reshape tensor:  `torch.view`:

```python
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
print(x.size(), y.size(), z.size())
```

```shell
torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
```

one element tensor,   `.item()` get  a Python number

```python
x = torch.randn(1)
print(x)
print(x.item())
```

```python
tensor([-1.0658])
-1.0657769441604614
```



> 100+ Tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random numbers, etc., are described [here](https://pytorch.org/docs/torch).



## NumPy Bridge

Torch Tensor and NumPy array will share their underlying memory locations (if the Torch Tensor is on CPU), and **changing one will change the other**.

### Converting a Torch Tensor to a NumPy Array

```python
a = torch.ones(5)
b = a.numpy()
print(b)
```

```python
[1. 1. 1. 1. 1.]
```

See how the numpy array changed in value.

```python
a.add_(1)
print(a)
print(b)
```

```python
tensor([2., 2., 2., 2., 2.])
[2. 2. 2. 2. 2.]
```

### Converting NumPy Array to Torch Tensor

```python
import numpy as np
a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out=a)
print(a)
print(b)
```

```
[2. 2. 2. 2. 2.]
tensor([2., 2., 2., 2., 2.], dtype=torch.float64)
```

All the Tensors on the CPU except a CharTensor support converting to NumPy and back.



## CUDA Tensors

Tensors can be moved onto any device using the `.to` method.

```python
# let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA device object
    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU
    x = x.to(device)                       # or just use strings ``.to("cuda")``
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # ``.to`` can also change dtype together!
```

```
tensor([-0.0658], device='cuda:0')
tensor([-0.0658], dtype=torch.float64)
```



# AUTOGRAD: AUTOMATIC DIFFERENTIATION

Central to all neural networks in PyTorch is the `autograd` package. It is a **define-by-run** framework, which means that your **backprop is defined by how your code is run**, and that **every single iteration can be different**.

## Tensor

`torch.Tensor` is the central class of the package. If you set its attribute `.requires_grad` as `True`, it **starts to track all operations** on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.

To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked. 看成常量

To **prevent tracking history** (and using memory), you can also wrap the code block in `with torch.no_grad():`. This can be particularly helpful when **evaluating a model** because the model may have trainable parameters with `requires_grad=True`, but for which we don’t need the gradients.

There’s one more class which is very important for autograd implementation - a `Function`.

`Tensor` and `Function` are interconnected and build up an **acyclic graph**, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a `Function` that has **created the** `Tensor` (except for Tensors created by the user - their `grad_fn is None`).

If you want to **compute the derivatives**, you can call `.backward()` on a `Tensor`. If `Tensor` is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a `gradient` argument that is a tensor of **matching shape**.



```python
x = torch.ones(2, 2, requires_grad=True)
print(x)
```

```python
tensor([[1., 1.],
        [1., 1.]], requires_grad=True)
```

```python
y = x + 2
print(y)
```

```python
tensor([[3., 3.],
        [3., 3.]], grad_fn=<AddBackward0>)
```

`y` was created as a result of an operation, so it has a `grad_fn`.

```python
print(y.grad_fn)
```

```python
<AddBackward0 object at 0x7fac3a8d0080>
```

Do more operations on `y`

```python
z = y * y * 3
out = z.mean()
print(z, out)
```

```python
tensor([[27., 27.],
        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)
```

`.requires_grad_( ... )` changes an existing Tensor’s `requires_grad` flag in-place. The input flag defaults to `False` if not given.

```python
a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(a.requires_grad)
a.requires_grad_(True)
print(a.requires_grad)
b = (a * a).sum()
print(b.grad_fn)
```

```python
False
True
<SumBackward0 object at 0x7fac3a8d0c18>
```

## Gradients

Let’s backprop now. Because `out` contains a single scalar, `out.backward()` is equivalent to `out.backward(torch.tensor(1.))`.

```python
out.backward()
```

Print gradients d(out)/dx

```python
print(x.grad)
```

```python
tensor([[4.5000, 4.5000],
        [4.5000, 4.5000]])
```



Mathematically, if you have a vector valued function $\vec{y}=f(\vec{x})$, then the gradient of $\vec{y}$ with respect to $\vec{x}$
is a Jacobian matrix:

$$
\begin{align}J=\left(\begin{array}{ccc}
   \frac{\partial y{1}}{\partial x{1}} & \cdots & \frac{\partial y{1}}{\partial x{n}}\\
   \vdots & \ddots & \vdots\\
   \frac{\partial y{m}}{\partial x{1}} & \cdots & \frac{\partial y{m}}{\partial x{n}}
   \end{array}\right)\end{align}
$$

Generally speaking, ``torch.autograd`` is an engine for **computing vector-Jacobian product**. That is, given any vector $v=\left(\begin{array}{cccc} v_{1} & v_{2} & \cdots & v_{m}\end{array}\right)^{T}$, **compute the product** $v^{T}\cdot J$. If $v$ happens to be the gradient of a scalar function $l=g\left(\vec{y}\right)$, that is, $v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} & \cdots & \frac{\partial l}{\partial y_{m}}\end{array}\right)^{T}$, then by the chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\vec{x}$:  计算目标函数对当前变量的梯度, 目标函数就是正向的程序流程

$$
\begin{align}J^{T}\cdot v=\left(\begin{array}{ccc}
   \frac{\partial y{1}}{\partial x{1}} & \cdots & \frac{\partial y{m}}{\partial x{1}}\\
   \vdots & \ddots & \vdots\\
   \frac{\partial y{1}}{\partial x{n}} & \cdots & \frac{\partial y{m}}{\partial x{n}}
   \end{array}\right)\left(\begin{array}{c}
   \frac{\partial l}{\partial y{1}}\\
   \vdots\\
   \frac{\partial l}{\partial y{m}}
   \end{array}\right)=\left(\begin{array}{c}
   \frac{\partial l}{\partial x{1}}\\
   \vdots\\
   \frac{\partial l}{\partial x{n}}
   \end{array}\right)\end{align}
$$



This characteristic of vector-Jacobian product makes it very convenient to feed external gradients into a model that has non-scalar output.

an example of vector-Jacobian product:

```python
x = torch.randn(3, requires_grad=True)

y = x * 2
while y.data.norm() < 1000:
    y = y * 2

print(y)
```

```python
tensor([ 933.9816,  285.7694, -848.8997], grad_fn=<MulBackward0>)
```

Now in this case `y` is no longer a scalar. `torch.autograd` could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to `backward` as argument:

```python
v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)
y.backward(v)

print(x.grad)
```

```python
tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])
```

**stop autograd** from tracking history on Tensors with `.requires_grad=True` by wrapping the code block in `with torch.no_grad():`

```python
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)
```

```python
True
True
False
```



Document about `autograd.Function` is at https://pytorch.org/docs/stable/autograd.html#function



# NEURAL NETWORKS

Neural networks can be constructed using the `torch.nn` package.



## Define the network

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 3x3 square convolution
        # kernel
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]  # all dimensions except the batch dimension
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)
```

```python
Net(
  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))
  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))
  (fc1): Linear(in_features=576, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=10, bias=True)
)
```

You just **have to** define the `forward` function, and the `backward` function (where gradients are computed) is automatically defined for you using `autograd`. You can use any of the Tensor operations in the `forward` function.

The learnable parameters of a model are returned by `net.parameters()`

```python
params = list(net.parameters())
print(len(params))
print(params[0].size())  # conv1's .weight
```

```python
10
torch.Size([6, 1, 3, 3])
```

```python
input = torch.randn(1, 1, 32, 32)
out = net(input)
print(out)
```

Zero the gradient buffers of all parameters and backprops with random gradients:

```python
net.zero_grad()
out.backward(torch.randn(1, 10))
```

NOTE

> `torch.nn` **only supports mini-batches**. The entire `torch.nn` package only supports inputs that are a mini-batch of samples, and not a single sample.
>
> For example, `nn.Conv2d` will take in a 4D Tensor of `nSamples x nChannels x Height x Width`.
>
> If you have a single sample, just use `input.unsqueeze(0)` to add a fake batch dimension.

 

## Loss Function

A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.

```python
output = net(input)
target = torch.randn(10)  # a dummy target, for example
target = target.view(1, -1)  # make it the same shape as output
criterion = nn.MSELoss()

loss = criterion(output, target)
print(loss)
```

if you follow `loss` in the backward direction, using its `.grad_fn` attribute, you will see a graph of computations that looks like this:

```python
input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d
      -> view -> linear -> relu -> linear -> relu -> linear
      -> MSELoss
      -> loss
```

So, when we call `loss.backward()`, the whole graph is differentiated w.r.t. the loss, and all Tensors in the graph that has `requires_grad=True` will have their `.grad` Tensor accumulated with the gradient.

```python
print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU
```



## Backprop

need to clear the existing gradients though, else gradients will be **accumulated** to existing gradients.

```python
net.zero_grad()     # zeroes the gradient buffers of all parameters

print('conv1.bias.grad before backward')
print(net.conv1.bias.grad)

loss.backward()

print('conv1.bias.grad after backward')
print(net.conv1.bias.grad)
```

```python
conv1.bias.grad before backward
tensor([0., 0., 0., 0., 0., 0.])
conv1.bias.grad after backward
tensor([-0.0189,  0.0574, -0.0527, -0.0099,  0.0280,  0.0115])
```



> The neural network package contains various modules and loss functions that form the building blocks of deep neural networks. A full list with documentation is [here](https://pytorch.org/docs/nn).



## Update the weights

SGD using simple python code:

```python
learning_rate = 0.01
for f in net.parameters():
    f.data.sub_(f.grad.data * learning_rate)
```

 `torch.optim` 

```python
import torch.optim as optim

# create your optimizer
optimizer = optim.SGD(net.parameters(), lr=0.01)

# in your training loop:
optimizer.zero_grad()   # zero the gradient buffers
output = net(input)
loss = criterion(output, target)
loss.backward()
optimizer.step()    # Does the update
```



# TRAINING A CLASSIFIER

## Load Data

- For images, packages such as Pillow, OpenCV are useful
- For audio, packages such as scipy and librosa
- For text, either raw Python or Cython based loading, or NLTK and SpaCy are useful



Specifically for vision,  package `torchvision`, that has data loaders for common datasets such as Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz., `torchvision.datasets` and `torch.utils.data.DataLoader`.

## Training an image classifier

```python
import torch
import torchvision
import torchvision.transforms as transforms

transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                          shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                       download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                         shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
```

```python
import matplotlib.pyplot as plt
import numpy as np

# functions to show an image
def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()


# get some random training images
dataiter = iter(trainloader)
images, labels = dataiter.next()

# show images
imshow(torchvision.utils.make_grid(images))
# print labels
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))
```



```python
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x
      
net = Net()
```

```python
import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
```

```python
for epoch in range(2):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %
                  (epoch + 1, i + 1, running_loss / 2000))
            running_loss = 0.0

print('Finished Training')
```



```python
PATH = './cifar_net.pth'
torch.save(net.state_dict(), PATH) # save model
```



```python
dataiter = iter(testloader)
images, labels = dataiter.next()

# print images
imshow(torchvision.utils.make_grid(images))
print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))
```

```python
net = Net()
net.load_state_dict(torch.load(PATH))

outputs = net(images)

_, predicted = torch.max(outputs, 1)

print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]
                              for j in range(4)))
```



```python
correct = 0
total = 0
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the 10000 test images: %d %%' % (
    100 * correct / total))
```

```shell
Accuracy of the network on the 10000 test images: 53 %
```



```python
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))
with torch.no_grad():
    for data in testloader:
        images, labels = data
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze()
        for i in range(4):
            label = labels[i]
            class_correct[label] += c[i].item()
            class_total[label] += 1


for i in range(10):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))
```



## Training on GPU

Just like how you transfer a Tensor onto the GPU, you **transfer the neural net onto the GPU**.

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# Assuming that we are on a CUDA machine, this should print a CUDA device:
print(device)
```

Then these methods will **recursively go over all modules and convert their parameters and buffers to CUDA tensors**:

```python
net.to(device)
```

Remember that you will have to **send the inputs and targets at every step to the GPU too**:

```python
inputs, labels = data[0].to(device), data[1].to(device)
```



# DATA PARALLELISM

**use multiple GPUs** using `DataParallel`.

put the model on a GPU:

```python
device = torch.device("cuda:0")
model.to(device)
```

copy all your tensors to the GPU:

```python
mytensor = my_tensor.to(device)
```

calling `my_tensor.to(device)` returns a **new copy** of `my_tensor` on GPU instead of rewriting `my_tensor`. You need to **assign it to a new tensor and use that tensor on the GPU.**

Pytorch only **use one GPU by default**. You can easily run your operations on multiple GPUs by making your model run parallelly using `DataParallel`:

```
model = nn.DataParallel(model)
```



## Imports and parameters

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Parameters and DataLoaders
input_size = 5
output_size = 2

batch_size = 30
data_size = 100
```

Device

```python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

## Dummy DataSet

Make a dummy (random) dataset. You just need to implement the getitem

```python
class RandomDataset(Dataset):

    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return self.len

rand_loader = DataLoader(dataset=RandomDataset(input_size, data_size),
                         batch_size=batch_size, shuffle=True)
```

## Simple Model

can use `DataParallel` on any model (CNN, RNN, Capsule Net etc.)

We’ve placed a print statement inside the model to monitor the size of input and output tensors. Please pay attention to what is printed at batch rank 0.

```python
class Model(nn.Module):
    # Our model

    def __init__(self, input_size, output_size):
        super(Model, self).__init__()
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, input):
        output = self.fc(input)
        print("\tIn Model: input size", input.size(),
              "output size", output.size())

        return output
```



## Create Model and DataParallel

core :  check  multiple GPUs.  wrap our model using `nn.DataParallel`. Then put model on GPUs by `model.to(device)`

```python
model = Model(input_size, output_size)
if torch.cuda.device_count() > 1:
  print("Let's use", torch.cuda.device_count(), "GPUs!")
  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs
  model = nn.DataParallel(model)

model.to(device)
```



## Run the Model

Now we can see the sizes of input and output tensors.

```python
for data in rand_loader:
    input = data.to(device)
    output = model(input)
    print("Outside: input size", input.size(),
          "output_size", output.size())
```



## Results

If you have no GPU or one GPU, when we batch 30 inputs and 30 outputs, the model gets 30 and outputs 30 as expected. But if you have multiple GPUs, then you can get results like this.



## Summary

DataParallel **splits your data automatically** and sends job orders to multiple models on several GPUs. After each model finishes their job, DataParallel **collects and merges the results** before returning it to you.

https://pytorch.org/tutorials/beginner/former_torchies/parallelism_tutorial.html.





# LEARNING PYTORCH WITH EXAMPLES

implement the network using numpy

```python
import numpy as np

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random input and output data
x = np.random.randn(N, D_in)
y = np.random.randn(N, D_out)

# Randomly initialize weights
w1 = np.random.randn(D_in, H)
w2 = np.random.randn(H, D_out)

learning_rate = 1e-6
for t in range(500):
    # Forward pass: compute predicted y
    h = x.dot(w1)
    h_relu = np.maximum(h, 0)
    y_pred = h_relu.dot(w2)

    # Compute and print loss
    loss = np.square(y_pred - y).sum()
    print(t, loss)

    # Backprop to compute gradients of w1 and w2 with respect to loss
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.T.dot(grad_y_pred)
    grad_h_relu = grad_y_pred.dot(w2.T)
    grad_h = grad_h_relu.copy()
    grad_h[h < 0] = 0  # 这个实现比较简洁
    grad_w1 = x.T.dot(grad_h)

    # Update weights
    w1 -= learning_rate * grad_w1
    w2 -= learning_rate * grad_w2
```

For modern deep neural networks, GPUs often provide speedups of [50x or greater](https://github.com/jcjohnson/cnn-benchmarks)

use PyTorch Tensors and autograd to implement our two-layer network

```python
import torch

dtype = torch.float
device = torch.device("cpu")
# device = torch.device("cuda:0") # Uncomment this to run on GPU

N, D_in, H, D_out = 64, 1000, 100, 10

x = torch.randn(N, D_in, device=device, dtype=dtype)
y = torch.randn(N, D_out, device=device, dtype=dtype)

w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)
w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
    y_pred = x.mm(w1).clamp(min=0).mm(w2)
    loss = (y_pred - y).pow(2).sum()
    if t % 100 == 99:
        print(t, loss.item())

    loss.backward()

    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad
        # Manually zero the gradients after updating weights
        w1.grad.zero_()
        w2.grad.zero_()
```



### Defining new autograd functions

Under the hood, each **primitive autograd operator** is really **two functions** that operate on Tensors. The **forward** function computes output Tensors from input Tensors. The **backward** function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value. 一个正向一个反向

define our own autograd operator by defining a **subclass** of `torch.autograd.Function` and implementing the `forward` and `backward` functions. 

In this example we define our own custom autograd function for performing the ReLU nonlinearity, and use it to implement our two-layer network:

```python
import torch

class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        """
        In the forward pass we receive a Tensor containing the input and return
        a Tensor containing the output. ctx is a context object that can be used
        to stash information for backward computation. You can cache arbitrary
        objects for use in the backward pass using the ctx.save_for_backward method.
        """
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def backward(ctx, grad_output):
        """
        In the backward pass we receive a Tensor containing the gradient of the loss
        with respect to the output, and we need to compute the gradient of the loss
        with respect to the input.
        """
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input


dtype = torch.float
device = torch.device("cpu")
# device = torch.device("cuda:0") # Uncomment this to run on GPU

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold input and outputs.
x = torch.randn(N, D_in, device=device, dtype=dtype)
y = torch.randn(N, D_out, device=device, dtype=dtype)

# Create random Tensors for weights.
w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)
w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
    # To apply our Function, we use Function.apply method. We alias this as 'relu'.
    relu = MyReLU.apply

    # Forward pass: compute predicted y using operations; we compute
    # ReLU using our custom autograd operation.
    y_pred = relu(x.mm(w1)).mm(w2)

    # Compute and print loss
    loss = (y_pred - y).pow(2).sum()
    if t % 100 == 99:
        print(t, loss.item())

    # Use autograd to compute the backward pass.
    loss.backward()

    # Update weights using gradient descent
    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad

        # Manually zero the gradients after updating weights
        w1.grad.zero_()
        w2.grad.zero_()
```



use the `nn` and `optim` to implement our two-layer network:

```python
import torch

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold inputs and outputs
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# Use the nn package to define our model and loss function.
model = torch.nn.Sequential(
    torch.nn.Linear(D_in, H),
    torch.nn.ReLU(),
    torch.nn.Linear(H, D_out),
)
loss_fn = torch.nn.MSELoss(reduction='sum')

# Use the optim package to define an Optimizer that will update the weights of
# the model for us. Here we will use Adam; the optim package contains many other
# optimization algoriths. The first argument to the Adam constructor tells the
# optimizer which Tensors it should update.
learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
for t in range(500):
    # Forward pass: compute predicted y by passing x to the model.
    y_pred = model(x)

    # Compute and print loss.
    loss = loss_fn(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    optimizer.zero_grad()

    # Backward pass: compute gradient of the loss with respect to model
    # parameters
    loss.backward()

    # Calling the step function on an Optimizer makes an update to its
    # parameters
    optimizer.step()
```



### Custom nn Modules

define your own Modules by subclassing `nn.Module` and defining a `forward` 

```python
import torch


class TwoLayerNet(torch.nn.Module):
    def __init__(self, D_in, H, D_out):
        """
        In the constructor we instantiate two nn.Linear modules and assign them as
        member variables.
        """
        super(TwoLayerNet, self).__init__()
        self.linear1 = torch.nn.Linear(D_in, H)
        self.linear2 = torch.nn.Linear(H, D_out)

    def forward(self, x):
        """
        In the forward function we accept a Tensor of input data and we must return
        a Tensor of output data. We can use Modules defined in the constructor as
        well as arbitrary operators on Tensors.
        """
        h_relu = self.linear1(x).clamp(min=0)
        y_pred = self.linear2(h_relu)
        return y_pred


# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold inputs and outputs
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# Construct our model by instantiating the class defined above
model = TwoLayerNet(D_in, H, D_out)

# Construct our loss function and an Optimizer. The call to model.parameters()
# in the SGD constructor will contain the learnable parameters of the two
# nn.Linear modules which are members of the model.
criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
for t in range(500):
    # Forward pass: Compute predicted y by passing x to the model
    y_pred = model(x)

    # Compute and print loss
    loss = criterion(y_pred, y)
    if t % 100 == 99:
        print(t, loss.item())

    # Zero gradients, perform a backward pass, and update the weights.
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```



# WRITING CUSTOM DATASETS, DATALOADERS AND TRANSFORMS

一些data处理方法

- `scikit-image`: For image io and transforms
- `pandas`: For easier csv parsing



## Dataset class

`torch.utils.data.Dataset` is an abstract class representing a dataset. Your custom dataset should inherit `Dataset` and override the following methods:

- `__len__` so that `len(dataset)` returns the size of the dataset.
- `__getitem__` to support the indexing such that `dataset[i]` can be used to get iith sample

This is memory efficient because all the images are not stored in the memory at once but **read as required**.



- Batching the data
- Shuffling the data
- Load the data in parallel using `multiprocessing` workers.

`torch.utils.data.DataLoader` is an iterator which provides all these features. 



## torchvision

`torchvision` package provides some common datasets and transforms.  为图片数据准备的util类.   `ImageFolder` assumes that images are organized in the following way:

```shell
root/ants/xxx.png
root/ants/xxy.jpeg
root/ants/xxz.png
root/bees/123.jpg
```

```python
import torch
from torchvision import transforms, datasets

data_transform = transforms.Compose([
        transforms.RandomSizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406],
                             std=[0.229, 0.224, 0.225])
    ])
hymenoptera_dataset = datasets.ImageFolder(root='hymenoptera_data/train',
                                           transform=data_transform)
dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,
                                             batch_size=4, shuffle=True,
                                             num_workers=4)
```



# TRANSFER LEARNING FOR COMPUTER VISION TUTORIAL



These two major transfer learning scenarios look as follows:

- **Finetuning the convnet**: Instead of random initializaion, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.
- **ConvNet as fixed feature extractor**: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.



```python
from __future__ import print_function, division

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
import numpy as np
import torchvision
from torchvision import datasets, models, transforms
import matplotlib.pyplot as plt
import time
import os
import copy

plt.ion()   # interactive mode
```

```python
# Data augmentation and normalization for training
# Just normalization for validation
data_transforms = {
    'train': transforms.Compose([
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
    'val': transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ]),
}

data_dir = 'data/hymenoptera_data'
image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),
                                          data_transforms[x])
                  for x in ['train', 'val']}
dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,
                                             shuffle=True, num_workers=4)
              for x in ['train', 'val']}
dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}
class_names = image_datasets['train'].classes

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
```

```python
    """Imshow for Tensor."""
    inp = inp.numpy().transpose((1, 2, 0))
    mean = np.array([0.485, 0.456, 0.406])
    std = np.array([0.229, 0.224, 0.225])
    inp = std * inp + mean
    inp = np.clip(inp, 0, 1)
    plt.imshow(inp)
    if title is not None:
        plt.title(title)
    plt.pause(0.001)  # pause a bit so that plots are updated


# Get a batch of training data
inputs, classes = next(iter(dataloaders['train']))

# Make a grid from batch
out = torchvision.utils.make_grid(inputs)

imshow(out, title=[class_names[x] for x in classes])
```



```python
def train_model(model, criterion, optimizer, scheduler, num_epochs=25):
    since = time.time()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    for epoch in range(num_epochs):
        print('Epoch {}/{}'.format(epoch, num_epochs - 1))
        print('-' * 10)

        # Each epoch has a training and validation phase
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Set model to training mode
            else:
                model.eval()   # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0

            # Iterate over data.
            for inputs, labels in dataloaders[phase]:
                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # backward + optimize only if in training phase
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)
            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                phase, epoch_loss, epoch_acc))

            # deep copy the model
            if phase == 'val' and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Best val Acc: {:4f}'.format(best_acc))

    # load best model weights
    model.load_state_dict(best_model_wts)
    return model
```

```python
def visualize_model(model, num_images=6):
    was_training = model.training
    model.eval()
    images_so_far = 0
    fig = plt.figure()

    with torch.no_grad():
        for i, (inputs, labels) in enumerate(dataloaders['val']):
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)

            for j in range(inputs.size()[0]):
                images_so_far += 1
                ax = plt.subplot(num_images//2, 2, images_so_far)
                ax.axis('off')
                ax.set_title('predicted: {}'.format(class_names[preds[j]]))
                imshow(inputs.cpu().data[j])

                if images_so_far == num_images:
                    model.train(mode=was_training)
                    return
        model.train(mode=was_training)
```

## Finetuning the convnet

Load a pretrained model and reset final fully connected layer.

```python
model_ft = models.resnet18(pretrained=True)
num_ftrs = model_ft.fc.in_features
# Here the size of each output sample is set to 2.
# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).
model_ft.fc = nn.Linear(num_ftrs, 2)

model_ft = model_ft.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)
```

It should take around 15-25 min on CPU. On GPU though, it takes less than a minute.

```python
model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,
                       num_epochs=25)
visualize_model(model_ft)
```



## ConvNet as fixed feature extractor

**freeze all the network** except the final layer. set `requires_grad == False` to freeze the parameters so that the gradients are not computed in `backward()`.

```python
model_conv = torchvision.models.resnet18(pretrained=True)
for param in model_conv.parameters():
    param.requires_grad = False

# Parameters of newly constructed modules have requires_grad=True by default
num_ftrs = model_conv.fc.in_features
model_conv.fc = nn.Linear(num_ftrs, 2)

model_conv = model_conv.to(device)

criterion = nn.CrossEntropyLoss()

# Observe that only parameters of final layer are being optimized as
# opposed to before.
optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)

# Decay LR by a factor of 0.1 every 7 epochs
exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)
```

On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed.

```python
model_conv = train_model(model_conv, criterion, optimizer_conv,
                         exp_lr_scheduler, num_epochs=25)
```

```python
visualize_model(model_conv)

plt.ioff()
plt.show()
```



# TORCHAUDIO TUTORIAL



## Transformations

torchaudio supports a growing list of [transformations](https://pytorch.org/audio/transforms.html).

- **Resample**: Resample waveform to a different sample rate.
- **Spectrogram**: Create a spectrogram from a waveform.
- **MelScale**: This turns a normal STFT into a Mel-frequency STFT, using a conversion matrix.
- **AmplitudeToDB**: This turns a spectrogram from the power/amplitude scale to the decibel scale.
- **MFCC**: Create the Mel-frequency cepstrum coefficients from a waveform.
- **MelSpectrogram**: Create MEL Spectrograms from a waveform using the STFT function in PyTorch.
- **MuLawEncoding**: Encode waveform based on mu-law companding.
- **MuLawDecoding**: Decode mu-law encoded waveform.

## Migrating to torchaudio from Kaldi

 [Kaldi](http://github.com/kaldi-asr/kaldi), a toolkit for speech recognition. torchaudio offers compatibility with it in `torchaudio.kaldi_io`.

https://pytorch.org/tutorials/beginner/audio_preprocessing_tutorial.html




# Reference

https://pytorch.org/tutorials/



- [Image classification (MNIST) using Convnets](https://github.com/pytorch/examples/blob/master/mnist)
- [Word level Language Modeling using LSTM RNNs](https://github.com/pytorch/examples/blob/master/word_language_model)
- [Training Imagenet Classifiers with Residual Networks](https://github.com/pytorch/examples/blob/master/imagenet)
- [Generative Adversarial Networks (DCGAN)](https://github.com/pytorch/examples/blob/master/dcgan)
- [Variational Auto-Encoders](https://github.com/pytorch/examples/blob/master/vae)
- [Superresolution using an efficient sub-pixel convolutional neural network](https://github.com/pytorch/examples/blob/master/super_resolution)
- [Hogwild training of shared ConvNets across multiple processes on MNIST](https://github.com/pytorch/examples/blob/master/mnist_hogwild)
- [Training a CartPole to balance in OpenAI Gym with actor-critic](https://github.com/pytorch/examples/blob/master/reinforcement_learning)
- [Natural Language Inference (SNLI) with GloVe vectors, LSTMs, and torchtext](https://github.com/pytorch/examples/blob/master/snli)
- [Time sequence prediction - use an LSTM to learn Sine waves](https://github.com/pytorch/examples/blob/master/time_sequence_prediction)
- [Implement the Neural Style Transfer algorithm on images](https://github.com/pytorch/examples/blob/master/fast_neural_style)



