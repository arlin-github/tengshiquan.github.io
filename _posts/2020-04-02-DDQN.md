---
layout:     post
title:      Double DQN
subtitle:   DQN from Deepmind
date:       2020-04-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - DeepMind
    - Reinforcement Learning
    - DQN

---

 

# Double DQN

 

## Deep Reinforcement Learning with Double Q-learning

2015   主要解决 Q-learning **overestimate** 的问题 ,  double , **一个取a, 一个evaluate**

Q-learning tends to prefer overestimating. Q-learning 倾向高估.     
本文得出结论, overestimations can occur when the action values are inaccurate. 只要估值函数不准就会造成高估. 

高估了并不一定是坏的: 如果所有值都均匀高估.  sometimes it is good to be optimistic.  但如果高估不均匀, 就会有不好的影响 , 会造成次优策略(甚至渐近逼近次优) suboptimal policies, even asymptotically.

就算DQN这样, 也会高估.  

本文揭示了 Double Q-learning(van Hasselt, 2010) 背后的思想, 并将原始tabular的情况推广. 



#### Background

standard Q-learning update for the parameters after taking action $A_t$ in state $S_t$ and observing the immediate reward $R_{t+1}$ and resulting state $S_{t+1}$ ,

$$
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_{t}+\alpha\left(Y_{t}^{\mathrm{Q}}-Q\left(S_{t}, \boldsymbol{A}_{t} ; \boldsymbol{\theta}_{t}\right)\right) \nabla_{\boldsymbol{\theta}_{t}} Q\left(S_{t}, A_{t} ; \boldsymbol{\theta}_{t}\right) \\
Y_{t}^{Q} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right)
$$

该算法update类似SGD. This update resembles stochastic gradient descent

##### Deep Q Networks

action out  
DQN的两个要点:  target network,  experience replay

$$
Y_{t}^{\mathrm{DQN}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}^{-}\right)
$$

##### Double Q-Learning 

用同一个网络选择和评估造成过于乐观.   
Double Q-learning  的思想就是分离选择与评估 selection evaluation.
原始的double Q里面,有两个独立训练出来的网络, **一个用来选a,一个用来评估**. In the original Double Q-leaming algorithm, two value functions are learned by assigning each experience randomly to update one of the two value functions, such that there are two sets of weights, $\boldsymbol{\theta}$ and $\boldsymbol{\theta}^{\prime}$. For each update, one set of weights is used to determine the greedy policy and the other to determine its value. 
$$
Y_{t}^\text{DoubleQ}  \equiv  R_{t+1}+\gamma Q\left(S_{t+1}, \underset{a}{\operatorname{argmax}}    Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right) ; \boldsymbol{\theta'}_{t}\right)
$$



#### Overoptimism due to estimation errors

Thrun and Schwartz (1993) 第一个研究这个问题,  

1. 给了一个误差上限. if the action values contain random errors uniformly distributed in an interval $[-\epsilon, \epsilon]$ then each target is overestimated up to $\gamma \epsilon \frac{m-1}{m+1}$ where $m$ is the number of actions.  
2. 一个渐近到次优解的例子. give a concrete example in which these overestimations even asymptotically lead to sub-optimal policies.
3. using function approximation , overestimations的表现

Later van Hasselt (2010) argued that noise in the environment can lead to overestimations even when using tabular representation, and proposed Double Q-learning. 就算用tabular但如果有噪声都会overestimation,  然后提出Double Q.



下面作者更普遍地证明了无论任何类型的估计误差( 环境噪声、函数近似、非稳态性, environmental noise, function approximation, non-stationarity, or any other source )都会诱发 向上的偏差 upward bias .  
这一点很重要，因为在实践中，任何方法在学习过程中都会产生一些不准确的误差，这仅仅是由于最初的真实值是未知的。

下面这个定理, 确定了误差的下限.  假设了, 所有actionQ整体无偏, 所以必然有Q(a)>V* , 只要有action的Q估值不准并且相对V*高估,  max后必然有误差.  

**Theorem 1**. Consider a state s in which all the true optimal action values are equal at $$Q_{*}(s, a)=V_{*}(s)$$ for some $$V_{*}(s)$$ . Let $$Q_{t}$$ be arbitrary value estimates that are on the whole unbiased in the sense that $$\sum_{a}\left(Q_{t}(s, a)-V_{*}(s)\right)=0$$, but that are not all correct, such that $$\frac{1}{m} \sum_{a}\left(Q_{t}(s, a)-V_{*}(s)\right)^{2}=C$$ for some $C>0$,  where $m \geq 2$ is the number of actions in $s$ Under these conditions, $$\max _{a} Q_{t}(s, a) \geq V_{*}(s)+\sqrt{\frac{C}{m-1}}$$ .     
This lower bound is tight. Under the same conditions, the lower bound on the absolute error of the Double Q-learning estimate is zero. (Proof in appendix.)

注意，不需要假设不同行为的估计误差是独立的。  

等号成立的一个条件是 $\epsilon_{a}=\sqrt{\frac{C}{m-1}}$ for $a=1, \ldots, m-1$ and $\epsilon_{m}=-\sqrt{(m-1) C}$  

在Double Q-learning的情况下,  $$\vert Q_{t}^{\prime}\left(s, \operatorname{argmax}_{a} Q_{t}(s, a)\right)-V_{*}(s) \vert$$ 下限可以是0;  构造满足约束条件的误差
$$
Q_{t}\left(s, a_{1}\right)=V_{*}(s)+\sqrt{C \frac{m-1}{m}}
$$
 和 
$$
Q_{t}\left(s, a_{i}\right)=V_{*}(s)-\sqrt{C \frac{1}{m(m-1)}}, \text { for } i>1
$$
显然argmax 选出来的action是 a1 , 如果有 $Q_{t}^{\prime}\left(s, a_{1}\right)=V_{*}(s)$ , then the error is zero.  这里假设Q'在a1上是估值是准确的.  这里只是说明, Double Q-learning有可能突破该定理Q-learning的下限.



这个定理表明，即使Q的平均估计值是正确的，**任何来源的估计误差都会使估计值上升并远离真正的最优值**。  
定理1中的**下限随着行动的数量而减小**。 如图所示, overoptimism 过分乐观性随着动作数的增加而增加, 而Double Q-learning则是无偏的。图中, 假设 $Q(s,a)= V_*(s)+ \epsilon_a$ , $\lbrace \epsilon_a \rbrace^m_{a=1}$ 是独立的标准正态分布变量. 橙色是一次Q-learning update的bias, 显然是找集合里面误差最大的; 蓝色是Double Q-learning的. 结果是100次实验的平均.

![image-20200408024345755](/img/2020-04-01-DQN.assets/image-20200408024345755.png)





下面看function approximation的情况. 考虑一个连续状态的例子. In each state (x-axis), there are 10 actions. 为了简单，本例中真正的最优动作值只取决于状态，因此在每个状态下，所有的动作都有相同的真值 $$Q_*(s, a)= V_*(s)$$。 下图中,  
**left column**, 紫色是真值, 定义为 $$Q_*(s, a)=\sin (s)$$ (top row) or $$Q_{*}(s, a)=2 \exp \left(-s^{2}\right)$$ (middle and bottom rows).  The green line shows estimated values Q(s,a) for one action as a function of state, fitted to the true value at several sampled states (green dots). The estimate is a d-degree polynomial that is fit to the true values at sampled states, where d = 6 (top and middle rows) or d = 9 (bottom row).上面两行, 由于函数的灵活度不够, 没能完全拟合sample点, 第三行呢又过拟合. 注意, 左侧的采样点间隔比右边的大, 所以在左边, 拟合函数的估值误差也大. 这是一个典型的learning setting, 在每个时间点上，我们只有有限的数据。   
**middle column** plots show all the estimated values (green), and the maximum of these values (dashed black). The maximum is higher than the true value (purple, left plot) almost everywhere. 因为有10个action, 所以有10条绿色的估值函数,因为从紫色真值函数上用了不同的采样点集合, 所以拟合出来的近似函数也都不一样.  黑色虚线的max函数基本上总是高于紫色的真值.  
**right column** plots shows the difference in orange. The blue line in the right plots is the estimate used by Double Q-learning with a second set of samples for each state. The blue line is much closer to zero, indicating less bias.  橙色部分,基本都是正的, 说明有向上的偏差.蓝色的double则可以减少overoptimism.

![image-20200408030503204](/img/2020-04-01-DQN.assets/image-20200408030503204.png)

这个实验说明, 对于不同的真值函数, 都会被高估.  第二行, 拟合函数flexibility灵活度不够, 造成有些sample点的估值不对;  第三行, 拟合函数灵活度很高 , 但这导致了对于没有见过的state的估值误差非常大. 这点很重要, 因为RL中, 喜欢用NN.  
该例也说明, 即使假设我们在某些状态下有真实的行动值样本，也会出现overestimation。如果我们从已经过于乐观的行动值中bootstrap，那么估计值会进一步恶化，因为这将导致高估值在整个估计中传播。虽然统一的高估值可能不会对结果的政策造成损害，但实际,  overestimation errors will differ for different states and actions.   
**Overestimation** combined with **bootstrapping** then has the pernicious effect of **propagating** the **wrong relative information** about which states are more valuable than others, directly affecting the quality of the learned policies.  传播错误的相对信息，即哪些状态比其他状态更有价值，直接影响到所学策略的质量。

overestimations 与 optimism in the face of uncertainty 不能混淆. 后者是 exploration bonus 探索的奖励, 有助于去学习最优策略.  overestimations occur only after updating , 高估是取max更新造成的, 是有害的.



#### Double DQN

The idea of Double Q-learning is to reduce overestimations by **decomposing** the **max** operation in the target into action **selection** and action **evaluation**.   
虽然没有完全解耦(decoupled),但DQN里面就天然有target network. 所以无需引入新的网络.  使用online网络来选择max, 使用target network 来evaluate. 其他不用改.
$$
Y_{t}^{\text {DoubledQN }} \equiv R_{t+1}+\gamma Q\left(S_{t+1}, \underset{a}{\operatorname{argmax}} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right), \boldsymbol{\theta}_{t}^{-}\right)
$$



#### Empirical results

本节分析了 overestimations of DQN, 表明了 DoubleDQN 提升了 **value accuracy** and **policy quality**.

网络结构与之前的DQN一样, 3 CNN and 1 fully-connected hidden layer (1.5M parameters). 

每个游戏,在一个GPU上训练, 2亿帧,一个礼拜. trained on a single GPU for 200M frames, or approximately 1 week.

##### Results on overoptimism

![image-20200408112546474](/img/2020-04-01-DQN.assets/image-20200408112546474.png)

如图, DQN与Double DQN, 完全一样的设置, 就是更新公式不一样.   
第一行说明, Qvalue 比 水平线的真值要高估.  真值是 actual discounted value of the best learned policy , 直接run学到的最好策略, 然后累积reward.  Qvalue的 (averaged) value estimates are computed regularly during training with full evaluation phases of length T = 125, 000, $\frac{1}{T} \sum_1^T \arg\max_a Q(S_T, a; \theta)$.  如果没有高估情况的话, 这两个线的右边应该一样.    
蓝色的DoubleDQN不仅改善了高估, 还改善了策略. 

有两个游戏, 高估情况非常严重. 最下面一行显示了, 如果DQN开始高估了, 那么策略就开始退化. 之前都认为是off-policy learning with function approximation 算法本身的不稳定性造成的. 但DoubleDQN却表现的很稳.说明, 不稳定是高估造成的. **instabilities is in fact Q- learning’s overoptimism**.

##### Quality of the learned policies

下面讨论 DoubleDQN减少Overoptimism对策略质量的提升

对某些简单问题, 高估并不一定影响策略的质量;  但降低高估值可以显著地提高学习的稳定性. 

评估学好的策略的方法跟上篇一样. each evaluation episode starts by executing a special no-op action that does not affect the environment up to 30 times, to provide different starting points for the agent. 评估的时候,开头会有些空操作,让开局不一样.  The learned policies are evaluated for **5 mins** of emulator time (18,000 frames) with an ε-greedy policy where ε = 0.05.The scores are averaged over 100 episodes. Double与DQN之前用的超参都一样, 所以这样对DQN更有利一些,因为之前是为DQN 调参过.

normalize the score for each game:
$$
\text {score}_\text {normalized} =\frac{\text { score }_\text{agent }-\text { score }_{\text {random }}}{\text { score }_{\text {human }}-\text { score }_{\text {random }}}
$$

$$
\begin{array}{|c|rcc|}
\hline & \text { DQN } & \text { Double DQN }   \\
\hline \text { Median } & 93.5 \% & 114.7 \%   \\
\hline \text { Mean } & 241.1 \% & 330.3 \%   \\
\hline
\end{array}
$$

可以看到整体上都有提升, 并且有个别游戏提升特别大.

Gorila obtained median and mean normalized scores of 96% and 495%

##### Robustness to Human starts

作者担心之前的评估方式,对deterministic游戏, DQN可以对sequence的死记硬背,不能泛化. solution 不够 robust. 那么 By testing the agents from various starting points, 可以测试solution是不是能泛化的好.

使用人类的trajectory, 得到100个start points , 再run 108,000 frames(**30 mins** at 60Hz including the trajectory before the starting point). 直接累加之后的reward.

这次对DoubleDQN也tuned.   所有下面的手段都提升了结果

1. 增加了在交换target network参数之间的训练次数10,000 to 30,000, 以进一步减少高估, 因为每次交换参数之后,  Double Q-learning 就会退化到Q-learning. 
2. learning的时候减少 ε = 0.01 , 评估的时候用  ε = 0.001 
3. tuned version uses a single shared bias for all action values in the top layer of the network. 最后一层使用一个共享的bias. 


$$
\begin{array}{|c|rcc|}
\hline & \text { DQN } & \text { Double DQN } & \text { Double DQN (tuned) } \\
\hline \text { Median } & 47.5 \% & 88.4 \% & 116.7 \% \\
\hline \text { Mean } & 122.0 \% & 273.1 \% & 475.2 \% \\
\hline
\end{array}
$$


Gorila DQN  median  78%   mean   259%

Double DQN 健壮性更好, 并不是利用确定性来死记硬背,在寻找一般性的解决方案方面取得了进展.  Double DQN appears more robust to this more challenging evaluation, suggesting that appropriate generalizations occur and that the found solutions do **not exploit the determinism** of the environments. This is appealing, as it indicates progress towards finding general solutions rather than a deterministic sequence of steps that would be less robust.

<img src="/img/2020-04-01-DQN.assets/image-20200408160552859.png" alt="image-20200408160552859" style="zoom:50%;" />

#### Discussion

1. 证明为什么会高估, 因为固有的估计误差. why Q-learning can be overoptimistic in large-scale problems, even if these are deterministic, due to the inherent estimation errors of learning.
2. overestimations are more common and severe in practice than pre- viously acknowledged.
3. Double Q-learning can be used at scale to successfully reduce this overoptimism, resulting in more stable and reliable learning.
4. 利用 DQN 实现 Double DQN
5. Double DQN finds better policies




## Reference

Deep Reinforcement Learning with Double Q-learning  https://arxiv.org/abs/1509.06461























