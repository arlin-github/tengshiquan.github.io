---
layout:     post
title:      DeepStack
subtitle:   DeepStack:Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-dice.jpg"
catalog: true
tags:
    - AI
    - Game Theory 
    - Imperfect Information

---



# DeepStack: Expert-Level Artificial Intelligence in Heads-Up No-Limit Poker

2017 



### Abstract

近些年来，人工智能领域出现了很多突破，其中游戏往往被用作重要的里程碑。过去实现那些成功的游戏的一个常见的特征是它们都具有完美信息（perfect information）的性质。扑克是一个典型的不完美信息（imperfect information）游戏，而且其一直以来都是人工智能领域内的一个难题。在这篇论文中，我们介绍了 DeepStack，这是一种用于扑克这样的不完美信息环境的新算法。它结合了回归推理（recursive reasoning）来处理信息不对称性，还结合了分解（decomposition）来将计算集中到相关的决策上，以及一种形式的直觉（intuition）——该直觉可以使用深度学习进行自我玩牌而自动学习到。在一项涉及到 44000 手扑克的研究中，DeepStack 在一对一无限制德州扑克（heads-up no-limit Texas hold'em）上击败了职业扑克玩家。这种方法在理论上是可靠的，并且在实践中也能得出比之前的方法更难以被利用的策略。



### Introduction

perfect information是很多游戏成功关键 , 例如可以 play的时候 local search 

asymmetric information 不对称信息

Heads-up no-limit Texas hold’em (HUNL)   一对一无限注德州扑克 ;  之前的AI是固定下注, $10^{14}$ 个decision points.  围棋, $10^{170}$ 个决策点.  information game HUNL与围棋相当, 有$10^{160}$ . 

reasoning 推理的重要性:  不完全信息博弈比同样大小的完全信息博弈需要更复杂的推理。在某一特定时刻的正确决策取决于对手所掌握的私人信息的概率分布，而这些信息是通过对手过去的行动所揭示的。然而，我们的对手的行动如何揭示出这些信息，取决于他们对我们的私人信息的了解，以及我们的行动如何揭示出这些信息。**这种递归推理是不能轻易对博弈情境进行孤立推理的原因，这也是完美信息博弈的启发式搜索方法的核心。**不完全信息博弈中的竞争性人工智能方法通常会对整个博弈进行推理，并在博弈前产生一个完整的策略（14-16）。**反事实遗憾最小化（CFR）**（14,17,18）就是这样一种技术，它通过在连续的迭代中对自己的策略进行递归推理，利用自我博弈来做递归推理。如果博弈太大，无法直接解决，常见的反应是解决一个较小的、抽象的博弈。要想玩好原始博弈，就要把原始博弈中的情境和动作翻译成抽象博弈。

尽管这种方法使程序在HUNL这样的游戏中进行推理是可行的，但它是通过将HUNL的$10^{160}$种情况压缩到$10^{14}$种抽象情况的顺序来实现的。可能是由于这种信息丢失的结果，这样的程序在专家级的人类游戏中落后于专家级的人类。在2015年，计算机程序Claudico以91 mbb/g的优势输给了一个职业扑克玩家团队，这是一个 "巨大的胜率"(20)。此外，最近有研究表明，在年度计算机扑克大赛中，基于抽象概念的程序存在巨大的缺陷(21)。我们使用一种局部最佳反应技术对四种这样的程序（包括2016年比赛中的顶级程序）进行了评估，该技术产生了一个关于策略能输多少的近似下限。这四种基于抽象的程序都能以超过3000mbb/g 被击败，这是每场比赛简单folding的四倍。

DeepStack采取了一种根本不同的方法。它继续使用CFR的递归推理来处理信息不对称。然而，它不计算和存储一个完整的策略，因此不需要显式抽象。相反，它考虑的是每一个特定的情况，因为它是在博弈过程中出现的，但不是孤立的。它通过用一个快速的近似估计来代替超过一定深度的计算，避免了对整个游戏的剩余部分进行推理。这种估计可以被认为是DeepStack的直觉：在任何可能的牌局中，对任何可能的私人牌的价值的直觉。最后，DeepStack的直觉，就像人类的直觉一样，需要进行训练。我们用深度学习（22）训练它，使用从随机扑克牌情境中生成的例子。我们表明，DeepStack在理论上是健全的，产生的策略比基于抽象的技术更难利用，并且在统计学意义上击败了HUNL的职业扑克玩家。



### DeepStack

DeepStack是一个用于一大类 顺序的不完全信息博弈的通用算法。为了清楚起见，我们将在HUNL游戏中描述它的操作。扑克游戏的状态可以分为玩家的私人信息、两张牌面朝下发的手牌和公共状态，公共状态由牌面朝上铺在桌面上的牌和玩家的下注动作序列组成。游戏中可能的公共状态序列形成了一个公共树，每个公共状态都有一个相关的公共子树（图1）。

<img src="2020-04-16-DeepStack.assets/image-20200501022903288.png" alt="image-20200501022903288" style="zoom:50%;" />

图1 , HUNL中的公共树的一部分。节点代表公共状态，而边缘代表行动：红色和蓝绿色表示玩家的下注行动，绿色代表偶然发现的公共牌。游戏在终端节点结束，显示为一个带有相关值的筹码。对于没有玩家fold的终端节点，其私人牌形成更大的组合的玩家收到该状态的价值。

玩家的策略定义了每个决策点的有效行动的概率分布，其中一个决策点是公共状态和行动玩家的手牌的组合。给出一个玩家的策略，对于任何公共状态，我们可以计算出玩家的范围，也就是在达到公共状态的情况下，玩家可能的手牌的概率分布。

在固定了双方的策略后，在游戏结束的终端公共状态下，特定玩家在终端公共状态下的效用是由游戏规则确定的报酬矩阵决定的玩家范围的双线性函数。玩家在任何其他公共状态（包括初始状态）下的预期效用是在玩家的固定策略下，在可达到的终端状态上的预期效用。最佳反应策略是指玩家的预期效用最大化的策略，它与对手的策略相比，玩家的预期效用最大化。在双人零和博弈中，如HUNL，当与最佳反应对手策略对弈时，解或纳什均衡策略(23)使预期效用最大化。策略的可利用性是指在对阵最佳反应对手时的预期效用与纳什均衡下的预期效用之差。











