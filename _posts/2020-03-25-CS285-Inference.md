---
layout:     post
title:      CS 285. Reframing Control as an Inference Problem
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

 

## Reframing Control as an Inference Problem

Inference, 推理,推断.  怎么给模型提供一个解释.     逆强化学习的基础

各种soft算法的理论

### Lecture

1. Does reinforcement learning and optimal control provide a reasonable model of human behavior?
2. Is there a better explanation?
3. Can we derive optimal control, reinforcement learning, and planning as *probabilistic inference*?
4. How does this change our RL algorithms?
5. (next week) We’ll see this is crucial for *inverse* reinforcement learning

#### Goals:

- Understand the connection between inference and control
- Understand how specific RL algorithms can be instantiated in this framework 
- Understand why this might be a good idea





#### Optimal Control as a Model of Human Behavior

- 之前都是拿到reward函数, 解决最优化问题, 给出plan或者 policy.
- 现在要利用最优化理论来解释观察到的行为数据.  如果观察到一个人的行为, 那么能不能利用最优化理论,  推导出其背后的动机(reward函数).

optimize this to explain the data
$$
\begin{aligned}
&\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}=\arg \max _{\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}} \sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\\
&\mathbf{s}_{t+1}=f\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\\
\\
&\pi=\arg \max _{\pi} E_{\mathbf{S}_{t+1} \sim p\left(\mathbf{s}_{t+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right), \mathbf{a}_{t} \sim \pi\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\\
&\mathbf{a}_{t} \sim \pi\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)
\end{aligned}
$$


#### What if the data is not optimal?

<img src="/img/CS285.assets/image-20200326113446553.png" alt="image-20200326113446553" style="zoom:33%;" /> <img src="/img/CS285.assets/image-20200326113506556.png" alt="image-20200326113506556" style="zoom:33%;" />

猴子到达奖励点的路线并不是直线, 可能猴子只是想花最小的成本拿到奖励, 而不是费精力去走最优化的路线. 

那么能不能提出一个新的随机最优控制模型来让上面常见的弯路出现的概率高, 不常见的路径出现的概率低. 

之前的强化学习都是会产出两点之间的直线这一个策略.

- some mistakes matter more than others!  一些错误比较常见
- behavior is **stochastic**
- but good behavior is still the most likely



### A probabilistic graphical model of decision making

提出一个概率图模型, 可以解释 好的行为 为什么 概率高, 但各种各样的好的行为可能出现的概率一样高.  要能解释为什么猴子会有一点点出错. 



$$p( \underbrace{\mathbf{s}' \vert \mathbf{s},\mathbf{a} }_{\tau}) = ??$$  no assumption of optimal behavior! 这个公式并不能解释猴子为什么总的趋势还是去目标点, 只是描述了这个轨迹的概率. 



<img src="/img/CS285.assets/image-20200326115712731.png" alt="image-20200326115712731" style="zoom:33%;" />

引入新变量: $$\mathcal O$$ , 表示当前是否是最优的.  是个binary ,  true, 表明你在努力成为最优的, false , 你在随机做action.  对于猴子, 可以认为他是个好猴子, trying to be optimal, 当整个路径中总会有点情况, 所以会有一点随机. 

可以给整个轨迹配一个这样的变量; 但在轨迹的每个点都配上这个变量, 用起来很方便, 表示每个点的意图. 

$$p(\tau \vert \mathcal{O}_{1:T})$$ ,  下面推断概率, 假定猴子在每个时间步都是optimal, 看见一个特定轨迹的概率. 所以这个是猴子行为的generative model. 如果模型不错, 会给到常见路径比较高的几率. 

$$p(\mathcal{O}_t \vert \mathbf{s}_t,\mathbf{a}_t)\propto\exp(r(\mathbf{s}_t,\mathbf{a}_t))$$ , 假设, 每步是不是optimal与该步reward是正比关系. 后面会发现该假设非常的方便计算. 
$$
p(\tau \vert \mathcal{O}_{1:T})=\frac{p(\tau,\mathcal{O}_{1:T})}{p(\mathcal{O}_{1:T})}\propto p(\tau)\prod_t\exp(r(\mathbf{s}_t,\mathbf{a}_t))=p(\tau)\exp\left(\sum_tr(\mathbf{s}_t,\mathbf{a}_t)\right)
$$
上式最后的部分, 一个表示该轨迹出现的概率, 后面表示该轨迹的总reward的指数, 



- Can model **suboptimal** behavior (important for inverse RL)
- Can apply **inference** algorithms to solve control and planning problems
- Provides an explanation for why stochastic behavior might be preferred (useful for exploration and transfer learning)



#### Inference = planning

how to do inference?

1. compute backward messages $$\beta_t(\mathbf{s}_t,\mathbf{a}_t)=p(\mathcal{O}_{t:T} \vert \mathbf{s}_t,\mathbf{a}_t)$$  , 从t时刻, 在s_t执行a_t, 到最后,  optimal的概率,  probability that we can be optimal at steps t through T given that we take action at in state St
2. compute policy  $$\pi(\mathbf{a}_t \vert \mathbf{s}_t)=p(\mathbf{a}_t \vert \mathbf{s}_t,\mathcal{O}_{1:T})$$, 有了上面1, 容易计算整个
3. compute forward messages  $$\alpha_t(\mathbf{s}_t)=p(\mathbf{s}_t \vert \mathcal{O}_{1:t-1})$$ , t时刻之前都是optimal的话, t时刻在特定s_t的几率



##### Backward messages

$$
\begin{aligned} \beta_t(\mathbf{s}_t,\mathbf{a}_t)&=p(\mathcal{O}_{t:T} \vert \mathbf{s}_t,\mathbf{a}_t) \\&=\int p(\mathcal{O}_{t:T},\mathbf{s}_{t+1} \vert \mathbf{s}_t,\mathbf{a}_t)\mathrm{d}\mathbf{s}_{t+1} \\ &=\int p(\mathcal{O}_{t+1:T} \vert \mathbf{s}_{t+1})p(\mathbf{s}_{t+1} \vert \mathbf{s}_t,\mathbf{a}_t)p(\mathcal{O}_t \vert \mathbf{s}_t,\mathbf{a}_t)\mathrm{d}\mathbf{s}_{t+1}

\end{aligned}
$$

$$
p(\mathcal{O}_{t+1:T} \vert \mathbf{s}_{t+1})=\int \underbrace{p(\mathcal{O}_{t+1:T} \vert \mathbf{s}_{t+1},\mathbf{a}_{t+1})}_{\beta_t(\mathbf{s}_{t+1},\mathbf{a}_{t+1})}p(\mathbf{a}_{t+1} \vert \mathbf{s}_{t+1})\mathrm{d}\mathbf{a}_{t+1}
$$

$$p(\mathbf{a}_{t+1} \vert \mathbf{s}_{t+1})$$ 表示s下选择a的可能性, 先选择均匀分布, 后面为解释为啥这个不影响. 

$$
\begin{aligned}
&\text{for } t= T-1 \text{to} 1: \\
& \quad \beta_t(\mathbf{s}_t,\mathbf{a}_t)=p(\mathcal{O}_t \vert \mathbf{s}_t,\mathbf{a}_t)\mathbf{E}_{\mathbf{s}_{t+1}\sim p(\mathbf{s}_{t+1} \vert \mathbf{s}_t,\mathbf{a}_t)}[\beta_{t+1}(\mathbf{s}_{t+1})] \\
&\quad \beta_t(\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim p(\mathbf{a}_t \vert \mathbf{s}_t)}[\beta_t(\mathbf{s}_t,\mathbf{a}_t)]
\end{aligned}
$$



##### A closer look at the backward pass

有些类似 value iteration. 

- let $$V_{t}\left(\mathbf{s}_{t}\right)=\log \beta_{t}\left(\mathbf{s}_{t}\right)$$
- let $$Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\log \beta_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$

- $$V_{t}\left(\mathbf{s}_{t}\right)=\log \int \exp \left(Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t}$$ .  (“soft max”)
- $$V_{t}\left(\mathbf{s}_{t}\right) \rightarrow \max _{\mathbf{a}_{t}} Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$ ,  as $$Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$ gets bigger!



**Value iteration algorithm**:

1. Set  $$Q(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V(\mathbf{s}')]$$
2. Set  $$V(\mathbf{s})=\max_\mathbf{a}Q(\mathbf{s},\mathbf{a})$$



- $$Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log E\left[\exp \left(V_{t+1}\left(\mathbf{s}_{t+1}\right)\right)\right]$$ .   “optimistic” transition (not a good idea!)  
- deterministic transition:    $$Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+V_{t+1}\left(\mathbf{s}_{t+1}\right)$$
- we'll come back to the stochastic case later!



##### The action prior

what if the action prior is not uniform?  现在去掉上面 $$p(\mathbf{a}_{t+1} \vert \mathbf{s}_{t+1})$$ 用的均匀分布的假设 .

- $$V_t(\mathbf{s}_t)=\log\int\exp(Q_t(\mathbf{s}_t,\mathbf{a}_t)+\log p(\mathbf{a}_t \vert \mathbf{s}_t))\mathrm{d}\mathbf{a}_t$$ .
- $$Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log E\left[\exp \left(V\left(\mathbf{s}_{t+1}\right)\right)\right]$$ .



let $$\tilde{Q}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log p\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)+\log E\left[\exp \left(V\left(\mathbf{s}_{t+1}\right)\right)\right]$$

$$V\left(\mathbf{s}_{t}\right)=\log \int \exp \left(\tilde{Q}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) \mathbf{a}_{t} \quad \Leftrightarrow \quad V\left(\mathbf{s}_{t}\right)=\log \int \exp \left(Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log p\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right) \mathbf{a}_{t}$$

can **always** fold the action prior into the reward! uniform action prior can be assumed without loss of generality .  可以并到reward里面去.



#### Policy computation

compute policy : $$p(\mathbf{a}_t \vert \mathbf{s}_t,\mathcal{O}_{1:T})$$
$$
\begin{aligned}
p(\mathbf{a}_t \vert \mathbf{s}_t,\mathcal{O}_{1:T}) 
&= \pi(\mathbf{a}_t \vert \mathbf{s}_t)
\\&=  p(\mathbf{a}_t \vert \mathbf{s}_t,\mathcal{O}_{t:T})
\\&= \frac{p(\mathbf{a}_t,\mathbf{s}_t \vert \mathcal{O}_{t:T})}{p(\mathbf{s}_t \vert \mathcal{O}_{t:T})}
\\&=\frac{p(\mathcal{O}_{t:T} \vert \mathbf{a}_t,\mathbf{s}_t)p(\mathbf{a}_t,\mathbf{s}_t)/p(\mathcal{O}_{t:T})}{p(\mathcal{O}_{t:T} \vert \mathbf{s}_t)p(\mathbf{s}_t)/p(\mathcal{O}_{t:T})}
\\&=\frac{p(\mathcal{O}_{t:T} \vert \mathbf{a}_t,\mathbf{s}_t)}{p(\mathcal{O}_{t:T} \vert \mathbf{s}_t)}\frac{p(\mathbf{a}_t,\mathbf{s}_t)}{p(\mathbf{s}_t)}
\\&=\frac{\beta_t(\mathbf{s}_t,\mathbf{a}_t)}{\beta_t(\mathbf{s}_t)}p(\mathbf{a}_t \vert \mathbf{s}_t)
\end{aligned}
$$
先验$$p(\mathbf{a}_t \vert \mathbf{s}_t)$$，如果我们假设先验是均匀分布的话那么后者是常数，因此可以去掉. 

得到
$$
\pi(\mathbf{a}_t \vert \mathbf{s}_t)=\frac{\beta_t(\mathbf{s}_t,\mathbf{a}_t)}{\beta_t(\mathbf{s}_t)}
$$


##### Policy computation with value functions

for $$t=T-1$$ to 1:

- $$Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log E\left[\exp \left(V_{t+1}\left(\mathbf{s}_{t+1}\right)\right)\right]$$ .
- $$V_{t}\left(\mathbf{s}_{t}\right)=\log \int \exp \left(Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) \mathbf{a}_{t} $$ .

$$
\pi\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)=\frac{\beta_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}{\beta_{t}\left(\mathbf{s}_{t}\right)} \quad
$$

$$
\pi\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)=\exp \left(Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-V_{t}\left(\mathbf{s}_{t}\right)\right)=\exp \left(A_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)
$$

一个变种 , 类似于 discount $$\gamma$$ . 

with temperature : 
$$
\pi\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)=\exp  \left(\frac{1}{\alpha}Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\frac{1}{\alpha}V_{t}\left(\mathbf{s}_{t}\right)\right)=\exp \left(\frac{1}{\alpha}A_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)
$$


##### Policy computation summary

- Natural interpretation: better actions are more probable 
- Random **tie-breaking**
- Analogous to **Boltzmann exploration**
- Approaches greedy policy as temperature decreases



#### Forward messages

$$\alpha_t(\mathbf{s}_t)=p(\mathbf{s}_t \vert \mathcal{O}_{1:t-1})$$,  对逆强化学习很重要. 





#### Forward/backward message intersection

<img src="/img/CS285.assets/image-20200326162128880.png" alt="image-20200326162128880" style="zoom:33%;" />

这整个理论和HMM很像。注意到这个概率是前向消息和后项消息的乘积，可以说是一个交汇。考虑上图左边圆点是起点，右边叉叉是终点的路径。黄色锥区域是能够有高概率到达终点的状态，蓝色锥区域是有高概率从初始状态以高收益到达的状态，然后基本上就是两者的交（概率上相乘）。Li and Todorov (2006) 做了人和猴子类似从一个点到另一个点的实验，记录空间位置变化，基本上也是中间部分方差最大。



### Summary

1. Probabilistic graphical model for optimal control

2. Control = inference (similar to HMM, EKF, etc.)
3. Very similar to dynamic programming, value iteration, etc. (but “soft”)







### The optimism problem

$$
\begin{aligned}&\text{for } t= T-1 \text{ to } 1: \\& \quad \beta_t(\mathbf{s}_t,\mathbf{a}_t)=p(\mathcal{O}_t \vert \mathbf{s}_t,\mathbf{a}_t)\mathbf{E}_{\mathbf{s}_{t+1}\sim p(\mathbf{s}_{t+1} \vert \mathbf{s}_t,\mathbf{a}_t)}[\beta_{t+1}(\mathbf{s}_{t+1})] \\&\quad \beta_t(\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim p(\mathbf{a}_t \vert \mathbf{s}_t)}[\beta_t(\mathbf{s}_t,\mathbf{a}_t)]\end{aligned}
$$

- let $$V_{t}\left(\mathbf{s}_{t}\right)=\log \beta_{t}\left(\mathbf{s}_{t}\right)$$
- let $$Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\log \beta_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$

$$Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\log E\left[\exp \left(V_{t+1}\left(\mathbf{s}_{t+1}\right)\right)\right]$$ .   “optimistic” transition (not a good idea!)  

why did this happen?

- the inference problem: $$\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}  \vert  \mathcal{O}_{1: T}\right)$$ 
- marginalizing and conditioning, we get: $$p\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}, \mathcal{O}_{1: T}\right)$$ (the policy) we want this
  - "given that you obtained high reward, what was your action probability?" 
- marginalizing and conditioning, we get: $$p\left(\mathbf{s}_{t+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}, \mathcal{O}_{1: T}\right) \neq p\left(\mathbf{s}_{t+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right)$$ 与真实的转移概率不一样 ,  but not this!  运气很好的情况下,中彩票了, 这时的转移概率是啥, 这时的转移概率肯定会估的很乐观. 对control来说没啥用, 因为不能预设自己一定能得到高回报.
  - "given that you obtained high reward, what was your transition probability?

- "given that you obtained high reward, what was your action probability, given that your transition probability did not change?" 就是你运气还是一般, 但是得到了高回报,这时的转移概率是多少.  这个假设要work 需要一个trick.
- can we find another distribution $$q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)$$ that is close to $$p\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}  \vert  \mathcal{O}_{1: T}\right)$$ but has dynamics $$p\left(\mathbf{s}_{t+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right)$$
- where have we seen this before? $$\quad$$ let $$\mathbf{x}=\mathcal{O}_{1: T}$$ and $$\mathbf{z}=\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right) \quad$$ find $$q(\mathbf{z})$$ to approximate $$p(\mathbf{z}  \vert  \mathbf{x})$$
- let's try variational inference!



#### Control via variational inference

- let $$q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)=p\left(\mathbf{s}_{1}\right) \prod_{t} p\left(\mathbf{s}_{t+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right) q\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)$$

  - same dynamics and only new thing initial state as $$p$$ 
  - q only new thing

- let $$\mathbf{x}=\mathcal{O}_{1: T}$$ and $$\mathbf{z}=\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right) \quad$$

- <img src="/img/CS285.assets/image-20200326172239697.png" alt="image-20200326172239697" style="zoom:33%;" /> 

  - $$p(\mathbf{s}_{1: T}, \mathbf{a}_{1: T} \vert \mathcal{O}_{1: T} ) = p(\mathbf{z} \vert \mathbf{x})$$ .

- <img src="/img/CS285.assets/image-20200326172441315.png" alt="image-20200326172441315" style="zoom:33%;" />

  - $$q(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}  ) = q(\mathbf{z} )$$ .

  

#### The variational lower bound

- $$\log p(\mathbf{x}) \geq E_{\mathbf{z} \sim q(\mathbf{z})}[\log p(\mathbf{x}, \mathbf{z})-\log q(\mathbf{z})]$$ .

- $$\log q(\mathbf{z})$$:   the entropy   $$\mathcal{H}(q) $$ 

- let $$q\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right)=p\left(\mathbf{s}_{1}\right) \prod_{t} p\left(\mathbf{s}_{t+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right) q\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)$$.

  

$$
\begin{aligned}
\log p\left(\mathcal{O}_{1: T}\right) \geq E_{\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right) \sim q} &\left[\log  p\left(\mathbf{s}_{1}\right)+\sum_{t=1}^{T} \log p\left(\mathbf{s}_{\mathcal{t}+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right)+\sum_{t=1}^{T} \log p\left(\mathcal{O}_{t}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right)\right.\\
&\left. {-\log p\left(\mathbf{s}_{1}\right)} -\sum_{t=1}^{T} \log p\left(\mathbf{s}_{\mathcal{t}+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right)-\sum_{t=1}^{T} \log q\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right] \\
=& E_{\left(\mathbf{s}_{1: T}, \mathbf{a}_{1: T}\right) \sim q}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log q\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right] \\
=& \sum_{t} E_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim q}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\mathcal{H}\left(q\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right)\right]
\end{aligned}
$$

​	      maximize reward and maximize action entropy!  跟强化学习的目标很像, 只是最后多了一个entropy 项. 



##### Optimizing the variational lower bound

推导过程没讲,  结论就是 得到了常规 bellman 方程的.



#### Backward pass summary - variational

for $$t=T-1$$ to 1:

1. $$Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+E\left[\left(V_{t+1}\left(\mathbf{s}_{t+1}\right)\right]\right.$$ . 
2. $$V_{t}\left(\mathbf{s}_{t}\right)=\log \int \exp \left(Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t}$$ . 



value iteration algorithm:
1. set $$Q(\mathbf{s}, \mathbf{a}) \leftarrow r(\mathbf{s}, \mathbf{a})+\gamma E\left[V\left(\mathbf{s}^{\prime}\right)\right]$$
2. set $$V(\mathbf{s}) \leftarrow \max _{\mathbf{a}} Q(\mathbf{s}, \mathbf{a})$$



soft value iteration algorithm:

1. set $$Q(\mathbf{s}, \mathbf{a}) \leftarrow r(\mathbf{s}, \mathbf{a})+\gamma E\left[V\left(\mathbf{s}^{\prime}\right)\right]$$ .
2. set $$V(\mathbf{s}) \leftarrow$$ soft $$\max _{\mathbf{a}} Q(\mathbf{s}, \mathbf{a})$$



#### Summary

variants:

- discounted $$\operatorname{SOC}: Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma E\left[V_{t+1}\left(\mathbf{s}_{t+1}\right)\right]$$
- explicit temperature: $$V_{t}\left(\mathbf{s}_{t}\right)=\alpha \log \int \exp \left(\frac{1}{\alpha} Q_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right) d \mathbf{a}_{t}$$



For more details, see: Levine. (2018). Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review.



#### Q-learning with soft optimality

- standard Q-learning: $$\phi \leftarrow \phi+\alpha \nabla_{\phi} Q_{\phi}(\mathbf{s}, \mathbf{a})\left(r(\mathbf{s}, \mathbf{a})+\gamma V\left(\mathbf{s}^{\prime}\right)-Q_{\phi}(\mathbf{s}, \mathbf{a})\right)$$
- target value: $$V\left(\mathbf{s}^{\prime}\right)=\max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)$$



- **soft Q-learning**: $$\phi \leftarrow \phi+\alpha \nabla_{\phi} Q_{\phi}(\mathbf{s}, \mathbf{a})\left(r(\mathbf{s}, \mathbf{a})+\gamma V\left(\mathbf{s}^{\prime}\right)-Q_{\phi}(\mathbf{s}, \mathbf{a})\right)$$
- target value: $$V\left(\mathbf{s}^{\prime}\right)=\operatorname{soft} \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)=\log \int \exp \left(Q_{\phi}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right) d \mathbf{a}^{\prime}$$
- $$\pi(\mathbf{a}  \vert  \mathbf{s})=\exp \left(Q_{\phi}(\mathbf{s}, \mathbf{a})-V(\mathbf{s})\right)=\exp (A(\mathbf{s}, \mathbf{a}))$$.  策略函数也变成了类似Boltzmann探索的



1. take some action $$\mathbf{a}_{i}$$ and observe $$\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right),$$ add it to $$\mathcal{R}$$
2. sample mini-batch $$\left\{\mathbf{s}_{j}, \mathbf{a}_{j}, \mathbf{s}_{j}^{\prime}, r_{j}\right\}$$ from $$\mathcal{R}$$ uniformly
3. compute $$y_{j}=r_{j}+\gamma \operatorname{soft} \max _{\mathbf{a}_{j}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \mathbf{a}_{j}^{\prime}\right)$$ using target network $$Q_{\phi^{\prime}}$$
4. $$\phi \leftarrow \phi-\alpha \sum_{j} \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{j}, \mathbf{a}_{j}\right)\left(Q_{\phi}\left(\mathbf{s}_{j}, \mathbf{a}_{j}\right)-y_{j}\right)$$ .
5. update $$\phi^{\prime}:$$ copy $$\phi$$ every $$N$$ steps, or Polyak average $$\phi^{\prime} \leftarrow \tau \phi^{\prime}+(1-\tau) \phi$$  , goto 1



#### Policy gradient with soft optimality

$$\pi(\mathbf{a}  \vert  \mathbf{s})=\exp \left(Q_{\phi}(\mathbf{s}, \mathbf{a})-V(\mathbf{s})\right)$$ optimizes $$\sum_{t} E_{\pi\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]+E_{\pi\left(\mathbf{s}_{t}\right)}\left[\mathcal{H}\left(\pi\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right)\right]$$

$$\mathcal{H}\left(\pi\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right)$$ : policy entropy

intuition: $$\pi(\mathbf{a}  \vert  \mathbf{s}) \propto \exp \left(Q_{\phi}(\mathbf{s}, \mathbf{a})\right)$$ when $$\pi$$ minimizes $$D_{\mathrm{KL}}\left(\pi(\mathbf{a}  \vert  \mathbf{s})  \Vert  \frac{1}{Z} \exp (Q(\mathbf{s}, \mathbf{a}))\right)$$
$$
D_{\mathrm{KL}}\left(\pi(\mathbf{a}  \vert  \mathbf{s})  \Vert  \frac{1}{Z} \exp (Q(\mathbf{s}, \mathbf{a}))\right)=E_{\pi(\mathbf{a}  \vert  \mathbf{s})}[Q(\mathbf{s}, \mathbf{a})]-\mathcal{H}(\pi)
$$
PG 问题变成了 KL散度最小化问题. 



- often referred to as “entropy regularized” policy gradient combats premature entropy collapse 相当于增加了一个熵的正则化项, 反正policy 变得过于 deterministic.
  - 高斯策略下，策略梯度法通常希望减少策略的方差 , 减少方差一般让策略更好. 但不用natural gradient, 这样会让PG 过早地减少 exploration, 在找到一个好的 mean 之前.
  - 该技术早就有了, 做为一个启发式版本的PG算法 
- turns out to be closely related to soft Q-learning:  下面分析
  - see Haarnoja et al. ‘17 and Schulman et al. ‘17

Ziebart et al. ‘10 “Modeling Interaction via the Principle of Maximum Causal Entropy”



##### Policy gradient vs Q-learning

policy gradient derivation:
$$
J(\theta) = \sum_{t} E_{\pi\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]+E_{\pi\left(\mathbf{s}_{t}\right)} [ \underbrace{\mathcal{H}\left(\pi\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right) }_{\mathbf{E}_{\pi(\mathbf{a}_t \vert \mathbf{s}_t)}[-\log\pi(\mathbf{a}_t \vert \mathbf{s}_t)]}   ] =  \sum_t\mathbf{E}_{\pi(\mathbf{s}_t,\mathbf{a}_t)}[r(\mathbf{s}_t,\mathbf{a}_t)-\log\pi(\mathbf{a}_t \vert \mathbf{s}_t)]
$$

$$
\nabla_\theta \sum_t\mathbf{E}_{\pi(\mathbf{s}_t,\mathbf{a}_t)}[r(\mathbf{s}_t,\mathbf{a}_t)-\log\pi(\mathbf{a}_t \vert \mathbf{s}_t)]
\\ 
\approx\frac{1}{N}\sum_i\sum_t\nabla_\theta\log\pi_\theta(\mathbf{a}_t \vert \mathbf{s}_t)\left(r(\mathbf{s}_t,\mathbf{a}_t)+ \underbrace {\left( \sum_{t'=t+1}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})-\log\pi(\mathbf{a}_{t'} \vert \mathbf{s}_{t'}) \right)}_{ \approx Q(\mathbf{s}_{t+1},\mathbf{a}_{t+1})}-\log\pi(\mathbf{a}_t \vert \mathbf{s}_t)-1\right)
$$

最后的1 , 是baseline , can ignore ; 又由于  $$\log\pi(\mathbf{a}_t \vert \mathbf{s}_t)=Q(\mathbf{s}_t,\mathbf{a}_t)-V(\mathbf{s}_t) $$ ,代入上式
$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_i\sum_t(\nabla_\theta Q(\mathbf{a}_t \vert \mathbf{s}_t)-\nabla_\theta V(\mathbf{s}_t))\left(r(\mathbf{s}_t,\mathbf{a}_t)+Q(\mathbf{s}_{t+1},\mathbf{a}_{t+1})-Q(\mathbf{s}_t,\mathbf{a}_t)+V(\mathbf{s}_t)\right)
$$
最后的V也可以看做baseline, 可以忽略. 

soft Q-learning:  off-policy
$$
-\frac{1}{N}\sum_i\sum_t\nabla_\theta Q(\mathbf{a}_t \vert \mathbf{s}_t)\left(r(\mathbf{s}_t,\mathbf{a}_t)+\text{soft}\max_{\mathbf{a}_{t+1}}Q(\mathbf{s}_{t+1},\mathbf{a}_{t+1})-Q(\mathbf{s}_t,\mathbf{a}_t)\right)
$$
符号不同是因为Q学习是梯度下降，而策略梯度法是梯度上升。



#### Benefits of soft optimality

- **Improve exploration** and prevent entropy collapse
- Easier to specialize (finetune) policies for more specific tasks
- **Principled approach to break ties**
- Better **robustness** (due to wider coverage of states)
- Can reduce to hard optimality as reward magnitude increases  
- Good model for modeling human behavior (more on this later)

 



### Review

- Reinforcement learning can be viewed as inference in a graphical model
  - Value function is a backward message
  - Maximize reward and entropy (the bigger the rewards, the less entropy matters)
  - Variational inference to remove optimism

- Soft Q-learning
- Entropy-regularized policy gradient



#### Example

##### Stochastic models for learning control

Stochastic energy-based policies

Soft方法可以让两条线路有更均衡的被选择概率，防止某一块被过度增强，不错失可能性。

Haarnoja*, Tang*, Abbeel, L., Reinforcement Learning with Deep Energy-Based Policies. ICML 2017



##### Stochastic energy-based policies provide pretraining

<img src="/img/CS285.assets/image-20200326192645273.png" alt="image-20200326192645273" style="zoom:33%;" />

<img src="/img/CS285.assets/image-20200326193519134.png" alt="image-20200326193519134" style="zoom:33%;" />

Soft Q探索性更高, 如果知道了哪些方向可以走, 就会丢掉那些不好的方向.  DDPG需要unlearn已经学会的, 再去学好的



<img src="/img/CS285.assets/image-20200326193714845.png" alt="image-20200326193714845" style="zoom:33%;" />



#### Soft optimality suggested readings

- Todorov. (2006). Linearly solvable Markov decision problems: one framework for reasoning about soft optimality.

- Todorov. (2008). General duality between optimal control and estimation: primer on the equivalence between inference and control.

- Kappen. (2009). Optimal control as a graphical model inference problem: frames control as an inference problem in a graphical model.

- Ziebart. (2010). Modeling interaction via the principle of maximal causal entropy: connection between soft optimality and maximum entropy modeling.

- Rawlik, Toussaint, Vijaykumar. (2013). On stochastic optimal control and reinforcement learning by approximate inference: temporal difference style algorithm with soft optimality.

- Haarnoja*, Tang*, Abbeel, L. (2017). Reinforcement learning with deep energy based models: soft Q-learning algorithm, deep RL with continuous actions and soft optimality

- Nachum, Norouzi, Xu, Schuurmans. (2017). Bridging the gap between value and policy based reinforcement learning.

- Schulman, Abbeel, Chen. (2017). Equivalence between policy gradients and soft Q-learning.

- Haarnoja, Zhou, Abbeel, L. (2018). Soft Actor-Critic: Off-Policy Maximum Entropy Deep

  Reinforcement Learning with a Stochastic Actor.

- Levine. (2018). Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review