---
layout:     post
title:      Approximate Dynamic Programming
subtitle:   Note on "Approximate Dynamic Programming" (2014)
date:       2020-01-08 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-sunset.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - Dynamic Programming
    - ADP

---

# Approximate Dynamic Programming

**近似动态规划**  ADP course given by BERTSEKAS at THU, 2014 

比较清晰的教程





### OUTLINE

**Large-scale DP** based on approximations andin part on simulation. 主题是基于近似 (一部分基于仿真) 的大规模动态规划. 

AI或者ML机器学习 叫它**强化学习**，作者叫它**神经动态规划 NDP**或者**近似动态规划 ADP**，目前主要有两个方向，某种程度上是互相融合的，一个方向是强化学习，主要依赖于基于**特征表达**的输入输出之间的函数关系，也可以叫基于观察样本或者仿真进行学习(强化学习特别强调与env交互)，另一个方向是**最优控制理论 optimization/control theory**，关注重点是传统的优化方法和算法，比如策略迭代和值迭代。 

Deals with control of dynamic systems under **uncertainty**, but applies more broadly (e.g., discrete deterministic optimization) ，这个不确定性，不仅仅指随机，还有可能是解决 **minmax** 的问题，这个时候不确定性就可以来源于对手做出的行为。实际上动态规划可以求解动态系统的控制问题，比如离散优化问题和组合最优化问题。

Bellman 提出了**动态规划 (Dynamic Programming)** 方法并给出**“维数灾难”(Curse of Dimensionality)** 的概念，Powell 认为维数灾有三种: 动作空间，状态空间和随机干扰. 



### LECTURE 1

#### DP AS AN OPTIMIZATION METHODOLOGY

从更抽象的角度来看，只有一种优化问题，目标就是最小化cost函数，可以是标量，向量，或者更复杂的。优化的时候要受到约束集合的限制，优化就是从约束集合给出的决策集合 $U$ 中选择合适的决策 $u$ 让目标值最小。



Generic optimization problem: $\min_{u \in U} g(u)$ ,  
where $u$ **optimization/decision variable**, $g(u)$ **cost** function,   $U$  **constraint set**



##### Categories of problems

根据 g 和 U, 对优化问题进行分类

- **Discrete** (U is finite) or **continuous**
- **Linear** (g is linear and U is polyhedral 多边形集) or **nonlinear**
- **Stochastic** or **deterministic**: In stochastic problems,  the cost involves a stochastic parameter w,  $g(u) = E_w \lbrace G(u, w) \rbrace$  ,  env包含随机性, 所以是个期望 ;  确定性问题, 比如旅行商TSP问题



##### DP deals with multistage stochastic problems

动态规划能求解的问题 需要满足某些特性 (马尔科夫性);  这里用的词 stage, 类似于time-step ; DP问题, 大问题拆成子问题, 不一定是按照时间维度去拆分的



#### BASIC STRUCTURE OF STOCHASTIC DP

##### Discrete-time system

**状态转移函数** $x_{k+1}=f_{k}\left(x_{k}, u_{k}, w_{k}\right), \quad k=0,1, \ldots, N-1$  

- $k$  Discrete **time**  
- $x_{k}:$ **State**; summarizes past information that is relevant for future optimization
- $u_{k}:$ **Control**; decision to be selected at time $k$ from a given set 
- $w_{k}:$ Random parameter (also called "disturbance" or "**noise**" depending on the context)  某个状态的中的随机因素
- $N$: **Horizon** 界限 or number of times control is applied 

也可以用  **状态转移概率** $P\left(x_{k+1} \vert x_{k}, u_{k}\right)$ 来描述系统

f 与 p 就是代表着系统, env.

两者缺陷,  有时需要仿真采样来补充

- 状态转移函数f的缺陷是绝大多数难以得到解析表达的函数形式

- 状态转移概率p的缺陷

  - 使用数值概率，状态转移矩阵的空间非常大，甚至会导致计算机内存溢出，如果状态和控制有连续的，数值概率矩阵会有无数个维度

  - 使用解析的概率分布函数, 连续离散都可以表达，但这个解析关系非常难得到


##### Cost function that is additive over time

$$
E \left \{ g_{N} \left(x_{N} \right)+\sum_{k=0}^{N-1} g_{k} \left(x_{k}, u_{k}, w_{k} \right) \right \}
$$

与策略相关; 最后一步策略的t是N-1 , 然后得到的cost是 $g_N$

 

##### INVENTORY CONTROL EXAMPLE

一个例子, 主要看下 x, w的差异.    其实可以把 (x,w) 联合起来看作 state

举一个库存控制的例子，仓库中有一些货物，然后来了一批客户，这时决策者 需要把货物卖给客户满足他们的需求，然后下订单补充货物。在这个库存系统中, $x_k$ 是阶段 k 时的库存状态，比如一个存放汽车的仓库，放了 $x_k$ 数量的汽车，来了几个客户，对汽车的需求是 $w_k$，如果完全满足了他们的需求，库存量 减少为 $x_k −w_k$，下了订货量为 $u_k$ 的订单补充库存，订单到达后 (假设订单到 达前没有卖出汽车给客户) 新的库存状态为 $x_k −w_k +u_k$

##### ASSUMPTIONS

马尔科夫性，或无后效性 MDP.   对于MDP, 都是基于time-step的, 符合Markov性.  DP解法是一个大问题嵌套一个子问题的, 基于状态,子状态维度来的.  在MDP的某个step, 对所有可能出现的state进行一轮DP.

**policies** ( also called feedback control laws)
$$
u_{k}=\mu_{k}\left(x_{k}\right), \quad k=0, \ldots, N-1
$$

这里$\mu$ 函数代表策略$\pi$, 不过是写作函数的形式



#### FINITE-HORIZON PROBLEM

- **System** $x_{k+1}=f_{k}\left(x_{k}, u_{k}, w_{k}\right), k=0, \ldots, N-1$

- **Control contraints** $u_{k} \in U_{k}\left(x_{k}\right)$ 

- **Probability distribution** $P_{k}\left(\cdot \  \vert x_{k}, u_{k}\right)$ of $w_k$ 

- **Policies** $\pi=\{ \mu_{0}, \ldots, \mu_{N-1}\}$ ; $\mu_{k}$ maps  $x_{k}$ into controls $u_k = \mu_k(x_k)$ and  $\mu_k(x_k) \in U_{k}(x_{k})$ for all $x_{k}$  
  这里策略$\pi$, 是用 每个时刻control的取值集合来表示的. 即  $\mu: x \to \pi$ 

- **Expected cost** of $\pi$ starting at $x_{0}$ is :  **cost的期望  J**   一定与一个策略相关
  $$
  J_{\pi} (x_{0} )=E\left\{g_{N} (x_{N} )+\sum_{k=0}^{N-1} g_{k}\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}
  $$
  
- **Optimal cost** function  $J^\ast=\min_{\pi} J_{\pi}\left(x_{0}\right)$ ,  定义式

- **Optimal polity** $\pi^\ast$  satisfies  $J_{\pi^\ast} (x_0) = J^\ast(x_0)$

  

#### PRINCIPLE OF OPTIMALITY 

Let $\pi^ \ast = \{   \mu_{0}^\ast, \mu_{1}^\ast, \ldots, \mu_{N-1}^\ast \}$ be optimal policy

Consider the “**tail subproblem**”子问题  whereby we are at $x_k$ at time k and wish to minimize the “**cost- to-go**” from time k to time $N$ and "**tail policy**"

<img src="/img/2020-01-08-ADP.assets/image-20200108232901646.png" alt="image-20200108232901646" style="zoom:50%;" />
$$
E\left\{g_{N}\left(x_{N}\right)+\sum_{\ell =k}^{N-1} g_\ell\left(x_{\ell }, \mu_{\ell }\left(x_{\ell }\right), w_{\ell }\right)\right\}
$$


Principle of optimality : The tail policy is optimal for the tail subproblem (optimization of the future does not depend on what we did in the past). The principle of optimality says that the tail of an optimal sequence is optimal for the tail subproblem.

大问题的**最优解**可以由小问题的**最优解**推出，这个性质叫做“最优子结构性质”。 



#### DP ALGORITHM

- Start with Initial condition: $J_{N}\left(x_{N}\right)=g_{N}\left(x_{N}\right)$  最后一个状态的g作为初值
- Go **backwards**, $k=N-1, \ldots, 0,$ using 

$$
J_{k} (x_{k} )=\min_{u_{k} \in U_{k} (x_{k} )}  \underset{w_{k}}{E}\left\{g_{k} (x_{k}, u_{k}, w_{k} ) 
 +J_{k+1} (f_{k} (x_{k}, u_{k}, w_{k}  ) )\right \}
$$

- Then $J_{0}\left(x_{0}\right)$ generated at the last step, is equal to the optimal cost $J^{*} (x_{0})$ Also, the policy $\pi^\ast = \left \lbrace \mu_0^\ast, \ldots, \mu_{N-1}^\ast \right \rbrace$ is optimal. 

这里对每个状态x, 每一步都找的最优的control, 对exact DP方法,   J是从最后一步倒推回来的, 只需要这个是真值, 同时,模型是完全已知的, 每个control对应的cost已知, 执行完DP流程后, $J \to J^\ast$ , 所以DP可以看成两个函数间的映射, 原函数J只需最后一步的值, 其他值都被更新掉了,所以其他步的值可以任意 ; J的实现可以用table, 如果把 J 看成函数, 则要认为这个函数是可以被赋值更新的, 直接看成映射关系比较容易理解. **mapping** 



#### PRACTICAL DIFFICULTIES OF DP

- The **curse** of **dimensionality**
  - Exponential growth of the computational and storage requirements as the number of state variables and control variables increases
  - Quick explosion of the number of states in combinatorial problems

- The curse of **modeling**
  - Sometimes a simulator of the system is easier to construct than a model

- **real-time** solution constraints
  - 一类问题, 需要很高实时性, 拿到数据后时间很少 The data of the problem to be solved is given with little advance notice
  - The problem data may change as the system is controlled – need for on-line replanning

所以要近似求解, All of the above are motivations for **approximation** and **simulation**



#### COST APPROXIMATION

近似成本不是唯一的近似思路，但是是一个很主要的近似思路

optimal cost-to-go function $J_{k+1}$  replaced by  approximation $\tilde J_{k+1}$

- Problem Approximation: Use J derived from a related but simpler problem 使用

  一个更容易计算的问题的cost

- **Parametric** Cost-to-Go Approximation:  parameters are tuned by some heuristic or systematic scheme 

- **Rollout** Approach:  the cost of some suboptimal policy, which is calculated either analytically or by simulation



#### ROLLOUT ALGORITHMS

At each k and state $x_k$, use the control $\bar \mu(x_k)$ that minimizes in $\min_{u_{k} \in U_{k} (x_{k} ) }  {E}\left [g_{k} (x_{k}, u_{k}, w_{k} )  + \tilde J_{k+1} \left(f_{k} (x_{k}, u_{k}, w_{k}  ) \right)\right ]$ ,  where $\tilde J_{k+1}$ is the cost-to-go of some **heuristic** policy (**base policy**).

从这个启发式策略出发，通过前向仿真计算这个策略执行后的成本，再选择控制，一般情况下会得到一个不会更差(可能更优)的策略，这种方法很适合在线计算

Cost improvement property: The rollout algorithm achieves no worse (and usually much better) cost than the base policy starting from the same state.

Main difficulty:  Calculating  $\tilde J_{k+1}$  maybe computationally intensive if the cost-to-go of the base policy cannot be analytically calculated.

- May involve **Monte Carlo simulation** if the problem is **stochastic**.
- Things improve in the deterministic case (an important application is discrete optimization).
- Connection w/ Model Predictive Control (MPC). 在该领域rollout很有效



#### INFINITE HORIZON PROBLEMS

- The number of stages is infinite.
- The system is **stationary**.

两种角度:  总成本,  平均成本;  平均成本的近似理论还在研究中, 下面用总成本.  因为总成本可能无穷大, 所以要处理:  使用折扣系数与截断阶段;  截断阶段是把趋于 $\infty$ 的 N 截断，令 N 等于一个固定的值，将无限期问题近似转化为有限期问题，然后求解。

Total cost:  minimize

$$
J_{\pi}\left(x_{0}\right)=\lim _{N \rightarrow \infty} \underset{w_k \atop k=0,1, \ldots}{E}\left\{\sum_{k=0}^{N-1} \alpha^{k} g\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}
$$

- Discounted problems (α < 1, bounded g)
- Stochastic shortest path problems (α = 1, finite-state system with a termination state) 
- Discounted and undiscounted problems with unbounded cost per stage  无穷大问题, 不讨论

生活中很少 (甚至是完全) 不会产生无限期问题，那么为什么要研究无限期问题呢。原因是无限期问题在数学上更优雅，我们可以把注意力 放在平稳策略上，这样就可以在任何阶段都使用相同的策略进行控制，大量的研究产生了大量有效的算法，很多近似理论也是为无限期问题服务的. 



##### DISCOUNTED PROBLEMS/BOUNDED COST

- **Stationary system** $x_{k+1}=f\left(x_{k}, u_{k}, w_{k}\right), \quad k=0,1, \ldots$      时间维度
  平稳系统: 每一个阶段的系统状态都在同一个状态空间内取值，随机干扰 w 在每一阶段都有相同的概率分布. 
- cost of a policy $\pi=\lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$ :

  $$
  J_{\pi}\left(x_{0}\right)=\lim _{N \rightarrow \infty} \underset{w_k \atop k=0,1, \ldots}{E}\left\{\sum_{k=0}^{N-1} \alpha^{k} g\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}
  $$
  
  with $\alpha<1,$ and $g$ is bounded [for some $M,$ we have $\vert g(x, u, w)\vert \leq M \text { for all }(x, u, w)]$ 
- Optimal cost function: $J^\ast (x)=\min_{\pi} J_{\pi}(x)$
- Boundedness of $g$ guarantees that all costs are well-defined and bounded: $\vert J_{\pi}(x) \vert \leq \frac{M}{1-\alpha}$



#### SHORTHAND NOTATION FOR DP MAPPINGS  简洁符号

算子是从函数到函数的映射;  下面的两个算子, 根据定义公式, 都对所有状态x成立, 把所有state都过了一轮.  $J$本来对应一个默认的策略. 是什么没所谓,因为会在某个时刻 k, control被替换掉; 算子本身如果不带时刻i下标,则没指定哪个时刻k, 表示任意某个时刻,对应的那一步control的改变.

引入 算子T,  For **any function** J of x, denote:   $(T J)(x)=\min _{u \in U(x)} \underset{w}{E}\{g(x, u, w)+\alpha J(f(x, u, w))\}, \forall x$

$TJ$ is the optimal cost function for the one-stage problem with stage cost g and **terminal cost function** $\alpha J$.  根据前面的DP算法, 整个算法的迭代过程中, 只需要最后一步的cost函数的真值.  这个算子, 只对应策略里的一个greedy control.  如果该问题只有一个stage, 期望的cost 变为 最优化cost $TJ$ .  

对 discounted 问题, T operates on bounded functions of x to produce other bounded functions of x. 有界函数到有界函数的映射. 

For any stationary policy $\mu,$  引入策略算子 $T_{\mu}$ ,  下式是定义式,  $J$ 只是一个有界函数
$\left(T_{\mu} J\right)(x)=\underset{w}{E}\{g(x, \mu(x), w)+\alpha J(f(x, \mu(x), w))\}, \forall x$

The critical structure of the problem is captured in $T$ and $T_{\mu}$    算子包含了问题的关键结构信息. $T$ and $T_{\mu}$ provide a powerful unifying framework for DP:  essence of  "**Abstract Dynamic Programming**"

$T = \min_\mu T_\mu$



#### FINITE-HORIZON COST EXPRESSIONS

这边定义的算子, 都是有时间下标的, 对应策略里面的k下标, 即在时刻k,执行该control, J函数的改变. 下式中的$\ell$ 时间步, 都是原先的默认策略的. 

Consider an $N$-stage policy $\pi_{0}^{N}=\lbrace \mu_{0}, \mu_{1}, \ldots, \mu_{N-1}\rbrace$ with a **terminal cost** $J$( 这里的$J$是最后一步的cost函数, 是递推的源头,但不涉及策略):

$$
\begin{aligned}
J_{\pi_{0}^{N}}\left(x_{0}\right)&=E\left\{\alpha^{N} J\left(x_{k}\right)+\sum_{\ell=0}^{N-1} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\} \\
 &=E\left\{g\left(x_{0}, \mu_{0}\left(x_{0}\right), w_{0}\right)+\alpha J_{\pi_{1}^{N}}\left(x_{1}\right)\right\}  \\
 &=\left(T_{\mu_{0}} J_{\pi_{1}^{N}}\right)\left(x_{0}\right) 
\end{aligned}
$$

where $\pi_{1}^{N}=\lbrace \mu_{1}, \mu_{2}, \ldots, \mu_{N-1}\rbrace$  , 上式对应了策略里面的第一步control造成的改变, 后面的看成一个整体

By induction we have  $J_{\pi_{0}^{N}}(x)=\left(T_{\mu_{0}} T_{\mu_{1}} \cdots T_{\mu_{N-1}} J\right)(x), \quad \forall x$ ;   T执行顺序, 对$J$而言, 从右到左, 离得最近的先执行

For a stationary policy $\mu$ the $N$-stage cost function (with terminal cost $J$ ) is $J_{\pi_{0}^{N}}=T_{\mu}^{N} J$,  where $T_{\mu}^{N}$ is the $N$-fold composition of $T_{\mu}$ ;     $T_\mu^N$ 表示一个完整策略每一步的映射, 上标表示按时间执行N步optimal

optimal $N$-stage cost function  (  with terminal cost $J$ )  is $T^{N} J$  ;  从最后一步开始逆向N步执行optimal

$T^{N} J=T\left(T^{N-1} J\right)$ is just the **DP algorithm** , DP算法的简洁表达,  意义: 左边, 每一步都optimal ; 右边, 括号里面的先执行 , 表示从最后的J一直optimal到时刻1, 然后时刻0再来一个T 取optimal.  这里的括号不能少,源头也关键.



#### "SHORTHAND" THEORY-A SUMMARY

- Infinite horizon cost function expressions [with $\left.J_{0}(x) \equiv 0\right]$ 

  对于无限问题, 是没有最后一步, 对discounted问题, 只需把每步的g加起来, 因为g是在第一步control之后才有的, 所以$J_0 = 0$ , 下式的意思就是, 对一个无限问题, 默认的策略都会被覆盖掉, 得到一个新的J函数
  $$
  J_{\pi}(x)=\lim _{N \rightarrow \infty}\left(T_{\mu_{0}} T_{\mu_{1}} \cdots T_{\mu_{N}} J_{0}\right)(x), \quad J_{\mu}(x)=\lim _{N \rightarrow \infty}\left(T_{\mu}^{N} J_{0}\right)(x)
  $$
  
- **Bellman's equation**: $J^\ast=T J^\ast, \quad J_{\mu}=T_{\mu} J_{\mu}$     已收敛

- **Optimality** condition:  $\mu: \text { optimal }    \iff  T_{\mu} J^\ast=T J^\ast$   一个策略函数最优, 表示该策略在任意k时刻,都取的cost最小值的control , 定义

- **Value iteration**: For any (bounded) $J$ , $J^{*}(x)=\lim_{k \rightarrow \infty}\left(T^{k} J\right)(x), \quad \forall x$  这个公式里面没时间下标, 可以看成, 对无论stage多长的问题, 逆序地每个stage进行一次T

- **Policy iteration**: Given $\mu^{k}$ , 注意这里是上标,  表示策略的一个版本. 算子的下标都没有时间i, 对任意状态都成立

  - Policy evaluation: Find $J_{\mu^{k}}$ by solving , $J_{\mu^{k}}=T_{\mu^{k}} J_{\mu^{k}}$    求解$J$, 即evaluate到收敛
  
  - Policy improvement: Find $\mu^{k+1}$ such that  $T_{\mu^{k+1}} J_{\mu^{k}}=T J_{\mu^{k}}$   improve, 在所有状态上都select min, 选择cost最小的那个control, 然后要重新evaluate
  
    $\mu^k \to J_{\mu^k} \stackrel{improve}{\longrightarrow} \mu^{k+1} \to J_{\mu_{k+1}} \to  \dots $



##### TWO KEY PROPERTIES

- **Monotonicity** property **单调性**: For any $J$ and $J′$ such that $J(x) ≤ J′(x)$ for all $x$, and any $\mu$,  $(TJ)(x) ≤ (TJ′)(x), \quad (T_\mu J)(x) ≤ (T_\mu J′)(x), \quad \forall x$

  Monotonicity is present in all DP models  单调性对所有DP问题都成立

- Constant Shift property : For any $J$, any scalar $r$, and any $\mu$,  $\left(T(J+re) \right)(x) = (TJ)(x)+ \alpha r,\left(T_\mu(J+re) \right)(x) = (T_\mu J)(x)+ \alpha r, \quad \forall x$,   where $e$ is the unit function $[e(x) \equiv 1]$.   

  $(J + re)(x)$ 的意思是 $J(x) + r$ , 即在$J(x)$加上一个常数 , 因为乘以T算子相当于执行了一个时间步, 所以r有discount $\alpha$ 
  
  Constant shift is special to discounted models  只对discounted问题成立

**contraction mappings** : for Discounted problem



#### CONVERGENCE OF VALUE ITERATION

for all bounded $J$

$$
J^{*}(x)=\lim _{k \rightarrow \infty}\left(T^{k} J\right)(x), \quad \text { for all } x
$$

Proof: For simplicity we give the proof for $J \equiv 0$ For any initial state $x_{0},$ and policy $\pi=\lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$

$$
\begin{aligned}
J_{\pi}\left(x_{0}\right)&=
E\left\{\sum_{\ell=0}^{\infty} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\} \\
&=E\left\{\sum_{\ell=0}^{k-1} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\} \\
& \quad\quad +E\left\{\sum_{\ell=k}^{\infty} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\}
\end{aligned}
$$

The tail portion satisfies

$$
\left|E\left\{\sum_{\ell=k}^{\infty} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\}\right| \leq \frac{\alpha^{k} M}{1-\alpha}
$$

where $M \geq \vert g(x, u, w) \vert$  ;  上面是说, $J_\pi$ 是有界的, 然后两边都上T算子, 每步都取最小,再取极限, 会收敛到固定值. 

<u>Take min</u> over $\pi$ of both sides, then $\lim$ as $k \rightarrow \infty $  **Q.E.D.**   



##### BELLMAN’S EQUATION

The optimal cost function $J^\ast$ is a solution of Bellman's equation,   $J^\ast=T J^\ast$,   i.e., for all $x,$
$$
J^\ast(x)=\min _{u \in U(x)} E\left\{g(x, u, w)+\alpha J^\ast(f(x, u, w))\right\}
$$

Proof: For all $x$ and $k$

$$
J^{*}(x)-\frac{\alpha^{k} M}{1-\alpha} \leq\left(T^{k} J_{0}\right)(x) \leq J^{*}(x)+\frac{\alpha^{k} M}{1-\alpha}
$$

where $J_{0}(x) \equiv 0$ and $M \geq \vert g(x, u, w)\vert$ .  Applying $T$ to this relation, and using Monotonicity and Constant Shift,   两边夹逼

$$
\begin{aligned}\left(T J^{*}\right)(x)-\frac{\alpha^{k+1} M}{1-\alpha} & \leq\left(T^{k+1} J_{0}\right)(x) \\ & \leq\left(T J^{*}\right)(x)+\frac{\alpha^{k+1} M}{1-\alpha} \end{aligned}
$$

Taking the limit as $k \rightarrow \infty$ and using the fact

$$
\lim_{k \rightarrow \infty}\left(T^{k+1} J_{0}\right)(x)=J^*(x)
$$

we obtain $J^∗ = TJ^∗$. **Q.E.D.**



#### THE CONTRACTION PROPERTY

**Contraction** property **压缩**性质: For any bounded functions $J$ and $J^{\prime},$ and any $\mu$  
$$
\max _{x}\left|(T J)(x)-\left(T J^{\prime}\right)(x)\right| \leq \alpha \max _{x}\left|J(x)-J^{\prime}(x)\right| \\
\max _{x}\left|\left(T_{\mu} J\right)(x)-\left(T_{\mu} J^{\prime}\right)(x)\right| \leq \alpha \max _{x}\left|J(x)-J^{\prime}(x)\right|
$$

映射后产生的两个新的点距离要比映射前的点距离要近

Proof: Denote $c=\max _{x \in S}\vert J(x)-J^{\prime}(x)\vert$ ,  Then 
$$
J(x)-c \leq J^{\prime}(x) \leq J(x)+c, \quad \forall x
$$

Apply $T$ to both sides, and use the Monotonicity and Constant Shift properties:

$$
(T J)(x)-\alpha c \leq\left(T J^{\prime}\right)(x) \leq(T J)(x)+\alpha c, \quad \forall x
$$

Hence

$$
\left|(T J)(x)-\left(T J^{\prime}\right)(x)\right| \leq \alpha c, \quad \forall x
$$

**Q.E.D.**
Note: This implies that $J^{*}$ is the **unique** solution of $J^{*}=T J^{*},$ and $J_{\mu}$ is the **unique** solution



#### NEC. AND SUFFICIENT OPT. CONDITION 最优的充要条件

A stationary policy $\mu$ is optimal if and only if $\mu(x)$ attains the minimum in Bellman's equation for each $x ;$ i.e.,

$$
T J^{*}=T_{\mu} J^{*}
$$

or, equivalently, for all $x$

$$
\mu(x) \in \arg \min _{u \in U(x)} E\left\{g(x, u, w)+\alpha J^{*}(f(x, u, w))\right\}
$$

Proof: If $T^\ast=T_{\mu} J^\ast$, then using Bellman's equation $\left(J^\ast=T J^\ast \right)$,  we have

$$
J^{*}=T_{\mu} J^{*}
$$

so by uniqueness of the fixed point of $T_{\mu},$ we obtain $J^{*}=J_{\mu} ;$ i.e., $\mu$ is optimal. 

Conversely, if the stationary policy $\mu$ is optimal, we have $J^{*}=J_{\mu},$ so

$$
J^{*}=T_{\mu} J^{*}
$$

Combining this with Bellman's Eq. $\left(J^\ast=T J^\ast \right)$ obtain $T J^\ast=T_{\mu} J^\ast$ . **Q.E.D.**



### LECTURE 2

#### VI AND PI

- Value iteration: For any (bounded) $J$ 

$$
J^{*}(x)=\lim _{k \rightarrow \infty}\left(T^{k} J\right)(x), \quad \forall x
$$

- Policy iteration: Given $\mu^{k}$ 

  - Policy evaluation: Find $J_{\mu^{k}}$ by solving,  到收敛
    $$
    J_{\mu^{k}}(x)=\underset{w}{E}\left\{g\left(x, \mu^{k}(x), w\right)+\alpha J_{\mu^{k}}\left(f\left(x, \mu^{k}(x), w\right)\right)\right\}, \forall x \\
    J_{\mu^{k}}=T_{\mu^{k}} J_{\mu^{k}}
    $$
    
  - Policy improvement: Let $\mu^{k+1}$ be such that , 下式符号是属于, 右边是集合
    $$
    \mu^{k+1}(x) \in \arg \min _{u \in U(x)} \underset{w}{E}\left\{g(x, u, w)+\alpha J_{\mu^{k}}(f(x, u, w))\right\}, \forall x \\
    T_{\mu^{k+1}} J_{\mu^{k}}=T J_{\mu^{k}}
    $$

    For the case of $n$ states, policy evaluation is equivalent to solving an $n \times n$ linear system of equations: $J_{\mu}=g_{\mu}+\alpha P_{\mu} J_{\mu}$  ; 直接求解析解不可行

    For large $n,$ exact $\mathrm{PI}$ is out of the question (even though it terminates finitely as we will show)



#### JUSTIFICATION OF POLICY ITERATION

 $J_{\mu^{k}} \geq J_{\mu^{k+1}}$ for all $k$  ;  证明PI是一直改进的

Proof : For given $k,$ we have $J_{\mu^{k}}=T_{\mu^{k}} J_{\mu^{k}} \geq T J_{\mu^{k}}=T_{\mu^{k+1}} J_{\mu^{k}}$ ,  其中$T_{\mu^{k+1}} J_{\mu^{k}}$是一个还没evaluate的cost函数,  可以作为一个中间值, 然后继续执行$\mu^{k+1}$, 就可以收敛到 $J_{\mu^{k+1}}$.

Using the monotonicity property of DP,  $J_{\mu^{k}} \geq T_{\mu^{k+1}} J_{\mu^{k}} \geq T_{\mu^{k+1}}^{2} J_{\mu^{k}} \geq \dots \geq \lim_{N \rightarrow \infty} T_{\mu^{k+1}}^{N} J_{\mu^{k}}$, since $\lim_{N \rightarrow \infty} T_{\mu^{k+1}}^{N} J_{\mu^{k}}=J_{\mu^{k+1}}$  we have $J_{\mu^{k}} \geq J_{\mu^{k+1}}$ . 

If $J_{\mu^{k}}=J_{\mu^{k+1}},$ all above inequalities hold as equations, so $J_{\mu^{k}}$ solves Bellman's equation. Hence $J_{\mu^{k}}=J^{*}$ , 

Thus at iteration k either the algorithm generates a **strictly improved** policy or it finds an **optimal** policy

- For a finite spaces MDP, the algorithm **terminates** with an optimal policy

- For infinite spaces MDP, **convergence** (in an infinite number of iterations) can be shown



#### OPTIMISTIC POLICY ITERATION

策略迭代的策略评价算法标准情况下是迭代直到收敛的，但是 Optimistic PI不会迭代到收敛，而是迭代若干次 (m 次)，不等到收敛就停止，然后进行策略改进，继续做策略评价，这也是一种近似方法。

**Optimistic PI**:  This is PI, where policy evaluation is done approximately, with a finite number of VI

approximate the policy evaluation $J_\mu \approx  T^m_\mu  J $ ,   inital $J$   ; 先近似evaluate, 这里,  把收敛值记为 $J_{\mu^{k}}$ , 未收敛的近似值记为 $J_k$ , 下式

Shorthand definition: For some integers $m_k$,  

$$
T_{\mu^k} J_k = T J_k, \quad  \text{improve, get } \mu^k \\
J_{k+1} = T_{\mu^k}^{m_k} J_k , \quad  \mu^k \text{execed, get approx evaluate } J_{k+1}
$$

- If $m_k ≡1$ it becomes VI , 上式, 如果在improve的时候直接把$J_{k+1}$ 记下来, 就相当于evaluate一次
- If $m_k = \infty$ it becomes PI

$ J_k \stackrel{improve}{\longrightarrow} \mu^k \stackrel{eval}{\dashrightarrow } J_{k+1} \to \mu^{k+1} \dashrightarrow J_{k+2} \to  \dots $ 

详细定义: Optimistic Policy Iteration: Discounted Problems
Given the typical function $J_{k}$:
**Policy improvement** computes a policy $\mu^{k}$ such that
$$
\mu^{k}(i) \in \arg \min _{u \in U(i)} \sum_{j=1}^{n} p_{i j}(u)\left(g(i, u, j)+\alpha J_{k}(j)\right), \quad i=1, \ldots, n
$$

**Optimistic policy evaluation** starts with $\hat J_{k, 0}=J_k$,  and uses $m_k$ VI iterations for policy $\mu^k$ to compute $\hat J_{k, 1}, \dots, \hat J_{k, m_{k}}$ according to:
$$
\hat{J}_{k, m+1}(i)=\sum_{j=1}^{n} p_{i j}\left(\mu^{k}(i)\right)\left(g\left(i, \mu^{k}(i), j\right)+\alpha \hat{J}_{k, m}(j)\right)
$$

for all $i=1, \ldots, n, m=0, \ldots, m_{k}-1,$ and sets $J_{k+1}=\hat J_{k, m_k}$
​

之前 : $\mu^k \to J_{\mu^k} \stackrel{improve}{\longrightarrow} \mu^{k+1} \to J_{\mu_{k+1}} \to  \dots $



**Converges** for both finite and infinite spaces discounted problems (in an infinite number of iterations). Typically works  **faster**  than VI and PI (for large problems)



#### APPROXIMATE PI

policy evaluation is approximate,

$$
\|J_{k}-J_{\mu^{k}} \| \leq \delta, \quad k=0,1
$$

policy improvement is approximate, 这里, 在$J_k$的基础上改进,必定就是近似的

$$
\|T_{\mu^{k+1}} J_{k}-T J_{k}  \| \leq \epsilon, \quad k=0,1
$$

where $\delta$ and $\epsilon$ are some positive scalars.

Error Bound I:误差上界 The sequence $\lbrace \mu^{k} \rbrace$ generated by approximate policy iteration satisfies

$$
\limsup _{k \rightarrow \infty}\left\|J_{\mu^{k}}-J^{*}\right\| \leq \frac{\epsilon+2 \alpha \delta}{(1-\alpha)^{2}}
$$

Typical practical behavior: The method makes steady progress up to a point and then the iterates
$J_\mu^{k}$ oscillate within a neighborhood of  $J^\ast$  最优附近震荡

Error Bound II: If in addition the sequence $\lbrace \mu^{k} \rbrace$ "terminates" at $\bar{\mu}$ (i.e., keeps generating $\bar{\mu}$ , 策略稳定后) 

$$
\left\|J_{\bar{\mu}}-J^{*}\right\| \leq \frac{\epsilon+2 \alpha \delta}{1-\alpha}
$$

α 是折扣问题中的折扣因子，这个折扣因子需要人为调整，如果折扣因子 特别接近一，分母就会非常小导致误差很大，但是折扣因子非常小，趋近于零的时候，bellman 方程就会很接近于贪心算子。

这两个界限用于理论分析. 



#### Q-FACTORS

Optimal Q-factor of $(x, u):$

$$
Q^{*}(x, u)=E\left\{g(x, u, w)+\alpha J^{*}(\bar{x})\right\}
$$

with $\bar{x}=f(x, u, w)$, 表示后继状态.  It is the cost of starting at $x$ applying $u$ is the 1st stage, and an optimal policy after the 1st stage. 定义为当前系统 状态 x 下执行控制 u 产生的即时成本, 加上 之后一直走optimal 策略的cost 期望。

Bellman's equation:	 $J^\ast (x)=\min_{u \in U(x)} Q^\ast (x, u), \quad \forall x$

VI method: 

$$
J_{k+1}(x)=\min _{u \in U(x)} Q_{k+1}(x, u), \quad \forall x
$$

where $Q_{k+1}$ is generated by $Q_{k+1}(x, u)= E \lbrace g(x, u, w)+\alpha \min_{v \in U(\bar x)} Q_k(\bar x, v)\rbrace$ with $\bar x=f(x, u, w)$

Q-factors are costs in an “augmented” problem where states are (x, u) ,   强化学习的(s, a)

They satisfy a Bellman equation $Q^\ast = FQ^\ast$ where ,  这里用F表示贝尔曼算子
$$
(FQ)(x, u)= E \lbrace g(x, u, w)+\alpha \min_{v \in U(\bar x)} Q_k(\bar x, v)\rbrace
$$
VI and PI for Q-factors are mathematically equivalent to VI and PI for costs

They require equal amount of computation ... they just need more storage 需要更多资源, 因为要存模型的信息

Having optimal Q-factors is convenient when implementing an optimal policy on-line by
$$
\mu^*(x) = \min_{u \in U(x)} Q^*(x, u)
$$
Once $Q^\ast(x,u)$ are known, the model [$g$ and $E\{\cdot \}$] is not needed. **Model-free** operation, 引入Q, 是为了解决不知道model的那些问题

**Q-Learning** (to be discussed later) is a sampling method that calculates $Q^\ast(x,u)$ using a simulator of the system (no model needed)



#### OTHER DP MODELS

之前讨论是 discounted model. 

- Undiscounted problems (α = 1)
- Continuous-time finite-state MDP:  连续时间是相邻两个状态产生的时间间隔不是均匀的，任意时刻都可能有新状态产生.  有两种连续时间模型，第一种是每时每刻都在产生新状态，即状态 x(t) 是 关于时间 t 的函数，比如电机控制等系统，另一种是两个状态以满 足某概率分布的时间间隔进行跳转，一个比较典型的例子，是排队系统，一个客户 (可能以泊松分布等概率分布) 到达，状态增加 1， 客户以随机时间离开，随机的时间长短依赖于控制 ， 实际上，排队系统详细划分应该属于半马尔科夫过程 (semi-markov decision process, semi-MDP)





#### CONTINUOUS-TIME MODELS

- **System equation**: $d x(t) / d t=f(x(t), u(t))$
- **cost** function: $\int_{0}^{\infty} g(x(t), u(t))$
- Optimal cost starting from $x: J^{*}(x)$ 
- $\delta$-Discretization of time: $x_{k+1}=x_{k}+\delta \cdot f\left(x_{k}, u_{k}\right)$   连续问题离散化
- Bellman equation for the $\delta$ -discretized problem:
  $$J_{\delta}^{*}(x)=\min _{u}\left\{\delta \cdot g(x, u)+J_{\delta}^{*}(x+\delta \cdot f(x, u))\right\}$$
- Take $\delta \rightarrow 0,$ to obtain the **Hamilton-Jacobi-Bellman equation** [assuming $ \lim_{\delta \rightarrow 0} J_{\delta}^\ast(x)=J^\ast(x) $]

  $$
  0=\min _{u}\left\{g(x, u)+\nabla J^{*}(x)^{\prime} f(x, u)\right\}, \quad \forall x
  $$
  这里是对 状态x的导数,  $x_{k+1} - x_k$,   公式里的'应该可以去掉
  

Policy Iteration (informally):

- Policy evaluation: Given current $\mu,$ solve
  $0=g(x, \mu(x))+\nabla J_{\mu}(x)^{\prime} f(x, \mu(x)), \quad \forall x$
- Policy improvement: Find

$$
\bar{\mu}(x) \in \arg \min _{u}\left\{g(x, u)+\nabla J_{\mu}(x)^{\prime} f(x, u)\right\}
$$
Note: Need to learn $\nabla J_{\mu}(x)$ NOT $J_{\mu}(x)$



#### MORE GENERAL/ABSTRACT VIEW OF DP

Let $Y$ be a real vector space with a norm $\|\cdot\|$

A function $F: Y \mapsto Y$ is said to be a **contraction mapping 压缩映射** if for some $\rho \in(0,1),$ we have

$$
\|F y-F z\| \leq \rho\|y-z\|, \quad \text { for all } y, z \in Y
$$

$\rho$ is called the **modulus of contraction 压缩模量** of $F$

Important example: Let $X$ be a set (e.g., state space in DP),   $v: X \mapsto \Re$ be a **positive-valued** function.  Let $B(X)$  be the set of all functions $J: X \mapsto \Re$ , 所有cost函数空间 , such that $J(x) / v(x)$ is bounded over x. 
We define a norm on $B(X)$ , called the **weighted sup-norm**, by
$$
\|J\|=\max _{x \in X} \frac{|J(x)|}{v(x)}
$$

Important special case: The discounted problem mappings $T$ and $T_μ$,  [for $v(x) ≡ 1, ρ = α$].



##### CONTRACTION MAPPINGS: AN EXAMPLE

Consider extension from finite to countable state space, $X=\{1,2, \ldots\},$ and a weighted sup norm with respect to which the one stage costs are bounded. 比如一个排队系统，系统状态为客户数量，客户数量可能是 1, 2, · · · ， 每个阶段的成本依赖于系统状态而且没有上界， 之前的分析可以知道这种系统很难求解，现在要给出一个让成本函数有界的方法，即使用加权最大范数 (sup-weighted norm) 处理这个问题让它有界。
Suppose that $T_{\mu}$ has the form
$$
\left(T_{\mu} J\right)(i)=b_{i}+\alpha \sum_{j \in X} a_{i j} J(j), \quad \forall i=1,2,
$$

where $b_{i}$ and $a_{i j}$ are some scalars. Then $T_{\mu}$ is a contraction with modulus $\rho$ if and only if

$$
\frac{\sum_{j \in X}\left|a_{i j}\right| v(j)}{v(i)} \leq \rho, \quad \forall i=1,2
$$

Consider $T$, 
$$(T J)(i)=\min _{\mu}\left(T_{\mu} J\right)(i), \quad \forall i=1,2, \ldots$$
where for each $\mu \in M, T_{\mu}$ is a contraction mapping with modulus $\rho$.  Then $T$ is a contraction mapping with modulus  $\rho$.  对这类问题,  $T$ 与 $T_\mu$ 都是压缩映射

Allows extensions of main DP results from bounded one-stage cost to interesting unbounded one-stage cost cases.



##### CONTRACTION MAPPING FIXED-POINT TH.

**Contraction Mapping Fixed-Point Theorem 压缩映射不动点理论**: If $F: B(X) \mapsto B(X)$ is a contraction with modulus $\rho \in(0,1),$ then there exists a unique $J^{*} \in B(X)$ such that
$$
J^*=F J^{*}
$$

Furthermore, if $J$ is any function in $B(X),$ then $\lbrace F^{k} J\rbrace$ **converges** to $J^{*}$ and we have

$$
\left\|F^{k} J-J^{*}\right\| \leq \rho^{k}\left\|J-J^{*}\right\|, \quad k=1,2, \dots
$$

This is a special case of a general result for contraction mappings $F: Y \mapsto Y$ over normed vector spaces $Y$ that are complete: every sequence $\lbrace y_{k}\rbrace$ that is **Cauchy** (satisfies $\left\|y_{m}-y_{n}\right\| \rightarrow 0$ as $m, n \rightarrow \infty)$ converges. 如果一个范数向量空间  $Y$ 中所有柯西序列 $\lbrace y_{k}\rbrace$  都收敛， 那么这个空间  $Y$ 是一个完备空间
The space $B(X)$ is complete 完备空间 



#### ABSTRACT FORMS OF DP

本页给出了满足单调性和压缩性的动态规划的更加抽象形式. We consider an abstract form of DP based on monotonicity and contraction

**Abstract Mapping**: Denote $R(X)$:  set of realvalued functions  $J: X \mapsto \Re$  , $R(X)$表示实函数空间   ,  and let $H: X \times U \times R(X) \mapsto \Re$ be a given mapping,  H是所有J的映射的空间.  We consider the mapping
$$
(T J)(x)=\min _{u \in U(x)} H(x, u, J), \quad \forall x \in X
$$

We assume that $(T J)(x)>-\infty$ for all $x \in X$ so $T$ maps $R(X)$ into $R(X)$ 

**Abstract Policies**: Let $\mathcal{M}$ be the set of "policies"所有策略函数的集合, i.e., functions $\mu$ such that $\mu(x) \in U(x)$ for all $x \in X$
For each $\mu \in \mathcal{M},$ we consider the mapping $T_{\mu}: R(X) \mapsto R(X)$ defined by
$$
\left(T_{\mu} J\right)(x)=H(x, \mu(x), J), \quad \forall x \in X
$$

Find a function $J^\ast \in R(X)$ such that
$$J^{*}(x)=\min _{u \in U(x)} H\left(x, u, J^{*}\right), \quad \forall x \in X$$



##### EXAMPLES

对不同类型的问题, 核心部分就是 J函数的映射,  这是一个通用性很强的框架

- Discounted problems
  $$H(x, u, J)=E\{g(x, u, w)+\alpha J(f(x, u, w))\}$$

- Discounted "discrete-state continuous-time" Semi-Markov Problems (e.g., queueing)

$$
H(x, u, J)=G(x, u)+\sum_{y=1}^{n} m_{x y}(u) J(y)
$$

​		where $m_{x y}$ are "discounted" transition probabilities, defined by the distribution of transition times
- Minimax Problems/Games
$H(x, u, J)=\max _{w \in W(x, u)}[g(x, u, w)+\alpha J(f(x, u, w))]$

- Shortest Path Problems

$$
H(x, u, J)=\left\{\begin{array}{ll}
{a_{x u}+J(u)} & {\text { if } u \neq d} \\
{a_{x d}} & {\text { if } u=d}
\end{array}\right.
$$



##### ASSUMPTIONS

对于 H 算子有一些假设，下面说一说这些假设和假设带来的性质

**Monotonicity**: If $J, J^{\prime} \in R(X)$ and $J \leq J^{\prime}$,  $H(x, u, J) \leq H\left(x, u, J^{\prime}\right), \quad \forall x \in X, u \in U(x)$

​	We can show all the standard analytical and computational results of discounted DP if monotonicity and the following assumption holds:  如果映射具有单调性，那么所有的动态规划问题都可以使用相同的标准分析过程和计算结果。

**Contraction**:

- For every $J \in B(X),$ the functions $T_{\mu} J$ and $T J$ belong to $B(X)$ 
- For some $\alpha \in(0,1),$ and all $\mu$ and $J, J^{\prime} \in$ $B(X),$ we have

$$
\left\|T_{\mu} J-T_{\mu} J^{\prime}\right\| \leq \alpha\left\|J-J^{\prime}\right\|
$$
​	如果 H 具有压缩性，那么可以知道 T 和 Tμ 也具有压缩性。

With just monotonicity assumption 只有单调性 (as in undiscounted problems) we can still show various forms of the basic results under appropriate conditions.  

A weaker substitute for contraction assumption is **semicontractiveness** 部分压缩性: (roughly) for some $\mu, T_{\mu}$ is a contraction and for others it is not; also the "noncontractive" $\mu$ are not optimal. 这时候由于算子没法保证压缩性，也就没法保证最优性 

##### RESULTS USING CONTRACTION

使用压缩映射,得到的推论:  不动点 以及 收敛

Proposition 1: The mappings $T_{\mu}$ and $T$ are weighted sup-norm contraction mappings with modulus $\alpha$ over $B(X),$ and have unique **fixed points** in $B(X),$ denoted $J_{\mu}$ and $J^\ast,$ respectively $(\mathrm{cf}$ Bellman's equation).

Proof: From the contraction property of $H$

Proposition 2: For any $J \in B(X)$ and $\mu \in \mathcal{M}$
$$
\lim _{k \rightarrow \infty} T_{\mu}^{k} J=J_{\mu}, \quad \lim _{k \rightarrow \infty} T^{k} J=J^\ast
$$

(cf. convergence of value iteration).

Proof: From the contraction property of $T_{\mu}$ and $T$

Proposition 3: We have $T_{\mu} J^\ast=T J^\ast$ if and only if $J_{\mu}=J^\ast$ (cf. optimality condition).

Proof: $T_{\mu} J^\ast=T J^\ast,$ then $T_{\mu} J^\ast=J^\ast,$ implying $J^\ast=J_{\mu} .$ Conversely, if $J_{\mu}=J^\ast,$ then $T_{\mu} J^\ast=$ $T_{\mu} J_{\mu}=J_{\mu}=J^\ast=T J^\ast$

##### RESULTS USING MON. AND CONTRACTION

使用单调性以及压缩,得到的结论

-  Optimality of fixed point:  单调改进, 则不动点是最优的

$$
J^{*}(x)=\min _{\mu \in \mathcal{M}} J_{\mu}(x), \quad \forall x \in X
$$

-  Existence of a nearly optimal policy: For every $\epsilon>0$, there exists $\mu_{\epsilon} \in \mathcal{M}$ such that

$$
J^{*}(x) \leq J_{\mu_{\epsilon}}(x) \leq J^{*}(x)+\epsilon, \quad \forall x \in X
$$

-  Nonstationary policies 非稳态: Consider the set $\Pi$ of all sequences $\pi= \lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$ with $\mu_{k} \in \mathcal{M}$  for all $k$, and define

$$
J_{\pi}(x)=\liminf _{k \rightarrow \infty}\left(T_{\mu_{0}} T_{\mu_{1}} \cdots T_{\mu_{k}} J\right)(x), \quad \forall x \in X
$$

​	with $J$ being any function (the choice of $J$ does not matter), We have 囊括非稳态的最优解的定义
$$
J^{*}(x)=\min _{\pi \in \Pi} J_{\pi}(x) , \quad \forall x \in X
$$





#### ASYNCHRONOUS ALGORITHMS 异步算法

Motivation for asynchronous algorithms 

- Faster convergence
- Parallel and distributed computation 并行计算
- Simulation-based implementations

General framework: Partition $X$ into disjoint nonempty subsets $X_{1}, \dots, X_{m}$,  and use separate processor $\ell$ updating $J(x)$ for $x \in X_{\ell}$ ,  把状态分成不相交非空子集以及对应的cost函数集.  Let $J$ be partitioned as $J=\left(J_{1}, \dots, J_{m}\right)$
where $J_{\ell}$ is the restriction of $J$ on the set $X_{\ell}$

-  Synchronous VI algorithm:  更新所有状态子集的J

$$
J_{\ell}^{t+1}(x)=T\left(J_{1}^{t}, \ldots, J_{m}^{t}\right)(x), \quad x \in X_{\ell}, \ell=1, \ldots, m
$$

-  Asynchronous VI algorithm: For some subsets of times $\mathcal{R}_{\ell}$

$$
J_{\ell}^{t+1}(x)=\left\{\begin{array}{ll}{T\left(J_{1}^{\tau_{\ell 1}(t)}, \ldots, J_{m}^{\tau_{\ell m}(t)}\right)(x)} & {\text { if } t \in \mathcal{R}_{\ell}} \\ {J_{\ell}^{t}(x)} & {\text { if } t \notin \mathcal{R}_{\ell}}\end{array}\right.
$$

where $t-\tau_{\ell j}(t)$ are communication "delays"



##### ONE-STATE-AT-A-TIME ITERATIONS

一个比较重要的例子，假设有 n 个状态，分成了 n 个子集，使用仿真方法生 成系统状态，每一个状态都会产生无数次，使用 n 个机器去计算 J(x) 的值， 这种情况下算法是没有通信延迟的。

Important special case: Assume $n$ "states", a separate processor for each state, and no delays

Generate a sequence of states $\lbrace x^{0}, x^{1}, \ldots\rbrace$,  generated in some way, possibly by simulation (each state is generated infinitely often)

Asynchronous VI:
$$
J_{\ell}^{t+1}=\left\{\begin{array}{ll}
{T\left(J_{1}^{t}, \ldots, J_{n}^{t}\right)(\ell)} & {\text { if } \ell=x^{t}} \\
{J_{\ell}^{t}} & {\text { if } \ell \neq x^{t}}
\end{array}\right.
$$
where $T\left(J_{1}^{t}, \ldots, J_{n}^{t}\right)(\ell)$ denotes the $\ell$ -th component of the vector

$$
T\left(J_{1}^{t}, \ldots, J_{n}^{t}\right)=T J^{t}
$$

The special case where
$$
\left\{x^{0}, x^{1}, \ldots\right\}=\{1, \ldots, n, 1, \ldots, n, 1, \ldots\}
$$

is the **Gauss-Seidel method**  高斯-赛德尔迭代.   迭代时就先访问第一个状态，更新第一个状态的成本，然后访问第二个状态的成本，一次访问下去直到最后一个状态 $x^n$ 的成本，再从第一个状态重 新开始直到收敛，这种迭代方法很经典



##### ASYNCHRONOUS CONV. THEOREM

异步迭代的收敛性,  需要对普通版本的值迭代和策略迭代做一点修改 才能让异步算法工作，这个修改是建立在两个假设的基础上，第一个假设是 每一个机器都能够进行无限次数的更新，即每个状态都会被访问无数次，也就是课件中给的 $\ell, j=1, \dots, m, \mathcal R_\ell$ is infinite 想要表达的假设，第二个假设是延迟通信会一直进 行下去，算法会一直传递 (延迟的) 信息，τ 与 t 永远会存在一个 gap，即课 件中 $\lim_{t \rightarrow \infty} \tau_{\ell j}(t)=\infty$ 要表达的内容。这就是两个比较基本的假设。

KEY FACT: VI and also PI (with some modifications) still work when implemented asynchronously

Assume that for all $\ell, j=1, \dots, m, \mathcal R_\ell$ is infinite and $\lim_{t \rightarrow \infty} \tau_{\ell j}(t)=\infty$

Proposition: Let $T$ have a unique fixed point $J^\ast$ and assume that there is a sequence of nonempty subsets $\{S(k)\} \subset R(X)$ with $S(k+1) \subset S(k)$ for all $k,$ and with the following properties:

- Synchronous Convergence Condition: Every sequence $\lbrace J^{k}\rbrace$ with $J^{k} \in S(k)$ for each $k$ converges pointwise to $J^\ast$.  Moreover,

$$
T J \in S(k+1), \quad \forall J \in S(k), k=0,1, \ldots
$$
- Box Condition: For all $k, S(k)$ is a Cartesian product 笛卡儿积 of the form

$$
S(k)=S_{1}(k) \times \cdots \times S_{m}(k)
$$
where $S_{\ell}(k)$ is a set of real-valued functions on $X_{\ell}, \ell=1, \dots, m$

Then for every $J \in S(0),$ the sequence $\lbrace J^{t} \rbrace$ generated by the asynchronous algorithm converges pointwise to $J^\ast$

同步值迭代和异步值迭代，主要区别是同步值迭代每次迭代都一次性更新所有 J 的值，异步值迭代只有根据集合 $\mathcal R_\ell$在特定迭代次数的时候更新特定子集的J，有些时候异步值迭代有通信延迟，有些时候异步迭代没有通信延迟，这就是同步值迭代和异步值迭代的区别。 由于 exact DP的单调性, 所有的子线程都在往好的方向努力.

Interpretation of assumptions:  下面用图表示异步DP

<img src="2020-01-08-ADP.assets/image-20200112232013512.png" alt="image-20200112232013512" style="zoom:50%;" />

A synchronous iteration from any J in S(k) moves into S(k + 1) (component-by-component)

Convergence mechanism:

<img src="2020-01-08-ADP.assets/image-20200112232035104.png" alt="image-20200112232035104" style="zoom:50%;" />

Key: “Independent” component-wise improve- ment. An asynchronous component iteration from any J in S(k) moves into the corresponding com- ponent portion of S(k + 1)



### LECTURE 3

#### MDP - TRANSITION PROBABILITY NOTATION

assume the system is an n-state (controlled) Markov chain,  switch to Markov chain notation 

- States  $i = 1, \dots, n$  [instead of x]
- **Transition probabilities** $p_{i_{k} i_{k+1}}\left(u_{k}\right)$ [instead of $x_{k+1}=f (x_{k}, u_{k}, w_{k} )$]
- Stage cost $g\left(i_{k}, u_{k}, i_{k+1}\right)\left[\text { instead of } g\left(x_{k}, u_{k}, w_{k}\right)\right]$ 
- Cost functions $J=(J(1), \ldots, J(n))$ (vectors in $\left.\Re^{n}\right)$
- cost of a policy $\pi=\lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$
  $$J_{\pi}(i)=\lim _{N \rightarrow \infty} \underset{r_{k} \atop k=1,2, \ldots}{E}\left\{\sum_{k=0}^{N-1} \alpha^{k} g\left(i_{k}, \mu_{k}\left(i_{k}\right), i_{k+1}\right) | i_{0}=i\right\}$$
- Shorthand notation for DP mappings
  $$(T J)(i)=\min _{u \in U(i)} \sum_{j=1}^{n} p_{i j}(u)(g(i, u, j)+\alpha J(j)), \quad i=1, \ldots, n$$
  $$\left(T_{\mu} J\right)(i)=\sum_{j=1}^{n} p_{i j}(\mu(i))(g(i, \mu(i), j)+\alpha J(j)), \quad i=1, \ldots, n$$



#### GENERAL ORIENTATION TO ADP

APPROXIMATE DP
-  “reinforcement learning” (RL). 关注的是让计算机从数据中学习决策，从机器学习的角度
- “neuro-dynamic programming” (NDP). 从数学的角度
- “adaptive dynamic programming” (ADP)  从自适应控制 (adaptive control) 发展而来的技术, 控制领域关注得更多的是系统的收敛性与稳定性

mainly adopt an n-state discounted model (the easiest case - but think of HUGE n). Extensions to other DP models (continuous space, continuous-time, not discounted) are possible.

many approaches:

- Problem approximation
- Simulation-based approaches (we will focus on these) 
  - Rollout (we will not discuss further)
  - Approximation in value space
  - Approximation in policy space



#### WHY SIMULATION?

One reason: Computational complexity advantage in computing sums/expectations involving a very large number of terms  优化计算复杂度
Any sum
$$
\sum_{i=1}^{n} a_{i}
$$
can be written as an expected value:
$$
\sum_{i=1}^{n} a_{i}=\sum_{i=1}^{n} \xi_{i} \frac{a_{i}}{\xi_{i}}=E_{\xi}\left\{\frac{a_{i}}{\xi_{i}}\right\}
$$
where $\xi$ is any prob. distribution over $\lbrace 1, \ldots, n\rbrace$.  It can be approximated by generating many samples $\lbrace i_{1}, \ldots, i_{k}\rbrace$ from $\{1, \ldots, n\}$, according to distribution $\xi,$ and Monte Carlo averaging:
$$
\sum_{i=1}^{n} a_{i}=E_{\xi}\left\{\frac{a_{i}}{\xi_{i}}\right\} \approx \frac{1}{k} \sum_{t=1}^{k} \frac{a_{i_{t}}}{\xi_{i_{t}}}
$$


Simulation is also convenient when <u>an analytical model of the system is unavailable</u>, but a simulation/computer model is possible.   解析模型不可知



#### APPROXIMATION IN VALUE SPACE

Approximate $J^\ast$ or $J_\mu$ from a parametric class $\tilde J(i; r)$ ,  r weights;

may also use parametric approximation for Q-factors or cost function differences.  (Advantage 函数)

#### APPROXIMATION ARCHITECTURES

linear and nonlinear : Linear architectures are easier to train, but non-linear ones (e.g., neural networks) are richer

<img src="2020-01-08-ADP.assets/image-20200114013526095.png" alt="image-20200114013526095" style="zoom:50%;" />

board position as state and move as control

##### LINEAR APPROXIMATION ARCHITECTURES

the features encode much of the nonlinearity inherent in the cost function approximated  特征能越多体现cost函数里面的非线性越好

With well-chosen features, we can use a linear architecture: $\tilde{J}(i ; r)=\phi(i)^{\prime} r, i=1, \ldots, n,$ or
$$
\tilde{J}(r)=\Phi r=\sum_{i=1}^{s} \Phi_{j} r_{j}
$$
<img src="2020-01-08-ADP.assets/image-20200114014632697.png" alt="image-20200114014632697" style="zoom:50%;" />

This is approximation on the subspace
$$
S=\left\{\Phi r | r \in \Re^{s}\right\}
$$
spanned by the columns of $\Phi$ (**basis functions 基函数**) 

Many examples of feature types: Polynomial approximation, radial basis functions, etc

##### ILLUSTRATIONS: POLYNOMIAL TYPE

- Polynomial Approximation 多项式近似
- **Interpolation 插值法** :从状态空间中选择特殊并且具有 代表性的状态子集 $I$，然后使用近似参数 $r_i, i ∈ I $来近似状态 i 对应的成本函数$\tilde{J}(i ; r)=r_{i}, \quad i \in I$ ，对于不在状态子集 $I$ 中的状态 i，使用插值法 (片段插 值、线性插值、多项式插值、样条曲线插值、三角内插法、有理内插、小波内 插等方法) 进行估计，然后只需要调整近似参数 r 就可以了。

$$
\tilde{J}(i ; r)=r_{i}, \quad i \in I
$$


##### Tetris



#### APPROX. PI

Use simulation to approximate the cost $J_\mu$ or Q-factors of the current policy $\mu$

<img src="2020-01-08-ADP.assets/image-20200114024432273.png" alt="image-20200114024432273" style="zoom:50%;" />

<img src="2020-01-08-ADP.assets/image-20200114024459209.png" alt="image-20200114024459209" style="zoom:50%;" />



##### APPROXIMATING $J^∗$ OR $Q^∗$

Approximation of the optimal cost function $J^\ast$ 

- Q-Learning: Use a simulation algorithm to approximate the Q-factors

$$
Q^{*}(i, u)=g(i, u)+\alpha \sum_{j=1}^{n} p_{i j}(u) J^{*}(j)
$$
and the optimal costs
$$
J^{*}(i)=\min _{u \in U(i)} Q^{*}(i, u)
$$
- Bellman Error approach: Find $r$ to

$$
\min _{r} E_{i}\left\{(\tilde{J}(i ; r)-(T \tilde{J})(i ; r))^{2}\right\}
$$
where $E_{i}\{\cdot\}$ is taken with respect to some distribution over the states 

- Approximate Linear Programming (we will not discuss here)

Q-learning can also be used with approximations

Q-learning and Bellman error approach can also be used for policy evaluation

#### APPROXIMATION IN POLICY SPACE

Use parametrization $\mu(i ; r)$ of policies with a vector $r=\left(r_{1}, \ldots, r_{s}\right) .$ Examples:

-  Polynomial, e.g.,  $\mu(i ; r)=r_{1}+r_{2} \cdot i+r_{3} \cdot i^{2}$
- Linear feature-based $\mu(i ; r)=\phi_{1}(i) \cdot r_{1}+\phi_{2}(i) \cdot r_{2}$

Optimize the cost over $r .$ For example:

- Each value of $r$ defines a stationary policy, with cost starting at state $i$ denoted by $\tilde{J}(i ; r)$ 
- Let $\left(p_{1}, \ldots, p_{n}\right)$ be some probability distribution over the states, and minimize over $r$

$$
\sum_{i=1}^{n} p_{i} \tilde{J}(i ; r)
$$
- Use a random search, gradient, or other method
  A special case: The parameterization of the policies is indirect, through a cost approximation architecture $\hat{J},$ i.e.
  $$\mu(i ; r) \in \arg \min _{u \in U(i)} \sum_{j=1}^{n} p_{i j}(u)(g(i, u, j)+\alpha \hat{J}(j ; r))$$



#### APPROXIMATE POLICY EVALUATION METHODS

















































### Reference

http://web.mit.edu/dimitrib/www/publ.html

https://github.com/Fengyuan-Shi/ADP_course_given_by_BERTSEKAS_2014_THU









