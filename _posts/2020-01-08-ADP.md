---
layout:     post
title:      Approximate Dynamic Programming
subtitle:   Note on "Approximate Dynamic Programming" (2014)
date:       2020-01-08 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-sunset.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - Dynamic Programming
    - ADP

---

# Approximate Dynamic Programming

**近似动态规划**  ADP course given by BERTSEKAS at THU, 2014 

DP方面的数学归纳总结比较多





## OUTLINE

**Large-scale DP** based on approximations andin part on simulation. 主题是基于近似 (一部分基于仿真) 的大规模动态规划. 

AI或者ML机器学习 叫它**强化学习**，作者叫它**神经动态规划 NDP**或者**近似动态规划 ADP**，目前主要有两个方向，某种程度上是互相融合的，一个方向是强化学习，主要依赖于基于**特征表达**的输入输出之间的函数关系，也可以叫基于观察样本或者仿真进行学习(强化学习特别强调与env交互)，另一个方向是**最优控制理论 optimization/control theory**，关注重点是传统的优化方法和算法，比如策略迭代和值迭代。 

Deals with control of dynamic systems under **uncertainty**, but applies more broadly (e.g., discrete deterministic optimization) ，这个不确定性，不仅仅指随机，还有可能是解决 **minmax** 的问题，这个时候不确定性就可以来源于对手做出的行为。实际上动态规划可以求解动态系统的控制问题，比如离散优化问题和组合最优化问题。

Bellman 提出了**动态规划 (Dynamic Programming)** 方法并给出**“维数灾难”(Curse of Dimensionality)** 的概念，Powell 认为维数灾有三种: 动作空间，状态空间和随机干扰. 



## LECTURE 1

#### DP AS AN OPTIMIZATION METHODOLOGY, DP作为一个优化方法

从更抽象的角度来看，只有一种优化问题，目标就是最小化cost函数，可以是标量，向量，或者更复杂的。优化的时候要受到约束集合的限制，优化就是从约束集合给出的决策集合 $U$ 中选择合适的决策 $u$ 让目标值最小。

- Generic optimization problem: $\min_{u \in U} g(u)$ ,  一般优化问题
  where $u$ **optimization/decision variable**, $g(u)$ **cost** function,   $U$  **constraint set**

- Categories of problems  根据 g 和 U, 对优化问题进行分类
  - **Discrete** (U is finite) or **continuous**
  - **Linear** (g is linear and U is polyhedral 多边形集) or **nonlinear**
  - **Stochastic** or **deterministic**: In stochastic problems,  the cost involves a stochastic parameter w,  $g(u) = E_w \lbrace G(u, w) \rbrace$  ,  env包含随机性, 所以是个期望 ;  确定性问题, 比如旅行商TSP问题

- DP deals with multistage stochastic problems

  动态规划能求解的问题需要满足某些特性 (马尔科夫性);  这里用的词 stage, 类似于time-step ; DP问题, 大问题拆成子问题, 不一定是按照时间维度去拆分的



#### BASIC STRUCTURE OF STOCHASTIC DP

- Discrete-time system:
  - **状态转移函数** $x_{k+1}=f_{k}\left(x_{k}, u_{k}, w_{k}\right), \quad k=0,1, \ldots, N-1$  
    - $k$ : Discrete **time**  , timestep的下标
    - $x_{k}:$ **State**; summarizes past information that is relevant for future optimization
    - $u_{k}:$ **Control**; decision to be selected at time $k$ from a given set 
    - $w_{k}:$ **Random parameter** (also called "disturbance" or "**noise**" depending on the context)  某个状态的中的随机因素
    - $N$: **Horizon** 界限 or number of times control is applied 
  - **状态转移概率** $P\left(x_{k+1} \vert x_{k}, u_{k}\right)$ 也可以用来描述系统

- $f$ 与 $p$ 就是代表着系统 Env.  两者的缺陷,  有时需要仿真采样来补充:
  - 状态转移函数f的缺陷是绝大多数难以得到解析表达的函数形式
  - 状态转移概率p的缺陷
    - 使用数值，状态转移矩阵的空间非常大，如果状态和控制有连续的，数值概率矩阵会有无数个维度
    - 使用解析的概率分布函数, 连续离散都可以表达，但这个解析关系非常难得到

- Cost function that is additive over time
与具体策略相关; 最后一步策略的t是N-1 , 对应的cost是 $g_N$.

$$
E \left \{ g_{N} \left(x_{N} \right)+\sum_{k=0}^{N-1} g_{k} \left(x_{k}, u_{k}, w_{k} \right) \right \}
$$

##### INVENTORY CONTROL EXAMPLE

- 下面一个例子, 主要看下 x, w的差异.    其实可以把 (x,w) 联合起来看作 state
- 库存控制的例子，仓库中有一些货物，然后来了一批客户，这时决策者 需要把货物卖给客户满足他们的需求，然后下订单补充货物。在这个库存系统中, $x_k$ 是阶段 k 时的库存状态，比如一个存放汽车的仓库，放了 $x_k$ 数量的汽车，来了几个客户，对汽车的需求是 $w_k$，如果完全满足了他们的需求，库存量 减少为 $x_k −w_k$，下了订货量为 $u_k$ 的订单补充库存，订单到达后 (假设订单到达前没有卖出汽车给客户) 新的库存状态为 $x_k −w_k +u_k$
- 这个例子符合马尔科夫性，或无后效性, 即当前决策只受到当前状态影响，下一状态也只受上一个状态与控制影响，而不受到更早的信息的影响, MDP
- **policies** ( also called feedback control laws),  策略, 状态到控制的映射; 这里$\mu$ 函数代表策略$\pi$, 表达为函数的形式,  相当于直接输出action的下标
$$
  u_{k}=\mu_{k}\left(x_{k}\right), \quad k=0, \ldots, N-1
$$



#### FINITE-HORIZON PROBLEM 有限时段问题

- **System** $x_{k+1}=f_{k}\left(x_{k}, u_{k}, w_{k}\right), k=0, \ldots, N-1$

- **Control contraints** $u_{k} \in U_{k}\left(x_{k}\right)$ 

- **Probability distribution** $P_{k}\left(\cdot \  \vert x_{k}, u_{k}\right)$ of $w_k$ 

- **Policies** $\pi=\{ \mu_{0}, \ldots, \mu_{N-1}\}$ ; $\mu_{k}$ maps  $x_{k}$ into controls $u_k = \mu_k(x_k)$ and  $\mu_k(x_k) \in U_{k}(x_{k})$ for all $x_{k}$  
  这里策略$\pi$, 是用每个时刻control的取值集合来表示的. 即  $\mu: x \to \pi$ 

- **Expected cost** of $\pi$ starting at $x_{0}$ is :  **cost的期望  J**   一定与一个策略相关
  $$
  J_{\pi} (x_{0} )=E\left\{g_{N} (x_{N} )+\sum_{k=0}^{N-1} g_{k}\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}
  $$
  
- **Optimal cost** function  $J^\ast=\min_{\pi} J_{\pi}\left(x_{0}\right)$ ,  定义式

- **Optimal polity** $\pi^\ast$  satisfies  $J_{\pi^\ast} (x_0) = J^\ast(x_0)$

  

#### PRINCIPLE OF OPTIMALITY 

Let $\pi^ \ast = \{   \mu_{0}^\ast, \mu_{1}^\ast, \ldots, \mu_{N-1}^\ast \}$ be optimal policy

Consider the “**tail subproblem**” 尾部子问题  whereby we are at $x_k$ at time k and wish to minimize the “**cost-to-go**” from time k to time $N$ and "**tail policy**"

<img src="/img/2020-01-08-ADP.assets/image-20200108232901646.png" alt="image-20200108232901646" style="zoom:50%;" />
$$
E\left\{g_{N}\left(x_{N}\right)+\sum_{\ell =k}^{N-1} g_\ell\left(x_{\ell }, \mu_{\ell }\left(x_{\ell }\right), w_{\ell }\right)\right\}
$$

**Principle of optimality** : The tail policy is optimal for the tail subproblem (optimization of the future does not depend on what we did in the past). The principle of optimality says that the tail of an optimal sequence is optimal for the tail subproblem.

大问题的**最优解**可以由小问题的**最优解**推出，这个性质叫做“最优子结构性质”。 



#### DP ALGORITHM

- Start with Initial condition: $J_{N}\left(x_{N}\right)=g_{N}\left(x_{N}\right)$  最后一个状态的g作为初值
- Go **backwards**, $k=N-1, \ldots, 0,$ using 

$$
J_{k} (x_{k} )=\min_{u_{k} \in U_{k} (x_{k} )}  \underset{w_{k}}{E}\left\{g_{k} (x_{k}, u_{k}, w_{k} ) 
 +J_{k+1} (f_{k} (x_{k}, u_{k}, w_{k}  ) )\right \}
$$

- Then $J_{0}\left(x_{0}\right)$ generated at the last step, is equal to the optimal cost $J^{*} (x_{0})$ Also, the policy $\pi^\ast = \left \lbrace \mu_0^\ast, \ldots, \mu_{N-1}^\ast \right \rbrace$ is optimal. 

这里对每个状态x, 每一步都找的最优的control, 对exact DP方法,   J是从最后一步倒推回来的, 只需要这个是真值, 同时,模型是完全已知的, 每个control对应的cost已知, 执行完DP流程后, $J \to J^\ast$ , 所以DP可以看成两个函数间的映射, 原函数J只需最后一步的值, 其他值都被更新掉了,所以其他步的值可以任意 ; J的实现可以用table, 如果把 J 看成函数, 则要认为这个函数是可以被赋值更新的, 直接看成映射关系比较容易理解. **mapping** 



#### PRACTICAL DIFFICULTIES OF DP

- The **curse** of **dimensionality**
  - Exponential growth of the computational and storage requirements as the number of state variables and control variables increases
  - Quick explosion of the number of states in combinatorial problems

- The **curse** of **modeling**
  - Sometimes a simulator of the system is easier to construct than a model

- **real-time** solution constraints
  - 一类问题, 需要很高实时性, 获取数据后计算时间的很少 The data of the problem to be solved is given with little advance notice
  - The problem data may change as the system is controlled – need for on-line replanning

所以要近似求解, All of the above are motivations for **approximation** and **simulation**



#### COST APPROXIMATION

optimal cost-to-go function $J_{k+1}$  replaced by approximation $\tilde J_{k+1}$

- Problem Approximation: Use J derived from a related but simpler problem 使用一个更容易计算的问题的cost, 用处很窄

- **Parametric** Cost-to-Go Approximation:  parameters are tuned by some heuristic or systematic scheme 

- **Rollout** Approach:  the cost of some suboptimal policy, which is calculated either analytically or by simulation



#### ROLLOUT ALGORITHMS

At each k and state $x_k$, use the control $\bar \mu(x_k)$ that minimizes in $\min_{u_{k} \in U_{k} (x_{k} ) }  {E}\left [g_{k} (x_{k}, u_{k}, w_{k} )  + \tilde J_{k+1} \left(f_{k} (x_{k}, u_{k}, w_{k}  ) \right)\right ]$ ,  where $\tilde J_{k+1}$ is the cost-to-go of some **heuristic** policy (**base policy**).

从这个启发式策略出发，通过前向仿真计算这个策略执行后的成本，再选择控制，一般情况下会得到一个不会更差(可能更优)的策略，这种方法很适合在线计算

Cost improvement property: The rollout algorithm achieves no worse (and usually much better) cost than the base policy starting from the same state.

Main difficulty:  Calculating  $\tilde J_{k+1}$  maybe computationally intensive if the cost-to-go of the base policy cannot be analytically calculated.

- May involve **Monte Carlo simulation** if the problem is **stochastic**.
- Things improve in the deterministic case (an important application is discrete optimization).
- Connection w/ Model Predictive Control (MPC). 在该领域rollout很有效



#### INFINITE HORIZON PROBLEMS  无限时段问题

- The number of stages is infinite.
- The system is **stationary**.

两种角度:  总成本,  平均成本;  平均成本的近似理论还在研究中, 下面用总成本.  因为总成本可能无穷大, 所以要处理:  使用折扣系数与截断阶段;  截断阶段是把趋于 $\infty$ 的 N 截断，令 N 等于一个固定的值，将无限期问题近似转化为有限期问题，然后求解。

Total cost:  minimize

$$
J_{\pi}\left(x_{0}\right)=\lim _{N \rightarrow \infty} \underset{w_k \atop k=0,1, \ldots}{E}\left\{\sum_{k=0}^{N-1} \alpha^{k} g\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}
$$

- Discounted problems (α < 1, bounded g)
- Stochastic shortest path problems (α = 1, finite-state system with a termination state) 
- Discounted and undiscounted problems with unbounded cost per stage  无穷大问题, 不讨论

生活中很少 (甚至是完全) 不会产生无限期问题，那么为什么要研究无限期问题呢。原因是无限期问题在数学上更优雅，我们可以把注意力 放在平稳策略上，这样就可以在任何阶段都使用相同的策略进行控制，大量的研究产生了大量有效的算法，很多近似理论也是为无限期问题服务的. 



##### DISCOUNTED PROBLEMS/BOUNDED COST

- **Stationary system** $x_{k+1}=f\left(x_{k}, u_{k}, w_{k}\right), \quad k=0,1, \ldots$      时间维度
  平稳系统: 每一个阶段的系统状态都在同一个状态空间内取值，随机干扰 w 在每一阶段都有相同的概率分布. 
- cost of a policy $\pi=\lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$ :

  $$
  J_{\pi}\left(x_{0}\right)=\lim _{N \rightarrow \infty} \underset{w_k \atop k=0,1, \ldots}{E}\left\{\sum_{k=0}^{N-1} \alpha^{k} g\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}
  $$
  
  with $\alpha<1,$ and $g$ is bounded [for some $M,$ we have $\vert g(x, u, w)\vert \leq M \text { for all }(x, u, w)]$ 
- Optimal cost function: $J^\ast (x)=\min_{\pi} J_{\pi}(x)$
- **Boundedness** of $g$ guarantees that all costs are well-defined and bounded: $\vert J_{\pi}(x) \vert \leq \frac{M}{1-\alpha}$ , 用等比数列证明. 



#### SHORTHAND NOTATION FOR DP MAPPINGS,  DP速记符号

算子是从函数到函数的映射;  下面的两个算子, 根据定义公式, 都对所有状态x成立, 把所有state都过了一轮.  $J$本来对应一个默认的策略. 是什么没所谓,因为会在某个时刻 k, control被替换掉; 算子本身如果不带时刻i下标,则没指定哪个时刻k, 表示任意某个时刻,对应的那一步control的改变.

引入 算子T,  For **any function** J of x, denote:   $(T J)(x)=\min _{u \in U(x)} \underset{w}{E}\{g(x, u, w)+\alpha J(f(x, u, w))\}, \forall x$

$TJ$ is the optimal cost function for the one-stage problem with stage cost g and **terminal cost function** $\alpha J$.  根据前面的DP算法, 整个算法的迭代过程中, 只需要最后一步的cost函数的真值.  这个算子, 只对应策略里的一个greedy control.  如果该问题只有一个stage, 期望的cost 变为 最优化cost $TJ$ .  

对 discounted 问题, T operates on bounded functions of x to produce other bounded functions of x. 有界函数到有界函数的映射. 

For any stationary policy $\mu,$  引入策略算子 $T_{\mu}$ ,  下式是定义式,  $J$ 只是一个有界函数
$\left(T_{\mu} J\right)(x)=\underset{w}{E}\{g(x, \mu(x), w)+\alpha J(f(x, \mu(x), w))\}, \forall x$

The critical structure of the problem is captured in $T$ and $T_{\mu}$    算子包含了问题的关键结构信息. $T$ and $T_{\mu}$ provide a powerful unifying framework for DP:  essence of  "**Abstract Dynamic Programming**"

$T = \min_\mu T_\mu$



#### FINITE-HORIZON COST EXPRESSIONS, Cost的算子表达式

这边定义的算子, 都是有时间下标的, 对应策略里面的k下标, 即在时刻k,执行该control, J函数的改变. 下式中的$\ell$ 时间步, 都是原先的默认策略的. 

Consider an $N$-stage policy $\pi_{0}^{N}=\lbrace \mu_{0}, \mu_{1}, \ldots, \mu_{N-1}\rbrace$ with a **terminal cost** $J$( 这里的$J$是最后一步的cost函数, 是递推的源头,但不涉及策略):

$$
\begin{aligned}
J_{\pi_{0}^{N}}\left(x_{0}\right)&=E\left\{\alpha^{N} J\left(x_{k}\right)+\sum_{\ell=0}^{N-1} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\} \\
 &=E\left\{g\left(x_{0}, \mu_{0}\left(x_{0}\right), w_{0}\right)+\alpha J_{\pi_{1}^{N}}\left(x_{1}\right)\right\}  \\
 &=\left(T_{\mu_{0}} J_{\pi_{1}^{N}}\right)\left(x_{0}\right) 
\end{aligned}
$$

where $\pi_{1}^{N}=\lbrace \mu_{1}, \mu_{2}, \ldots, \mu_{N-1}\rbrace$  , 上式对应了策略里面的第一步control造成的改变, 后面的看成一个整体

By induction we have  $J_{\pi_{0}^{N}}(x)=\left(T_{\mu_{0}} T_{\mu_{1}} \cdots T_{\mu_{N-1}} J\right)(x), \quad \forall x$ ;   T执行顺序, 对$J$而言, 从右到左, 离得最近的先执行

For a stationary policy $\mu$ the $N$-stage cost function (with terminal cost $J$ ) is $J_{\pi_{0}^{N}}=T_{\mu}^{N} J$,  where $T_{\mu}^{N}$ is the $N$-fold composition of $T_{\mu}$ ;     $T_\mu^N$ 表示一个完整策略每一步的映射, 上标表示按时间执行N步optimal

optimal $N$-stage cost function  (  with terminal cost $J$ )  is $T^{N} J$  ;  从最后一步开始逆向N步执行optimal

$T^{N} J=T\left(T^{N-1} J\right)$ is just the **DP algorithm** , DP算法的简洁表达,  意义: 左边, 每一步都optimal ; 右边, 括号里面的先执行 , 表示从最后的J一直optimal到时刻1, 然后时刻0再来一个T 取optimal.  这里的括号不能少,源头也关键.



#### "SHORTHAND" THEORY-A SUMMARY

- Infinite horizon cost function expressions [with $\left.J_{0}(x) \equiv 0\right]$ 
  对于无限问题, 是没有最后一步, 对discounted问题, 只需把每步的g加起来, 因为g是在第一步control之后才有的, 所以$J_0 = 0$ , 下式的意思就是, 对一个无限问题, 默认的策略都会被覆盖掉, 得到一个新的J函数
  $$
  J_{\pi}(x)=\lim _{N \rightarrow \infty}\left(T_{\mu_{0}} T_{\mu_{1}} \cdots T_{\mu_{N}} J_{0}\right)(x), \quad J_{\mu}(x)=\lim _{N \rightarrow \infty}\left(T_{\mu}^{N} J_{0}\right)(x)
  $$
  
- **Bellman's equation**: $J^\ast=T J^\ast, \quad J_{\mu}=T_{\mu} J_{\mu}$     已收敛

- **Optimality** condition:  $\mu: \text { optimal }    \iff  T_{\mu} J^\ast=T J^\ast$   一个策略函数最优, 表示该策略在任意k时刻,都取的cost最小值的control , 定义

- **Value iteration**: For any (bounded) $J$ , $J^{*}(x)=\lim_{k \rightarrow \infty}\left(T^{k} J\right)(x), \quad \forall x$  这个公式里面没时间下标, 可以看成, 对无论stage多长的问题, 逆序地每个stage进行一次T

- **Policy iteration**: Given $\mu^{k}$ , 注意这里是上标,  表示策略的一个版本. 算子的下标都没有时间i, 对任意状态都成立

  - Policy evaluation: Find $J_{\mu^{k}}$ by solving , $J_{\mu^{k}}=T_{\mu^{k}} J_{\mu^{k}}$    求解$J$, 即evaluate到收敛
  
  - Policy improvement: Find $\mu^{k+1}$ such that  $T_{\mu^{k+1}} J_{\mu^{k}}=T J_{\mu^{k}}$   improve, 在所有状态上都select min, 选择cost最小的那个control, 然后要重新evaluate
  
    $\mu^k \to J_{\mu^k} \stackrel{improve}{\longrightarrow} \mu^{k+1} \to J_{\mu_{k+1}} \to  \dots $



##### TWO KEY PROPERTIES

- **Monotonicity** property **单调性**: For any $J$ and $J′$ such that $J(x) ≤ J′(x)$ for all $x$, and any $\mu$,  $(TJ)(x) ≤ (TJ′)(x), \quad (T_\mu J)(x) ≤ (T_\mu J′)(x), \quad \forall x$

  Monotonicity is present in all DP models  单调性对所有DP问题都成立

- Constant Shift property : For any $J$, any scalar $r$, and any $\mu$,  $\left(T(J+re) \right)(x) = (TJ)(x)+ \alpha r,\left(T_\mu(J+re) \right)(x) = (T_\mu J)(x)+ \alpha r, \quad \forall x$,   where $e$ is the unit function $[e(x) \equiv 1]$.   

  $(J + re)(x)$ 的意思是 $J(x) + r$ , 即在$J(x)$加上一个常数 , 因为乘以T算子相当于执行了一个时间步, 所以r有discount $\alpha$ 
  
  Constant shift is special to discounted models  只对discounted问题成立

**contraction mappings** : for Discounted problem



笔记里面的几个定义以及性质:

- **metric space**
  一个 metric space 由一个二元组 ⟨M,d⟩ 组成，其中 M 表示集合，d 表示M 的一个 metric，即映射 $d : M × M → \mathbb R$ 。其中 d 满足如下的距离的三条公理:对于任意 $x, y, z ∈ M$ , 有

  1. (非负性) d(x,y)≥0 ，且 $d(x,y)=0⇔x=y$
  2. (对称性) d(x, y) = d(y, x)
  3.  (三角不等式) $d(x, z) ≤ d(x, y) + d(y, z)$
  
  Metric Space 表示的是某个集合中任意两个点之间的距离都被定义了，那么这些点的集合就叫做 Metric Space，这些距离被叫做这个 space 的一个 metric，同一个 space 可以有无数个 metric
  
- **Cauchy sequence**

  - 设 $x_n$ 是距离空间 X 中的点列，如果对于任意的 $ε > 0$，存在自然数 N，当m, n > N 时，$\vert x_n−x_m \vert < ε$，称 $x_n$ 是一个 Cauchy sequence . 
  - Cauchy sequence 指的是如果一个 metric space 中的序列上的点随着点的序号变大，两个点之间的距离 (定义 metrix space 时定义的距离) 越来 越近直到无限接近的趋势. 

- **complete metric space**
  
  - 一个 metric space ⟨M, d⟩ 是完备的 (或者说是 Cauchy 的)，当且仅当所有在M中的 Cauchy 序列，都会收敛到 M 中。
  - complete metric space 是说某个 metric space 中所有 Cauchy sequence 最后都会收敛于该 metric space 中的元素，这个 metric space 才是 complete metric space，所以想要证明一个 metric space 不是 complete matric space 很简单，只要找到一个反例就可以了。
  
- **contraction mapping**
  
  - In mathematics, a contraction mapping, or contraction or contractor, on a metric space ⟨M, d⟩ is a function f from M to itself, with the property that there is some nonnegative real number 0 ≤ k < 1 such that for all x and y in M, $d(f (x), f (y)) ≤ kd(x, y)$ . 
  - contraction mapping 是说某个 matric space 中的映射 f 如果满足任意两个 点经过映射后产生的两个新的点距离要比映射前的点距离要近，这个映射 f 就是该 matric space 中的一个 contraction mapping，满足条件的最小的 k 被叫做 f 的 Lipschitz constant，k 越小说明压缩得越厉害。
  
- **Contracting mapping theorem**

  对于一个 complete metric space ⟨M, d⟩ ，如果 f : M 􏰀→ M 是它的一个压缩映射，那么

  1. 在该 complete metric space 中，存在唯一的不动点 $x^\ast$:  $f(x^\ast) = x^\ast$

  2. 并且，对于任意的 x ∈ M , 定义序列 $f^2(x) = f(f(x)) , f^3(x) = f(f^2(x)),··· ,f^n(x) = f(f^{n−1}(x))$, 该序列会收敛于$x^\ast$,  即 $\lim_{n→∞} f^n(x) = x^\ast$ 

     证明, 利用压缩性以及三角不等式, 
     $$
     \begin{aligned}
     d(x, y) & \leq d(x, f(x)) + d(f(x), y) \\& \leq d(x, f(x)) + d(f(x), f(y)) + d(f(y), y) \\&\leq d(x, f(x)) + kd(x, y) + d(f(y), y) = d(f(x), x) + kd(x, y) + d(f(y), y)
     \\ \Rightarrow  d(x, y) & \leq \frac{d(f(x),x)+d(f(y),y)}{1-k}
     \end{aligned}
     $$
     序列 $f^n(x)$ 中的任意两个点 $f^M (x), f^N (x)$，满足
     $$
     \begin{aligned}
     d(f^M (x), f^N (x)) \leq & \frac{d(f^{M+1} (x), f^M (x))+d(f^{N+1}(x), f^N (x))}{1-k}
     \\ \leq & \frac{k^M d(f(x), x) + k^N d(f(x), x)}{1-k} \quad{利用压缩性,可以去f}
     \\ = & \frac{k^M + k^N}{1-k} d(f(x),x)
     \end{aligned}
     $$
     得到 $\lim_{M,N \to \infty} d(f^M (x), f^N (x)) = 0$, 所以序列$f^n(x)$ 收敛. 

     
  
     唯一不动点证明: 由于序列 $f^n(x)$ 是收敛的, 同时 $f^n(x)$ 处于 complete metric space 中，假设它收敛于 metric space 中某个不动点 $x^\ast = lim_{n\to \infty} f^n(x)$ ，根据压缩映射 的定义可知 f 连续，所以 $f(x^\ast) = f(lim_{n\to \infty} f^n(x)) = lim_{n\to \infty} f(f^n(x)) = lim_{n\to \infty} f^{n+1}(x) = x^\ast$。
     
     证明不动点存在后用反证法证明 $x^\ast$ 的唯一性，假设有两个点$x^\ast,y^\ast$ 满足 $f(x^\ast)=x^\ast,f(y^\ast)=y^\ast$，那么$0<d(x^\ast,y^\ast)=d(f(x^\ast),f(y^\ast))≤kd(x^\ast,y^\ast)< d(x^\ast, y^\ast)$得到 $d(x^\ast, y^\ast) < d(x^\ast, y^\ast)$ ，与假设矛盾，因此假设不成立，$x^\ast$ 是唯 一的不动点。
  
  
  
  由于 $x^\ast =f (x^\ast)$,  所以 $d(f^{n}(x), x^{*} )=d (f^{n}(x), f(x^\ast))$,根据压缩映射的 定义，可以得到 $d(f^n(x),f(x^\ast)) ≤ kd(f^{n−1}(x),x^\ast) ≤ k^nd(x,x^\ast)$ , 所以序列 $f^n(x)$ 以线性速率 k 收敛。

现在分析策略评价、值迭代和策略迭代的收敛性:假设状态集合 $S=\lbrace s_{1}, s_{2}, \cdots, s_{n} \rbrace$, 行动集合$A=\lbrace a_{1}, a_{2}, \cdots, a_{n}\rbrace$, 定义状态值向量函数: $\mathbf{v}=\left[v (s_{1} ), v (s_{2}), \cdots, v(s_{n} )\right]^{T}$ , 距离函数$$d (\mathbf v_1, \mathbf v_2)=\max _{s \in \mathcal S}  \vert \mathbf v_1(s)-\mathbf v_2 (s)  \vert$$ ,  由于 $$\mathcal{V}=\mathcal{R}^{n}$$ , 所以所有序列的点一定都在空间$\mathcal{V}$中, 因此$\langle\mathcal{V}, d\rangle$ 是一个 complete metric space. 定义策略函数向量: $\pi=\left[\mu (s_{1} ), \mu (s_{2}), \cdots, \mu (s_{n} )\right]^{T}$ ,  属于行动空间 $\mathcal A$,  定义$\mathcal A$ 中的距离函数 $$d\left(\mu_{1}, \mu_{2}\right)=\max _{s \in \mathcal{S}}  \vert \left( \Vert a_{1}(s)-a_{2}(s) \Vert_{2}\right)  \vert$$ , 

Policy evaluation 策略评价: bellman 期望方程向量形式为 $\mathbf u_{new} = T^\pi(\mathbf u) = \mathcal R^\pi + \gamma \mathcal P^\pi \mathbf u$，其中 $\mathcal P^\pi$ 是 策略$\pi$下状态转移的概率分布函数，可以计算得到 $\mathbf u$ 与 $\mathbf v$ 使用算子 $T^\pi$ 后新的点距离为 
$$
\begin{aligned}
d\left(T^{\pi}(\mathbf{u}), T^{\pi}(\mathbf{v})\right) &=\left\|\left(\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} \mathbf{u}\right)-\left(\mathcal{R}^{\pi}+\gamma \mathcal{P}^{\pi} \mathbf{v}\right)\right\|_{\infty} \\
&=\left\|\gamma \mathcal{P}^{\pi}(\mathbf{u}-\mathbf{v})\right\|_{\infty} \\
& \leq\left\|\gamma \mathcal{P}^{\pi}\right\| \mathbf{u}-\mathbf{v}\left\|_{\infty}\right\|_{\infty} \\
& \leq \gamma\|\mathbf{u}-\mathbf{v}\|_{\infty} \\
&=\gamma d(\mathbf{u}, \mathbf{v})
\end{aligned}
$$


所以 $T^\pi$ 是一个压缩映射，由压缩映射定理 (Contracting mapping theorem) 可以得到

1. 值向量空间$\mathcal{V}$中存在唯一的点$\mathbf v_\pi^\ast$ 满足 $\mathbf v_\pi^\ast = T^\pi(\mathbf v_\pi^\ast)$
2. 对于任意的 $\mathbf v \in \mathcal V$ 在映射 $T^\pi$ 的作用下都会收敛于 $\mathbf v_\pi^\ast$，即 $\lim_{n \to \infty} T^n_\pi(\mathbf v) = \mathbf v_\pi^\ast$
3. 策略评价迭代以线性速率 $\gamma$ 收敛于 $\mathbf v_\pi^\ast$

value iteration 值迭代:  bellman 最优方程 (值迭代) 的矩阵形式迭代式: $\mathbf u_{new} = T^\ast(\mathbf u) = \max_{a \in \mathcal A} \mathcal R^a + \gamma \mathcal P^a \mathbf u$   , 与策略评价迭代式相似，可以得到 $d(T^\ast(\mathbf u), T^\ast(\mathbf v)) \leq \gamma d(\mathbf u, \mathbf v)$ , 所以$T^\ast$也是压缩映射，由压缩映射定理可以得到相同的结论:

1. 值向量空间$\mathcal{V}$中存在唯一的点$\mathbf v^\ast$ 满足 $\mathbf v^\ast = T^\ast(\mathbf v^\ast)$
2. 对于任意的 $\mathbf v \in \mathcal V$ 在映射 $T^\ast$ 的作用下都会收敛于 $\mathbf v^\ast$，即 $\lim_{n \to \infty} (T^\ast)^n(\mathbf v) = \mathbf v^\ast$
3. 值迭代以线性速率 $\gamma$ 收敛于 $\mathbf v^\ast$

Policy improvement 策略改进: 策略改进矩阵形式表达式: $\mathbf \pi_{new} = I^\ast(\mathbf \pi) = \arg \max_{\pi} \mathcal R^\pi + \gamma \mathcal P^\pi \mathbf v $ ,  与策略评价迭代式相似，可以得到 $d(I^\ast(\mathbf \pi_1), I^\ast(\mathbf \pi_2)) \leq \gamma d(\mathbf \pi_1, \mathbf \pi_2)$ , 所以 $I^\ast$ 也是压缩映射，由压缩映射定理可以得到相同的结论:

1. 策略空间$\mathcal{\Pi}$中存在唯一的点$\mathbf \pi^\ast$ 满足 $\mathbf \pi^\ast = T^\ast(\mathbf \pi^\ast)$
2. 对于任意的 $\mathbf \pi \in \mathcal \Pi$ 在映射 $I^\ast$ 的作用下都会收敛于 $\mathbf \pi^\ast$，即 $\lim_{n \to \infty} (I^\ast)^n(\pi) = \mathbf \pi^\ast$
3. 以线性速率 $\gamma$ 收敛于 $\mathbf \pi^\ast$







#### CONVERGENCE OF VALUE ITERATION

- for all bounded $J$ 

  $$
  J^{*}(x)=\lim _{k \rightarrow \infty}\left(T^{k} J\right)(x), \quad \text { for all } x
  $$

- Proof: For simplicity we give the proof for $J \equiv 0$ For any initial state $x_{0},$ and policy $\pi=\lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$

  $$
\begin{aligned}
J_{\pi}\left(x_{0}\right)&=
E\left\{\sum_{\ell=0}^{\infty} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\} \\
&=E\left\{\sum_{\ell=0}^{k-1} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\} \\
& \quad\quad +E\left\{\sum_{\ell=k}^{\infty} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\}
\end{aligned}
  $$

  The tail portion satisfies

  $$
\left|E\left\{\sum_{\ell=k}^{\infty} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\}\right| \leq \frac{\alpha^{k} M}{1-\alpha}
  $$

  where $M \geq \vert g(x, u, w) \vert$  ;  上面是说, $J_\pi$ 是有界的, 然后两边都上T算子, 每步都取最小,再取极限, 会收敛到固定值. 
<u>Take min</u> over $\pi$ of both sides, then $\lim$ as $k \rightarrow \infty $  **Q.E.D.**   



##### BELLMAN’S EQUATION

- The optimal cost function $J^\ast$ is a solution of Bellman's equation,   $J^\ast=T J^\ast$,   i.e., for all $x,$
$$
J^\ast(x)=\min _{u \in U(x)} E\left\{g(x, u, w)+\alpha J^\ast(f(x, u, w))\right\}
$$

- Proof: For all $x$ and $k$

  $$
J^{*}(x)-\frac{\alpha^{k} M}{1-\alpha} \leq\left(T^{k} J_{0}\right)(x) \leq J^{*}(x)+\frac{\alpha^{k} M}{1-\alpha}
  $$

  where $J_{0}(x) \equiv 0$ and $M \geq \vert g(x, u, w)\vert$ .  Applying $T$ to this relation, and using Monotonicity and Constant Shift,   两边夹逼

  $$
\begin{aligned}\left(T J^{*}\right)(x)-\frac{\alpha^{k+1} M}{1-\alpha} & \leq\left(T^{k+1} J_{0}\right)(x) \\ & \leq\left(T J^{*}\right)(x)+\frac{\alpha^{k+1} M}{1-\alpha} \end{aligned}
  $$

  Taking the limit as $k \rightarrow \infty$ and using the fact

  $$
\lim_{k \rightarrow \infty}\left(T^{k+1} J_{0}\right)(x)=J^*(x)
  $$

  we obtain $J^∗ = TJ^∗$. **Q.E.D.**



#### THE CONTRACTION PROPERTY

- **Contraction** property **收缩**性质: For any bounded functions $J$ and $J^{\prime},$ and any $\mu$  
$$
  \max _{x}\left|(T J)(x)-\left(T J^{\prime}\right)(x)\right| \leq \alpha \max _{x}\left|J(x)-J^{\prime}(x)\right| \\
\max _{x}\left|\left(T_{\mu} J\right)(x)-\left(T_{\mu} J^{\prime}\right)(x)\right| \leq \alpha \max _{x}\left|J(x)-J^{\prime}(x)\right|
$$

映射后产生的两个新的点距离要比映射前的点距离要近

- Proof: Denote $c=\max _{x \in S}\vert J(x)-J^{\prime}(x)\vert$ ,  Then 

  $$
J(x)-c \leq J^{\prime}(x) \leq J(x)+c, \quad \forall x
  $$

  Apply $T$ to both sides, and use the Monotonicity and Constant Shift properties:

  $$
(T J)(x)-\alpha c \leq\left(T J^{\prime}\right)(x) \leq(T J)(x)+\alpha c, \quad \forall x
  $$

  Hence
  $$
\left|(T J)(x)-\left(T J^{\prime}\right)(x)\right| \leq \alpha c, \quad \forall x
  $$

  **Q.E.D.**
  
- Note: This implies that $J^{*}$ is the **unique** solution of $J^{*}=T J^{*},$ and $J_{\mu}$ is the **unique** solution



#### NEC. AND SUFFICIENT OPT. CONDITION 最优的充要条件

- A stationary policy $\mu$ is optimal if and only if $\mu(x)$ attains the minimum in Bellman's equation for each $x ;$ i.e.,
  $$
T J^{*}=T_{\mu} J^{*}
  $$

  or, equivalently, for all $x$
  $$
\mu(x) \in \arg \min _{u \in U(x)} E\left\{g(x, u, w)+\alpha J^{*}(f(x, u, w))\right\}
  $$

- Proof: If $T^\ast=T_{\mu} J^\ast$, then using Bellman's equation $\left(J^\ast=T J^\ast \right)$,  we have
  $$
J^{*}=T_{\mu} J^{*}
  $$

  so by uniqueness of the fixed point of $T_{\mu},$ we obtain $J^{*}=J_{\mu} ;$ i.e., $\mu$ is optimal. 

  Conversely, if the stationary policy $\mu$ is optimal, we have $J^{*}=J_{\mu},$ so
  $$
J^{*}=T_{\mu} J^{*}
  $$

  Combining this with Bellman's Eq. $\left(J^\ast=T J^\ast \right)$ obtain $T J^\ast=T_{\mu} J^\ast$ . **Q.E.D.**



## LECTURE 2

#### VI AND PI

- Value iteration: For any (bounded) $J$ 

$$
J^{*}(x)=\lim _{k \rightarrow \infty}\left(T^{k} J\right)(x), \quad \forall x
$$

- Policy iteration: Given $\mu^{k}$ 

  - Policy evaluation: Find $J_{\mu^{k}}$ by solving,  到收敛
    $$
    J_{\mu^{k}}(x)=\underset{w}{E}\left\{g\left(x, \mu^{k}(x), w\right)+\alpha J_{\mu^{k}}\left(f\left(x, \mu^{k}(x), w\right)\right)\right\}, \forall x \\
    J_{\mu^{k}}=T_{\mu^{k}} J_{\mu^{k}}
    $$
    
  - Policy improvement: Let $\mu^{k+1}$ be such that , 下式符号是属于, 右边是集合
    $$
    \mu^{k+1}(x) \in \arg \min _{u \in U(x)} \underset{w}{E}\left\{g(x, u, w)+\alpha J_{\mu^{k}}(f(x, u, w))\right\}, \forall x \\
    T_{\mu^{k+1}} J_{\mu^{k}}=T J_{\mu^{k}}
    $$

    For the case of $n$ states, policy evaluation is equivalent to solving an $n \times n$ linear system of equations: $J_{\mu}=g_{\mu}+\alpha P_{\mu} J_{\mu}$  ; 直接求解析解不可行

    For large $n,$ exact $\mathrm{PI}$ is out of the question (even though it terminates finitely as we will show)



#### JUSTIFICATION OF POLICY ITERATION, PI的证明

 $J_{\mu^{k}} \geq J_{\mu^{k+1}}$ for all $k$  ;  证明PI是一直改进的

Proof : For given $k,$ we have $J_{\mu^{k}}=T_{\mu^{k}} J_{\mu^{k}} \geq T J_{\mu^{k}}=T_{\mu^{k+1}} J_{\mu^{k}}$ ,  其中$T_{\mu^{k+1}} J_{\mu^{k}}$是一个还没evaluate的cost函数,  可以作为一个中间值, 然后继续执行$\mu^{k+1}$, 就可以收敛到 $J_{\mu^{k+1}}$.

Using the monotonicity property of DP,  $J_{\mu^{k}} \geq T_{\mu^{k+1}} J_{\mu^{k}} \geq T_{\mu^{k+1}}^{2} J_{\mu^{k}} \geq \dots \geq \lim_{N \rightarrow \infty} T_{\mu^{k+1}}^{N} J_{\mu^{k}}$, since $\lim_{N \rightarrow \infty} T_{\mu^{k+1}}^{N} J_{\mu^{k}}=J_{\mu^{k+1}}$  we have $J_{\mu^{k}} \geq J_{\mu^{k+1}}$ . 

If $J_{\mu^{k}}=J_{\mu^{k+1}},$ all above inequalities hold as equations, so $J_{\mu^{k}}$ solves Bellman's equation. Hence $J_{\mu^{k}}=J^{*}$ 

Thus at iteration k either the algorithm generates a **strictly improved** policy or it finds an **optimal** policy

- For a finite spaces MDP, the algorithm **terminates** with an optimal policy

- For infinite spaces MDP, **convergence** (in an infinite number of iterations) can be shown



#### OPTIMISTIC POLICY ITERATION

策略迭代的策略评价算法标准情况下是迭代直到收敛的，但是 Optimistic PI不会迭代到收敛，而是迭代若干次 (m 次)，不等到收敛就停止，然后进行策略改进，继续做策略评价，这也是一种近似方法。

**Optimistic PI**:  This is PI, where policy evaluation is done approximately, with a finite number of VI

approximate the policy evaluation $J_\mu \approx  T^m_\mu  J $ ,   inital $J$   ; 先近似evaluate, 这里,  把收敛值记为 $J_{\mu^{k}}$ , 未收敛的近似值记为 $J_k$ , 下式

Shorthand definition: For some integers $m_k$,  

$$
T_{\mu^k} J_k = T J_k, \quad  \text{improve, get } \mu^k \\
J_{k+1} = T_{\mu^k}^{m_k} J_k , \quad  \mu^k \text{execed, get approx evaluate } J_{k+1}
$$

- If $m_k ≡1$ it becomes VI , 上式, 如果在improve的时候直接把$J_{k+1}$ 记下来, 就相当于evaluate一次
- If $m_k = \infty$ it becomes PI

$ J_k \stackrel{improve}{\longrightarrow} \mu^k \stackrel{eval}{\dashrightarrow } J_{k+1} \to \mu^{k+1} \dashrightarrow J_{k+2} \to  \dots $ 

详细定义: Optimistic Policy Iteration: Discounted Problems
Given the typical function $J_{k}$:
**Policy improvement** computes a policy $\mu^{k}$ such that
$$
\mu^{k}(i) \in \arg \min _{u \in U(i)} \sum_{j=1}^{n} p_{i j}(u)\left(g(i, u, j)+\alpha J_{k}(j)\right), \quad i=1, \ldots, n
$$

**Optimistic policy evaluation** starts with $\hat J_{k, 0}=J_k$,  and uses $m_k$ VI iterations for policy $\mu^k$ to compute $\hat J_{k, 1}, \dots, \hat J_{k, m_{k}}$ according to:
$$
\hat{J}_{k, m+1}(i)=\sum_{j=1}^{n} p_{i j}\left(\mu^{k}(i)\right)\left(g\left(i, \mu^{k}(i), j\right)+\alpha \hat{J}_{k, m}(j)\right)
$$

for all $i=1, \ldots, n, m=0, \ldots, m_{k}-1,$ and sets $J_{k+1}=\hat J_{k, m_k}$


之前 : $\mu^k \to J_{\mu^k} \stackrel{improve}{\longrightarrow} \mu^{k+1} \to J_{\mu_{k+1}} \to  \dots $



**Converges** for both finite and infinite spaces discounted problems (in an infinite number of iterations). Typically works  **faster**  than VI and PI (for large problems)



#### APPROXIMATE PI

policy evaluation is approximate,

$$
\|J_{k}-J_{\mu^{k}} \| \leq \delta, \quad k=0,1
$$

policy improvement is approximate, 这里, 在$J_k$的基础上改进,必定就是近似的

$$
\|T_{\mu^{k+1}} J_{k}-T J_{k}  \| \leq \epsilon, \quad k=0,1
$$

where $\delta$ and $\epsilon$ are some positive scalars.

Error Bound I:误差上界 The sequence $\lbrace \mu^{k} \rbrace$ generated by approximate policy iteration satisfies

$$
\limsup _{k \rightarrow \infty}\left\|J_{\mu^{k}}-J^{*}\right\| \leq \frac{\epsilon+2 \alpha \delta}{(1-\alpha)^{2}}
$$

Typical practical behavior: The method makes steady progress up to a point and then the iterates
$J_\mu^{k}$ oscillate within a neighborhood of  $J^\ast$  最优附近震荡

Error Bound II: If in addition the sequence $\lbrace \mu^{k} \rbrace$ "terminates" at $\bar{\mu}$ (i.e., keeps generating $\bar{\mu}$ , 策略稳定后) 

$$
\left\|J_{\bar{\mu}}-J^{*}\right\| \leq \frac{\epsilon+2 \alpha \delta}{1-\alpha}
$$

α 是折扣问题中的折扣因子，这个折扣因子需要人为调整，如果折扣因子 特别接近一，分母就会非常小导致误差很大，但是折扣因子非常小，趋近于零的时候，bellman 方程就会很接近于贪心算子。

这两个界限用于理论分析. 



#### Q-FACTORS (x,u), (s,a)

- Optimal Q-factor of $(x, u):$

$$
Q^{*}(x, u)=E\left\{g(x, u, w)+\alpha J^{*}(\bar{x})\right\}
$$

with $\bar{x}=f(x, u, w)$, 表示后继状态.  It is the cost of starting at $x$ applying $u$ is the 1st stage, and an optimal policy after the 1st stage. 定义为当前系统状态 x 下执行控制 u 产生的即时成本, 加上之后一直走optimal 策略的cost期望。

- Bellman's equation:	 $J^\ast (x)=\min_{u \in U(x)} Q^\ast (x, u), \quad \forall x$
- VI method: 

$$
J_{k+1}(x)=\min _{u \in U(x)} Q_{k+1}(x, u), \quad \forall x
$$

where $Q_{k+1}$ is generated by $Q_{k+1}(x, u)= E \lbrace g(x, u, w)+\alpha \min_{v \in U(\bar x)} Q_k(\bar x, v)\rbrace$ with $\bar x=f(x, u, w)$

- Q-factors are costs in an “augmented” problem where states are (x, u) ,   强化学习的(s, a)

They satisfy a Bellman equation $Q^\ast = FQ^\ast$ where ,  这里用F表示贝尔曼算子
$$
(FQ)(x, u)= E \lbrace g(x, u, w)+\alpha \min_{v \in U(\bar x)} Q_k(\bar x, v)\rbrace
$$
VI and PI for Q-factors are mathematically equivalent to VI and PI for costs

They require equal amount of computation ... they just need more storage 需要更多资源, 因为要存模型的信息

Having optimal Q-factors is convenient when implementing an optimal policy on-line by
$$
\mu^*(x) = \min_{u \in U(x)} Q^*(x, u)
$$
Once $Q^\ast(x,u)$ are known, the model [$g$ and $E\{\cdot \}$] is not needed. **Model-free** operation, 引入Q, 是为了解决不知道model的那些问题

**Q-Learning** (to be discussed later) is a sampling method that calculates $Q^\ast(x,u)$ using a simulator of the system (no model needed)



#### OTHER DP MODELS

之前讨论是 discounted model. 

- Undiscounted problems (α = 1)
- Continuous-time finite-state MDP:  连续时间是相邻两个状态产生的时间间隔不是均匀的，任意时刻都可能有新状态产生.  有两种连续时间模型，第一种是每时每刻都在产生新状态，即状态 x(t) 是 关于时间 t 的函数，比如电机控制等系统，另一种是两个状态以满 足某概率分布的时间间隔进行跳转，一个比较典型的例子，是排队系统，一个客户 (可能以泊松分布等概率分布) 到达，状态增加 1， 客户以随机时间离开，随机的时间长短依赖于控制 ， 实际上，排队系统详细划分应该属于半马尔科夫过程 (semi-markov decision process, semi-MDP)





#### CONTINUOUS-TIME MODELS

- **System equation**: $d x(t) / d t=f(x(t), u(t))$
- **cost** function: $\int_{0}^{\infty} g(x(t), u(t))$
- Optimal cost starting from $x: J^{*}(x)$ 
- $\delta$-Discretization of time: $x_{k+1}=x_{k}+\delta \cdot f\left(x_{k}, u_{k}\right)$   连续问题离散化
- Bellman equation for the $\delta$ -discretized problem:
  $$J_{\delta}^{*}(x)=\min _{u}\left\{\delta \cdot g(x, u)+J_{\delta}^{*}(x+\delta \cdot f(x, u))\right\}$$
- Take $\delta \rightarrow 0,$ to obtain the **Hamilton-Jacobi-Bellman equation** [assuming $ \lim_{\delta \rightarrow 0} J_{\delta}^\ast(x)=J^\ast(x) $]

  $$
  0=\min _{u}\left\{g(x, u)+\nabla J^{*}(x)^{\prime} f(x, u)\right\}, \quad \forall x
  $$
  这里是对 状态x的导数,  $x_{k+1} - x_k$,   公式里的'是转置的
  

Policy Iteration (informally):

- Policy evaluation: Given current $\mu,$ solve
  $0=g(x, \mu(x))+\nabla J_{\mu}(x)^{\prime} f(x, \mu(x)), \quad \forall x$
- Policy improvement: Find

$$
\bar{\mu}(x) \in \arg \min _{u}\left\{g(x, u)+\nabla J_{\mu}(x)^{\prime} f(x, u)\right\}
$$
Note: Need to learn $\nabla J_{\mu}(x)$ NOT $J_{\mu}(x)$



#### MORE GENERAL/ABSTRACT VIEW OF DP

Let $Y$ be a real vector space with a norm $\|\cdot\|$

A function $F: Y \mapsto Y$ is said to be a **contraction mapping 压缩映射** if for some $\rho \in(0,1),$ we have

$$
\|F y-F z\| \leq \rho\|y-z\|, \quad \text { for all } y, z \in Y
$$

$\rho$ is called the **modulus of contraction 收缩模量** of $F$

Important example: Let $X$ be a set (e.g., state space in DP),   $v: X \mapsto \Re$ be a **positive-valued** function.  Let $B(X)$  be the set of all functions $J: X \mapsto \Re$ , 所有cost函数空间 , such that $J(x) / v(x)$ is bounded over x. 
We define a norm on $B(X)$ , called the **weighted sup-norm 加权最大范数**, by
$$
\|J\|=\max _{x \in X} \frac{|J(x)|}{v(x)}
$$

Important special case: The discounted problem mappings $T$ and $T_μ$,  [for $v(x) ≡ 1, ρ = α$]. v(x)就是加权weight.



##### CONTRACTION MAPPINGS: AN EXAMPLE

- Consider extension from finite to countable state space, $X=\{1,2, \ldots\},$ and a weighted sup norm with respect to which the one stage costs are bounded. 比如一个排队系统，系统状态为客户数量，客户数量可能是 1, 2, · · · ， 每个阶段的成本依赖于系统状态而且没有上界， 之前的分析可以知道这种系统很难求解，现在要给出一个让成本函数有界的方法，即使用加权最大范数 (sup-weighted norm) 处理这个问题让它有界。
- Suppose that $T_{\mu}$ has the form

$$
\left(T_{\mu} J\right)(i)=b_{i}+\alpha \sum_{j \in X} a_{i j} J(j), \quad \forall i=1,2,
$$

where $b_{i}$ and $a_{i j}$ are some scalars. Then $T_{\mu}$ is a contraction with modulus $\rho$ if and only if

$$
\frac{\sum_{j \in X}\left|a_{i j}\right| v(j)}{v(i)} \leq \rho, \quad \forall i=1,2
$$

  这个条件如果满足, 相当于对加权系数有了一些约定, 可以让最终cost有界
- Consider $T$,     $$(T J)(i)=\min _{\mu}\left(T_{\mu} J\right)(i), \quad \forall i=1,2, \ldots$$
  where for each $\mu \in M, T_{\mu}$ is a contraction mapping with modulus $\rho$.  Then $T$ is a contraction mapping with modulus  $\rho$.  对这类问题,  $T$ 与 $T_\mu$ 都是压缩映射

Allows extensions of main DP results from bounded one-stage cost to interesting unbounded one-stage cost cases.



##### CONTRACTION MAPPING FIXED-POINT TH.

**Contraction Mapping Fixed-Point Theorem 压缩映射不动点理论**: If $F: B(X) \mapsto B(X)$ is a contraction with modulus $\rho \in(0,1),$ then there exists a unique $J^{*} \in B(X)$ such that
$$
J^*=F J^{*}
$$

Furthermore, if $J$ is any function in $B(X),$ then $\lbrace F^{k} J\rbrace$ **converges** to $J^{*}$ and we have

$$
\left\|F^{k} J-J^{*}\right\| \leq \rho^{k}\left\|J-J^{*}\right\|, \quad k=1,2, \dots
$$

This is a special case of a general result for contraction mappings $F: Y \mapsto Y$ over normed vector spaces $Y$ that are complete: every sequence $\lbrace y_{k}\rbrace$ that is **Cauchy** (satisfies $\left\|y_{m}-y_{n}\right\| \rightarrow 0$ as $m, n \rightarrow \infty)$ converges. 如果一个范数向量空间  $Y$ 中所有柯西序列 $\lbrace y_{k}\rbrace$  都收敛， 那么这个空间  $Y$ 是一个完备空间
The space $B(X)$ is complete 完备空间 



#### ABSTRACT FORMS OF DP

本页给出了满足单调性和压缩性的动态规划的更加抽象形式. We consider an abstract form of DP based on monotonicity and contraction

**Abstract Mapping**: Denote $R(X)$:  set of realvalued functions  $J: X \mapsto \Re$  , $R(X)$表示实函数空间   ,  and let $H: X \times U \times R(X) \mapsto \Re$ be a given mapping,  H是所有J的映射的空间.  We consider the mapping
$$
(T J)(x)=\min _{u \in U(x)} H(x, u, J), \quad \forall x \in X
$$

We assume that $(T J)(x)>-\infty$ for all $x \in X$ so $T$ maps $R(X)$ into $R(X)$ 

**Abstract Policies**: Let $\mathcal{M}$ be the set of "policies"所有策略函数的集合, i.e., functions $\mu$ such that $\mu(x) \in U(x)$ for all $x \in X$
For each $\mu \in \mathcal{M},$ we consider the mapping $T_{\mu}: R(X) \mapsto R(X)$ defined by
$$
\left(T_{\mu} J\right)(x)=H(x, \mu(x), J), \quad \forall x \in X
$$

Find a function $J^\ast \in R(X)$ such that
$$J^{*}(x)=\min _{u \in U(x)} H\left(x, u, J^{*}\right), \quad \forall x \in X$$



##### EXAMPLES

对不同类型的问题, 核心部分就是 J函数的映射,  这是一个通用性很强的框架

- Discounted problems
  $$H(x, u, J)=E\{g(x, u, w)+\alpha J(f(x, u, w))\}$$

- Discounted "discrete-state continuous-time" Semi-Markov Problems (e.g., queueing)

$$
H(x, u, J)=G(x, u)+\sum_{y=1}^{n} m_{x y}(u) J(y)
$$

​		where $m_{x y}$ are "discounted" transition probabilities, defined by the distribution of transition times
- Minimax Problems/Games
$H(x, u, J)=\max _{w \in W(x, u)}[g(x, u, w)+\alpha J(f(x, u, w))]$

- Shortest Path Problems

$$
H(x, u, J)=\left\{\begin{array}{ll}
{a_{x u}+J(u)} & {\text { if } u \neq d} \\
{a_{x d}} & {\text { if } u=d}
\end{array}\right.
$$



##### ASSUMPTIONS

对于 H 算子有一些假设，下面说一说这些假设和假设带来的性质

**Monotonicity**: If $J, J^{\prime} \in R(X)$ and $J \leq J^{\prime}$,  $H(x, u, J) \leq H\left(x, u, J^{\prime}\right), \quad \forall x \in X, u \in U(x)$

​	We can show all the standard analytical and computational results of discounted DP if monotonicity and the following assumption holds:  如果映射具有单调性，那么所有的动态规划问题都可以使用相同的标准分析过程和计算结果。

**Contraction**:

- For every $J \in B(X),$ the functions $T_{\mu} J$ and $T J$ belong to $B(X)$ 
- For some $\alpha \in(0,1),$ and all $\mu$ and $J, J^{\prime} \in$ $B(X),$ we have

$$
\left\|T_{\mu} J-T_{\mu} J^{\prime}\right\| \leq \alpha\left\|J-J^{\prime}\right\|
$$
​	如果 H 具有压缩性，那么可以知道 T 和 Tμ 也具有压缩性。

With just monotonicity assumption 只有单调性 (as in undiscounted problems) we can still show various forms of the basic results under appropriate conditions.  

A weaker substitute for contraction assumption is **semicontractiveness** 部分压缩性: (roughly) for some $\mu, T_{\mu}$ is a contraction and for others it is not; also the "noncontractive" $\mu$ are not optimal. 这时候由于算子没法保证压缩性，也就没法保证最优性 

##### RESULTS USING CONTRACTION

使用压缩映射,得到的推论:  不动点 以及 收敛

Proposition 1: The mappings $T_{\mu}$ and $T$ are weighted sup-norm contraction mappings with modulus $\alpha$ over $B(X),$ and have unique **fixed points** in $B(X),$ denoted $J_{\mu}$ and $J^\ast,$ respectively $(\mathrm{cf}$ Bellman's equation).

Proof: From the contraction property of $H$

Proposition 2: For any $J \in B(X)$ and $\mu \in \mathcal{M}$
$$
\lim _{k \rightarrow \infty} T_{\mu}^{k} J=J_{\mu}, \quad \lim _{k \rightarrow \infty} T^{k} J=J^\ast
$$

(cf. convergence of value iteration).

Proof: From the contraction property of $T_{\mu}$ and $T$

Proposition 3: We have $T_{\mu} J^\ast=T J^\ast$ if and only if $J_{\mu}=J^\ast$ (cf. optimality condition).

Proof: $T_{\mu} J^\ast=T J^\ast,$ then $T_{\mu} J^\ast=J^\ast,$ implying $J^\ast=J_{\mu} .$ Conversely, if $J_{\mu}=J^\ast,$ then $T_{\mu} J^\ast=$ $T_{\mu} J_{\mu}=J_{\mu}=J^\ast=T J^\ast$

##### RESULTS USING MON. AND CONTRACTION

使用单调性以及压缩,得到的结论

-  Optimality of fixed point:  单调改进, 则不动点是最优的

$$
J^{*}(x)=\min _{\mu \in \mathcal{M}} J_{\mu}(x), \quad \forall x \in X
$$

-  Existence of a nearly optimal policy: For every $\epsilon>0$, there exists $\mu_{\epsilon} \in \mathcal{M}$ such that

$$
J^{*}(x) \leq J_{\mu_{\epsilon}}(x) \leq J^{*}(x)+\epsilon, \quad \forall x \in X
$$

-  Nonstationary policies 非稳态: Consider the set $\Pi$ of all sequences $\pi= \lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$ with $\mu_{k} \in \mathcal{M}$  for all $k$, and define

$$
J_{\pi}(x)=\liminf _{k \rightarrow \infty}\left(T_{\mu_{0}} T_{\mu_{1}} \cdots T_{\mu_{k}} J\right)(x), \quad \forall x \in X
$$

​	with $J$ being any function (the choice of $J$ does not matter), We have 囊括非稳态的最优解的定义
$$
J^{*}(x)=\min _{\pi \in \Pi} J_{\pi}(x) , \quad \forall x \in X
$$





#### ASYNCHRONOUS ALGORITHMS 异步算法

Motivation for asynchronous algorithms 

- Faster convergence
- Parallel and distributed computation 并行计算
- Simulation-based implementations

General framework: Partition $X$ into disjoint nonempty subsets $X_{1}, \dots, X_{m}$,  and use separate processor $\ell$ updating $J(x)$ for $x \in X_{\ell}$ ,  把状态分成不相交非空子集以及对应的cost函数集.  Let $J$ be partitioned as $J=\left(J_{1}, \dots, J_{m}\right)$
where $J_{\ell}$ is the restriction of $J$ on the set $X_{\ell}$

-  Synchronous VI algorithm:  更新所有状态子集的J

$$
J_{\ell}^{t+1}(x)=T\left(J_{1}^{t}, \ldots, J_{m}^{t}\right)(x), \quad x \in X_{\ell}, \ell=1, \ldots, m
$$

-  Asynchronous VI algorithm: For some subsets of times $\mathcal{R}_{\ell}$

$$
J_{\ell}^{t+1}(x)=\left\{\begin{array}{ll}{T\left(J_{1}^{\tau_{\ell 1}(t)}, \ldots, J_{m}^{\tau_{\ell m}(t)}\right)(x)} & {\text { if } t \in \mathcal{R}_{\ell}} \\ {J_{\ell}^{t}(x)} & {\text { if } t \notin \mathcal{R}_{\ell}}\end{array}\right.
$$

where $t-\tau_{\ell j}(t)$ are communication "delays"



##### ONE-STATE-AT-A-TIME ITERATIONS

一个比较重要的例子，假设有 n 个状态，分成了 n 个子集，使用仿真方法生 成系统状态，每一个状态都会产生无数次，使用 n 个机器去计算 J(x) 的值， 这种情况下算法是没有通信延迟的。

Important special case: Assume $n$ "states", a separate processor for each state, and no delays

Generate a sequence of states $\lbrace x^{0}, x^{1}, \ldots\rbrace$,  generated in some way, possibly by simulation (each state is generated infinitely often)

Asynchronous VI:
$$
J_{\ell}^{t+1}=\left\{\begin{array}{ll}
{T\left(J_{1}^{t}, \ldots, J_{n}^{t}\right)(\ell)} & {\text { if } \ell=x^{t}} \\
{J_{\ell}^{t}} & {\text { if } \ell \neq x^{t}}
\end{array}\right.
$$
where $T\left(J_{1}^{t}, \ldots, J_{n}^{t}\right)(\ell)$ denotes the $\ell$ -th component of the vector

$$
T\left(J_{1}^{t}, \ldots, J_{n}^{t}\right)=T J^{t}
$$

The special case where
$$
\left\{x^{0}, x^{1}, \ldots\right\}=\{1, \ldots, n, 1, \ldots, n, 1, \ldots\}
$$

is the **Gauss-Seidel method**  高斯-赛德尔迭代.   迭代时就先访问第一个状态，更新第一个状态的成本，然后访问第二个状态的成本，一次访问下去直到最后一个状态 $x^n$ 的成本，再从第一个状态重 新开始直到收敛，这种迭代方法很经典



##### ASYNCHRONOUS CONV. THEOREM

异步迭代的收敛性,  需要对普通版本的值迭代和策略迭代做一点修改 才能让异步算法工作，这个修改是建立在两个假设的基础上，第一个假设是 每一个机器都能够进行无限次数的更新，即每个状态都会被访问无数次，也就是课件中给的 $\ell, j=1, \dots, m, \mathcal R_\ell$ is infinite 想要表达的假设，第二个假设是延迟通信会一直进 行下去，算法会一直传递 (延迟的) 信息，τ 与 t 永远会存在一个 gap，即课 件中 $\lim_{t \rightarrow \infty} \tau_{\ell j}(t)=\infty$ 要表达的内容。这就是两个比较基本的假设。

KEY FACT: VI and also PI (with some modifications) still work when implemented asynchronously

Assume that for all $\ell, j=1, \dots, m, \mathcal R_\ell$ is infinite and $\lim_{t \rightarrow \infty} \tau_{\ell j}(t)=\infty$

Proposition: Let $T$ have a unique fixed point $J^\ast$ and assume that there is a sequence of nonempty subsets $\{S(k)\} \subset R(X)$ with $S(k+1) \subset S(k)$ for all $k,$ and with the following properties:

- Synchronous Convergence Condition: Every sequence $\lbrace J^{k}\rbrace$ with $J^{k} \in S(k)$ for each $k$ converges pointwise to $J^\ast$.  Moreover,

$$
T J \in S(k+1), \quad \forall J \in S(k), k=0,1, \ldots
$$
- Box Condition: For all $k, S(k)$ is a Cartesian product 笛卡儿积 of the form

$$
S(k)=S_{1}(k) \times \cdots \times S_{m}(k)
$$
where $S_{\ell}(k)$ is a set of real-valued functions on $X_{\ell}, \ell=1, \dots, m$

Then for every $J \in S(0),$ the sequence $\lbrace J^{t} \rbrace$ generated by the asynchronous algorithm converges pointwise to $J^\ast$

同步值迭代和异步值迭代，主要区别是同步值迭代每次迭代都一次性更新所有 J 的值，异步值迭代只有根据集合 $\mathcal R_\ell$在特定迭代次数的时候更新特定子集的J，有些时候异步值迭代有通信延迟，有些时候异步迭代没有通信延迟，这就是同步值迭代和异步值迭代的区别。 由于 exact DP的单调性, 所有的子线程都在往好的方向努力.

Interpretation of assumptions:  下面用图表示异步DP

<img src="/img/2020-01-08-ADP.assets/image-20200112232013512.png" alt="image-20200112232013512" style="zoom:50%;" />

A synchronous iteration from any J in S(k) moves into S(k + 1) (component-by-component)

Convergence mechanism:

<img src="/img/2020-01-08-ADP.assets/image-20200112232035104.png" alt="image-20200112232035104" style="zoom:50%;" />

Key: “Independent” component-wise improve- ment. An asynchronous component iteration from any J in S(k) moves into the corresponding com- ponent portion of S(k + 1)



## LECTURE 3

首先介绍近似动态规划，然后介绍近似结构，比如参数化近似形式和近似 结构，然后会讲一下使用仿真的近似策略迭代，同时讨论为什么仿真技术在 动态规划领域这么重要，接下来会讨论一些策略评估的近似方法和策略改进 的问题，最后讲一些近似和方针的一般性的问题。

### APPROXIMATE DP

#### MDP - TRANSITION PROBABILITY NOTATION

assume the system is an n-state (controlled) Markov chain,  switch to Markov chain notation 

- States  $i = 1, \dots, n$  [instead of x]
- **Transition probabilities** $p_{i_{k} i_{k+1}}\left(u_{k}\right)$ [instead of $x_{k+1}=f (x_{k}, u_{k}, w_{k} )$]
- Stage cost $g\left(i_{k}, u_{k}, i_{k+1}\right)$  instead of [$  g\left(x_{k}, u_{k}, w_{k}\right) $] 
- Cost functions $J=(J(1), \ldots, J(n))$ (vectors in $\Re^{n}$)
- cost of a policy $\pi=\lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$
  $$J_{\pi}(i)=\lim _{N \rightarrow \infty} \underset{r_{k} \atop k=1,2, \ldots}{E}\left\{\sum_{k=0}^{N-1} \alpha^{k} g\left(i_{k}, \mu_{k}\left(i_{k}\right), i_{k+1}\right) | i_{0}=i\right\}$$
- Shorthand notation for DP mappings
  $$(T J)(i)=\min _{u \in U(i)} \sum_{j=1}^{n} p_{i j}(u)(g(i, u, j)+\alpha J(j)), \quad i=1, \ldots, n$$
  $$\left(T_{\mu} J\right)(i)=\sum_{j=1}^{n} p_{i j}(\mu(i))(g(i, \mu(i), j)+\alpha J(j)), \quad i=1, \ldots, n$$



#### GENERAL ORIENTATION TO ADP

- APPROXIMATE DP, ADP
  -  “reinforcement learning” (RL). 关注的是让计算机从数据中学习决策，从机器学习的角度
  - “neuro-dynamic programming” (NDP). 从数学的角度
  - “adaptive dynamic programming” (ADP)  从自适应控制 (adaptive control) 发展而来的技术, 控制领域关注得更多的是系统的收敛性与稳定性
- mainly adopt an n-state discounted model (the easiest case - but think of HUGE n).
-  Extensions to other DP models (continuous space, continuous-time, not discounted) are possible.
- many approaches:
  - Problem approximation  问题近似, 换个类似的简单问题
  - Simulation-based approaches (we will focus on these) 
    - Rollout
    - Approximation in value space
    - Approximation in policy space



#### WHY SIMULATION?

One reason: Computational complexity advantage in computing sums/expectations involving a very large number of terms  通过抽样来优化计算复杂度
Any sum
$$
\sum_{i=1}^{n} a_{i}
$$
can be written as an expected value:
$$
\sum_{i=1}^{n} a_{i}=\sum_{i=1}^{n} \xi_{i} \frac{a_{i}}{\xi_{i}}=E_{\xi}\left\{\frac{a_{i}}{\xi_{i}}\right\}
$$
where $\xi$ is any prob. distribution over $\lbrace 1, \ldots, n\rbrace$.  It can be approximated by generating many samples $\lbrace i_{1}, \ldots, i_{k}\rbrace$ from $\{1, \ldots, n\}$, according to distribution $\xi,$ and Monte Carlo averaging:
$$
\sum_{i=1}^{n} a_{i}=E_{\xi}\left\{\frac{a_{i}}{\xi_{i}}\right\} \approx \frac{1}{k} \sum_{t=1}^{k} \frac{a_{i_{t}}}{\xi_{i_{t}}}
$$

Simulation is also convenient when <u>an analytical model of the system is unavailable</u>, but a simulation/computer model is possible.   当解析模型不可知



### APPROXIMATION IN VALUE AND POLICY SPACE

#### APPROXIMATION IN VALUE SPACE

Approximate $J^\ast$ or $J_\mu$ from a parametric class $\tilde J(i; r)$ ,  r weights;    直接拟合$J$, $V$的值

may also use parametric approximation for Q-factors or cost function differences.  (Advantage 函数)

##### APPROXIMATION ARCHITECTURES  近似的模型结构

**linear** and **nonlinear** : Linear architectures are easier to train, but non-linear ones (e.g., neural networks) are richer

<img src="/img/2020-01-08-ADP.assets/image-20200114013526095.png" alt="image-20200114013526095" style="zoom:50%;" />

board position as state and move as control

##### LINEAR APPROXIMATION ARCHITECTURES 线性模型

the features encode much of the nonlinearity inherent in the cost function approximated  特征能越多体现cost函数里面的非线性越好

注意该公式里面, '符号表示转置;   With well-chosen **features**,  linear architecture: $\tilde{J}(i ; r)=\phi(i)^{\prime} r, i=1, \ldots, n,$ or
$$
\tilde{J}(r)=\Phi r=\sum_{i=1}^{s} \Phi_{j} r_{j}
$$
<img src="/img/2020-01-08-ADP.assets/image-20200114014632697.png" alt="image-20200114014632697" style="zoom:50%;" />

This is approximation on the subspace
$$
S=\left\{\Phi r | r \in \Re^{s}\right\}
$$
spanned by the columns of $\Phi$ (**basis functions 基函数**) 

Many examples of feature types: Polynomial approximation, radial basis functions, etc

##### ILLUSTRATIONS: POLYNOMIAL TYPE

- **Polynomial Approximation** 多项式近似
- **Interpolation 插值法** :从状态空间中选择特殊并且具有 代表性的状态子集 $I$，然后使用近似参数 $r_i, i ∈ I $来近似状态 i 对应的成本函数$\tilde{J}(i ; r)=r_{i}, \quad i \in I$ ，对于不在状态子集 $I$ 中的状态 i，使用插值法 (片段插 值、线性插值、多项式插值、样条曲线插值、三角内插法、有理内插、小波内 插等方法) 进行估计，然后只需要调整近似参数 r 就可以了。

$$
\tilde{J}(i ; r)=r_{i}, \quad i \in I
$$


##### Tetris



##### APPROX. PI : OPTION TO APPROX. $J\mu$ OR $Q_\mu$

Use **simulation** to approximate the cost $J_\mu$ or Q-factors of the current policy $\mu$.  先sample再train

<img src="/img/2020-01-08-ADP.assets/image-20200114024432273.png" alt="image-20200114024432273" style="zoom: 33%;" />

<img src="/img/2020-01-08-ADP.assets/image-20200114024459209.png" alt="image-20200114024459209" style="zoom: 33%;" />



##### APPROXIMATING $J^∗$ OR $Q^∗$

另一种方法是近似$J^∗$ or $Q^∗$,  思路是先计算某些状态的最优值, 然后用模型拟合

- Approximation of the optimal cost function $J^\ast$ ;   知道了 $J^\ast$ 或者 $Q^\ast$ 中的任意一个，就可以根据这个关系计算另一个,  J与Q都可以用来PI .  Q learning是一个计算最优Q值的算法. 
  - **Q-Learning**: Use a simulation algorithm to approximate the Q-factors

  $$
Q^{*}(i, u)=g(i, u)+\alpha \sum_{j=1}^{n} p_{i j}(u) J^{*}(j)
  $$

  ​       and the optimal costs  
  $$
J^{*}(i)=\min _{u \in U(i)} Q^{*}(i, u)
  $$
  - **Bellman Error** approach: Find $r$ to     ;  TD 方法
  $$
\min _{r} E_{i}\left\{(\tilde{J}(i ; r)-(T \tilde{J})(i; r))^{2}\right\}
  $$
​       where $E_{i}\{\cdot\}$ is taken with respect to some distribution over the states 

  - Approximate Linear Programming 近似线性规划

- Q-learning can also be used with approximations 
- Q-learning and Bellman error approach can also be used for policy evaluation



#### APPROXIMATION IN POLICY SPACE

直接拟合$\pi$函数,  用参数化模型

Use parametrization $\mu(i ; r)$ of policies with a vector $r=\left(r_{1}, \ldots, r_{s}\right) .$ Examples:

-  Polynomial, e.g.,  $\mu(i ; r)=r_{1}+r_{2} \cdot i+r_{3} \cdot i^{2}$
- Linear feature-based $\mu(i ; r)=\phi_{1}(i) \cdot r_{1}+\phi_{2}(i) \cdot r_{2}$

Optimize the cost over $r .$ For example:

- Each value of $r$ defines a stationary policy, with <u>cost starting at state $i$</u> denoted by $\tilde{J}(i ; r)$ ; 下面需要做的就是搜索成本最小的参数 r，每一组参数 r 都对应一个平稳策略, 计算出个起始状态近似函数 $\tilde{J}(i ; r)$ 的值
- Let $\left(p_{1}, \ldots, p_{n}\right)$ be some probability distribution over the states, and minimize over $r$  , 计算总的期望cost, 开始优化r

$$
\sum_{i=1}^{n} p_{i} \tilde{J}(i ; r)
$$
- Use a random search, gradient, or other method  可以使用随机搜索, 策略梯度等方法
  

A special case: The parameterization of the policies is indirect, through a cost approximation architecture $\hat{J},$ i.e.    先用参数r计算近似cost函数, 再计算control策略
  $$\mu(i ; r) \in \arg \min _{u \in U(i)} \sum_{j=1}^{n} p_{i j}(u)(g(i, u, j)+\alpha \hat{J}(j ; r))$$



### APPROXIMATE POLICY EVALUATION METHODS

前面都是模型拟合 J 和 Q , 现在讨论怎么评估policy

#### DIRECT POLICY EVALUATION

给定了一个策略, 直接评估policy的步骤:  model拟合J, J 采样得到, 通过最小二乘法算出来

- Approximate the cost of the current policy by **least squares** and **simulation** cost samples. 
- Amounts to **projection** of $J_\mu$ onto the approximation subspace.  $J_\mu$是一个高维度空间内的高维度向量，在高维空间内找到一个更低维度的空间并找到一个合理的近似, 通过projection映射到 $\Pi J_\mu$ 
- Solution by **least squares methods**.    define some kind of projection matrix some norm , project to $\Pi J_\mu$,   假设这个 Euclidean norm 欧几里得范数会将目标函数变为最小二乘问题，那么这个问题就变成了最小化所有$J_\mu$和它的投影的平方误差之和. 这是一个quadratic problem二次问题，可以使用通过策略$\mu$观测到的数据进行求解
- Regular and optimistic policy iteration.
- Nonlinear approximation architectures may also be used

策略$\mu$的成本函数 $J_\mu$ 维度非常大，搜索空间也非常大，根本没法算出精确解，所以要把原问题的$J_\mu$ 空间使用算子 $\Pi$ 降维映射到低维度空间 $\Pi J_\mu$进行搜索，其实就是使用近似结构在参数空间中搜索近似效果最好的近似函数。

对于任意的特征函数 $\Phi$，参数 r 在空间中$r \in \Re^{n}$对应的 $\Pi J_\mu$集合理论上与原问题的 $J_\mu$ 集合是相等的集合，但是对于给定的特征函数 $\Phi$，参数 r 在空间中 $r \in \Re^{n}$ 对应的 $\Pi J_\mu$集合就不能保证与原问题的集合 $J_\mu$相等了，现在需要做的有两件事，第一件事是给一个合适的特征函数 $\Phi$，然后需要在这个 $\Phi$ 的基础上在参数空间中搜索最优的参数 r 让近似效果最好，可以理解成需要 解决一个回归问题，使用最小二乘模型求解参数 r，最小二乘模型需要知道各近似成本函数的值，这个值可以通过各种方法求解，比如仿真技术之类的方法估计成本函数，估计成本函数后使用这些值计算近似效果最好的参数 r，然后使用这个参数 r 计算其他状态下的成本，如果近似结构是非线性结构，也可以使用同样的思路和方法进行近似。

##### DIRECT EVALUATION BY SIMULATION

直接求解策略cost非常困难, 可以通过 simulation;  即用一个model拟合sample来的J

在使用投影 (可以理解成近似) 求 bellman 方程次优解的时候，如果系统模型不可知或者模型规模庞大，可以使用仿真方法估计成本函数的值，由于仿真采样无法获取所有状态的样本，所以只能使用不完全采样的结果进行估算，同时采样使用的概率分布可能不准确，所以在估值的时候需要使用加权欧几里得范数 (weighted Euclidean norm).   

由于无法计算$J_\mu$所有的成本向量，所以我们可以有选择性地计算一部分状态的成本, 对某个策略从状态i开始进行很多次仿真, 仿真过程中我们获得了很多不同的轨迹，可以计算这些轨迹的平均成本来近似从某个状态开始时某策略的期望成本. 换句话说, 评估$J_\mu$的投影需要求解这个最小二乘问题，$r^\ast$是与最优投影相关的量，最小化$J_\mu$和近似值的欧几里得二范数误差可以得到$r^\ast$的值.    

- Projection by **Monte Carlo Simulation**: Compute the projection $\Pi_{\mu}$ of $J_{\mu}$ on subspace $S= \lbrace \Phi r  \vert r \in \Re^{s} \rbrace$, with respect to a **weighted Euclidean norm** $\Vert \cdot \Vert_{\xi}$ 
- Equivalently, find $\Phi r^{*}$, where

$$
r^{*}=\arg \min _{r \in \Re^{s}}\left\|\Phi r-J_{\mu}\right\|_{\xi}^{2}=\arg \min _{r \in \Re^{s}} \sum_{i=1}^{n} \xi_{i}\left(\phi(i)^{\prime} r-J_{\mu}(i)\right)^{2}
$$

- Setting to 0 the gradient at $r^{*}$, 最小二乘法: $Xb = y$ 的解: $b = (X^TX)^{-1}X^Ty$

$$
r^{*}=\left(\sum_{i=1}^{n} \xi_{i} \phi(i) \phi(i)^{\prime}\right)^{-1} \sum_{i=1}^{n} \xi_{i} \phi(i) J_{\mu}(i)
$$

上面有两个问题, 一个是无法知道$J_\mu$的精确值, 二是n非常大才能精确计算; 所以下面要上仿真, 通过采样得到的J来拟合模型.

即使经过近似后，最优参数 $r^\ast$ 由于状态空间巨大依然难以计算，同时这个矩阵求逆计算是逆矩阵的期望，之前讲过的使用仿真技术代替大规模计算就可以使用了，使用仿真技术使用状态概率分布 ξ 采样 k 次，得到 k 个样本 $\{\left(i_{1}, J_{\mu}\left(i_{1}\right)\right), \ldots,\left(i_{k}, J_{\mu}\left(i_{k}\right)\right)\}$, 然后使用蒙特卡洛方法计算 r 的 期望值 $\hat r$ 来代替最小二乘的最优参数 $r^\ast$，直接对策略成本投影，很好用但不是最好的方法。

- Generate samples $\{\left(i_{1}, J_{\mu}\left(i_{1}\right)\right), \ldots,\left(i_{k}, J_{\mu}\left(i_{k}\right)\right)\}$ using distribution $\xi$  
- Approximate by Monte Carlo the two "expected values" with low-dimensional calculations

$$
\hat{r}_{k}=\left(\sum_{t=1}^{k} \phi\left(i_{t}\right) \phi\left(i_{t}\right)^{\prime}\right)^{-1} \sum_{t=1}^{k} \phi\left(i_{t}\right) J_{\mu}\left(i_{t}\right)
$$

- Equivalent least squares alternative calculation:  相当于把之前的精确值最小二乘法的精确值J换成采样值的期望

$$
\hat{r}_{k}=\arg \min _{r \in \Re^s} \sum_{t=1}^{k}\left(\phi\left(i_{t}\right)^{\prime} r-J_{\mu}\left(i_{t}\right)\right)^{2}
$$



##### INDIRECT POLICY EVALUATION

间接的PI,  这里的间接指的是, 不再直接以 $J_\mu$为目标拟合, 而是以$T_\mu$为目标, 就是各种**TD算法**.

An example: Galerkin approximation

Solve the projected equation $\Phi r=\Pi T_{\mu}(\Phi r)$ where $\Pi$ is projection w/ respect to a suitable weighted Euclidean norm

Solution methods that use simulation (to manage the calculation of $\Pi$ ) 

- $TD(\lambda)$: Stochastic iterative algorithm for solving $\Phi r=\Pi T_{\mu}(\Phi r)$

- $LSTD(\lambda)$: Solves a simulation-based approximation w/ a standard solver 

- $LSPE(\lambda)$: A simulation-based form of projected value iteration; essentially $\Phi_{r_{k+1}}=\Pi T_{\mu}\left(\Phi r_{k}\right)+$ simulation noise

  

##### BELLMAN EQUATION ERROR METHODS

Another example of indirect approximate policy evaluation:  $\min_\tau \Vert \Phi_{r} -T_{\mu} (\Phi_{r} ) \Vert_{\xi}^{2}$
where $\Vert \cdot \Vert_{\xi}$ is Euclidean norm, weighted with respect to some distribution $\xi$

It is closely related to the projected equation/Galerkin approach (with a special choice of projection norm)

Several ways to implement projected equation and **Bellman error methods** by simulation. They involve:
- Generating many random samples of states $i_k$ using the distribution ξ
- Generating many samples of transitions$(i_k,j_k)$ using the policy μ
- Form a simulation-based approximation of the optimality condition for projection problem or problem (*) (use sample averages in place of inner products)
- Solve the Monte-Carlo approximation of the optimality condition

工作流程有两个问题，第一个是产生样本，如果使用固定的分布，很容易 出现的情况是状态集中出现在某个区域内，这样蒙特卡洛平均得到的成本就 不准确，需要尽可能随机地产生状态，第二个问题是求解参数的时候，大规模 计算非常消耗计算资源，这时候可以使用蒙特卡洛平均代替大规模线性计算。

##### ANOTHER INDIRECT METHOD: AGGREGATION

聚合

- A first idea: Group similar states together into “aggregate states” $x_1,...,x_s$; assign a common cost value $r_i$ to each group $x_i$.
- Solve an “aggregate” DP problem, involving the aggregate states, to obtain $r = (r_1, . . . , r_s)$. This is called hard aggregation

<img src="/img/2020-01-08-ADP.assets/image-20200114170440020.png" alt="image-20200114170440020" style="zoom:50%;" />

<img src="/img/2020-01-08-ADP.assets/image-20200114170937272.png" alt="image-20200114170937272" style="zoom: 50%;" />



聚合其实是做 问题近似，从高维度的问题变成低维度的问题，然后使用精确方法求解这个低维度的问题，再使用 Φ 得到原问题的近似成本函数进行控制.  聚合问题有一个限制，就是近似结构收到限制，因为聚合方法需要有概率矩阵 D，这是一种比较特殊的投影方法，所以近似结构收到限制，而一般的 投影方法可以使用更一般的近似结构。

### APPROXIMATE POLICY ITERATION ISSUES

之前是 model拟合 V, 近似去评估 policy,  现在开始改进policy; 改进要比评估困难. 下面就是给了个上限.

#### THEORETICAL BASIS OF APPROXIMATE PI

假设，PE 误差上限 $\delta$ ，PI 误差上限 $\epsilon$ , $\epsilon$ 趋近0，这样生成的策略序列与最优解间会有一个上界

- If policies are approximately evaluated using an approximation architecture such that
  $$
  \max _{i}\left|\tilde{J}\left(i, r_{k}\right)-J_{\mu^{k}}(i)\right| \leq \delta \quad k=0,
  $$
  
- If policy improvement is also approximate, 
  $$
\max _{i}\left|\left(T_{\mu^{k+1}} \tilde{J}\right)\left(i, r_{k}\right)-(T \tilde{J})\left(i, r_{k}\right)\right| \leq \epsilon, \quad k=0,1
  $$
  
- Error bound: The sequence $\lbrace \mu^{k}\rbrace$ generated by approximate policy iteration satisfics 
  $$
\limsup _{k \rightarrow \infty} \max_i \left(J_{\mu^{\prime}}(i)-J^{\prime *}(i)\right) \leq \frac{\epsilon+2 \alpha \delta}{(1-\alpha)^{2}}
  $$
  
- Typical practical behavior: The method makes steady progress up to a point and then the iterates $J_{\mu^k}$ oscillate within a neighborhood of $J^\ast$. 获得的策略的成本函数会在这个最优解附近的区间内震荡. 

- Oscillations are quite unpredictable.

  - Some bad examples of oscillations have been constructed.
  - In practice oscillations between policies is probably not the major concern.

投影法的状态转换容易导致震荡，而聚合法完全不会震荡，原本会震荡的状态最终会到达同一个聚合状态中。



#### THE ISSUE OF EXPLORATION

评估策略 μ 的成本时，需要使用这个策略产生样本，而这样采样会导致状态 比较集中地出现，不经常出现的状态估值会产生很大的误差，这就是不充分探索导致的误差，确定性系统这种现象尤其严重，因为一个确定性的策略不会随机地访问所有的状态, 所以充分采样就比较重要

- To evaluate a policy $\mu,$ we need to generate cost samples using that policy - this biases the simulation by underrepresenting states that are unlikely to occur under $\mu$
- Cost-to-go estimates of underrepresented states may be highly inaccurate
- This seriously impacts the improved policy $\bar  \mu$
- This is known as **inadequate exploration** - a particularly acute difficulty when the randomness embodied in the transition probabilities is “relatively small” (e.g., a deterministic system)
- Some remedies:
  - Frequently restart the simulation and ensure that the initial states employed form a rich and representative subset 有规律地使用有代表性的初始状态重新开始仿真轨迹
  - Occasionally generate transitions that use a randomly selected control rather than the one dictated by the policy μ .    random pick
  - Other methods: Use **two Markov chains** (one is the chain of the policy and is used to gen- erate the transition sequence, the other is used to generate the state sequence). off-policy



#### APPROXIMATING Q-FACTORS

- Given $\tilde{J}(i ; r)$, policy improvement requires a model  [knowledge of $p_{ij}(u)$ for all controls $u \in U (i)$]

- Model-free alternative: Approximate Q-factors
  $$
  \bar {Q}(i, u ; r) \approx \sum_{i=1}^{n} p_{i j}(u)\left(g(i, u, j)+\alpha J_{\mu}(j)\right)
  $$
  and use for policy improvement the minimization 
  $$
  \bar{\mu}(i) \in \arg \min _{u \in U(i)} \tilde{Q}(i, u ; r) \\
  \tilde {Q}(i, u ; r)=\sum_{m=1}^{s} r_{m} \phi_{m}(i, u)
  $$
  
- We can adapt any of the cost approximation approaches, e.g., projected equations, aggregation

- Major concern: Acutely diminished exploration  也必须充分探索



### SOME GENERAL ISSUES

#### STOCHASTIC ALGORITHMS: GENERALITIES

- Consider solution of a linear equation $x = b + Ax$ by using $m$ simulation samples $b+w_{k}$ and $A+W_{k}, k=1, \ldots, m,$ where $w_{k}, W_{k}$ are random, c.g. "simulation noise"    一个线性系统, 可以用矩阵求逆来求解, 很多人用仿真来求解

- Think of $x=b+A x$ as approximate policy evaluation (projected or aggregation equations)

- **Stoch. approx. (SA) 随机近似** approach: For $k=1, \ldots, m$,  这种方法一边采样一边学习, 有个步长.  $x_{k+1}=\left(1-\gamma_{k}\right) x_{k}+\gamma_{k}\left(\left(b+w_{k}\right)+\left(A+W_{k}\right) x_{k}\right)$

- **Monte Carlo estimation (MCE)** approach: Form Monte Carlo estimates of $b$ and A
  $b_{m}=\frac{1}{m} \sum_{k=1}^{m}\left(b+w_{k}\right), \quad A_{m}=\frac{1}{m} \sum_{k=1}^{m}\left(A+W_{k}\right)$    所有采样MC求平均
  Then solve $x=b_{m}+A_{m} x$ by matrix inversion or iteratively  解这个方程可通过矩阵求逆或者其他迭代方法. 
  $$
  x_m = (1 - A_m)^{-1}b_m
  $$
  

  
- TD(λ) and Q-learning are SA methods
- LSTD(λ) and LSPE(λ) are MCE methods




#### COSTS OR COST DIFFERENCES?

- 学习defference;  Consider the exact policy improvement process. 
  To compare two controls $u$ and $u^{\prime}$ at $x,$ we need
  $$E\left\{g(x, u, w)-g\left(x, u^{\prime}, w\right)+\alpha\left(J_{\mu}(\bar{x})-J_{\mu}\left(\bar{x}^{\prime}\right)\right)\right\}$$
  where $\bar{x}=f(x, u, w)$ and $\bar{x}^{\prime}=f\left(x, u^{\prime}, w\right)$
- Approximate $J_{\mu}(\bar{x})$ or $D_{\mu}\left(\bar{x}, \bar{x}^{\prime}\right)=J_{\mu}(\bar{x})-J_{\mu}\left(\bar{x}^{\prime}\right) $   ?  如果已经得到一个策略的cost函数, 并且把这个策略函数的每个值都提升相同的幅度, 则会得到一样的策略, 说明J的值不是关键, 策略的改进,也就是评价两个control的好坏, 不取决于J值的绝对大小,而是差值
- Approximating $D_{\mu}\left(\bar{x}, \bar{x}^{\prime}\right)$ avoids "noise differencing". 避免噪声. This can make a big difference
- Important point: $D_{\mu}$ satisfies a Bellman equation for a system with "state" $\left(x, x^{\prime}\right)$

  $$
D_{\mu}\left(x, x^{\prime}\right)=E\left\{G_{\mu}\left(x, x^{\prime}, w\right)+\alpha D_{\mu}\left(\bar{x}, \bar{x}^{\prime}\right)\right\}
  $$
  
  where $\bar{x}=f(x, \mu(x), w), \bar{x}^{\prime}=f\left(x^{\prime}, \mu\left(x^{\prime}\right), w\right)$ and
  
  $$
G_{\mu}\left(x, x^{\prime}, w\right)=g(x, \mu(x), w)-g\left(x^{\prime}, \mu\left(x^{\prime}\right), w\right)
  $$
- $D_{\mu}$ can be "learned" by the standard methods
(TD, LSTD, LSPE, Bellman error, aggregation, etc). This is known as **differential training**.

一个例子来解释学习成本的梯度或者差值效果比直接学习成本值效果更好, 这个例子, J函数里面有x项以及control项, x的取值过大,其误差都能覆盖了control的影响.

- System and cost per stage:

  $$
x_{k+1}=x_{k}+\delta u_{k}, \quad g(x, u)=\delta\left(x^{2}+u^{2}\right)
  $$

  $\delta>0$ is very small; think of discretization of continuous-time problem involving $dx(t)/dt = u(t)$   

- Consider policy $\mu(x)=-2 x$. Its cost function
  $$
J_{\mu}(x)=\frac{5 x^{2}}{4}(1+\delta)+O\left(\delta^{2}\right)
  $$
  and its Q-factor is  $Q_{\mu}(x, u)=\frac{5 x^{2}}{4}+\delta\left(\frac{9 x^{2}}{4}+u^{2}+\frac{5}{2} x u\right)+O\left(\delta^{2}\right)$

- The important part for policy improvement
  $$
\delta\left(u^{2}+\frac{5}{2} x u\right)
  $$
  When $J_{\mu}(x)$ [or $Q_{\mu}(x, u)$] is approximated by $\tilde J_\mu(x ; r)$ [or by $\tilde Q_\mu(x, u ; r)$], it will be dominated by $\frac{5 x^{2}}{4}$ and will be "lost". 由于近似的不准确性，不同的 u 带来的成本函数的变化就会被$\frac{5 x^{2}}{4}$ 的近似误差干扰导致无法区分不同控制的好坏关系，这时候近似成本函数的差值进行控制得到的成本就会好于只近似成本函数的值进行控制得到的成本。



## LECTURE 4

介绍在值空间内对值和策略进行近似。值空间近似指的是在更低维度的空间内对原空间内的一个函数进行近似，之前 我们讨论的是使用仿真来进行近似，这里也会继续用仿真进行近似，这个近似可能是成本函数，或者问题的某个策略的成本函数。

### APPROXIMATE VALUE ITERATION 近似值迭代

#### APPROXIMATE (FITTED) VI 拟合值迭代, 古老的算法

值迭代是计算 $J_0, TJ_0, T^2J_0,\dots$  , 该算法是, 从$J_0$开始, 计算$TJ_0$, 然后通过近似获得$\tilde J_1$, 然后计算$T\tilde J_1$, 再去近似... 感觉上就很容易不收敛.. 	

- Approximates sequentially $J_k (i) = (T^k J_0 )(i), k = 1,2,...$, with $\tilde J (i;r)$

- starting function $J_0$ is given (e.g., $J_0 \equiv 0$)

- Approximate (Fitted) Value Iteration:  A sequential “fit” to produce  $\tilde  J_{k+1} \approx T\tilde J_k$ or (for a single policy $\mu$)  $\tilde  J_{k+1} \approx T_\mu\tilde J_k$   每一步都去拟合,高纬投影到低纬

  <img src="/img/2020-01-08-ADP.assets/image-20200114222634683.png" alt="image-20200114222634683" style="zoom:50%;" />

- After a large enough number $N$ of steps, $\tilde J_N (i ; r_N)$ is used as approxoximation $\tilde{J}(i ; r)$ to $J^\ast(i)$ 经过很多次 (N 次) 迭代后，就可以得到一个比较接近成本向量的参数向量 r 了

- Possibly use (approximate) projection $\Pi$ with respect to some projection norm,
  $$
  \tilde{J}_{k+1} \approx \Pi T \tilde{J}_{k}
  $$



##### WEIGHTED EUCLIDEAN PROJECTIONS 加权欧式投影

- Consider a weighted Euclidean norm

  $$
\|J\|_{\xi}=\sqrt{\sum_{i=1}^{n} \xi_{i}(J(i))^{2}}
  $$
  
  where $\xi=\left(\xi_{1}, \ldots, \xi_{n}\right)$ is a positive distribution $\left(\xi_{i}>0 \text { for all } i\right)$ 

- Let $\Pi$ denote the projection operation onto,     特征的线性组合
  $$
S=\{\Phi r | r \in \Re^s\}
  $$
  with respect to this norm, i.e., for any $J \in \Re^{n}$;   目标就是要找到一组参数向量$r*$, 等于这个投影
  
  $$
\Pi=\Phi r^{*}
  $$
  
  where
  
  $$
r^{*}=\arg \min _{r \in \mathbb{\Re}^{s}}\|\Phi r-J\|_{\xi}^{2}
  $$
  
- Recall that weighted Euclidean projection can be implemented by **simulation** and **least squares**,i.e., sampling $J(i)_{k}$ according to $\xi$ and solving  在低维空间计算可以通过采样控制投影近似的结果
  
  $$
\min _{r \in {\Re}^{s}} \sum_{t=1}^{K}\left(\phi\left(i_{t}\right)^{\prime} r-J\left(i_{t}\right)\right)^{2}
  $$

##### FITTED VI - NAIVE IMPLEMENTATION

- Select/sample a "small" subset $I_{k}$ of representative states

- For each $i \in I_{k},$ given $\tilde{J}_{k},$ compute 
  $$
  \left(T \tilde{J}_{k}\right)(i)=\min _{u \in U^{(1)}} \sum_{j=1}^{n} p_{i j}(u)\left(g(i, u, j)+\alpha \tilde{J}_{k}(j ; r)\right)
  $$
  
- "Fit" the function $\tilde J_{k+1} (i ; r_{k+1})$ to the "small" set of values $(T J_{k})(i), i \in I_k$ (for example use some form of approximate projection) 

- Simulation can be used for "model-free" implementation

- Error Bound: If the fit is uniformly accurate within $\delta>0,$ i.e.,   只能保证误差上界

$$
\max _{i}\left|\tilde{J}_{k+1}(i)-T \tilde{J}_{k}(i)\right| \leq \delta
$$

$$
\lim _{k \rightarrow \infty} \max _{i=1, \ldots, n}\left(\tilde{J}_{k}\left(i, r_{k}\right)-J^{*}(i)\right) \leq \frac{2 \alpha \delta}{(1-\alpha)^{2}}
$$

  But there is a potential problem!

一个该方法不灵的例子: 

- Consider two-state discounted MDP with states 1 and 2, and a single policy. 每个状态就一个control
  - Deterministic transitions: $1 \rightarrow 2$ and $2 \rightarrow 2$
  - $\text { Transition costs } \equiv 0, \text { so } J^{*}(1)=J^{*}(2)=0$
  
- Consider (exact) fitted VI scheme that approximates cost functions within $S=\lbrace (r, 2 r) \vert r \in \Re\rbrace$ with a weighted least squares fit; here $\Phi=\left(\begin{array}{l}{1} \\ {2}\end{array}\right)$

- Given $\tilde J_{k}= (r_{k}, 2 r_{k} )$, we find $\tilde J_{k+1}= (r_{k+1}, 2 r_{k+1} )$ where $\tilde J_{k+1}=\Pi_\xi (T \tilde J_k)$, with weights $\xi= (\xi_1, \xi_2)$:
  $$
  r_{k+1}=\arg \min _{r}\left[\xi_{1}\left(r-\left(T \tilde{J}_{k}\right)(1)\right)^{2}+\xi_{2}\left(2 r-\left(T \tilde{J}_{k}\right)(2)\right)^{2}\right]
  $$

- With straightforward calculation
  $$
  r_{k+1}=\alpha \beta r_{k}, \text { where } \beta=2\left(\xi_{1}+2 \xi_{2}\right) /\left(\xi_{1}+4 \xi_{2}\right)>1
  $$

- So if $\alpha>1 / \beta\left(\text { e.g., } \xi_{1}=\xi_{2}=1\right),$ the sequence $\lbrace r_{k}\rbrace$ diverges and so does $\lbrace\tilde{J}_{k}\rbrace$

- Difficulty is that $T$ is a contraction, but $\Pi_{\xi} T$    (= least squares fit composed with T)  is not.导致算法失效的原因是原 bellman 方程中算子 T 是压缩映射，而投影后 $\Pi_{\xi} T$无法保证依然是压缩映射

$$
\begin{aligned}
& \xi_{1}\left(r-\left(T \tilde{J}_{k}(1)\right)\right)^{2}+\xi_{2}\left(2 r-\left(T \tilde{J}_{k}(2)\right)\right)^{2} \\
=&\left.\xi_{1}\left(r-\left(2 \alpha r_{k}\right)\right)^{2}+\xi_{2}\left(2 r_{k}\right)\right)^{2} \\
=& \xi_{1}\left(r^{2}-4 \alpha r r_{k}+4 \alpha^{2} r_{k}^{2}\right)+\xi_{2}\left(4 r_{k}\right)^{2} \\
=&\left(\xi_{1}+4 \xi_{2}\right) r^{2}-\left(4 \alpha \xi_{1} r_{k}+8 \alpha \xi_{2} r_{k}\right) r+R(1) \\
=& r^{2}-\frac{\left(4 \alpha \xi_{1} r_{k}+8 \alpha \xi_{2} r_{k}\right)}{\left(\xi_{1}+4 \xi_{2}\right)} r+\frac{\left(4 \alpha \xi_{1} r_{k}+8 \alpha \xi_{2} r_{k}\right)}{R(1)} \\
=&\left(r-\frac{2\left(\alpha \xi_{1} r_{k}+2 \alpha \xi_{2} r_{k}\right)}{\left(\xi_{1}+4 \xi_{2}\right)}\right)^{2}+R^{\prime}(1)
\end{aligned}
$$

$$
r=\frac{2\left(\alpha \xi_{1} r_{k}+2 \alpha \xi_{2} r_{k}\right)}{\left(\xi_{1}+4 \xi_{2}\right)}=\alpha \frac{2\left(\xi_{1}+2 \xi_{2}\right)}{\left(\xi_{1}+4 \xi_{2}\right)} r_{k}
$$

​	

##### NORM MISMATCH PROBLEM 范数不匹配问题

- For the method to converge, we need $\Pi_{\xi} T$ to be a **contraction**; the contraction property of $T$ not enough   需要都是压缩映射

  <img src="/img/2020-01-08-ADP.assets/image-20200114231659413.png" alt="image-20200114231659413" style="zoom:50%;" />

- We need a vector of weights $\xi$ such that $T$ is contraction with respect to the weighted Euclidean norm $\|\cdot\| \xi$

- Then we can show that $\Pi_{\xi} T$ is a contraction with respect to $\|\cdot\| \xi$



### APPROXIMATE POLICY ITERATION

#### APPROXIMATE PI

- **Evaluation** of typical policy $\mu:$ Linear cost function approximation $\tilde{J}_{\mu}(r)=\Phi r,$ where $\Phi$ is full rank $n \times s$ matrix with columns the basis functions, and $i$ th row denoted $\phi(i)^{\prime}$
- Policy "**improvement**" to generate  $\bar{\mu}$ :
$$
\bar{\mu}(i)=\arg \min _{u \in U(i)} \sum_{j=1}^{n} p_{i j}(u)\left(g(i, u, j)+\alpha \phi(j)^{\prime} r\right)
$$

- **Error Bound** (same as approximate VI): If
$$
\max _{i}\left| \tilde J_{\mu^{k}}\left(i, r_{k}\right)-J_{\mu^{k}}(i)\right| \leq \delta, \quad k=0,1, \ldots
$$

​    the sequence $\left\{\mu^{k}\right\}$ satisfies
$$
\limsup _{k \rightarrow \infty} \max _{i}\left(J_{\mu^{k}}(i)-J^{*}(i)\right) \leq \frac{2 \alpha \delta}{(1-\alpha)^{2}}
$$



##### POLICY EVALUATION

- Let's consider approximate evaluation of the cost of the current policy by using simulation. 
  - Direct policy evaluation - Cost samples generated by simulation, and optimization by least squares 
  - Indirect policy evaluation TD - solving the projected equation $\Phi r=\Pi T_{\mu}(\Phi r)$ where $\Pi$ is projection w/ respect to a suitable weighted Euclidean norm

- Recall that projection can be implemented by simulation and least squares



##### PI WITH INDIRECT POLICY EVALUATION

Given the current policy $\mu$: 
- We solve the projected Bellman's equation
  $$
\Phi r=\Pi T_{\mu}(\Phi r)
  $$
  
- We approximate the solution $J_{\mu}$ of Bellman's equation
  $$
J=T_{\mu} J
  $$
with the projected equation solution $\tilde{J}_{\mu}(r)$



#### KEY QUESTIONS AND RESULTS 关键问题: 保证收缩投影

- Does the projected equation have a solution? 投影方程是否有解
- Under what conditions is the mapping $\Pi T_{\mu}$ a contraction, so $\Pi T_{\mu}$ has unique fixed point?  什么情况下保证是收缩投影, 以及有不动点
- Assumption: The Markov chain corresponding to $\mu$ has a single recurrent class and no transient states, i.e., it has steady-state probabilities that are positive     假设有个马尔科夫链, 只有一个节点, 各状态按概率出现
  $$
\xi_{j}=\lim _{N \rightarrow \infty} \frac{1}{N} \sum_{k=1}^{N} P\left(i_{k}=j | i_{0}=i\right)>0
  $$
Note that $\xi_{j}$ is the long-term frequency of state $j$ 
- Proposition: (**Norm Matching Property**)   Assume that the projection $\Pi$ is with respect to $\|\cdot\|_{\xi}$ where $\xi=\left(\xi_{1}, \ldots, \xi_{n}\right)$ is the steady-state probability vector. Then:  利用上面的分布$\xi$来做权重的范数, 就是收缩投影

  - (a) $\Pi T_{\mu}$ is contraction of modulus $\alpha$ with respect to $\|\cdot\|_{\xi}$
  - (b) The unique fixed point $\Phi r^{*}$ of $\Pi T_{\mu}$ satisfies   
$$
\left\|J_{\mu}-\Phi r^{*}\right\|_{\xi} \leq \frac{1}{\sqrt{1-\alpha^{2}}}\left\|J_{\mu}-\Pi J_{\mu}\right\|_{\xi}
$$



##### PRELIMINARIES: PROJECTION PROPERTIES  预备知识: 投影的性质

介绍一下为什么投影操作能够工作

- Important property of the projection $\Pi$ on $S$ with weighted Euclidean norm $\Vert \cdot \Vert_{\xi}$ . For all $J \in \Re^{n}$, $\Phi r \in S$, the **Pythagorean Theorem** **勾股定理 (毕达哥拉斯定理)** holds: 

  <img src="/img/2020-01-08-ADP.assets/image-20200117030210285.png" alt="image-20200117030210285" style="zoom: 33%;" />

$$
\|J-\Phi r\|_{\xi}^{2}=\|J-\Pi J\|_{\xi}^{2}+\|\Pi J-\Phi r\|_{\xi}^{2}
$$

- The Pythagorean Theorem implies that the projection is nonexpansive, 使用勾股定理可以得到非扩张性  i.e.
  $$
\|\Pi J-\Pi \bar{J}\|_{\xi} \leq\|J-\bar{J}\|_{\xi}, \quad \forall J, \bar{J} \in \Re^{n}
  $$
 To see this, note that

$$
\|\Pi(J-\bar{J})\|_{\xi}^{2} \leq\|\Pi(J-\bar{J})\|_{\xi}^{2}+\|(I-\Pi)(J-\bar{J})\|_{\xi}^{2}  \\
=\|J-\bar{J}\|_{\xi}^{2}
$$



##### PROOF OF CONTRACTION PROPERTY

证明一下$\Pi T_{\mu}$ 是一个收缩投影

- Lemma: If $P$ is the transition matrix of $\mu$ 

  $$
\| P z\left\|_{\xi} \leq\right\| z \|_{\xi} , \quad z   \in \Re^n
  $$
  
- Proof: Let $p_{i j}$ be the components of $P$. For all $z \in \Re^{n}$, we have

  $$
\|P z\|_{\xi}^{2}=\sum_{i=1}^{n} \xi_{i}\left(\sum_{j=1}^{n} p_{i j} z_{j}\right)^{2} \leq \sum_{i=1}^{n} \xi_{i} \sum_{j=1}^{n} p_{i j} z_{j}^{2}
\\ =\sum_{j=1}^{n} \sum_{i=1}^{n} \xi_{i} p_{i j} z_{j}^{2}=\sum_{j=1}^{n} \xi_{j} z_{j}^{2}=\|z\|_{\xi}^{2}
  $$
  
  where the inequality follows from the convexity of the quadratic function, and the next to last equality follows from the defining property $\sum_{i=1}^{n} \xi_{i} p_{i j}=\xi_{j}$ of the steady-state probabilities.
  
- Using the lemma, the nonexpansiveness of $\Pi,$ and the definition $T_{\mu} J=g+\alpha P J,$ we have 

  $$
\left\|\Pi T_{\mu} J-\Pi T_{\mu} \bar{J}\right\|_{\xi} \leq\left\|T_{\mu} J-T_{\mu} \bar{J}\right\|_{\xi}=\alpha\|P(J-\bar{J})\|_{\xi} \leq \alpha \| J- \bar J \|_\xi
  $$

  for all $J, \bar{J} \in \Re^{n} .$ Hence $\Pi T_{\mu}$ is a contraction of modulus $\alpha$ .

##### PROOF OF ERROR BOUND

- Let $\Phi r^{*}$ be the fixed point of $\Pi T .$ We have

  $$
\left\|J_{\mu}-\Phi r^{*}\right\|_{\xi} \leq \frac{1}{\sqrt{1-\alpha^{2}}}\left\|J_{\mu}-\Pi J_{\mu}\right\|_{\xi}
  $$
- Proof: We have

  $$
\left\|J_{\mu}-\Phi r^{*}\right\|_{\xi}^{2}=\left\|J_{\mu}-\Pi J_{\mu}\right\|_{\xi}^{2}+\left\|\Pi J_{\mu}-\Phi r^{*}\right\|_{\xi}^{2}
\\=\left\|J_{\mu}-\Pi J_{\mu}\right\|_{\xi}^{2}+\left\|\Pi T J_{\mu}-\Pi T\left(\Phi r^{*}\right)\right\|_{\xi}^{2}
\\\leq\left\|J_{\mu}-\Pi J_{\mu}\right\|_{\xi}^{2}+\alpha^{2}\left\|J_{\mu}-\Phi r^{*}\right\|_{\xi}^{2}
  $$
  where 
  - The first equality uses the Pythagorean Theorem. 
  - The second equality holds because $J_{\mu}$ is the fixed point of $T$ and $\Phi r^{*}$ is the fixed point of $\Pi T$
  - The inequality uses the contraction property ef $\Pi T$



### SIMULATION-BASED SOLUTION OF PROJECTED EQUATION

#### MATRIX FORM OF PROJECTED EQUATION

- The solution $\Phi r^{*}$ satisfies the **orthogonality 正交** condition: The error
  $$
\Phi r^{*}-\left(g+\alpha P \Phi r^{*}\right)
  $$
  is "orthogonal" to the subspace spanned by the columns of $\Phi$
- This is written as
  $$
\Phi^{\prime} \Xi\left(\Phi r^{*}-\left(g+\alpha P \Phi r^{*}\right)\right)=0
  $$
  where $\Xi$ is the diagonal matrix with the steadystate probabilities $\xi_{1}, \ldots, \xi_{n}$ along the diagonal.
- Equivalently, $C r^{*}=d,$ where
  $$
C=\Phi^{\prime} \Xi(I-\alpha P) \Phi, \quad d=\Phi^{\prime} \Xi g
  $$
  but computing $C$ and $d$ is HARD (high-dimensional inner products)

#### SOLUTION OF PROJECTED EQUATION

- Solve $C r^\ast=d$ by matrix inversion: $r^\ast=C^{-1} d$
- Projected Value Iteration (PVI) method:
$$
\Phi r_{k+1}=\Pi T\left(\Phi r_{k}\right)=\Pi\left(g+\alpha P \Phi r_{k}\right)
$$
Converges to $r^{*}$ because $\Pi T$ is a contraction.

<img src="/img/2020-01-08-ADP.assets/image-20200114235504564.png" alt="image-20200114235504564" style="zoom: 33%;" />

- PVI can be written as:
  $$
  r_{k+1}=\arg \min _{r \in \Re^{s}}\left\|\Phi r-\left(g+\alpha P \Phi r_{k}\right)\right\|_{\xi}^{2}
  $$
  By setting to 0 the gradient with respect to $r$
  $$\Phi^{\prime} \Xi\left(\Phi r_{k+1}-\left(g+\alpha P \Phi r_{k}\right)\right)=0$$
  which yields
  $$r_{k+1}=r_{k}-\left(\Phi^{\prime} \Xi \Phi\right)^{-1}\left(C r_{k}-d\right)$$



##### SIMULATION-BASED IMPLEMENTATIONS

主要的思路就是使用低维线性计算代替高维先行计算进行估计。

- Key idea: Calculate simulation-based approximations based on $k$ samples
$$
C_{k} \approx C, \quad d_{k} \approx d
$$
- Matrix inversion $r^{*}=C^{-1} d$ is approximated
$$
\text { by } \quad \hat{r}_{k}=C_{k}^{-1} d_{k}
$$
This is the **LSTD (Least Squares Temporal Differences)​** Method.

- PVI method :  $r_{k+1}=r_{k}-\left(\Phi^{\prime} \Xi \Phi\right)^{-1}\left(C r_{k}-d\right)$ is approximated by

$$
r_{k+1}=r_{k}-G_{k}\left(C_{k} r_{k}-d_{k}\right)
$$
where
$$
G_{k} \approx\left(\Phi^{\prime} \Xi \Phi\right)^{-1}
$$
This is the **LSPE (Least Squares Policy Evaluation)** Method.

- Key fact: $C_{k}, d_{k},$ and $G_{k}$ can be computed with low-dimensional linear algebra (of order the number of basis functions).

#### SIMULATION MECHANICS 采样机制

- We generate an infinitely long trajectory $\left(i_{0}, i_{1}, \ldots\right)$ of the Markov chain, so states $i$ and transitions $(i, j)$ appear with long-term frequencies $\xi_{i}$ and $p_{i j}$ 
- After generating each transition $\left(i_{t}, i_{t+1}\right),$ we compute the row $\phi\left(i_{t}\right)^{\prime}$ of $\Phi$ and the cost component $g\left(i_{t}, i_{t+1}\right)$ 
- We form

$$d_{k}=\frac{1}{k+1} \sum_{t=0}^{k} \phi\left(i_{t}\right) g\left(i_{t}, i_{t+1}\right) \approx \sum_{i, j} \xi_{i} p_{i j} \phi(i) g(i, j)$$

$$k=\frac{1}{k+1} \sum_{t=0}^{k} \phi\left(i_{t}\right)\left(\phi\left(i_{t}\right)-\alpha \phi\left(i_{t+1}\right)\right)^{\prime} \approx \Phi^{\prime} \Xi(I-\alpha P) \Phi=C$$

Also in the case of LSPE

$$
G_{k}=\frac{1}{k+1} \sum_{t=0}^{k} \phi\left(i_{t}\right) \phi\left(i_{t}\right)^{\prime} \approx \Phi^{\prime} \Xi \Phi
$$

- Convergence based on law of large numbers.
- $C_k, d_{k}$, and $G_{k}$ can be formed incrementally.

Also can be written using the formalism of tem- poral differences (this is just a matter of style)

#### OPTIMISTIC VERSIONS

- Instead of calculating nearly exact approximations $C_{k} \approx C$ and $d_{k} \approx d,$ we do a less accurate approximation, based on few simulation samples
- Evaluate (coarsely) current policy $\mu,$ then do a policy improvement
- This often leads to faster computation (as optimistic methods often do)
- Very complex behavior (see the subsequent discussion on oscillations)
- The matrix inversion/LSTD method has serious problems due to large simulation noise (because of limited sampling $)$ - particularly if the $C$ matrix is ill-conditioned
- LSPE tends to cope better because of its iterative nature (this is true of other iterative methods as well)
- A stepsize $\gamma \in(0,1]$ in LSPE may be useful to damp the effect of simulation noise

$$
r_{k+1}=r_{k}-\gamma G_{k}\left(C_{k} r_{k}-d_{k}\right)
$$



### MULTISTEP PROJECTED EQUATIONS

#### MULTISTEP METHODS  多步方法, Eligibility Traces 

- Introduce a multistep version of Bellman's equation $J=T^{(\lambda)} J,$ where for $\lambda \in[0,1)$
  $$
T^{(\lambda)}=(1-\lambda) \sum_{\ell=0}^{\infty} \lambda^{\ell} T^{\ell+1}
  $$
  Geometrically weighted sum of powers of $T$.
- Note that $T^{\ell}$ is a contraction with modulus
$\alpha^{\ell},$ with respect to the weighted Euclidean norm $\|\cdot\|_{\xi},$ where $\xi$ is the steady-state probability vector of the Markov chain.
- Hence $T(\lambda)$ is a contraction with modulus
  $$
\alpha_{\lambda}=(1-\lambda) \sum_{\ell=0}^{\infty} \alpha^{\ell+1} \lambda^{\ell}=\frac{\alpha(1-\lambda)}{1-\alpha \lambda}
  $$
  Note that $\quad \alpha_{\lambda} \rightarrow 0$ as $\lambda \rightarrow 1$
- $T^{\ell}$ and $T^{(\lambda)}$ have the same fixed point $J_{\mu}$ and

$$
\left\|J_{\mu}-\Phi r_{\lambda}^{*}\right\|_{\xi} \leq \frac{1}{\sqrt{1-\alpha_{\lambda}^{2}}}\left\|J_{\mu}-\Pi J_{\mu}\right\|_{\xi}
$$

where $\Phi r_{\lambda}^{*}$ is the fixed point of $\Pi T(\lambda)$

- The fixed point $\Phi r_{\lambda}^{*}$ depends on $\lambda$

#### BIAS-VARIANCE TRADEOFF

<img src="/img/2020-01-08-ADP.assets/image-20200115000728821.png" alt="image-20200115000728821" style="zoom: 33%;" />

- Error bound $$\left\|J_{\mu}-\Phi r_{\lambda}^{*}\right\|_{\xi} \leq \frac{1}{\sqrt{1-\alpha_{\lambda}^{2}}}\left\|J_{\mu}-\Pi J_{\mu}\right\|_{\xi}$$
- As $\lambda \uparrow 1,$ we have $\alpha_{\lambda} \downarrow 0,$ so error bound (and the quality of approximation) improves as $\lambda \uparrow 1$
In fact
$$
\lim _{\lambda \uparrow 1} \Phi r_{\lambda}^{*}=\Pi J_{\mu}
$$
- But the simulation noise in approximating
$$
T^{(\lambda)}=(1-\lambda) \sum_{\ell=0}^{\infty} \lambda^{\ell} T^{\ell+1}
$$
increases
- Choice of $\lambda$ is usually based on trial and error



#### MULTISTEP PROJECTED EQ. METHODS

- The projected Bellman equation is
$$
\Phi r=\Pi T^{(\lambda)}(\Phi r)
$$

- In matrix form: $C^{(\lambda)} r=d^{(\lambda)},$ where
$$C(\lambda)=\Phi^{\prime} \Xi(I-\alpha P(\lambda)) \Phi, \quad d(\lambda)=\Phi^{\prime} \Xi g^{(\lambda)}$$
with 
$$P(\lambda)=(1-\lambda) \sum_{\ell=0}^{\infty} \alpha^{\ell} \lambda^{\ell} P^{\ell+1}, \quad g^{(\lambda)}=\sum_{\ell=0}^{\infty} \alpha^{\ell} \lambda^{\ell} P^{\ell} g$$
- The LSTD $(\lambda)$ method is
$$
\left(C_{k}^{(\lambda)}\right)^{-1} d_{k}^{(\lambda)}
$$
where $C_{k}^{(\lambda)}$ and $d_{k}^{(\lambda)}$ are simulation-based approximations of $C^{(\lambda)}$ and $d^{(\lambda)}$
- The LSPE(\lambda) method is
$$
r_{k+1}=r_{k}-\gamma G_{k}\left(C_{k}^{(\lambda)} r_{k}-d_{k}^{(\lambda)}\right)
$$
where $G_{k}$ is a simulation-based approx.to $\left(\Phi^{\prime} \Xi \Phi\right)^{-1}$ 
- $\mathrm{TD}(\lambda):$ An important simpler/slower iteration [similar to LSPE($\lambda$) with $G_{k}=I$ ]

#### MORE ON MULTISTEP METHODS

- The simulation process to obtain $C_{k}^{(\lambda)}$ and $d_{k}^{(\lambda)}$ is similar to the case $\lambda=0$ (single simulation trajectory $i_{0}, i_{1}, \ldots$more complex formulas )

$$C_{k}^{(\lambda)}=\frac{1}{k+1} \sum_{t=0}^{k} \phi\left(i_{t}\right) \sum_{m=t}^{k} \alpha^{m-t} \lambda^{m-t}\left(\phi\left(i_{m}\right)-\alpha \phi\left(i_{m+1}\right)\right)^{\prime}$$

$$
d_{k}^{(\lambda)}=\frac{1}{k+1} \sum_{t=0}^{k} \phi\left(i_{t}\right) \sum_{m=t}^{k} \alpha^{m-t} \lambda^{m-t} g_{i_{m}}
$$

- In the context of approximate policy iteration, we can use optimistic versions (few samples between policy updates
- Many different versions (see the text).
- Note the $\lambda$ -tradeoffs:
  - As $\lambda \uparrow 1, C_{k}^{(\lambda)}$ and $d_{k}^{(\lambda)}$ contain more "simulation noise", so more samples are needed for a close approximation of $r_{\lambda}$ (the solution of the projected equation) 
  - The error bound $\left\|J_{\mu}-\Phi r_{\lambda}\right\|_{\xi}$ becomes smaller 
  - As $\lambda \uparrow 1, \Pi T^{(\lambda)}$ becomes a contraction for arbitrary projection norm



## LECTURE 5

### ISSUES OF POLICY IMPROVEMENT

#### EXPLORATION

- 1st major issue: exploration. To evaluate μ, we need to generate cost samples using μ
- This **biases** the simulation by underrepresenting states that are unlikely to occur under μ.  策略有倾向性会造成偏差
- As a result, the cost-to-go estimates of these underrepresented states may be highly inaccurate, and seriously impact the “improved policy” μ.
- This is known as **inadequate exploration** - a particularly acute difficulty when the randomness embodied in the transition probabilities is “relatively small” (e.g., a deterministic system).
- To deal with this we must change the sampling mechanism and modify the simulation formulas.
- Solve
  $$
\Phi r=\bar{\Pi} T_{\mu}(\Phi r)
  $$
  where $\bar{\Pi}$ is projection with respect to an **exploration enhanced norm** [uses a weight distribution $\zeta=$ $\left.\left(\zeta_{1}, \ldots, \zeta_{n}\right)\right]$ 需要 找到一个探索能力更强的投影来构成投影方程
- $\zeta$ is more "balanced" than $\xi$ the steady-state distribution of the Markov chain of $\mu$ 新的权重对不常出现的状态有更高的权重. 
- This also addresses any lack of ergodicity of $\mu$ . 如果策略 μ 不是一个遍历性的策略，即使改变了采样权重，探索不足遇到 的问题依然存在

##### EXPLORATION MECHANISMS

- One possibility: Use multiple short simulation trajectories instead of single long trajectory starting from a rich mixture of states. This is known as **geometric sampling**, or **free-form sampling**. 使用短的代替完整的采样
  - By properly <u>choosing the starting states</u>, we enhance exploration
  - The simulation formulas for $LSTD(\lambda)$ and LSPE $(\lambda)$ have to be modified to yield the solution of $\Phi r=\bar{\Pi} T_{\mu}^{(\lambda)}(\Phi r)$ (see the DP text) 算法公式需要调整
- Another possibility: Use a modified policy to generate a single long trajectory. This is called an **off-policy** approach.
  - Modify the transition probabilities of $\mu$ to enhance exploration 
  - Again the simulation formulas for $L S T D(\lambda)$ and $\mathrm{LSPE}(\lambda)$ have to be modified to yield the solution of $\Phi r=\bar{\Pi} T_{\mu}^{(\lambda)}(\Phi r)$ (use of importance sampling; see the DP text)

- With larger values of $\lambda>0$ the contraction property of $\bar{\Pi} T_{\mu}^{(\lambda)}$ is maintained.

- LSTD may be used without $\bar{\Pi} T_{\mu}^{(\lambda)}$ being a contraction $\ldots$ LSPE and TD require a contraction.

  

#### POLICY ITERATION ISSUES: OSCILLATIONS 震荡

- 2nd major issue: oscillation of policies 
- Analysis using the greedy partition of the space of weights $r: R_{\mu}$ is the set of parameter vectors $r$ for which $\mu$ is greedy with respect to $\tilde{J}(\cdot ; r)=\Phi r$

$$
R_{\mu}=\left\{r | T_{\mu}(\Phi r)=T(\Phi r)\right\} \quad \forall \mu
$$
​       If we use $r$ in $R_{\mu}$ the next "improved" policy is $\mu$

- If policy evaluation is exact, there is a finite number of possible vectors $r_\mu$, (one per μ)   精确PE可以收敛到策略的真值. 
- 近似方法,  最终可以造成 在几个策略里面循环, 绕不出来. 
- 如果使用乐观算法 (optimistic policy iteration) 导致无法得到准确的新策略，然后策略估计用的是不准确的策略的成本函数，然后继续策略改进无法准确到达新策略，就这样一直迭代下去，如果策略评价越来越乐观，震荡幅度就会越来越小。另一个比较奇怪的现象就是权重向量收敛 到一个固定的值，但是策略可能还在震荡，因此不仅要检查权重向量，还要检 查策略是否震荡，也就是说，权重向量收敛了，但是策略发散了，这种现象有时候会出现，但是很难分析成因。
- In the case of **optimistic** policy iteration a different picture holds (policy evaluation does not produce exactly $r_\mu$) 乐观PI, 无法得到精确值
- Oscillations of weight vector r are less violent, but the “limit” point is meaningless
- Fundamentally, oscillations are due to the lack of **monotonicity** of the projection operator, i.e., J≤J′ does not imply $\Pi J \leq \Pi J′$ .本质上说, 震荡是由于, 投影算子不具备单调性引起的. 不具备单 调性指的是有两个函数 J 和 J′，在近似子空间内对他们进行投影的时候，他们的大小关系与原关系可能相反。
- If approximate PI uses an evaluation of the form

$$
\Phi r=\left(W T_{\mu}\right)(\Phi r)
$$
​       with $W:$ monotone and $W T_{\mu}$: contraction, the policies converge (to a possibly nonoptimal limit).如果把$\Pi$换成一个单调算子 W，同时保证 $W T_{\mu}$ 也 单调并且收缩，算法就不会出现震荡的现象，这个结论可以从震荡的数学角度进行解释。

- These conditions hold when aggregation is used . 下面要讲一下如何避免震荡，引入状态聚合。

### AGGREGATION  聚合 

#### PROBLEM APPROXIMATION - AGGREGATION

求解动态规划的另一个方法是状态聚合，状态聚合是一种问题近似的方法，把问题化简，让问题更容易求解，然后就可以使用任意一种方法，精确算法或者近似算法都可以。   聚合是人工的选择某种合并状态的算法, 来简化问题, 算是解决了一个近似问题,  是一种启发式的解决方法.  把类聚合, 类似于设计一种特征了.



## LECTURE 6

#### WHAT IS GOOD AND BAD ABOUT Q-FACTORS

对比Q值与cost,  Q值可以model-free

- All the exact theory and algorithms for costs applies to Q-factors 所有对cost可行的方法对Q值都适用

  - Bellman’s equations, contractions, optimality conditions, convergence of VI and PI   

- All the approximate theory and algorithms for costs applies to Q-factors 近似算法也是

  - Projected equations, sampling and exploration issues, oscillations, aggregation

- A **MODEL-FREE** (on-line) controller implementation, 这时不再需要利用bellman公式

  - Once we calculate $Q^\ast(i,u)$ for all (i,u),   
    $$
    \mu^\ast(i) = \arg \min_{u \in U(i)}Q^\ast(i, u) , \quad \forall i
    $$

  - Similarly, once we calculate a parametric approximation Q ̃(i, u; r) for all (i, u),
    $$
    \tilde \mu(i) = \arg \min_{u \in U(i)} \tilde Q(i, u;r) , \quad \forall i
    $$

- The main bad thing: Greater dimension and more storage! 

### Q-LEARNING

- approximate PI methods adapted for Q-factors: Q-LEARNING, a sampled form of VI
- Q-learning algorithm:
  - Sampling  采样
  - Iteration 迭代
  - Stepsize conditions: $\gamma_k \to 0$ 学习率非常小



##### NOTES AND QUESTIONS ABOUT Q-LEARNING

- Model free implementation

- Operates on only one state-control pair at a time. Convenient for simulation, no restrictions on sampling method. (Connection with asynchronous algorithms.)

- Aims to find the (exactly) optimal Q-factors.

- converge to $Q^\ast$

- a sampled version of VI  don't work

- important mathematical (fine) point: In the Q- factor version of Bellman’s equation the order of expectation and minimization is reversed relative to the cost version of Bellman’s equation
  $$
  J^\ast (i) = \min_{u \in U(i)} \sum_{j=1}^n p_{ij}(u)\left( g(i,u,j) + \alpha J^\ast(j)\right)
  $$

##### CONVERGENCE ASPECTS OF Q-LEARNING  Q-learning收敛性

- Q-learning can be shown to converge to true/exact Q-factors (under mild assumptions). 

- The proof is sophisticated, based on theories of stochastic approximation and asynchronous algorithms.   收敛性的证明比较复杂

-  Q-learning map F 算子是一个 sup-norm contraction  .  压缩映射.  为了让随机近似算法，压缩性很重要，由于有异步操作，sup-norm 也很重要
  $$
  (FQ)(i,u) = E_j \left\{g(i,u,j) + \alpha \min_{u'} Q(j, u')\right \}
  $$
  
- Generic stochastic approximation algorithm

  - Consider generic fixed point problem involving an expectation:
    $$
    x = E_w\{ f(x,w)\}
    $$

  - Assume $E_w\lbrace f(x,w)\rbrace$􏰁 is a contraction with respect to some norm, so the iteration $x_{k+1} = E_w\lbrace f(x_k,w)\rbrace$ , converges to the unique fixed point.

  - Approximate $E_w\lbrace f(x,w)\rbrace$ by sampling



##### STOCH. APPROX. CONVERGENCE IDEAS

思路, 之前迭代的时候, 使用一整个采样序列, 计算量太大, 所以只用随机一个采样来迭代.  有点类似 全量梯度下降 与  随机梯度下降.

##### Q-LEARNING COMBINED WITH OPTIMISTIC PI

被证明不work

##### Q-FACTOR APPROXIMATIONS  近似Q值

DQN

大规模精确 Q-learning 由于存储空间有限和维度过大无法使用，所以考虑使 用基函数近似来解决这个问题，使用状态 i 和控制 u 的特征向量函数 φ′ (i, u) 乘以权重向量 r 来近似 Q 值。在使用样本近似策略迭代搜索 r 的时候，从某 个策略出发，收集若干样本 (可能是几十个也可能是几百个，还有可能只有一 个)，评价策略，可以使用之前讲过的 LSTD 和 LSPE 之类的方法进行策略评 价，然后使用策略评价的结果进行策略迭代，实际操作中人们通常使用乐观 策略迭代来求解权重向量。

##### BELLMAN EQUATION ERROR APPROACH

思路,  用人工特征函数的线性参数r组合来近似Q,  然后再将近似值与 bellman error 的norm求最小,  其实bellman error 就作为近似的目标, 不过求解是最小二乘法的思路. 

对于随机问题，这种方法更适合确定性问题，因为对于一个确定性问题， 不存在转移概率，状态转移过程中能得到一个确定的新状态，生成样本之后， 就可以用这些样本计算最小二乘问题，如果使用所有状态求解 r，近似结果会 很精确，误差也会很小，如果没有用所有状态进行计算，能够采集具有代表性 的样本进行计算也能够得到一个误差比较小的近似权重向量。这是一个线性 的最小二乘问题，求解的方法很多，但是必须要保证矩阵可逆，方法是采集足 够的样本，样本数量不足的时候是没有办法保证矩阵可逆的







### ADAPTIVE CONTROL BASED ON ADP 自适应动态规划

#### LINEAR-QUADRATIC PROBLEM  线性二次问题



#### PI FOR LINEAR-QUADRATIC PROBLEM





### APPROXIMATION IN POLICY SPACE

#### APPROXIMATION IN POLICY SPACE

A policy approximator is called an **actor**, while a cost approximator is also called a **critic**. An actor and a critic may coexist.   **Actor-Critic** 算法



##### APPROXIMATION IN POLICY SPACE METHODS

求解近似策略的近似参数 r 有很多种方法

- Random search methods  随机搜索方法的工作原 理是在一个给定的近似参数 r 邻域随机生成若干新的近似参数 r′，找到一个 更好的近似参数。
- Gradient-type methods (known as **policy gradient** methods) 策略梯度算法



#### COMBINATION WITH APPROXIMATE PI

implement PI within the class of parametrized policies. 在使用策略近似策略迭代的时候，算法是这么工作的，首先使用近似值迭代方法评估当前的近似策略，然后使用优化算法 (比如之前提到的随机搜索或者 梯度方法) 优化策略的近似参数，然后再评估，一直这么迭代下去









### FINAL WORDS

#### TOPICS THAT WE HAVE NOT COVERED

- Extensions to discounted **semi-Markov**, stochastic shortest path problems, average cost problems, sequential games ...
- Extensions to continuous-space problems
- Extensions to continuous-time problems
- Adaptive DP - Continuous-time deterministic optimal control. Approximation of cost function derivatives or cost function differences
- Random search methods for approximate policy evaluation or approximation in policy space
- Basis function adaptation (automatic generation of basis functions, optimal selection of basis functions within a parametric class)
- Simulation-based methods for general linear problems, i.e., solution of linear equations, linear least squares, etc - Monte-Carlo linear algebra



#### CONCLUDING REMARKS  总结

There are major flaws in all methods:

- Oscillations and exploration issues in approximate PI with projected equations 近似方法会有震荡以及探索问题
- Restrictions on the approximation architecture in approximate PI with aggregation 聚合问题中近似结构会受到限制，也就是说基 函数需要按照概率分布来进行设计，否则算法很容易不工作
- Flakiness of optimization in policy space approximation 近似策略方法很容易实现，但是很容易不工作，不管优化策略近似参数 的时候使用基于仿真的梯度方法或者随机搜索什么的，都不可靠， 可能不工作;
















### Reference

http://web.mit.edu/dimitrib/www/publ.html

https://github.com/Fengyuan-Shi/ADP_course_given_by_BERTSEKAS_2014_THU









