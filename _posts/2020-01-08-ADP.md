---
layout:     post
title:      Approximate Dynamic Programming
subtitle:   Note on "Approximate Dynamic Programming" (2014)
date:       2020-01-08 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-sunset.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - Dynamic Programming
    - ADP

---

# Approximate Dynamic Programming

**近似动态规划**  ADP course given by BERTSEKAS at THU, 2014 

比较清晰的教程





### OUTLINE

**Large-scale DP** based on approximations andin part on simulation. 主题是基于近似 (一部分基于仿真) 的大规模动态规划. 

AI或者ML机器学习 叫它**强化学习**，作者叫它**神经动态规划 NDP**或者**近似动态规划 ADP**，目前主要有两个方向，某种程度上是互相融合的，一个方向是强化学习，主要依赖于基于**特征表达**的输入输出之间的函数关系，也可以叫基于观察样本或者仿真进行学习，另一个方向是**最优控制理论 optimization/control theory**，关注重点是传统的优化方法和算法，比如策略迭代和值迭代。

Deals with control of dynamic systems under **uncertainty**, but applies more broadly (e.g., discrete deterministic optimization) ，这个不确定性，不仅仅指随机，还有可能是解决 **minmax** 的问题，这个时候不确定性就可以来源于对手做出的行为。实际上动态规划可以求解动态系统的控制问题，比如离散优化问题和组合最优化问题。

Bellman 提出了**动态规划 (Dynamic Programming)** 方法并给出**“维数灾难”(Curse of Dimensionality)** 的概念，Powell 认为维数灾有三种: 动作空间，状态空间和随机干扰. 



### LECTURE 1

#### DP AS AN OPTIMIZATION METHODOLOGY

从更抽象的角度来看，只有一种优化问题，目标就是最小化cost函数，可以是标量，向量，或者更复杂的。优化的时候要受到约束集合的限制，优化就是从约束集合给出的决策集合 $U$ 中选择合适的决策 $u$ 让目标值最小。



Generic optimization problem: $\min_{u \in U} g(u)$ ,  
where $u$ **optimization/decision variable**, $g(u)$ **cost** function,   $U$  **constraint set**



##### Categories of problems

根据 g 和 U, 对优化问题进行分类

- **Discrete** (U is finite) or **continuous**
- **Linear** (g is linear and U is polyhedral 多边形集) or **nonlinear**
- **Stochastic** or **deterministic**: In stochastic problems,  the cost involves a stochastic parameter w,  $g(u) = E_w \lbrace G(u, w) \rbrace$  ,  env包含随机性, 所以是个期望 ;  确定性问题, 比如旅行商TSP问题



##### DP deals with multistage stochastic problems

动态规划能求解的问题 需要满足某些特性 (马尔科夫性);  这里用的词 stage, 类似于time-step ; DP问题, 大问题拆成子问题, 不一定是按照时间维度去拆分的



#### BASIC STRUCTURE OF STOCHASTIC DP

##### Discrete-time system

**状态转移函数** $x_{k+1}=f_{k}\left(x_{k}, u_{k}, w_{k}\right), \quad k=0,1, \ldots, N-1$  

- $k$  Discrete **time**  
- $x_{k}:$ **State**; summarizes past information that is relevant for future optimization
- $u_{k}:$ **Control**; decision to be selected at time $k$ from a given set 
- $w_{k}:$ Random parameter (also called "disturbance" or "**noise**" depending on the context)  某个状态的中的随机因素
- $N$: **Horizon** 界限 or number of times control is applied 

也可以用  **状态转移概率** $P\left(x_{k+1} \vert x_{k}, u_{k}\right)$ 来描述系统

f 与 p 就是代表着系统, env.

两者缺陷,  有时需要仿真采样来补充

- 状态转移函数f的缺陷是绝大多数难以得到解析表达的函数形式

- 状态转移概率p的缺陷

  - 使用数值概率，状态转移矩阵的空间非常大，甚至会导致计算机内存溢出，如果状态和控制有连续的，数值概率矩阵会有无数个维度

  - 使用解析的概率分布函数, 连续离散都可以表达，但这个解析关系非常难得到


##### Cost function that is additive over time

$$
E \left \{ g_{N} \left(x_{N} \right)+\sum_{k=0}^{N-1} g_{k} \left(x_{k}, u_{k}, w_{k} \right) \right \}
$$

与策略相关; 最后一步策略的t是N-1 , 然后得到的cost是 $g_N$

 

##### INVENTORY CONTROL EXAMPLE

一个例子, 主要看下 x, w的差异.    其实可以把 (x,w) 联合起来看作 state

举一个库存控制的例子，仓库中有一些货物，然后来了一批客户，这时决策者 需要把货物卖给客户满足他们的需求，然后下订单补充货物。在这个库存系统中, $x_k$ 是阶段 k 时的库存状态，比如一个存放汽车的仓库，放了 $x_k$ 数量的汽车，来了几个客户，对汽车的需求是 $w_k$，如果完全满足了他们的需求，库存量 减少为 $x_k −w_k$，下了订货量为 $u_k$ 的订单补充库存，订单到达后 (假设订单到 达前没有卖出汽车给客户) 新的库存状态为 $x_k −w_k +u_k$

马尔科夫性，或无后效性 MDP.   对于MDP,都是基于time-step的, 要求符合Markov性.  ??DP解法是一个大问题嵌套一个子问题的, 能否匹配?


Optiminization over **policies** ( also called feedback control laws)
$$
u_{k}=\mu_{k}\left(x_{k}\right), \quad k=0, \ldots, N-1
$$

这里$\mu$ 函数代表策略$\pi$, 不过是写作函数的形式



#### FINITE-HORIZON PROBLEM

- **System** $x_{k+1}=f_{k}\left(x_{k}, u_{k}, w_{k}\right), k=0, \ldots, N-1$

- **Control contraints** $u_{k} \in U_{k}\left(x_{k}\right)$ 

- **Probability distribution** $P_{k}\left(\cdot \  \vert x_{k}, u_{k}\right)$ of $w_k$ 

- **Policies** $\pi=\{ \mu_{0}, \ldots, \mu_{N-1}\}$ ; $\mu_{k}$ maps  $x_{k}$ into controls $u_k = \mu_k(x_k)$ and  $\mu_k(x_k) \in U_{k}(x_{k})$ for all $x_{k}$  
  这里策略$\pi$, 是用 每个时刻control的取值集合来表示的. 即  $\mu: x \to \pi$ 

- **Expected cost** of $\pi$ starting at $x_{0}$ is :  **cost的期望  J**   一定与一个策略相关
  $$
  J_{\pi} (x_{0} )=E\left\{g_{N} (x_{N} )+\sum_{k=0}^{N-1} g_{k}\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}
  $$
  
- **Optimal cost** function  $J^\ast=\min_{\pi} J_{\pi}\left(x_{0}\right)$

- **Optimal polity** $\pi^\ast$  satisfies  $J_{\pi^\ast} (x_0) = J^\ast(x_0)$

  

#### PRINCIPLE OF OPTIMALITY 

Let $\pi^ \ast = \{   \mu_{0}^\ast, \mu_{1}^\ast, \ldots, \mu_{N-1}^\ast \}$ be optimal policy

Consider the “**tail subproblem**”子问题  whereby we are at $x_k$ at time k and wish to minimize the “**cost- to-go**” from time k to time $N$ and "**tail policy**"

<img src="/img/2020-01-08-ADP.assets/image-20200108232901646.png" alt="image-20200108232901646" style="zoom:50%;" />
$$
E\left\{g_{N}\left(x_{N}\right)+\sum_{\ell =k}^{N-1} g_\ell\left(x_{\ell }, \mu_{\ell }\left(x_{\ell }\right), w_{\ell }\right)\right\}
$$


Principle of optimality : The tail policy is optimal for the tail subproblem (optimization of the future does not depend on what we did in the past). The principle of optimality says that the tail of an optimal sequence is optimal for the tail subproblem.

大问题的**最优解**可以由小问题的**最优解**推出，这个性质叫做“最优子结构性质”。 



#### DP ALGORITHM

- Start with Initial condition: $J_{N}\left(x_{N}\right)=g_{N}\left(x_{N}\right)$  最后一个状态的g作为初值
- Go **backwards**, $k=N-1, \ldots, 0,$ using 

$$
J_{k} (x_{k} )=\min_{u_{k} \in U_{k} (x_{k} )}  \underset{w_{k}}{E}\left\{g_{k} (x_{k}, u_{k}, w_{k} ) 
 +J_{k+1} (f_{k} (x_{k}, u_{k}, w_{k}  ) )\right \}
$$

- Then $J_{0}\left(x_{0}\right)$ generated at the last step, is equal to the optimal cost $J^{*} (x_{0})$ Also, the policy $\pi^\ast = \left \lbrace \mu_0^\ast, \ldots, \mu_{N-1}^\ast \right \rbrace$ is optimal. 

这里对每个状态x, 每一步都找的最优的control, 对exact DP方法,   J是从最后一步倒推回来的, 只需要这个是真值, 执行完DP流程后, $J \to J^\ast$ , 所以DP可以看成两个函数间的映射, 原函数J只需最后一步的值, 其他值都被更新掉了,所以其他步的值可以任意 ; J的实现可以用table, 如果把 J 看成函数, 则要认为这个函数是可以被赋值更新的, 直接看成映射关系比较容易理解. **mapping** ; 这里数学上的理解与程序的实现, 有点概念上的冲突. 



#### PRACTICAL DIFFICULTIES OF DP

- The **curse** of **dimensionality**
  - Exponential growth of the computational and storage requirements as the number of state variables and control variables increases
  - Quick explosion of the number of states in combinatorial problems

- The curse of **modeling**
  - Sometimes a simulator of the system is easier to construct than a model

- **real-time** solution constraints
  - 一类问题, 需要很高实时性, 拿到数据后时间很少 The data of the problem to be solved is given with little advance notice
  - The problem data may change as the system is controlled – need for on-line replanning

所以要近似求解, All of the above are motivations for **approximation** and **simulation**



#### COST APPROXIMATION

近似成本不是唯一的近似思路，但是是一个很主要的近似思路

optimal cost-to-go function $J_{k+1}$  replaced by  approximation $\tilde J_{k+1}$

- Problem Approximation: Use J derived from a related but simpler problem 使用

  一个更容易计算的问题的cost

- **Parametric** Cost-to-Go Approximation:  parameters are tuned by some heuristic or systematic scheme 

- **Rollout** Approach:  the cost of some suboptimal policy, which is calculated either analytically or by simulation



#### ROLLOUT ALGORITHMS

At each k and state $x_k$, use the control $\bar \mu(x_k)$ that minimizes in $\min_{u_{k} \in U_{k} (x_{k} ) }  {E}\left [g_{k} (x_{k}, u_{k}, w_{k} )  + \tilde J_{k+1} \left(f_{k} (x_{k}, u_{k}, w_{k}  ) \right)\right ]$ ,  where $\tilde J_{k+1}$ is the cost-to-go of some **heuristic** policy (**base policy**).

从这个启发式策略出发，通过前向仿真计算这个策略执行后的成本，再选择控制，一般情况下会得到一个不会更差(可能更优)的策略，这种方法很适合在线计算

Cost improvement property: The rollout algorithm achieves no worse (and usually much better) cost than the base policy starting from the same state.

Main difficulty:  Calculating  $\tilde J_{k+1}$  maybe computationally intensive if the cost-to-go of the base policy cannot be analytically calculated.

- May involve **Monte Carlo simulation** if the problem is **stochastic**.
- Things improve in the deterministic case (an important application is discrete optimization).
- Connection w/ Model Predictive Control (MPC). 在该领域rollout很有效



#### INFINITE HORIZON PROBLEMS

- The number of stages is infinite.
- The system is stationary.

两种角度:  总成本,  平均成本;  平均成本的近似理论还在研究中, 下面用总成本.  因为总成本可能无穷大, 所以要处理:  使用折扣系数与截断阶段;  截断阶段是把趋于 $\infty$ 的 N 截断，令 N 等于一个固定的值，将无限期问题近似转化为有限期问题，然后求解。

Total cost:  minimize

$$
J_{\pi}\left(x_{0}\right)=\lim _{N \rightarrow \infty} \underset{w_k \atop k=0,1, \ldots}{E}\left\{\sum_{k=0}^{N-1} \alpha^{k} g\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}
$$

- Discounted problems (α < 1, bounded g)
- Stochastic shortest path problems (α = 1, finite-state system with a termination state) 
- Discounted and undiscounted problems with unbounded cost per stage  无穷大问题, 不讨论

生活中很少 (甚至是完全) 不会产生无限期问题，那么为什么要研究无限期问题呢。原因是无限期问题在数学上更优雅，我们可以把注意力 放在平稳策略上，这样就可以在任何阶段都使用相同的策略进行控制，大量的研究产生了大量有效的算法，很多近似理论也是为无限期问题服务的. 



##### DISCOUNTED PROBLEMS/BOUNDED COST

- Stationary system $x_{k+1}=f\left(x_{k}, u_{k}, w_{k}\right), \quad k=0,1, \ldots$      时间维度
  平稳系统: 每一个阶段的系统状态都在同一个状态空间内取值，随机干扰 w 在每一阶段都有相同的概率分布. 
- cost of a policy $\pi=\lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$ :

  $$
  J_{\pi}\left(x_{0}\right)=\lim _{N \rightarrow \infty} \underset{w_k \atop k=0,1, \ldots}{E}\left\{\sum_{k=0}^{N-1} \alpha^{k} g\left(x_{k}, \mu_{k}\left(x_{k}\right), w_{k}\right)\right\}
  $$
  
  with $\alpha<1,$ and $g$ is bounded [for some $M,$ we have $\vert g(x, u, w)\vert \leq M \text { for all }(x, u, w)]$ 
- Optimal cost function: $J^\ast (x)=\min_{\pi} J_{\pi}(x)$
- Boundedness of $g$ guarantees that all costs are well-defined and bounded: $\vert J_{\pi}(x) \vert \leq \frac{M}{1-\alpha}$



#### SHORTHAND NOTATION FOR DP MAPPINGS  简洁符号

算子是从函数到函数的映射;  下面的两个算子, 都只对应于一步control.  J本来对应一个默认的策略. 是什么没所谓,因为会在某个时刻k, control被替换掉; 算子本身如果不带下标,则没指定哪个时刻k, 表示任意某个时刻.

引入 算子T,  For **any function** J of x, denote:   $(T J)(x)=\min _{u \in U(x)} \underset{w}{E}\{g(x, u, w)+\alpha J(f(x, u, w))\}, \forall x$

$TJ$ is the optimal cost function for the one-stage problem with stage cost g and **terminal cost function** $\alpha J$.  根据前面的DP算法, 整个算法的迭代过程中, 只需要最后一步的cost函数的真值.  这个算子, 只对应策略里的一个greedy control.  如果该问题只有一个stage, 期望的cost 变为 最优化cost $TJ$ .  

对 discounted 问题, T operates on bounded functions of x to produce other bounded functions of x. 有界函数到有界函数的映射. 

For any stationary policy $\mu,$  引入策略算子 $T_{\mu}$ ,  下式是定义式,  $J$ 只是一个有界函数
$\left(T_{\mu} J\right)(x)=\underset{w}{E}\{g(x, \mu(x), w)+\alpha J(f(x, \mu(x), w))\}, \forall x$

The critical structure of the problem is captured in $T$ and $T_{\mu}$    算子包含了问题的关键结构信息. $T$ and $T_{\mu}$ provide a powerful unifying framework for DP:  essence of  "**Abstract Dynamic Programming**"

$T = \min_\mu T_\mu$



#### FINITE-HORIZON COST EXPRESSIONS

这边定义的算子, 都是有时间下标的, 对应策略里面的k下标, 即在时刻k,执行该control, J函数的改变. 下式中的$\ell$ 时间步, 都是原先的默认策略的. 

Consider an $N$-stage policy $\pi_{0}^{N}=\lbrace \mu_{0}, \mu_{1}, \ldots, \mu_{N-1}\rbrace$ with a **terminal cost** $J$(最后一步的, 所以不涉及策略):

$$
\begin{aligned}
J_{\pi_{0}^{N}}\left(x_{0}\right)&=E\left\{\alpha^{N} J\left(x_{k}\right)+\sum_{\ell=0}^{N-1} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\} \\
 &=E\left\{g\left(x_{0}, \mu_{0}\left(x_{0}\right), w_{0}\right)+\alpha J_{\pi_{1}^{N}}\left(x_{1}\right)\right\}  \\
 &=\left(T_{\mu_{0}} J_{\pi_{1}^{N}}\right)\left(x_{0}\right) 
\end{aligned}
$$

where $\pi_{1}^{N}=\lbrace \mu_{1}, \mu_{2}, \ldots, \mu_{N-1}\rbrace$  , 上式对应了策略里面的第一步control造成的改变

By induction we have  $J_{\pi_{0}^{N}}(x)=\left(T_{\mu_{0}} T_{\mu_{1}} \cdots T_{\mu_{N-1}} J\right)(x), \quad \forall x$ ;   T执行顺序,从左到右, 按照time的顺序

For a stationary policy $\mu$ the $N$-stage cost function (with terminal cost $J$ ) is $J_{\pi_{0}^{N}}=T_{\mu}^{N} J$,  where $T_{\mu}^{N}$ is the $N$-fold composition of $T_{\mu}$ ;     $T_\mu^N$ 表示一个完整策略每一步的映射, 上标表示按时间执行N步optimal

optimal $N$-stage cost function  (  with terminal cost $J$ )  is $T^{N} J$  ;  $T$算子有个不清楚的地方, 就是不知道在哪个时间步进行greedy操作, 认为是在序列的最后一步进行的.

$T^{N} J=T\left(T^{N-1} J\right)$ is just the DP algorithm , DP算法的简洁表达,  左边表示每一步都optimal, 右边表示, N-1时刻选min后, 



#### "SHORTHAND" THEORY-A SUMMARY

- Infinite horizon cost function expressions [with $\left.J_{0}(x) \equiv 0\right]$ 

  对于无限问题, 是没有最后一步, 对discounted问题, 只需把每步的g加起来, 因为g是在第一步control之后才有的, 所以$J_0 = 0$ , 下式的意思就是, 对一个无限问题, 默认的策略都会被覆盖掉, 得到一个新的J函数
  $$
  J_{\pi}(x)=\lim _{N \rightarrow \infty}\left(T_{\mu_{0}} T_{\mu_{1}} \cdots T_{\mu_{N}} J_{0}\right)(x), \quad J_{\mu}(x)=\lim _{N \rightarrow \infty}\left(T_{\mu}^{N} J_{0}\right)(x)
  $$
  
- **Bellman's equation**: $J^\ast=T J^\ast, \quad J_{\mu}=T_{\mu} J_{\mu}$  这两个都已经收敛了

- **Optimality** condition:  $\mu: \text { optimal }    \iff  T_{\mu} J^\ast=T J^\ast$   一个策略函数最优, 表示该策略在任意k时刻,都取的cost最小值的control

- **Value** iteration: For any (bounded) $J$ , $J^{*}(x)=\lim_{k \rightarrow \infty}\left(T^{k} J\right)(x), \quad \forall x$  这个公式里面没时间下标, 可以看成, 对无论stage多长的问题, 对逆序地每个stage进行一次T

- **Policy** iteration: Given $\mu^{k}$  , 注意这里是上标,  应该是指一个策略的

  - Policy evaluation: Find $J_{\mu^{k}}$ by solving , $J_{\mu^{k}}=T_{\mu^{k}} J_{\mu^{k}}$    先evaluate到收敛
  - Policy improvement: Find $\mu^{k+1}$ such that  $T_{\mu^{k+1}} J_{\mu^{k}}=T J_{\mu^{k}}$  再 improve 一步



##### TWO KEY PROPERTIES

 下面两个公式对 $T$ , $T_\mu$ 都成立

- Monotonicity property 单调性: For any $J$ and $J′$ such that $J(x) ≤ J′(x)$ for all $x$, and any $\mu$,  $(TJ)(x) ≤ (TJ′)(x), \forall x$

  Monotonicity is present in all DP models  单调性对所有DP问题都成立

- Constant Shift property : For any $J$, any scalar $r$, and any $\mu$,  $\left(T(J+re) \right)(x) = (TJ)(x)+ \alpha r, \forall x$,   where $e$ is the unit function $[e(x) \equiv 1]$.   

  $(J + re)(x)$ 的意思是 $J(x) + r$ , 即在$J(x)$加上一个常数 , 因为T算子相当于执行了一个时间步, 所以r有discount $\alpha$ 
  
  Constant shift is special to discounted models

contraction mappings : for Discounted problem



#### CONVERGENCE OF VALUE ITERATION

for all bounded $J$

$$
J^{*}(x)=\lim _{k \rightarrow \infty}\left(T^{k} J\right)(x), \quad \text { for all } x
$$

Proof: For simplicity we give the proof for $J \equiv 0$ For any initial state $x_{0},$ and policy $\pi=\lbrace \mu_{0}, \mu_{1}, \ldots\rbrace$

$$
\begin{aligned}
J_{\pi}\left(x_{0}\right)&=
E\left\{\sum_{\ell=0}^{\infty} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\} \\
&=E\left\{\sum_{\ell=0}^{k-1} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\} \\
& \quad\quad +E\left\{\sum_{\ell=k}^{\infty} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\}
\end{aligned}
$$

The tail portion satisfies

$$
\left|E\left\{\sum_{\ell=k}^{\infty} \alpha^{\ell} g\left(x_{\ell}, \mu_{\ell}\left(x_{\ell}\right), w_{\ell}\right)\right\}\right| \leq \frac{\alpha^{k} M}{1-\alpha}
$$

where $M \geq \vert g(x, u, w) \vert$  ;  上面是说, $J_\pi$ 是有界的, 然后两边都上T算子, 每步都取最小,再取极限, 会收敛到固定值. 

<u>Take min</u> over $\pi$ of both sides, then $\lim$ as $k \rightarrow \infty $  **Q.E.D.**   



##### BELLMAN’S EQUATION

The optimal cost function $J^\ast$ is a solution of Bellman's equation, $J^\ast=T J^\ast$, i.e., for all $x,$
$$
J^\ast(x)=\min _{u \in U(x)} E\left\{g(x, u, w)+\alpha J^\ast(f(x, u, w))\right\}
$$

Proof: For all $x$ and $k$

$$
J^{*}(x)-\frac{\alpha^{k} M}{1-\alpha} \leq\left(T^{k} J_{0}\right)(x) \leq J^{*}(x)+\frac{\alpha^{k} M}{1-\alpha}
$$

where $J_{0}(x) \equiv 0$ and $M \geq \vert g(x, u, w)\vert$ .  Applying $T$ to this relation, and using Monotonicity and Constant Shift,   两边夹逼

$$
\begin{aligned}\left(T J^{*}\right)(x)-\frac{\alpha^{k+1} M}{1-\alpha} & \leq\left(T^{k+1} J_{0}\right)(x) \\ & \leq\left(T J^{*}\right)(x)+\frac{\alpha^{k+1} M}{1-\alpha} \end{aligned}
$$

Taking the limit as $k \rightarrow \infty$ and using the fact

$$
\lim_{k \rightarrow \infty}\left(T^{k+1} J_{0}\right)(x)=J^*(x)
$$

we obtain $J^∗ = TJ^∗$. **Q.E.D.**



#### THE CONTRACTION PROPERTY

Ctraction property: For any bounded functions $J$ and $J^{\prime},$ and any $\mu$
$$
\max _{x}\left|(T J)(x)-\left(T J^{\prime}\right)(x)\right| \leq \alpha \max _{x}\left|J(x)-J^{\prime}(x)\right| \\
\max _{x}\left|\left(T_{\mu} J\right)(x)-\left(T_{\mu} J^{\prime}\right)(x)\right| \leq \alpha \max _{x}\left|J(x)-J^{\prime}(x)\right|
$$

Proof: Denote $c=\max _{x \in S}\vert J(x)-J^{\prime}(x)\vert$ ,  Then 

$$
J(x)-c \leq J^{\prime}(x) \leq J(x)+c, \quad \forall x
$$

Apply $T$ to both sides, and use the Monotonicity and Constant Shift properties:

$$
(T J)(x)-\alpha c \leq\left(T J^{\prime}\right)(x) \leq(T J)(x)+\alpha c, \quad \forall x
$$

Hence

$$
\left|(T J)(x)-\left(T J^{\prime}\right)(x)\right| \leq \alpha c, \quad \forall x
$$

**Q.E.D.**
Note: This implies that $J^{*}$ is the **unique** solution of $J^{*}=T J^{*},$ and $J_{\mu}$ is the **unique** solution

#### NEC. AND SUFFICIENT OPT. CONDITION

A stationary policy $\mu$ is optimal if and only if $\mu(x)$ attains the minimum in Bellman's equation for each $x ;$ i.e.,

$$
T J^{*}=T_{\mu} J^{*}
$$

or, equivalently, for all $x$

$$
\mu(x) \in \arg \min _{u \in U(x)} E\left\{g(x, u, w)+\alpha J^{*}(f(x, u, w))\right\}
$$

Proof: If $T^\ast=T_{\mu} J^\ast$, then using Bellman's equation $\left(J^\ast=T J^\ast \right)$,  we have

$$
J^{*}=T_{\mu} J^{*}
$$

so by uniqueness of the fixed point of $T_{\mu},$ we obtain $J^{*}=J_{\mu} ;$ i.e., $\mu$ is optimal. 

Conversely, if the stationary policy $\mu$ is optimal, we have $J^{*}=J_{\mu},$ so

$$
J^{*}=T_{\mu} J^{*}
$$

Combining this with Bellman's Eq. $\left(J^\ast=T J^\ast \right)$ obtain $T J^\ast=T_{\mu} J^\ast$ . **Q.E.D.**








### Reference

http://web.mit.edu/dimitrib/www/publ.html

https://github.com/Fengyuan-Shi/ADP_course_given_by_BERTSEKAS_2014_THU









