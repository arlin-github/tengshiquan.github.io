---
layout:     post
title:      UCL Course on RL, Model-Free Control
subtitle:   David Silver 的课程笔记4
date:       2019-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---



# Model-Free Control

核心 :  SARSA, Q-learning



### Uses of Model-Free Control

对很多MDP问题

- MDP model is **unknown**, but **experience** can be sampled    , 未知
- MDP model is **known**, but is **too big** to use, except by **samples** , 太大

 **Model-free control** can solve these problems



#### On and Off-Policy 

- **On-policy** learning
  - Learn about policy $π$ from experience sampled from $π$    采样与改进的是同一个策略
  - 用于采样策略 $\pi$ 和**目标策略** $\pi$ 一致，根据该策略产生的一系列行为去获得奖励，然后更新状态函数，最后根据该更新的价值函数来优化策略得到较优的策略。

- **Off-policy** learning
  - Learn about policy $π$ from experience sampled from $μ$ 

> 采样策略 $\mu$ 可以选用随机性策略，目标策略 $\pi$ 选用确定性策略
> 需要结合**重要性采样**才能使用样本估计总体
> 方差更大，收敛性更差
> 数据利用性更好（可以使用其他智能体交互的样本）
> 采样策略需要比目标策略更具备**探索性**。即在每个状态下，目标策略的可行动作是采样策略的子集。  



### On-Policy Monte-Carlo Control

#### Generalised Policy Iteration With Monte-Carlo Evaluation

GPI中使用MC的评估

1. **因为MC是modelfree ， 所以值函数评估，不能只用V， 须用Q**  
因为modelfree，是不知s的转移概率的， 什么a导致了s’ ， 必须记录下来，所以求的是 q  
   如果只是为了evaluate，不为改进，那用MC评估V还是可以的

2. 因为不知道model，而且mc是采样的，所以不能完全greedy方式，否则会陷入局部   
   因为如果不探索，那找到一个稍微优秀点的就一直选这个，就没机会探索其他潜在更好的，对随机环境， 一开始不好的说不定最终是好的



#### Model-Free Policy Iteration Using Action-Value Function

- Greedy policy improvement over **V(s) requires model** of MDP     

$$
\pi'(s)=\mathop{\textrm {argmax}}_{a \in \cal{A}}\left( {\cal{R}}_s^a+{\cal{P}}_{ss'}^aV(s') \right)
$$

- Greedy policy improvement over **Q(s,a) is model-free**

$$
    \pi'(s)=\mathop{\textrm{argmax}}_{a \in \cal{A}}\ Q\left( s,a \right)
$$



### Exploration

policy is generally “**soft**”, meaning that:  $\pi(a \mid s)>0 \quad \forall s \in S, a \in A(s)$ 

- Uniformly random policy: $\pi(s, a)=1 / \vert A(s) \vert $ 
- $\varepsilon$ -soft policy: $\pi(s, a) \geq \epsilon / \vert A(s) \vert $ 
- ε-Greedy policy



#### ε-Greedy Exploration

- 完全使用贪婪算法改善策略通常不能得到最优策略

- Simplest idea for ensuring continual exploration

$$
\pi(a \vert s)=\begin{cases}\frac{\epsilon}{m}+1-\epsilon&\text{if a*=$\mathop{\textrm{argmax}}\limits_{a \in \cal{A}}\ Q(s,a)$}\\ \frac{\epsilon}{m}&\text{otherwise}\end{cases}
$$

- m代表全部可选的行为数量， 所有m个action都是 tried with non-zero probability



#### ε-Greedy Policy Improvement

- 策略提升定理   提升通过ε-greedy实现
- 对于任意 $\epsilon-{\textrm{greedy}}$ ，使用相应的 $\pi$ 得到的 $\epsilon-{\textrm{greedy}}$策略 $\pi^{'}$是在 $\pi$上的一 次策略提升，即 $v_{\pi^{'}}(s)\geq v_{\pi}\left( s \right)$   
  For any ε-greedy policy π, the ε-greedy policy π′ with respect to $q_π$ is an improvement, $v_{\pi^{'}}(s)\geq v_{\pi}\left( s \right)$
- 一个原本就是 $\epsilon -greedy$ 策略 $\pi$ ,  evaluate 一次以后，得到最新的$V_\pi$,  然后进行$\epsilon -greedy$步骤,得到$\pi '$，可以保证策略有提升 ， 因为 evaluate一次以后，那些a的q值就更准确了，提升了选真正最大q的那个a的几率，如果a不同的话；如果a相同，则期望回报还是一样，没有提升;    
- **关键是要能采样到那些有提升的s,a**  ， 如果一直采样不到那些比较少见的，可以认为是没有提升，原地不动；但根据概率，只要采样足够多，肯定是能采样到的，所以概率上必然是可以提升的，即**保证收敛**。
- 但对于用函数近似的，比如神经网络这种方式，一直在没有提升的数据上训练， 可能会造成**策略退化**的，所以**不保证收敛** 到最优

$$
\begin{align} q_{\pi}(s,\pi'(s))&=\sum_{a \in \cal{A}}\pi'(a \vert s)q_\pi(s,a)\\ &=\epsilon/m \sum_{a \in \cal{A}}q_\pi(s,a)+(1-\epsilon)\max_{a \in \cal{A}}q_\pi(s,a)\\ &\ge \epsilon/m \sum_{a \in \cal{A}}q_\pi(s,a)+(1-\epsilon)\sum_{a \in \cal{A}}\frac{\pi(a \vert s)-\frac{\epsilon}{m}}{1-\epsilon}q_\pi(s,a)\\ &=\sum_{a \in \cal{A}}\pi(a \vert s)q_\pi(s,a)=v_\pi(s) \end{align}
$$



### Monte-Carlo Policy Iteration

- **Policy evaluation** : Monte-Carlo policy evaluation, $Q = q_π$  这个需要无限多的采样
- **Policy improvement** : **ε-greedy** policy improvement  提升是通过ε-greedy实现 



#### Monte-Carlo Control

<img src="/img/2019-03-02-Silver.assets/image-20190203193449022.png" style="zoom: 50%;" />

- 实际应用中，更有效率的做法， MC的抽样没必要精确收敛到 $q_\pi$ ,每个episode可以估值，然后就可以improve了
- 即，采样一个episode，evaluation一次，就提升一次； 基于tabular的可以说是坚实的，不会退化

Every episode:

- **Policy evaluation** :  Monte-Carlo policy evaluation, $Q ≈ q_π$ 
- **Policy improvement** : ε-greedy policy improvement



#### GLIE

探索率衰减,  现在权衡  探索性 以及 最终策略的确定性，如果找到最优动作序列了，就不该很大的随机探索

**Greedy in the Limit with Infinite Exploration (GLIE)**  发音glee

1. All **state-action** pairs are explored **infinitely** many times： $\mathop{\textrm{lim}}_{k \rightarrow \infty}N_k(s,a)=\infty$ 
2. 策略最终收敛到一个**greedy policy**： $$\mathop{\textrm{lim}}_{k \rightarrow \infty}\pi_k(a \vert s)=\textbf{1}(a=\mathop{\textrm{argmax}}_{a' \in \cal{A}}Q_k(s,a'))$$ 

- 第一条要求所有的都遍历到足够多次, 第二条要求收敛到一个 纯的greedy策略,  而**不是soft** 的.

- 例如我们取 $\epsilon_k = 1/k$ （ k 为探索的episode数目），则 $\epsilon$ 值趋向于０，那么该 $\epsilon-{\textrm{greedy}}$ 蒙特卡洛控制就具备GLIE特性。

- 所以就是两点 ：要采样到所有情况， 以及  $\epsilon$  是越来越小的

- GLIE 算法，探索的越来越少， 对环境不变的时候是这样的，但一直在变的环境 non-stationary，探索可能是需要持续很久. 



##### GLIE Monte-Carlo Control

1. 对于给定策略 $\pi$，采样第 k 个episode： $$\left\{ {S_1,A_1,R_2,...,S_T} \right\}\sim \pi$$；

2. 对于该episode里出现的每一个状态行为对 $S_t$ 和 $A_t$ ，更新计数和Ｑ函数：

   $N(S_t, A_t) \leftarrow N(S_t, A_t) + 1$

   $Q(S_t, A_t) \leftarrow Q(S_t, A_t)+ \frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))$

3. 基于新的Ｑ函数改善以如下方式改善策略：

   $\epsilon \leftarrow 1/k$

   $\pi \leftarrow \epsilon-{\textrm{greedy}}(Q)$

- GLIE Monte-Carlo control **converges** to the **optimal** action-value function, $Q(s, a) → q∗(s, a)$



### On-Policy Temporal-Difference Learning

现在开始 把上面的MC,换成 TD， evaluate一步，improve一步

##### MC vs. TD Control

- Temporal-difference (TD) learning has several **advantages** over Monte-Carlo (MC) 
  - **Lower variance** 
  - Online
  - Incomplete sequences 

- Natural idea: use TD instead of MC in our control loop 
  - Apply TD to Q(S, A) 
  - Use ε-greedy policy improvement 
  - Update every time-step 



#### Sarsa(λ)

##### Updating Action-Value Functions with Sarsa  sɑ:sə

- update Action Value， 核心获得Qtable
- The **convergence** properties of the Sarsa algorithm depend on the **nature** of the policy’s dependence on Q.

- sarsa可以保证收敛到$q_\pi$ , soft-greedy , 看具体怎么选

<img src="/img/2019-03-02-Silver.assets/image-20190219145619102.png" style="zoom: 60%;" />
$$
Q(S,A) \leftarrow Q\left( S,A \right)+\alpha (R+\gamma Q\left( S',A' \right)-Q(S,A))
$$



##### On-Policy Control With Sarsa

- sarsa本身可以说是个evaluate的算法，improve靠policy自带的 soft-greedy；

- sarsa 的 on-policy，on-policy 关键是看采样的策略是不是正在improve的策略.   policy本身也是一直在improve的，也可以说是一直在改变的，为什么不是off-policy的？关键是采样的策略，sarsa的关键就是策略本身；如果策略本身就是$\epsilon -greedy$ 的，只要评估的更准一点本身，就是evalue update就会去improve一次，因为下一步选取策略的时候，就会找soft-greedy的那个，可以保证收敛到 optimal ；max的操作是放在**策略**本身里面的，不是放在**算法**里的！！这里与Q-Learning 不一样
- 对于Sarsa算法， S, a, r , s'  后的 a' 是在Q值更新之前取的，所以是按照improve前的策略选取的a’ ，用Q(s',a')来递推Q(s,a)， 然后执行了a‘，获得了improve前策略的采样，所以 **on-policy**  
- **可以看成， evaluate一个 $\epsilon-greedy$的算法， $\epsilon-greedy$ 自带improve属性**
- Every **time-step**:
  - **Policy evaluation**  ,  **Sarsa**, $Q ≈ q_π$  
  - **Policy improvement** , ε-greedy policy improvement

![image-20190219095103843](/img/2019-03-02-Silver.assets/image-20190219095103843.png)



#### Convergence of Sarsa

满足如下两个条件时， SARSA 算法将收敛至最优行为价值函数。

1. GLIE sequence of policies $π_t(a \vert s)$  , 符合GLIE特性；
2. Robbins-Monro sequence of step-sizes $α_t $ ,  步长系数 $\alpha_t$ 满足：$\sum_{t=1}^{\infty}{a_t} = \infty$  且 $\sum_{t=1}^{\infty}{a_t^2} < \infty$



#### n-Step Sarsa

n-step returns :

$$
n=1 \quad \left(  {\textrm{SARSA}} \right) \quad q_t^{\left( 1 \right)}=R_{t+1}+\gamma Q\left( S_{t+1} \right) 
\\
n=2 \quad \quad q_t^{\left( 2 \right)}=R_{t+1}+\gamma R_{t+2}+\gamma^2 Q\left( S_{t+2} \right)
\\
......
\\
n=\infty \quad \left(  {\textrm{MC}} \right) \quad  q_t^{\left( \infty \right)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-1} R_T
$$

the n-step Q-return:

$$
q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n})
$$

n-step Sarsa updates Q(s,a) towards the n-step Q-return

$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t)+\alpha (q_t^{(n)}-Q(S_t,A_t))
$$


##### Forward View Sarsa(λ)

The $q^λ$ return combines all n-step Q-returns $q^{(n)}_t$ 
$$
q_t^{\lambda}=(1-\lambda)\sum\limits_{n=1}^{\infty}\lambda^{n-1}q^{(n)}_t
$$

$$
Q(S_t, A_t) \leftarrow Q(S_t, A_t) + α (q_t^{(λ)} − Q(S_t, A_t))
$$

##### Backward View Sarsa(λ)

Just like TD(λ), we use eligibility traces in an online algorithm

$$
E_0(s, a) = 0
\\E_t(s, a) = γλE_{t-1}(s, a) + \textbf{1}(S_t=s, A_t=a)
$$

Q(s,a) is updated for every state s and action a

$$
\delta_t=R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)
\\ Q(s,a)\leftarrow Q(s,a)+\alpha \delta_t E_t(s,a)
$$



### Off-Policy Learning

- Evaluate **target policy** $π(a \vert s)$ to compute $v_π(s)$ or $q_π(s,a)$

- While following **behaviour policy** $μ(a \vert s)$

$$
{S_1,A_1,R_2,...,S_T} ∼ μ
$$
- Why is this important?
  - Learn from observing humans or other agents
  - **Re-use experience** generated from old policies $π_1, π_2, ..., π_{t−1}$  重用经验
  - Learn about <u>optimal</u> policy while following <u>exploratory</u> policy  使用一个探索性策略的同时学习最优
  - Learn about <u>multiple</u> policies while following one policy



#### Importance Sampling

**重要性采样实质上是按照两个概率分布对函数进行了加权**

Estimate the expectation of a different distribution

$$
\mathbb E_{X \sim P}[f(X)] = \sum P(X)f(X)  = \sum Q(X)\frac{P(X)}{Q(X)}f(X)
\\ = \mathbb E_{X \sim Q} \bigg[   \frac{P(X)}{Q(X)}f(X) \bigg]
$$


##### Importance Sampling for Off-Policy Monte-Carlo

- Use returns generated from μ to **evaluate π**

- **Weight** return $G_t$ according to similarity between policies

- Multiply importance sampling **corrections** along whole episode

$$
G_t^{\pi / \mu}=\frac{\pi(A_t \vert S_t)}{\mu(A_{t} \vert S_{t})} \frac{\pi(A_{t+1} \vert S_{t+1})}{\mu(A_{t+1} \vert S_{t+1})} ... \frac{\pi(A_T \vert S_T)}{\mu(A_T \vert S_T)}G_t
$$

- Update value towards scorrected return

$$
V(S_t)\leftarrow V(S_t)+\alpha (\color{red}{G_t^{\pi / \mu}}-V(S_t))
$$

- Cannot use if μ is zero when π is non-zero  , μ要包含π的动作空间 

- Importance sampling can dramatically **increase** **variance**

- **off-policy的MC算法，方差超级大，因为步数越多，相乘的系数越来越多，两个策略偏离的越来越多，不现实；这个时候要引入TD**



#### Importance Sampling for Off-Policy TD

- Use TD targets generated from μ to **evaluate π**  这里只评估V
- Weight TD target $R + γV (S′)$ by importance sampling
- Only need a single importance sampling correction

$$
V(S_t)\leftarrow V\left( S_t \right)+\alpha\left( \underline{\frac{\pi(A_t \vert S_t)}{\mu(A_t \vert S_t)}\left( R_{t+1}+\gamma V\left( S_{t+1} \right) \right)}-V\left( S_t \right) \right)
$$

- **Much lower variance** than Monte-Carlo importance sampling 
- Policies only need to **be similar over a single step**   也会放大误差，但比MC好
- 就算是TD, 两个策略的V依然是有明显不同的, 在每一步的时候，得到的R也不同，需要系数来修正



> 离线策略的方法就是，在状态 $S_t$ 时比较另一个策略 $\pi$ 和当前策略 $\mu$ 产生行为 A_t 的概率大小，如果策略 $\pi$ 得到的概率值与当前策略 $\mu$ 得到的概率值接近，说明根据状态 $S_{t+1}$ 价值来更新 $S_t$ 的价值同时得到两个策略的支持，这一更新操作比较有说服力。同时也说明在状态 $S_t$ 时，两个策略有接近的概率选择行为 $A_t$ 。假如这一概率比值很小，则表明如果依照被评估策略，选择 $A_t$ 的机会很小，这时候我们在更新 $S_t$ 价值的时候就不能过多的考虑基于当前策略得到的状态 $S_{t+1}$ 的价值。同样概率比值大于 1 时的道理也类似。这就相当于借鉴被评估策略的经验来更新我们自己的策略。



#### Q-Learning

1. s下采样用的 **ε-greedy** 选a 
2. 选择 a' 的时候, 用的target的greedy,  用做TD





- Off-policy  TD,  只有tabular的情况才收敛. 
- consider off-policy learning of action-values Q(s,a)   
- **No importance sampling** is required    直接就不需要
- 对于Q-learning 一个问题是，怎么解释为什么off-policy却不需要improtance sampling
- Next action is chosen using **behaviour policy** $A_{t+1} ∼ μ(· \vert S_t)$ 采样用的策略 
- **but** we consider **alternative** successor action $A′ ∼ π(· \vert S_t)$ ,  计算Qvalue的后继a'的选择是遵循着target策略的 ； 
- 下面更新 Qvalue , update $Q(S_t,A_t)$ **towards** value of alternative action

实际采样的动作值 朝着 alternative action的方向 ; when we bootstrap , bootstrap from the value of alternative action, because that's the thing tells us Q value under our target policy. sample from $\pi$ ,  tells what really happen under target policy. 就算$Q(S_t,A_t)$不是现在当前策略的Q ，一点点向着 实际R + Q(想要的策略)的方向, 这样能够保证基本上遵循着target policy,  因为backup，所以**backup的值是来自target的**，就能慢慢让Qtable转向target
$$
Q\left( S_t, A_t \right) \leftarrow Q\left( S_t, A_t \right) + \alpha\left( \color{red}{ \mathop{\underline{R_{t+1} + \gamma Q_{max}\left( S_{t+1}, A^{'} \right)}}_\textrm{TD目标} } − Q\left( S_t, A_t \right) \right)
$$

下面的QLearning，利用了目标策略的Q以及有点偏差的R来更新Q， 怎么保证收敛到 目标策略的Q值呢？？ 先考虑一个简单情况， 就是behavior 以及target 都是有几率选到所有动作的, 那么，真正有价值的其实就是s,a之后的r,s',a' ，r以及s'都是客观的； 如果a‘按照target来的话，Q(s',a')的参与会让Q走向target的！  具体证明不管了，结论先记着



##### Off-Policy Control with Q-Learning

> 下面对QLearning Control 来说， 为什么没有 importance sampling，也能说得通
>
> t 时刻与环境进行实际交互的行为 $A_t$ 由策略 $\mu$ 产生： $A_t\sim\mu\left( \cdot \vert S_t \right)$ ，策略 $\mu$ 是一个 $\epsilon-{\textrm{greedy}}$ 策略；
> t+1 时刻用来更新 Q 值的行为 $$A^{'}_{t+1}$$ 由下式产生： $$A^{'}_{t+1}\sim\pi\left( \cdot \vert S_{t+1} \right)$$ ，其中策略 $\pi$ 是一个完全贪婪策略。 maxQ 
>
> 因为target 策略是个greedy的， 所以在任何方式下都是选最优
>
> 重要性采样的因子 $\frac{\pi}{\mu}$ 就分两种情况。对于某个状态 s ， $\pi$ 策略是greedy的， 所以选择动作 a 的概率要么是 1(如果值相同的多个，则平均一下) ，要么是 0 。当为 1 时， $\mu$ 选择该动作的概率也较大（因为 $\epsilon$ 一般比较小），所以 $\frac{\pi}{\mu}\approx1$， 因为greedy ，所以总是选择值最大的那些，所以ratio总是近似于1
>
> 因此，虽然 Q-learnings是一种离策略算法，它却不需要重要性采样因子了，或者说，不需要引入重要性采样。

 

- 理论上来说，behavior策略可以是纯随机策略都行；提升是靠 target策略的greedy的max操作来实现的
- now allow both behaviour and target policies to **improve**
- **target** policy π is greedy w.r.t. Q(s,a)     ; 要学的是greedy的
  -  $$\pi\left( S_{t+1} \right)=\mathop{\textrm{argmax}}_{a^{'}}Q\left( S_{t+1},a^{'} \right)$$
- **behaviour** policy μ  is e.g. **ε-greedy** w.r.t. Q(s,a)    ;  交互的策略是有探索的
- The Q-learning target then simplifies:    

$$
\begin{align*} R_{t+1} + \gamma Q\left( S_{t+1}, A^{'} \right)&= R_{t+1} + \gamma Q\left( S_{t+1}, \mathop{\textrm{argmax}}_{a^{'}}Q\left( S_{t+1},a^{'} \right) \right)\\&= R_{t+1} + \max_{a^{'}}\gamma Q\left( S_{t+1}, a^{'} \right) \end{align*}
$$

##### Q-Learning Control Algorithm

<img src="/img/2019-03-02-Silver.assets/image-20190219154340323.png" style="zoom: 80%;" />
$$
Q\left( S,A \right) = Q\left( S,A \right) + \alpha\left( R+\gamma \max_aQ\left( S^{'},a \right) - Q\left( S,A \right) \right)
$$

Q-learning control **converges** to the optimal action-value function, $Q(s, a) → q_∗(s, a)$

![image-20190219154820731](/img/2019-03-02-Silver.assets/image-20190219154820731.png)

- Qlearning中，在s下取a是 Q表的$\epsilon -greedy$ ， 更新的是 s'的 maxQ的a ,  所以是offpolicy，但这两个策略基本上区别不大
- 如果Qlearning像sarsa一样是improve一个$\epsilon-greedy$ 策略，可以发现sarsa只是每次采样一次，然后evaluate一次，improve靠策略本身，进入的s’，a'由策略本身控制，并且用 Q(s',a')来backup； 
- Qlearning先按策略来选择a，进入的s‘ 是由策略影响的； update的时候不需要知道遵循该策略的a’，选择的就是max的a’ ；然后在s‘重新选择a‘


### Relationship Between DP and TD


|                       Full Backup (DP)                       | Sample Backup (TD)                                           |
| :----------------------------------------------------------: | ------------------------------------------------------------ |
| Iterative Policy Evaluation<br />$V(s) \leftarrow \mathbb E[R+\gamma V(S') \mid s]$ | TD Learning<br/>$V(S)\overset{\alpha}{\gets} R+γV(S′)$       |
| Q-Policy Iteration<br />$Q(s,a) \leftarrow \mathbb E[R+\gamma Q(S',A') \mid s,a]$ | Sarsa<br/>$Q(S,A)\overset{\alpha}{\gets} R+γQ(S',A')$        |
| Q-Value Iteration<br />$Q(s,a) \leftarrow \mathbb E[R+\gamma \max_{a' \in \mathcal A} Q(S',A'] \mid s,a]$ | **Q-Learning**<br/>$Q(S,A)\overset{\alpha}{\gets} R+γ \max_{a' \in \mathcal A}Q(S′,A')$ |

where $x \overset{\alpha}{\gets}  y≡x←x+α(y−x)$










