---
layout:     post
title:      TRPO PPO
subtitle:   
date:       2020-02-22 12:00:00
author:     "tengshiquan"
header-img: "img/about-bg.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - PPO
    - TRPO
---



# TRPO & PPO

#### Issue of Importance Sampling

PG 引入 IS 后, 从on-policy 变成 off-policy.  期望是一样的. 但方差不一样. 
$$
VAR[X] = E\left[X^{2}\right]-(E[X])^{2}
$$

$$
E_{x \sim p}[f(x)]=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]
$$

$$
\operatorname{Var}_{x \sim p}[f(x)] \quad \operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]
$$

$$
\operatorname{Var}_{x \sim p}[f(x)]=E_{x \sim p}\left[f(x)^{2}\right]-\left(E_{x \sim p}[f(x)]\right)^{2}  \tag{1}
$$


$$
\begin{aligned}
\operatorname{Var}_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]  &=E_{x \sim q}\left[\left(f(x) \frac{p(x)}{q(x)}\right)^{2}\right]-\left(E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]\right)^{2}\\
&=E_{x \sim p}\left[f(x)^{2} \frac{p(x)}{q(x)} \right] -\left(E_{x \sim p}[f(x)]\right)^{2} 
\end{aligned}  \tag{2}
$$


显然, 公式2与公式1, 差别就在于减号前面的第一项 .   公式2多了一个 p/q . 如果p,q的分布相差很大, 则该比值变化会很大 , 造成方差变大. 



<img src="/img/2020-02-22-TRPO-PPO.assets/image-20200330020120268.png" alt="image-20200330020120268" style="zoom:50%;" />

- 如图, 总体上, 按照p来采样, $E_{x \sim p}[f(x)]$是负数.   
- 如果按照q来采样, 因为q在右边几率高, 只采样到 右边的几个绿点, 则按照IS公式算 $E_{x \sim p}[f(x)]$ 可能就是正数了. 
- 如果碰巧采样到左边的那个绿色点, 因为p/q这个时候值超级大, 有可能把$E_{x \sim p}[f(x)]$ 给纠正回来. 
- 所以必须sample足够多次..





#### On-policy → Off-policy

下面用 $\theta'$ 采样, 做demostration

Gradient for update

$$
\nabla f(x)=f(x) \nabla \log f(x)
$$

$$
\begin{aligned}
\text{Gradient for update} 
&=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta}}\left[A^{\theta}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
&=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{P_{\theta}\left(s_{t}, a_{t}\right)}{P_{\theta^{\prime}}\left(s_{t}, a_{t}\right)}  A^{\theta \color{red}{^{\prime}} }\left(s_{t}, a_{t}\right)   \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right] \\
&=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} \frac{p_{\theta}\left(s_{t}\right)}{p_{\theta^{\prime}}\left(s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right) \nabla \log p_{\theta}\left(a_{t}^{n} | s_{t}^{n}\right)\right]
\end{aligned}
$$

第二行的 从$A^{\theta}$ 改为用 $A^{\theta'}$ 近似, 假设两个差不多.   第三行假设 $p_\theta(s_t)$ 与 $p_{\theta'}(s_t)$ 分布差不多, 分子分母消掉. 

下面由上面的近似gradient方式, 去反推一个近似的关于$\theta$ 目标函数. 

$$
J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]
$$


#### Add Constraint

上面的两个近似的前提,就是 $\theta$ 与 $\theta'$ 不能差太多.  所以加一个限制项, 类似正则项, 限制learn出来的$\theta$  与 $\theta'$不能过于不一样. (其实是指policy不能差异过大)

𝜃 cannot be very different from 𝜃′ ; Constraint on behavior not parameters.   
另外, 注意,公式中 这里KL divergence 不是指参数 $\theta$ 与 $\theta'$ distribution的距离, 而是两个model产出的action, 即policy的概率距离. 





##### **Proximal Policy Optimization (PPO)**

$$
\begin{array}{c}
J_{P P O}^{\theta^{\prime}}(\theta)=J^{\theta^{\prime}}(\theta)-\beta K L\left(\theta, \theta^{\prime}\right) \\
J^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right]
\end{array}
$$

PPO把限制项作为了一个正则项放入目标J函数中. 

##### **TRPO (Trust Region Policy Optimization)**

$$
\begin{aligned}
J_{T R P O}^{\theta^{\prime}}(\theta)=E_{\left(s_{t}, a_{t}\right) \sim \pi_{\theta^{\prime}}}\left[\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{\prime}}\left(a_{t} | s_{t}\right)} A^{\theta^{\prime}}\left(s_{t}, a_{t}\right)\right] & \\
& K L\left(\theta, \theta^{\prime}\right)<\delta
\end{aligned}
$$

TRPO是在做一个 带限制条件的 最优化问题 , 很难算.  PPO与TRPO performance差不多.



#### PPO algorithm

- Initial policy parameters $\theta^{0}$
- In each iteration
  - Using $\theta^{k}$ to interact with the environment to collect $$\left\{s_{t}, a_{t}\right\}$$ and compute advantage $A^{\theta^{k}}\left(s_{t}, a_{t}\right)$ 
  - Find $\theta$ optimizing $J_{P P O}(\theta)$
    $J_{P P O}^{\theta^{k}}(\theta)=J^{\theta^{k}}(\theta)-\beta K L\left(\theta, \theta^{k}\right) \begin{array}{l}\text { Update parameters } \\ \text { several times }\end{array}$

因为这里是off-policy , 所以可以 update 参数多次而不用立刻去重新采样. 



##### Adaptive KL Penalty

这个 KL 惩罚项 的超参可以动态调整. 

-  If $K L\left(\theta, \theta^{k}\right)>K L_{\max },$ increase $\beta$
-  If $K L\left(\theta, \theta^{k}\right)<K L_{\text {min }},$ decrease $\beta$ 
  



##### **PPO2 algorithm** 

因为算KL还是麻烦.  
$$
\begin{aligned}
J_\text{PPO2}^{\theta^k}(\theta) \approx \sum_{\left(s_{t}, \boldsymbol{a}_{t}\right)} \min \Bigg(\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{k}}\left(a_{t} | s_{t}\right)} A^{\theta^{k}}\left(s_{t}, a_{t}\right) ,   
& \operatorname{clip}\left(\frac{p_{\theta}\left(a_{t} | s_{t}\right)}{p_{\theta^{k}}\left(a_{t} | s_{t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A^{\theta^{k}\left(s_{t}, a_{t}\right)}\Bigg)
\end{aligned}
$$
<img src="/img/2020-02-22-TRPO-PPO.assets/image-20200330042218479.png" alt="image-20200330042218479" style="zoom:50%;" /> 横轴就是两个p的比值. 

<img src="/img/2020-02-22-TRPO-PPO.assets/image-20200330042255765.png" alt="image-20200330042255765" style="zoom:50%;" />

所以看图像, 最后的效果, 可以防止$p_\theta$  与 $p_{\theta'}$ 差距过大.  看A>0的情况, 会增加 $\frac{p_\theta}{p_{\theta'}}$ , 即该action出现的几率, 但不希望两个差距过大, 不可以超过$1+\varepsilon$, 所以 clip .  



##### Experimental Results

https://arxiv.org/abs/1707.06347

![image-20200330043234698](/img/2020-02-22-TRPO-PPO.assets/image-20200330043234698.png)





由于重要性采样的关系我们希望每次更新的时候策略分布之间差距并不是很大，这实际上是一种约束，即我们希望能每次更新的时候不大幅度地改变分布的形态，基于这种考虑openai的前辈们提出了TRPO算法，但是TRPO算法会有一些缺陷，他拿二次函数去近似约束条件，拿一次函数近似待优化的损失函数，这种近似会造成收敛上的困难，于是便有了第二次smart的改进，得到PPO系列的算法

策略梯度的硬伤就在于更新步长$\alpha$,当步长选的不合适的时候更新的参数会更差，因此很容易导致越学越差，最后崩溃，那什么样的步长叫做合适的步长呢，试想我们如果能找到一种步长，使他每次更新时都能保证回报函数**单调递增**，这样的步长就是好步长。TRPO的核心就是解决这个问题。









纠结的点, 是学习率  ;    还有问题能不能被函数表达;  如果可以的话, 感觉trpo还是有点合理性的

拟合函数本身的准确性,  采样,  等等各种问题



要迅速组成常见模式.. 



新策略 , 对可能出现的状态, 以及可能出现的风险, 要能预判, 并加入到 考虑中.. 

选择action, 由粗到细 ...



视野狭窄问题, 只能采样 ;  对图像, 至少能  CNN












