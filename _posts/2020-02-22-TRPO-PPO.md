---
layout:     post
title:      TRPO PPO
subtitle:   
date:       2020-02-22 12:00:00
author:     "tengshiquan"
header-img: "img/about-bg.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - PPO
    - TRPO
---



# TRPO & PPO

待整理



由于重要性采样的关系我们希望每次更新的时候策略分布之间差距并不是很大，这实际上是一种约束，即我们希望能每次更新的时候不大幅度地改变分布的形态，基于这种考虑openai的前辈们提出了TRPO算法，但是TRPO算法会有一些缺陷，他拿二次函数去近似约束条件，拿一次函数近似待优化的损失函数，这种近似会造成收敛上的困难，于是便有了第二次smart的改进，得到PPO系列的算法



策略梯度的硬伤就在于更新步长$\alpha$,当步长选的不合适的时候更新的参数会更差，因此很容易导致越学越差，最后崩溃，那什么样的步长叫做合适的步长呢，试想我们如果能找到一种步长，使他每次更新时都能保证回报函数**单调递增**，这样的步长就是好步长。TRPO的核心就是解决这个问题。





重要性采样的 合理性



纠结的点, 是学习率  ;    还有问题能不能被函数表达;  如果可以的话, 感觉trpo还是有点合理性的

拟合函数本身的准确性,  采样,  等等各种问题



要迅速组成常见模式.. 



新策略 , 对可能出现的状态, 以及可能出现的风险, 要能预判, 并加入到 考虑中.. 

选择action, 由粗到细 ...



视野狭窄问题, 只能采样 ;  对图像, 至少能  CNN











