---
layout:     post
title:      Trust Region Policy Optimization
subtitle:   Note on "Trust Region Policy Optimization"
date:       2020-01-02 12:00:00
author:     "tengshiquan"
header-img: "img/about-bg.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - TRPO
---



# Note on "Trust Region Policy Optimization"

论文笔记 2015

 

#### Abstract

- 描述了一个策略优化的迭代过程 iterative procedure for optimizing policies，可以保证单调的改进 guaranteed monotonic improvement。 
- 通过对理论上的过程进行若干近似 making several approximations to the theoretically-justified procedure, 提出了实际可行的TRPO算法。 
- 该算法类似于Policy Gradient，对于优化大型非线性策略（如神经网络）非常有效。 
- 在各种任务上都有稳定的表现 robust performance .  
- 尽管近似偏离了理论, TRPO倾向于单调改进, 对超参的调整很少. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with **little tuning** of hyperparameters. 



#### 1 Introduction

Policy optimization 可以分为三类: 

1. **policy iteration**, 交替 estimating value 和 improve. 动态规划,以及近似approximate动态规划 ADP
2. **policy gradient**, 直接沿着目标函数的梯度来优化策略函数本身
3. **derivative-free optimization**, 无梯度优化 ,  如**cross-entropy method (CEM)** , **covariance matrix adaptation (CMA)**. 将返回作为一个黑盒函数，根据策略参数进行优化.   易于理解与实现.  对连续动作问题, CMA表现不错.  (Szita & Lo ̈rincz, 2006)   

- 无梯度优化在很多问题上表现不错, 简单易实现.
- **ADP**和**Gradient-based** 方法 始终没法击败 **derivative-free random search**方法, 这令人不满. 因为基于梯度的优化算法比无梯度的方法具有更好的样本复杂度保证(Nemirovski, 2005).  
- 连续的基于梯度的优化在监督学习中很成功, 现引入到强化学习.

- 文章中, 首先证明，最小化某个替代目标函数surrogate objective function 保证能以不小的步长来改进策略
- 然后, 对理论算法进行了一系列近似，得出了一种实用的算法-TRPO. 
- 该算法有基于采样的方式, 两个变体: 
  1. **single-path** method , 可以 model-free 
  2. **vine** method, 需要将系统还原到特定状态，通常只能在仿真中使用。



#### 2 Preliminaries 

- $\rho_0$ : the distribution of the initial state $s_0$

- $\eta(\pi)$  : **expected discounted reward** of a stochastic policy $\pi$:  就是V

$$
{\eta(\pi)=\mathbb{E}_{s_{0}, a_{0}, \ldots}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}\right)\right] } \\ \text { where }
{s_{0} \sim \rho_{0}\left(s_{0}\right), a_{t} \sim \pi\left(a_{t} | s_{t}\right), s_{t+1} \sim P\left(s_{t+1} | s_{t}, a_{t}\right)}
$$

- $${Q_{\pi}\left(s_{t}, a_{t}\right)=\mathbb{E}_{s_{t+1}, a_{t+1}}, \ldots\left[\sum_{l=0}^{\infty} \gamma^{l} r\left(s_{t+l}\right)\right]}$$
- $${V_{\pi}\left(s_{t}\right)=\mathbb{E}_{a_{t}, s_{t+1}, \ldots}\left[\sum_{l=0}^{\infty} \gamma^{l} r\left(s_{t+l}\right)\right]} $$
- ${A_{\pi}(s, a)=Q_{\pi}(s, a)-V_{\pi}(s) } $
- Expected return of another policy $\tilde{\pi}$ in terms of the **advantage** over $\pi$

$$
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{s_{0}, a_{0}, \cdots \sim \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi}\left(s_{t}, a_{t}\right)\right] \tag{1}
$$

- let $\rho_{\pi}$ be the (unnormalized) **discounted visitation frequencies**  折扣的访问频率
  $\rho_{\pi}(s)=P\left(s_{0}=s\right)+\gamma P\left(s_{1}=s\right)+\gamma^{2} P\left(s_{2}=s\right)+\ldots$ , where $s_{0} \sim \rho_{0}$ 
- 按 $\pi$ 选取actions. 改写公式1, 从**timesteps**序列求和改为**state**分布的求和 :

$$
\begin{align}
\eta(\tilde{\pi}) &=\eta(\pi)+\sum_{t=0}^{\infty} \sum_{s} P\left(s_{t}=s | \tilde{\pi}\right) \sum_{a} \tilde{\pi}(a | s) \gamma^{t} A_{\pi}(s, a) 
\\
&=\eta(\pi)+\sum_{s} \sum_{t=0}^{\infty} \gamma^{t} P\left(s_{t}=s | \tilde{\pi}\right) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a)
\\&=\eta(\pi)+\sum_{s} \rho_{\tilde{\pi}}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a)
\end{align} \tag{2}
$$
- 该公式说明, 更新策略 $\pi \rightarrow \tilde{\pi}$ , 如果每个状态 $s$ 的优势函数都是非负数的话, 显然地就可以保证改进策略表现$\eta$, 至少不会变差.
- 该公式验证了经典结论,  对于基于精确值的策略迭代**exact policy iteration**, 使用deterministic policy $\tilde \pi (s)=\arg \max_a A_\pi(s, a)$ 可以改进策略, 如果找不到某个大于0的advantage value, 则说明策略收敛到最优.

- 然而,  in the approximate setting, 由于**estimation** and **approximation** error 估计以及近似带来的误差, 可能有些 states $s$, expected advantage is negative,  $\sum_{a} \tilde{\pi}(a \vert s) A_{\pi}(s, a)<0 .$ 
- <u>由于状态分布依赖于新策略$\tilde{\pi}$ , 必须按照新策略走的路径, 造成公式2很难直接优化.</u>

下面,引入 $\eta$ 的局部近似**local approximation**:    论文Aoarl 中 ,  $$  \mathbb{A}_{\pi}(\tilde{\pi}) = \sum_{s} \rho_{\pi}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a)  $$ 
$$
L_{\pi}(\tilde{\pi})=\eta(\pi)+\sum_{s} \rho_{\pi}(s) \sum_{a} \tilde{\pi}(a | s) A_{\pi}(s, a) \tag{3}
$$
- 关键近似: $L_{\pi}$ uses the visitation frequency $\rho_{\pi}$ rather than $\rho_{\tilde{\pi}}$,  使用了旧策略的状态分布$\rho_{\pi}$代替$\rho_{\tilde{\pi}}$, 忽略了策略改变造成的状态分布的变化. 
- 如果$\pi_{\theta}$ 函数对参数 $\theta$ 可导, 则 $L_{\pi}$ matches $\eta$ to first order 一阶导数相等, 即下式, $\pi_{\theta_0}$相当于$\pi_{old}$

$$
\begin{aligned}
L_{\pi_{\theta_{0}}}\left(\pi_{\theta_{0}}\right) &=\eta\left(\pi_{\theta_{0}}\right) \\
\left.\nabla_{\theta} L_{\pi_{\theta_{0}}}\left(\pi_{\theta}\right)\right|_{\theta=\theta_{0}} &=\left.\nabla_{\theta} \eta\left(\pi_{\theta}\right)\right|_{\theta=\theta_{0}}
\end{aligned} \tag{4}
$$
- 公式(4) 说明步长足够小的时候,  $\pi_{\theta_{0}} \rightarrow \tilde{\pi}$ 提升 $L_{\pi_{\theta_{\text {old }}}}$ 会同时提升新策略的回报 $\eta$ , 但这个步长多少没明确.  

- "Approximately optimal approximate reinforcement learning" 提出 **conservative policy iteration**, 提出了明确的策略改进的下界  lower bounds on the improvement of $\eta$.  令 $\pi'=\arg \max_{\pi'} L_{\pi_{old}} (\pi')$.  new policy :

$$
\pi_{\text {new }}(a | s)=(1-\alpha) \pi_{\text {old }}(a | s)+\alpha \pi^{\prime}(a | s)  \tag{5}
$$
- derived lower bound 导出下界:  下界较aoarl有放松,为了表达式简单

$$
\begin{aligned}
\eta\left(\pi_{\text {new }}\right) & \geq L_{\pi_{\text {old }}}\left(\pi_{\text {new }}\right)-\frac{2 \epsilon \gamma}{(1-\gamma)^{2}} \alpha^{2} \\
& \text { where } \epsilon=\max _{s}\left|\mathbb{E}_{a \sim \pi^{\prime}(a | s)}\left[A_{\pi}(s, a)\right]\right|
\end{aligned} \tag{6}
$$

- 公式5的混合策略**mixture policies** 不太实用, 需要推广到一般随机策略类型.



#### 3 Monotonic Improvement Guarantee for General Stochastic Policies 策略单调改进的保证

- 公式6说明, 只要提升右边, 可以保证提升$\eta$. 
- 本文的主要理论结果是使用π和π̃之间的距离替换α,  并适当调整常数ε , 将方程(6)中的下界从混合策略推广到一般随机策略. 
- 这里使用的距离是 **全变差距离** **total variation divergence**:  $D_{T V}(p \Vert q)=\frac{1}{2} \sum_{i}\vert p_{i}-q_{i} \vert$ .  定义:

$$
D_{\mathrm{TV}}^{\max }(\pi, \tilde{\pi})=\max _{s} D_{T V}(\pi(\cdot | s) \| \tilde{\pi}(\cdot | s)) \tag{7}
$$
**Theorem 1**. Let $\alpha=D_{\mathrm{TV}}^{\max }\left(\pi_{\text {old }}, \pi_{\text {new }}\right) .$ Then the following bound holds 下界成立:
$$
\eta\left(\pi_{\text {new }}\right) \geq L_{\pi_{\text {old }}}\left(\pi_{\text {new }}\right)-\frac{4 \epsilon \gamma}{(1-\gamma)^{2}} \alpha^{2}, \quad
\text { where } \epsilon=\max _{s, a}\left|A_{\pi}(s, a)\right| \tag{8}
$$

- 由于 total variation divergence 和 KL divergence 有如下关系: $D_{T V}(p \Vert q)^{2} \leq D_{\mathrm{KL}}(p \Vert q) $.  
- 令 $D_{\mathrm{KL}}^{\max }(\pi, \tilde{\pi}) = \max_s D_{\mathrm{KL}}(\pi(\cdot \vert s) \Vert \tilde \pi(\cdot \vert s))$ 则由定理1可得:

$$
\begin{aligned}
\eta(\tilde{\pi}) & \geq L_{\pi}(\tilde{\pi})-C D_{\mathrm{KL}}^{\max }(\pi, \tilde{\pi}) \\
& \text { where } C=\frac{4 \epsilon \gamma}{(1-\gamma)^{2}}
\end{aligned}  \tag{9}
$$

- 算法1描述了基于等式（9）下界的 近似策略改进迭代方案。
- 注意，现在假设Advantage值是精确的.

![image-20200125015548032](/img/2020-01-02-TRPO.assets/image-20200125015548032.png)

- 算法1保证单调改进:  $\eta\left(\pi_{0}\right) \leq \eta\left(\pi_{1}\right) \leq \eta\left(\pi_{2}\right) \leq \ldots$  
- 简单证明: let $M_{i}(\pi)=L_{\pi_{i}}(\pi)-C D_{\mathrm{KL}}^{\max }\left(\pi_{i}, \pi\right)$ , then

$$
\begin{array}{l}
{\eta\left(\pi_{i+1}\right) \geq M_{i}\left(\pi_{i+1}\right) \text { by Equation }( 9 )} \\
{\eta\left(\pi_{i}\right)=M_{i}\left(\pi_{i}\right), \text { therefore }} \\
{\eta\left(\pi_{i+1}\right)-\eta\left(\pi_{i}\right) \geq M_{i}\left(\pi_{i+1}\right)-M\left(\pi_{i}\right)}
\end{array}  \tag{10}
$$
- 通过在每个迭代中 最大化$M_i$, 可以保证 $\eta$ ,不会变差.  该算法是 **minorization-maximization (MM)** 算法中的一种. 

- TRPO 是算法1的一个近似,  which uses a constraint on the KL divergence rather than a penalty to robustly allow large updates, 使用对KL散度的约束而不是惩罚项来稳健地允许较大地更新.



#### 4 Optimization of Parameterized Policies 参数化策略的优化

- 之前都只考虑理论, 下面推导实际可行的算法, 在有限的采样以及参数化策略的情况.
- 对策略$\pi_{\theta}(a \vert s)$,  下面所有公式 , 使用 $\theta$ 代替 $\pi$.    $\theta_{\text {old }}$ :  previous policy parameters to improve.
- 上面已经证明  $\eta(\theta) \geq L_{\theta_{\text {old }}}(\theta)- C D_{\mathrm{KL}}^{\max }\left(\theta_{\mathrm{old}}, \theta\right)$
- 最大化右边部分,可以保证提升 $\eta$ :  $\underset{\theta}{\operatorname{maximize}}\left[L_{\theta_{\mathrm{old}}}(\theta)-C D_{\mathrm{KL}}^{\max }\left(\theta_{\mathrm{old}}, \theta\right)\right]$
- 实践中, 惩罚系数 penalty coefficient $C$ 会造成 step size很小. 
- 一个稳健的步长较大的方法: 约束新旧政策之间的DL散度的, 例如 **trust region** constraint:

$$
\begin{array}{l}
{\text { maximize } L_{\theta_{\text {old }}}(\theta)} \\
{\text { subject to } D_{\mathrm{KL}}^{\max }\left(\theta_{\text {old }}, \theta\right) \leq \delta}
\end{array}  \tag{11}
$$

- 这个问题强加了一个约束，即KL散度在状态空间的每个点上都是有界的。约束的点太多 
- 可以使用考虑启发式近似, 平均KL散度：

$$
\overline{D}_{\mathrm{KL}}^{\rho}\left(\theta_{1}, \theta_{2}\right):=\mathbb{E}_{s \sim \rho}\left[D_{\mathrm{KL}}\left(\pi_{\theta_{1}}(\cdot | s) \| \pi_{\theta_{2}}(\cdot | s)\right)\right]
$$

- 提出算法:

$$
\underset{\theta}{\operatorname{maximize}} L_{\theta_{\mathrm{old}}}(\theta)
\\\text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\text {old }}}\left(\theta_{\text {old }}, \theta\right) \leq \delta  \tag{12}
$$

- 本文实验表明, 上式的经验性能与公式11中的算法差不多



#### 5 Sample-Based Estimation of the Objective and Constraint

- 上一节提出了关于策略参数的约束优化问题（等式12）。 本节描述了如何使用蒙特卡洛模拟来逼近目标函数和约束函数。
- 展开公式12的$L_{\theta_{\text {old }}}$ :

$$
\begin{aligned}
\underset{\theta}{\operatorname{maximize}} \sum_{s} \rho_{\theta_{\text {old }}}(s) \sum_{a} \pi_{\theta}(a | s) A_{\theta_{\text {old }}}(s, a) \\
\text { subject to } \bar{D}_{\mathrm{KL}}^{\rho_{\theta_{\text {old }}}}\left(\theta_{\text {old }}, \theta\right) \leq \delta
\end{aligned}  \tag{13}
$$
- 对公式13, 
  - replace $\sum_{s} \rho_{\theta_{\text {old }}}(s)[\ldots]$   by expectation $\frac{1}{1-\gamma} \mathbb E_{s \sim \rho_{0} \text { old }}[\dots]$ . 	 MC

  - replace  $A_{\theta_{\text {old }}}$ by  $Q_{\theta_{\text {old }}}$ 

  - replace the <u>sum over the actions</u> by an **importance sampling** estimator. Using $q$ to denote the **sampling distribution**, the contribution of a single $s_{n}$ to the loss function is  每个$s_n$对loss的贡献
    $$
    \sum_{a} \pi_{\theta}\left(a | s_{n}\right) A_{\theta_{\mathrm{old}}}\left(s_{n}, a\right)=\mathbb{E}_{a \sim q}\left[\frac{\pi_{\theta}\left(a | s_{n}\right)}{q\left(a | s_{n}\right)} A_{\theta_{\mathrm{old}}}\left(s_{n}, a\right)\right]
    $$

  - 转为以下期望形式的公式:

$$
\begin{array}{l}
{\underset{\theta}{\operatorname{maximize}} \mathbb{E}_{s \sim \rho_{\theta_{\text {old }}}, a \sim q}\left[\frac{\pi_{\theta}(a | s)}{q(a | s)} Q_{\theta_{\text {old }}}(s, a)\right]} \\
{\text { subject to } \mathbb{E}_{s \sim \rho_{\theta \text { old }}}\left[D_{\mathrm{KL}}\left(\pi_{\theta_{\text {old }}}(\cdot | s) \| \pi_{\theta}(\cdot | s)\right)\right] \leq \delta}
\end{array}  \tag{14}
$$
- 剩下的就是用样本均值代替期望值，用经验估计代替Q值

- 有两种sample方案: 
  1. single path  
  2. vine

![image-20200125163009175](/img/2020-01-02-TRPO.assets/image-20200125163009175.png)

##### 5.1 Single Path

1. 对 $s_{0} \sim \rho_ {0}$进行采样,  起始
2. 然后模拟执行策略$\pi_{\theta_{i}}$ 若干timesteps，来生成轨迹$s_{0}, a_{0}, s_{1}, a_{1}, \dots, s_{T-1}, a_{T-1}, s_{T}$.  
3. 因此 $q(a \vert s)= \pi_{\theta_{\text {old }}(a \vert s)}$.     $Q_{\theta_{\text {old }}}(s, a)$ 对每个(s,a)计算沿轨迹的未来奖励的折扣总和。

##### 5.2 Vine 藤茎

1. 起始 $s_{0} \sim \rho_{0}$ , 执行 $\pi_{\theta_{i}}$ 生成很多 trajectories. 
2. 从这些轨迹中选择 $N$ states 作为一个子集, denoted $s_{1}, s_{2}, \ldots, s_{N},$  "**rollout set**".
3. 对rollout set中的每个 $s_{n}$ , 按照 $a_{n, k} \sim q\left(\cdot \vert s_{n}\right)$  再 sample $K$ actions
4. Any choice of $q\left(\cdot \vert s_{n}\right)$ with a support that includes the support of $\pi_{\theta_{i}}\left(\cdot \vert s_{n}\right)$ will produce a consistent estimator. 按照 $ \pi_{\theta_{i}}\left(\cdot \vert s_{n}\right)$ 来sample $q\left(\cdot \vert s_{n}\right)$ 会产生一致估计
5. 实践中,  $q\left(\cdot \vert s_{n}\right)=\pi_{\theta_{i}}\left(\cdot \vert s_{n}\right)$ works well on 连续问题 continuous problems, such as robotic locomotion ;  均匀分布 uniform distribution works well on 离散问题 discrete tasks, such as the Atari games, where it can sometimes achieve better exploration 因为更好的探索性,这时应该是有偏估计.
6. 对每个 $\hat Q_{\theta_{i}} (s_{n}, a_{n, k} )$ 执行 rollout (i.e., a short trajectory) . 
7. 采用相同随机数的技术可以极大的减少 rollout出来 的Q值的方差 We can greatly reduce the variance of the $Q$ -value differences between rollouts by using the same random number sequence for the noise in each of the $K$ rollouts, i.e., **common random numbers**.  (Ng & Jordan, 2000)

- 对有限小规模动作空间(总共K个action)问题, 可以rollout 一个state的所有action.  单独一个状态 $s_{n}$ 对  $L_{\theta_{\text {old }}}$ 的贡献为:

$$
L_{n}(\theta)=\sum_{k=1}^{K} \pi_{\theta}\left(a_{k} | s_{n}\right) \hat{Q}\left(s_{n}, a_{k}\right)  \tag{15}
$$
- 对大规模或连续状态问题, In large or continuous state spaces, we can construct an estimator of the surrogate objective using **importance sampling**. The 自归一化估计**self-normalized estimator**  of $L_{\theta_{\text {old }}}$ obtained at a single state $s_{n}$ is

$$
L_{n}(\theta)=\frac{\sum_{k=1}^{K} \frac{\pi_{\theta}\left(a_{n, k} | s_{n}\right)}{\pi_{\theta_{\text {old }}\left(a_{n, k} | s_{n}\right)}} \hat{Q}\left(s_{n}, a_{n, k}\right)}{\sum_{k=1}^{K} \frac{\pi_{\theta}\left(a_{n, k} | s_{n}\right)}{\pi_{\theta_{\text {old }}\left(a_{n, k} | s_{n}\right)}}}  \tag{16}
$$
 	??不再需要baseline This self-normalized estimator removes the need to use a baseline for the $Q$ -values (note that the gradient is unchanged by adding a constant to the $Q$ -values). Averaging over $s_{n} \sim \rho(\pi),$ we obtain an estimator for $L_{\theta_{\mathrm{old}}},$ as well as its gradient.

vine 比 single path 具有更小的方差. 但需要系统可以被设置为任意状态. 



#### 6 Practical Algorithm

提出两种基于上面理论的实践中的策略优化算法, 都有以下步骤

1. 使用single path 或 vine 采样得到(s,a) 集合,然后蒙特卡洛求Q
2. By averaging over samples, construct the estimated objective and constraint in Equation 14.   对sample平均, 计算公式14的目标以及约束
3. 近似求解, 更新参数.  使用共轭梯度,再线性搜索, 比直接计算梯度稍微更expensive一点.  Approximately solve this constrained optimization problem to update the policy’s parameter vector θ. We use the conjugate gradient algorithm followed by a line search, which is altogether only slightly more expensive than computing the gradient itself. 

对3,  we construct the **Fisher information matrix (FIM)** by analytically computing the Hessian of the KL divergence, rather than using the covariance matrix of the gradients. That is, we estimate $A_{i j}$ as $\frac{1}{N} \sum_{n=1}^{N} \frac{\partial^{2}}{\partial \theta_{i} \partial \theta_{j}} D_{\mathrm{KL}}\left(\pi_{\theta_{\mathrm{old}}}\left(\cdot \vert s_{n}\right) \Vert \pi_{\theta}\left(\cdot \vert s_{n}\right)\right)$,  rather than
$\frac{1}{N} \sum_{n=1}^{N} \frac{\partial}{\partial \theta_{i}} \log \pi_{\theta}\left(a_{n} \vert s_{n}\right) \frac{\partial}{\partial \theta_{j}} \log \pi_{\theta}\left(a_{n} \vert s_{n}\right) . \quad$ The analytic estimator integrates over the action at each state $s_{n}$ and does not depend on the action $a_{n}$ that was sampled. As described in Appendix $\mathrm{C},$ this analytic estimator has computational benefits in the large-scale setting, since it removes the need to store a dense Hessian or all policy gradients from a batch of trajectories. The rate of improvement in the policy is similar to the empirical FIM, as shown in the experiments.

下面总结下, 上面描述的理论算法与实际的算法的关系:

1. The theory justifies optimizing a surrogate objective with a penalty on KL divergence. However, the large penalty coefficient C leads to prohibitively small steps, so we would like to decrease this coefficient. Empirically, it is hard to robustly choose the penalty coefficient, so we use a hard constraint instead of a penalty, with parameter δ (the bound on KL divergence).
2. The constraint on $D_{\mathrm{KL}}^{\max }\left(\theta_{\mathrm{old}}, \theta\right)$ is hard for numerical optimization and estimation, so instead we constrain $\bar D_{\mathrm{KL}} (\theta_{\mathrm{old}}, \theta )$
3. Our theory ignores estimation error for the advantage function



#### 7 Connections with Prior Work

The natural policy gradient (Kakade, 2002 ) can be obtained as a special case of the update in Equation (12) by using a linear approximation to $L$ and a quadratic approximation to the $\bar D_{\mathrm{KL}}$ constraint, resulting in the following problem:


$$
\underset{\theta}{\operatorname{maximize}}\left[\left.\nabla_{\theta} L_{\theta_{\text {old }}}(\theta)\right|_{\theta=\theta_{\text {old }}} \cdot\left(\theta-\theta_{\text {old }}\right)\right] \\
\text { subject to } \frac{1}{2}\left(\theta_{\mathrm{old}}-\theta\right)^{T} A\left(\theta_{\mathrm{old}}\right)\left(\theta_{\mathrm{old}}-\theta\right) \leq \delta
\\
\text {where } A\left(\theta_{\text {old }}\right)_{i j}= 

\left.\frac{\partial}{\partial \theta_{i}} \frac{\partial}{\partial \theta_{j}} \mathbb{E}_{s \sim \rho_{\pi}}\left[D_{\mathrm{KL}}\left(\pi\left(\cdot | s, \theta_{\mathrm{old}}\right) \| \pi(\cdot | s, \theta)\right)\right]\right|_{\theta=\theta_{\mathrm{old}}}
$$


The update is $\theta_{\text {new }}=\theta_{\text {old }}+\left.\frac{1}{\lambda} A\left(\theta_{\text {old }}\right)^{-1} \nabla_{\theta} L(\theta)\right\vert_{\theta=\theta_{\text {old }}}$ , where the stepsize $\frac{1}{\lambda}$ is typically treated as an algorithm parameter. This differs from our approach, which enforces the constraint at each update. Though this difference might seem subtle, our experiments demonstrate that it sigificantly improves the algorithm’s performance on larger problems.

the standard policy gradient update by using an $\ell_{2}$ constraint or penalty:

$$
\begin{array}{l}
{\text { maximize }\left[\left.\nabla_{\theta} L_{\theta_{\text {old }}}(\theta)\right|_{\theta=\theta_{\text {old }}} \cdot\left(\theta-\theta_{\text {old }}\right)\right]} \\
{\text { subject to } \frac{1}{2}\left\|\theta-\theta_{\text {old }}\right\|^{2} \leq \delta}
\end{array}
$$

The policy iteration update can also be obtained by solving the unconstrained problem $$\text{maximize}_\pi L_{\pi_\text{old}}(\pi)$$,  using $L$ as defined in Equation (3). 

Several other methods employ an update similar to Equation (12). Relative entropy policy search (REPS) (Peters et al., 2010) constrains the state-action marginals p(s, a), while TRPO constrains the conditionals $p(a\vert s)$. Unlike REPS, our approach does not require a costly nonlinear optimization in the inner loop. Levine and Abbeel (2014) also use a KL divergence constraint, but its purpose is to encourage the policy not to stray from regions where the estimated dynamics model is valid, while we do not attempt to estimate the system dynamics explicitly. Pirotta et al. (2013) also build on and generalize Kakade and Langford’s results, and they derive different algorithms from the ones here.



#### 8 Experiments

验证以下问题:

1. What are the performance characteristics of the single path and vine sampling procedures?
2. TRPO is related to prior methods (e.g. natural policy gradient) but makes several changes, most notably by using a fixed KL divergence rather than a fixed penalty coefficient. How does this affect the performance of the algorithm?
3. Can TRPO be used to solve challenging large-scale problems? How does TRPO compare with other methods when applied to large-scale problems, with regard to final performance, computation time, and sample complexity?

##### 8.1 Simulated Robotic Locomotion

used δ = 0.01 for all experiments. 

Single path and vine TRPO solved all of the problems, yielding the best solutions. Natural gradient performed well on the two easier problems, but was unable to generate hopping and walking gaits that made forward progress. These results provide empirical evidence that constraining the KL divergence is a more robust way to choose step sizes and make fast, consistent progress, compared to using a fixed penalty.CEM and CMA are derivative-free algorithms, hence their sample complexity scales unfavorably with the number of parameters, and they performed poorly on the larger problems. The max KL method learned somewhat more slowly than our final method, due to the more restrictive form of the constraint, but overall the result suggests that the average KL divergence constraint has a similar effect as the theorecally justified maximum KL divergence.

![image-20200126023703845](/img/2020-01-02-TRPO.assets/image-20200126023703845.png)

##### 8.2 Playing Games from Images



#### 9 Discussion



#### A Proof of Policy Improvement Bound 策略改进界限的证明

**Lemma 1**. 

$$
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{\tau \sim \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi}\left(s_{t}, a_{t}\right)\right]
$$

This expectation is taken over trajectories $\tau:=\left(s_{0}, a_{0}, s_{1}, a_{1}, \ldots\right),$ and the notation $\mathbb{E}_{\tau \sim \tilde{\pi}}[\ldots]$ indicates that actions are sampled from $\tilde{\pi}$ to generate $\tau$

Proof.  $$A_{\pi}(s, a)=\mathbb{E}_{s^{\prime} \sim P\left(s^{\prime} \vert s, a\right)}\left[r(s)+\gamma V_{\pi}\left(s^{\prime}\right)-V_{\pi}(s)\right] $$          		Therefore,

$$
\begin{align}
\mathbb{E}_{\tau | \bar{\pi}} & \left[\sum_{t=0}^{\infty} \gamma^{t} A_{\pi}\left(s_{t}, a_{t}\right)\right]
 \\ &=\mathbb{E}_{\tau | \bar{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(s_{t}\right)+\gamma V_{\pi}\left(s_{t+1}\right)-V_{\pi}\left(s_{t}\right)\right)\right]
\\&=\mathbb{E}_{\tau | \bar{\pi}}\left[-V_{\pi}\left(s_{0}\right)+\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}\right)\right]
\\&=-\mathbb{E}_{s_{0}}\left[V_{\pi}\left(s_{0}\right)\right]+\mathbb{E}_{\tau | \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t}\right)\right]
\\ &=-\eta(\pi)+\eta(\tilde{\pi})
\end{align}
$$

Define $\bar{A}(s)$ to be the expected advantage of $\tilde{\pi}$ over $\pi$ at state $s$ : 在a上平均

$$
\bar{A}(s) :=\mathbb{E}_{a \sim \tilde{\pi}(\cdot | s)}\left[A_{\pi}(s, a)\right]
$$

- 引理1可以改写为:  去掉了 a 

$$
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{\tau \sim \tilde{\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} \bar{A}\left(s_{t}\right)\right]
$$

- $L_\pi$ 可以记为:

$$
L_\pi(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{\tau \sim  {\pi}}\left[\sum_{t=0}^{\infty} \gamma^{t} \bar{A}\left(s_{t}\right)\right]
$$

上面两个公式的主要区别是, 分别用$\tilde \pi$ 和 $\pi$ 来采样s. 为了限制两者之间的差异，我们将限制每个时间步产生的差异。 为此，首先需要引入一个度量$\tilde \pi$ 和 $\pi$ 一致的方法。 即,  将两个策略配对 couple, 以便定义一个 pairs of action 的联合分布。

**Definition 1**. $(\pi, \tilde{\pi})$ is an $\alpha$-coupled policy pair if it defines a joint distribution $(a, \tilde{a})  \vert s$, such that $P(a \neq \tilde{a} \vert s) \leq \alpha$ for s.  $\pi$ and  $\tilde{\pi}$ will denote the marginal distributions of a and $\tilde{a}$, respectively. 在每个s, 两个策略相对于所有action都有一个分布, 可以作为一个联合分布, 但具体是啥函数是未知的; 显然, 这两个边际分布都是相关的, 可能很类似.

Computationally, α-coupling means that if we randomly choose a seed for our random number generator, and then we sample from each of $\pi$ and $\tilde \pi$ after setting that seed, the results will agree for at least fraction 1 − α of seeds.  α-coupling 是指这两个策略不一致的几率为$\alpha$ 

**Lemma 2**. Given that $\pi, \tilde{\pi}$ are $\alpha$ -coupled policies, for all $s$

$$
|\bar{A}(s)| \leq 2 \alpha \max _{s, a}\left|A_{\pi}(s, a)\right|
$$

Proof.

$$
\begin{aligned}
\bar{A}(s) &=\mathbb{E}_{\tilde{a} \sim \tilde{\pi}}\left[A_{\pi}(s, \tilde{a})\right]=\mathbb{E}_{(a, \tilde{a}) \sim(\pi, \tilde{\pi})}\left[A_{\pi}(s, \tilde{a})-A_{\pi}(s, a)\right] \quad \text { since } \quad \mathbb{E}_{a \sim \pi}\left[A_{\pi}(s, a)\right]=0 \\
&=P(a \neq \tilde{a} | s) \mathbb{E}_{(a, \tilde{a})  \sim(\pi, \tilde{\pi}) | a \neq \tilde{a} }\left[A_{\pi}(s, \tilde{a})-A_{\pi}(s, a)\right] \quad  \text{如果相同则两个A相等}
\end{aligned}
$$

**Lemma 3.** Let $(\pi, \tilde{\pi})$ be an $\alpha$ -coupled policy pair. Then

$$
\left|\mathbb{E}_{s_{t} \sim \tilde{\pi}}\left[\bar{A}\left(s_{t}\right)\right]-\mathbb{E}_{s_{t} \sim \pi}\left[\bar{A}\left(s_{t}\right)\right]\right| \leq 2 \alpha \max _{s} \bar{A}(s) \leq 4 \alpha\left(1-(1-\alpha)^{t}\right) \max _{s}\left|A_{\pi}(s, a)\right|
$$

Proof.   

- Given the coupled policy pair $(\pi, \tilde{\pi}),$  obtain a coupling over the trajectory distributions produce by $\pi$ and $\tilde{\pi},$ respectively. Namely,  pairs of trajectories $\tau, \tilde{\tau}$,   where random seed is used to generate both trajectories. 
- 和论文 aoarl 思路一样, 将两个策略拆分为, 相同的部分以及不同的部分.  consider the advantage of $\tilde{\pi}$ over $\pi$ at timestep $t,$ and decompose this expectation based on whether $\pi$ agrees with $\tilde{\pi}$ at all timesteps  $i<t$. 
- Let $n_{t}$ denote the number of times that $a_{i} \neq \tilde{a}_{i}$ for $i<t,$ i.e., the number of times that $\pi$ and $\tilde{\pi}$ disagree before timestep t.  $n_t$ 表示 t时刻前的每个时刻i, 两个策略不一致的次数, 显然$n_t \leq t$ 

$$
\mathbb{E}_{s_{t} \sim \tilde{\pi}}\left[\bar{A}\left(s_{t}\right)\right]=P\left(n_{t}=0\right) \mathbb{E}_{s_{t} \sim \tilde{\pi} | n_{t}=0}\left[\bar{A}\left(s_{t}\right)\right]+P\left(n_{t}>0\right) \mathbb{E}_{s_{t} \sim \tilde{\pi} | n_{t}>0}\left[\bar{A}\left(s_{t}\right)\right]
$$

- The expectation decomposes similarly for actions are sampled using $\pi$: 两个策略都分解

$$
\mathbb{E}_{s_{t} \sim \pi}\left[\bar{A}\left(s_{t}\right)\right]=P\left(n_{t}=0\right) \mathbb{E}_{s_{t} \sim \pi | n_{t}=0}\left[\bar{A}\left(s_{t}\right)\right]+P\left(n_{t}>0\right) \mathbb{E}_{s_{t} \sim \pi | n_{t}>0}\left[\bar{A}\left(s_{t}\right)\right]
$$

- Note that the $n_t = 0$ terms are equal:  如果0步开始两个策略就一样

  $$
  \mathbb{E}_{s_{t} \sim \tilde{\pi} \vert n_t = 0}\left[\bar{A}\left(s_{t}\right)\right] = \mathbb{E}_{s_{t} \sim {\pi}\vert n_t = 0}\left[\bar{A}\left(s_{t}\right)\right]
  $$

- because $n_{t}=0$ indicates that $\pi$ and $\tilde{\pi}$ agreed on all timesteps less than $t .$ 

-  

-  

-  

- By definition of $\alpha, P(\pi, \tilde{\pi} \text { agree at timestep } i) \geq 1-\alpha,$ so $P\left(n_{t}=0\right) \geq(1-\alpha)^{t},$ and

$$
P\left(n_{t}>0\right) \leq 1-(1-\alpha)^{t}
$$

Subtracting Equations ( 33 ) and ( 34 ), we get
$$
\mathbb{E}_{s_{t} \sim \tilde{\pi}}\left[\bar{A}\left(s_{t}\right)\right]-\mathbb{E}_{s_{t} \sim \pi}\left[\bar{A}\left(s_{t}\right)\right]=P\left(n_{t}>0\right)\left(\mathbb{E}_{s_{t} \sim \tilde{\pi} | n_{t}>0}\left[\bar{A}\left(s_{t}\right)\right]-\mathbb{E}_{s_{t} \sim \pi\left[n_{t}>0\right.}\left[\bar{A}\left(s_{t}\right)\right]\right)
$$
By definition of $\alpha, P(\pi, \tilde{\pi} \text { agree at timestep } i) \geq 1-\alpha,$ so $P\left(n_{t}=0\right) \geq(1-\alpha)^{t},$ and
$$
P\left(n_{t}>0\right) \leq 1-(1-\alpha)^{t}
$$
Next, note that
$$
\begin{aligned}
\left|\mathbb{E}_{s_{t} \sim \tilde{\pi} | n_{t}>0}\left[\bar{A}\left(s_{t}\right)\right]-\mathbb{E}_{s_{t} \sim \pi | n_{t}>0}\left[\bar{A}\left(s_{t}\right)\right]\right| & \leq\left|\mathbb{E}_{s_{t} \sim \tilde{\pi} | n_{t}>0}\left[\bar{A}\left(s_{t}\right)\right]\right|+\left|\mathbb{E}_{s_{t} \sim \pi | n_{t}>0}\left[\bar{A}\left(s_{t}\right)\right]\right| \\
& \leq 4 \alpha \max _{s, a}\left|A_{\pi}(s, a)\right|
\end{aligned}
$$
Where the second inequality follows from Lemmal 3 . Plugging Equation ( 37) and Equation $(39 \text { ) into Equation }(36),$ we get
$$
\left|\mathbb{E}_{s_{t} \sim \vec{\pi}}\left[\bar{A}\left(s_{t}\right)\right]-\mathbb{E}_{s_{t} \sim \pi}\left[\bar{A}\left(s_{t}\right)\right]\right| \leq 4 \alpha\left(1-(1-\alpha)^{t}\right) \max _{s, a}\left|A_{\pi}(s, a)\right|
$$


#### B Perturbation Theory Proof of Policy Improvement Bound



#### C Efficiently Solving the Trust-Region Constrained Optimization Problem



#### D Approximating Factored Policies with Neural Networks