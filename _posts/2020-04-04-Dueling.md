---
layout:     post
title:      Dueling DQN
subtitle:   DQN from Deepmind
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - DeepMind
    - Reinforcement Learning
    - DQN

---

 

# Dueling DQN

Q = V + A ; 网络结构上的一个小的改进.   好处是只改了网络架构, 输入输出都没变, 利用起来方便.

 

## Dueling Network Architectures for Deep Reinforcement Learning



### Abstract

本文提出了一种新的neural network架构，用于model-free强化学习.  dueling network表达了两个独立的estimators：一个是value function，一个是action advantage function。这种分解的主要好处是可以在不改变底层强化学习算法的情况下，泛化action的学习。结果表明，在存在许多具有相似value的action的情况下，此架构可以更好地策略评估。



### Introduction

本文提出的是网络架构, 沿用的以前的具体算法. 

之前DQN架构,下面称为single-stream. 

**dueling architecture** :  将 state value 和 action advantage 两个stream分开。  
 网络前面的CNN  feature learning 模块共享.   
 two streams 通过一个聚合层aggregating layer来产出Q值.  
该网络分别自动产生value function和advantage function的估计，无需任何额外的supervision。

![image-20200410044412335](/img/2020-04-01-DQN.assets/image-20200410044412335.png)



  

<img src="/img/2020-04-01-DQN.assets/image-20200410045640711.png" alt="image-20200410045640711" style="zoom: 50%;" />

直观上，dueling架构可以学习哪些状态是有价值的，而不需要学习每个动作对每个状态的影响。这在其行动对环境没有任何相关影响的状态下特别有用。

图2所示, 上面一排看到 value网络流关注道路，特别是关注地平线，新车出现的地方。它也会注意到分数。dvantage stream 并不太关注视觉输入，因为当前面没有车的时候，它的动作选择实际上是无关紧要的。下面一排，dvantage 流会注意到前面有一辆车，这使得它的动作选择非常relevant。



实验证明了dueling架构可以在策略评估过程中，当冗余或类似的action被加入到学习问题中时，dueling架构可以更快速地识别出正确的动作。



#### Related Work

- original advantage updating algorithm, bellman residual update 公式分两部分: state value + advantage , Baird (1993)
- Advantage更新被证明比Q-learning在简单的连续时间内收敛得更快(Harmon et al., 1995).
- advantage learning algorithm只用 single advantage function (Harmon & Baird, 1996).
- policy gradient中的advantage函数的研究由来已久，始于(Sutton et al., 2000)。作为该系列工作的一个最近的例子，Schulman et al. (2015) 在线估计advantage value以减少policy gradient的方差

 





### Background

a sequential decision making setup,  environment $\mathcal{E}$ over discrete time steps   
agent  chooses an action from a discrete set $a_{t} \in \mathcal{A}=\{1, \ldots,|\mathcal{A}|\}$   
discounted return : $R_{t}= \sum_{\tau=t}^{\infty} \gamma^{\tau-t} r_{\tau}$  

$$
\begin{aligned}
Q^{\pi}(s, a) &=\mathbb{E}\left[R_{t} | s_{t}=s, a_{t}=a, \pi\right]  \\
V^{\pi}(s) &=\mathbb{E}_{a \sim \pi(s)}\left[Q^{\pi}(s, a)\right]
\end{aligned}
$$

dynamic programming 计算 Q:

$$
Q^{\pi}(s, a)=\mathbb{E}_{s^{\prime}}\left[r+\gamma \mathbb{E}_{a^{\prime} \sim \pi\left(s^{\prime}\right)}\left[Q^{\pi}\left(s^{\prime}, a^{\prime}\right)\right] | s, a, \pi\right]
$$

optimal $$Q^{*}(s, a)=\max _{\pi} Q^{\pi}(s, a)$$   
deterministic policy $$a=\arg \max _{a^{\prime} \in A} Q^{*}\left(s, a^{\prime}\right)$$    
$$V^{*}(s)=\max _{a} Q^{*}(s, a)$$ .  

$$
Q^{*}(s, a)=\mathbb{E}_{s^{\prime}}\left[r+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right) | s, a\right]
$$

advantage function:

$$
A^{\pi}(s, a)=Q^{\pi}(s, a)-V^{\pi}(s)
$$

 $\mathbb{E}_{a \sim \pi(s)}\left[A^{\pi}(s, a)\right]=0$  .



#### Deep Q-networks

Q-network  $Q(s, a ; \theta)$  approximate Q func   
optimize the following sequence of loss functions at iteration $i:$

$$
L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a, r, s^{\prime}}\left[\left(y_{i}^{D Q N}-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]  \\
y_{i}^{D Q N}=r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta^{-}\right)
$$

where $\theta^{-}$ represents the parameters of a fixed and separate **target network**.

gradient update : 

$$
\nabla_{\theta_{i}} L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a, r, s^{\prime}}\left[\left(y_{i}^{D Q N}-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{i}} Q\left(s, a ; \theta_{i}\right)\right]
$$

**model free** : states , rewards  from env  
**off-policy** :  states , rewards from behavior policy (epsilon greedy) , not online policy being learned.

**experience replay** (Lin, 1993; Mnih et al.. 2015).   
dataset $$\mathcal{D}_{t}=\left\{e_{1}, e_{2}, \ldots, e_{t}\right\}$$ of experiences $$e_{t}=\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$$ from many episodes.   
training the $Q$-network, instead using the current experience  temporal-difference learning,  sampling mini-batches of experiences from $\mathcal{D}$ **uniformly at random**. 

$$
L_{i}\left(\theta_{i}\right)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right) \sim u(\mathcal{D})}\left[\left(y_{i}^{D Q N}-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]
$$

Experience replay increases **data efficiency**  and **reduces variance** as uniform sampling from the replay buffer **reduces the correlation** among the samples used in the update.



#### Double Deep Q-networks

overoptimistic value estimates (van Hasselt, 2010)

DDQN:

$$
y_{i}^{D D Q N}=r+\gamma Q\left(s^{\prime}, \underset{a^{\prime}}{\arg \max } Q\left(s^{\prime}, a^{\prime} ; \theta_{i}\right) ; \theta^{-}\right)
$$

DDQN is the same as for DQN  with the target $y_{i}^{D Q N}$ replaced by $y_{i}^{D D Q N}$ 



![image-20200411012447815](/img/2020-04-01-DQN.assets/image-20200411012447815.png)



#### Prioritized Replay

key idea was to increase the replay probability of experience tuples that have a **high expected learning progress** (as measured via the proxy of **absolute TD-error**). This led to both faster learning and to better final policy quality.

dueling架构是对算法的补充, 对 uniform 和  prioritized replay  (easier implement rank-based) 都提升了性能, 产生了new state-of-the-art  **prioritized dueling** 版本.



### The Dueling Network Architecture

该架构的出发点:   for many states, it is **unnecessary to estimate the value of each action** choice.  
For bootstrapping based algorithms, the estimation of state values is important for every state.

跟之前DQN不同的就是, CNN后面最后一层FC层, 使用了两个 **sequences** (or **streams**) of fully connected layers.  然后再把这两个 treams 结合到一个FC层里.

将两个FC层结合起来输出Q估计的模块，需要精心设计。

for a deterministic policy, $$a^{*}=\arg \max _{a^{\prime} \in A} Q\left(s, a^{\prime}\right)$$,  $$Q\left(s, a^{*}\right)=V(s)$$ and hence $$A\left(s, a^{*}\right)=0$$ 

一个FC 输出 scalar $V(s ; \theta, \beta),$ 另一个FC输出 $|\mathcal{A}|$-dimensional vector $A(s, a ; \theta, \alpha) .$   
这里 $\theta$ 是CNN部分的网络参数, $\alpha$   $\beta$ 是这两个FC的参数. 则有:

$$
Q(s, a ; \theta, \alpha, \beta)=V(s ; \theta, \beta)+A(s, a ; \theta, \alpha) \tag{7}
$$

记住,  $Q(s, a ; \theta, \alpha, \beta)$ 是 Q-func 一个参数化近似估计.  并且, 不能推导出 $V(s ; \theta, \beta)$ 是state-value 的好的估计, 同样不能得出 $A(s, a ; \theta, \alpha)$ 是advantage function的一个好的估计.  
从这个角度上说, 公式7是无法识别的(unidentifiable) , 因为无法唯一的恢复V和A.   
由于缺乏可识别性**identifiability**, 当该公式被直接使用时，实际性能比较差。

为了解决identifiability问题, 可以强迫一个选定action, advantage function估计为0.  
即让网络的最后一个模块, 实现forward mapping:
$$
\begin{aligned}
Q(s, a ; \theta, \alpha, \beta) &=V(s ; \theta, \beta) \  + \\
&\left(A(s, a ; \theta, \alpha)-\max _{a^{\prime} \in|\mathcal{A}|} A\left(s, a^{\prime} ; \theta, \alpha\right)\right)  
\end{aligned} \tag{8}
$$

for $$a^{*}=\arg \max _{a^{\prime} \in \mathcal{A}} Q\left(s, a^{\prime} ; \theta, \alpha, \beta\right)= \arg \max _{a^{\prime} \in \mathcal{A}} A\left(s, a^{\prime} ; \theta, \alpha\right)$$,   
obtain $$Q\left(s, a^{*} ; \theta, \alpha, \beta\right)= V(s ; \theta, \beta)$$.   
Hence, stream $$V(s ; \theta, \beta)$$ 提供对V的估计, 另一个stream生成advantage的估计

另外一个模块的设计方案是用平均取代max: 

$$
\begin{aligned}
Q(s, a ; \theta, \alpha, \beta)&=V(s ; \theta, \beta)\ + \\
&\left(A(s, a ; \theta, \alpha)-\frac{1}{|\mathcal{A}|} \sum_{a^{\prime}} A\left(s, a^{\prime} ; \theta, \alpha\right)\right)
\end{aligned} \tag{9}
$$

一方面，这就失去了V和A的原始语义，因为它们现在离target偏离了一个常数，但另一方面，它增加了优化的稳定性。  
在(9)中，advantages只需要和mean 变化得一样快，而不需要像公式(8)中哪有, 对optimal action's advantage的任何变化进行补偿。  
我们还实验了公式(8)的softmax版本，但发现它的结果与公式(9)的简单模块相似。  
因此，本文的所有实验都使用了公式(9)的模块。

注意，虽然减去公式9中的均值有助于可识别性，但它并没有改变A值(以及Q)的相对排名，保留了公式7中基于Q值的任何greedy或ε-greedy策略。在运行时,  只需对advantage stream进行评估就可以做出决策。

重要, 公式(9)被视为网络的一部分，而不是单独的算法中的一步。与DQN一样，dueling architecture的训练只需要反向传播。V和A的估计是自动计算的，不需要任何额外的监督或算法修改. 

跟标准Q-network一样, 输入输出都没变. 所以之前的算法(DDQN, SARSA)都能复用.

 

### Experiments

#### Policy evaluation

先从简单的策略评估入手.  拟合$Q^\pi$
在这个实验中，采用TD-learning来学习Q值。  Expected SARSA

$$
y_{i}=r+\gamma \mathbb{E}_{a^{\prime} \sim \pi\left(s^{\prime}\right)}\left[Q\left(s^{\prime}, a^{\prime} ; \theta_{i}\right)\right]
$$

如图, 一个简单的env: corridor, 走廊.  agent 可以 上下左右no-op, 5个action. 从左下出发. 两个垂直的部分有10个方块, 水平的有50个.

![image-20200410193222218](/img/2020-04-01-DQN.assets/image-20200410193222218.png)

使用 ε-greedy policy 作为  behavior policy π.  ε = 0.001. 

比较single-stream Q与dueling框架  
在该env的三个版本上比较, 5, 10 , 20 actions.  10与20是另外再加了更多的空操作.   
性能衡量: Squared Error (SE) : $\sum_{s \in \mathcal{S}, a \in \mathcal{A}}(Q(s, a ; \theta)-\left.Q^{\pi}(s, a)\right)^{2}$ .    
single-stream : three layer MLP with 50 units on each hidden layer.   
dueling architecture : three layers.  first hidden layer of 50 units, two streams:  each, two layer MLP with 25 hidden units. 

实验结果,  5个action的时候, 收敛速度差不多, action越多性能差异越大.   
dueling network, the stream $V (s; θ, β)$ learns a general value that is shared across many similar actions at s, hence leading to faster convergence.  dueling中因为学到了V,所以收敛快.

这是个非常好的性质, 特别是对于有超多action空间的情况.



#### General Atari Game-Playing

- 底层网络结构还与之前的DQN一样, 最后的 dueling部分, 都是一个512的FC层.  
- 超参方面, 其他都一样, 不过学习率比之前低.  
- 由于advantage和value stream都会将梯度反向传播到最后一个卷积层，我们将进入最后一个卷积层的合并后梯度重新调整为$1/\sqrt 2$，这个简单的启发式方法会稍微增加稳定性。 
- clip the gradients,    norm <= 10. 
- 为了隔离dueling 的贡献,  我们重新train了一个 DDQN, 使用跟上一样的配置. 即 gradient clipping, 第一个FC层使用 1024 hidden units 使得两个网络参数一样 (dueling and single) .  这个重新训练的叫 Single Clip, 原始DDQN叫 Single.



开始游戏的时候, 有30个随机no-op操作, 好让agent处于一个随机的起始位置. 

性能衡量公式:

$$
\frac{\text { Score }_{\text {Agent }}-\text { Score }_{\text {Baseline }}}{\max \left\{\text { Score }_{\text {Human }}, \text { Score }_{\text {Baseline }}\right\}-\text { Score }_{\text {Random }}}
$$


我们取max人类和baseline agent的分数，因为这样可以防止在agent和baseline agent都没有取得好的表现时，不明显的变化表现为较大的改进。例如，当baseline agent达到2%的人类性能时，不应该被理解为是人类性能达到1%时的2倍。我们还选择不单独用人的性能百分比来衡量性能，因为在某些游戏中，相对于基线的微小差异可能会转化为几百个百分点的人的性能差异。



Table 1

$$
\begin{array}{l|cc|cc} 
& \ {\text { 30 no-ops }} & 
& {\text { Human Starts }} \\
\hline & \text { Mean } & \text { Median } & \text { Mean } & \text { Median } \\
\hline \text { Prior. Duel Clip } & \mathbf{5 9 1 . 9 \%} & \mathbf{1 7 2 . 1 \%} & \mathbf{5 6 7 . 0 \%} & \mathbf{1 1 5 . 3 \%} \\
\text { Prior. single } & 434.6 \% & 123.7 \% & 386.7 \% & 112.9 \% \\
\hline \text { Duel Clip } & \mathbf{3 7 3 . 1 \%} & \mathbf{1 5 1 . 5 \%} & \mathbf{3 4 3 . 8 \%} & \mathbf{1 1 7 . 1 \%} \\
\text { single Clip } & 341.2 \% & 132.6 \% & 302.8 \% & 114.1 \% \\
\text { single } & 307.3 \% & 117.8 \% & 332.9 \% & 110.9 \% \\
\hline \text { Nature DQN } & 227.9 \% & 79.1 \% & 219.6 \% & 68.5 \%
\end{array}
$$

Table 1 显示,  Single Clip的表现比Single更好。我们验证了这种增益主要是由gradient clipping带来的。因此，我们在所有的新方法中都加入了gradient clipping。



##### Robustness to human starts.

30 no-ops  的一个缺点是, 这种类起点不能generalize well.  由于雅达利环境的确定性，从独特的出发点出发，代理可以通过简单地记忆动作序列来学习获得良好的性能. 

为了得到一个更稳健的衡量标准，我们采用了Nair et al. (2015)的方法。具体来说，对于每个游戏，我们使用100个从人类专家的轨迹中采样的起始点。



##### Combining with Prioritized Experience Replay.

注意,虽然  prioritization, dueling , gradient clipping 这几种扩展方式是正交的. 它们之间的相互作用却很微妙。prioritization与gradient clipping相互作用，因为具有高TD-errors的采样transition更经常导致具有较高norm的梯度。为了避免不利的交互作用，我们对9个游戏子集的学习率和gradient clipping norm进行了粗略的重新调整。将学习率定为$6.25×10^{-5}$和gradient clipping norm 定为10（与上一节相同）。

就得到了目前最强的版本.



##### Saliency maps 显著性图

为了更好地理解V stream和A stream的作用,  我们计算出了saliency maps (Simonyan et al., 2013 ).  为了使V stream看到的图像中的突出部分可视化，我们计算出关于输入帧s的Jacobian矩阵:   $\left|\nabla_{s} \hat{V}(s ; \theta)\right| .$ 同意, Advantage的: $\left|\nabla_{s} \widehat{A}\left(s, \arg \max _{a^{\prime}} \widehat{A}\left(s, a^{\prime}\right) ; \theta\right)\right|$
因为这两个量与输入图片有相同的维度, 所以可以放一起可视化. 我们将灰度输入帧放在绿色和蓝色channel中，将saliency maps放在红色channel中,所有这三个channel共同组成了一个RGB图像。



### Discussion

**dueling架构的优势部分在于它能够有效地学习state value**。  
在dueling中, 每次更新Q值，value stream V就会被更新; 而在single-stream架构中 只有其中一个action的值被更新，其他action的值都不会被触及。  
在dueling中，这种更频繁的value stream的更新为V分配了更多的资源，因此可以更好地去近似 state value，而state value 的精确正是temporal-difference-based的方法(如Q-learning)所需要的 。  
这一现象在实验中得到了反映，当动作数量较多时，dueling架构相对于single-stream Q网络的优势会越来越大。
此外，相对于Q值的大小而言，一个给定状态的Q值之间的差异往往非常小.  即很多局面下,怎么挣扎都改变不大.  
例如，用DDQN对Seaquest游戏进行训练后，跨访问状态的平均Q差距（给定状态中的最佳action和次佳action的Q值之间的差距）大约为0.04，而这些状态的平均state value大约为15。  
这种尺度量级上的差异会导致更新中的少量噪声, 然后会导致action的重新排序，从而使nearly greedy策略突然切换。  
具有独立advantage stream的dueling architecture对这种影响是稳健的。






## Reference

Dueling Network Architectures for Deep Reinforcement Learning  https://arxiv.org/pdf/1511.06581.pdf 




