---
layout:     post
title:      Deep CFR
subtitle:   Deep Counterfactual Regret Minimization
date:       2020-04-23 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-poker.jpg"
catalog: true
tags:
    - AI
    - Imperfect Information
    - NFSP

---



# Deep CFR 算法

## Deep Counterfactual Regret Minimization

2019  Noam Brown from CMU



### Abstract

之前 tabular CFR +  Abstraction , 启发式;   =>  DeepCFR

**CFR** 是解决大型不完全信息博弈的领先框架。它通过**迭代遍历博弈树来收敛到均衡**。为了处理非常大的博弈，通常在运行CFR之前应用抽象化。抽象出来的博弈用表格式CFR求解，然后将其解映射回完整的博弈。这个过程可能是有问题的，因为抽象的方面往往是手工操作和领域特定的，抽象算法可能会漏掉博弈中重要的细微差别，而且有一个问题，因为确定一个好的抽象需要对博弈的均衡性有一定的了解。

本文介绍了Deep Counter-factual Regret Minimization，这是一种CFR的形式，它通过使用深度神经网络来近似CFR在全局博弈中的行为，从而避免了抽象的需要。我们表明，深度CFR是有原则的，并且在大型扑克游戏中实现了强大的性能。这是第一个在大型博弈中成功的CFR的非列表式变体。





### 1 Introduction

Imperfect-information games ,  find approximate equilibrium.

Counterfactual Regret Minimization,  CFR  (Zinkevich，2007）**converges** to a Nash equilibrium in two-player zero-sum games.   
Abstraction ( bucketing )  + tabular CFR  近似求解。可能只是真实均衡的粗略近似。

DeepRL 通过 DNN 而不是 tabular 来 **state representation**,  已经在 large state space 成功.  在 large MDPs 以及 零和完美信息博弈（如围棋）取得了一些突破。然而，**RL不能收敛到 imperfect-information games 的好策略（均衡）**。

本文介绍 DeepCFR ，使用DNN FA 来近似tabular CFR在完整的、未抽象的游戏中的行为。我们证明Deep CFR在双人零和游戏中收敛到ε-Nash均衡，并评估了在扑克游戏中的性能.  我们表明Deep CFR优于 NFSP（Heinrich & Silver, 2016），后者是不完全信息游戏的prior leading function逼近算法。



### 2 Notation and Background

imperfect-information extensive-form (that is, treeform) game:

- set of **players** ,   $\mathcal{P}$.
- **node** (or **history**) $h$ , all information of the current situation, including private knowledge known to only one player. 
- $A(h)$ denotes the **actions available** at a node 
- $P(h)$ is either **chance** or the unique player who acts at that node. 
- action $a \in A(h)$ **leads** from $h$ to $h^{\prime}$, then  $h \cdot a=h^{\prime}$
- $h \sqsubset h^{\prime}$ if a **sequence of actions leads** from $h$ to $h^{\prime}$    真子历史, 前因后果
- $H$  the set of **all nodes**. 
- $Z \subseteq H$ ,  **terminal nodes** 
- each player $p \in \mathcal{P},$  **payoff function** $u_{p}: Z \rightarrow \mathbb{R}$ .  
- $\mathcal{P}=\{1,2\}$ ,  $u_{1}=-u_{2}$ (**two-player zero-sum**)
- **range** of payoffs ,  $\Delta$ 
- Imperfect information is represented by **information sets** (**infosets**) 
- for any **infoset** $I$ belonging to $p,$  all nodes $h, h^{\prime} \in I$ are **indistinguishable** to $p$  
- every **non-terminal node** $h \in H$ belongs to exactly one infoset for each $p$  还是不可区分性
- set of all infosets belonging to $p$ where $p$ acts by $\mathcal{I}_{p}$ 
- set of all terminal nodes with a **prefix** in $I$ as $Z_{I}$ ,     从$I$走到终局的nodes;    
  the particular prefix $z[I]$
- **perfect recall**, if $h$ and $h^{\prime}$ do not share a player $p$ infoset then all nodes following $h$ do not share a player $p$ infoset with any node following $h^{\prime}$
- **strategy** (or **policy**) $\sigma(I)$ is a probability vector over actions for acting player $p$ in infoset $I .$ 
- set of actions in $I$ ,     $A(I)$
- probability of a particular action $a$ ,   $\sigma(I, a)$      ,  策略选a的几率
- $\sigma_{p}$ ,  a strategy for $p$ in every infoset  where $p$ acts.   玩家p的策略
- **strategy profile** $\sigma$ is a tuple of strategies
- strategy of every player other than $p$ ,   $\sigma_{-p}$   , 其他人的策略组合
- $u_{p}\left(\sigma_{p}, \sigma_{-p}\right)$ : **expected payoff** for $p$  , if  $p$  according to $\sigma_{p}$ , other play according to $\sigma_{-p}$ 
- $\pi^{\sigma}(h)=\Pi_{h^{\prime} \cdot a \sqsubseteq h} \sigma_{P\left(h^{\prime}\right)}\left(h^{\prime}, a\right)$ , **reach probability**,  $h$ is reached according to $\sigma$ ;  该定义就是到达几率是一路上各个节点玩家按策略选择action的几率的乘积(包含chance节点)
- $\pi_{p}^{\sigma}(h)$ , **contribution** of $p$ to this probability.  $\pi_{-p}^{\sigma}(h)$ contribution of chance and others, 到达几率拆分
- $\pi_{-p}^{\sigma}(I)=\sum_{h \in I} \pi_{-p}^{\sigma}(h)$ : probability of reaching $I$ if $p$ chooses actions leading toward $I$ ,  chance and others  according to $\sigma_{-p}$     ,  信息集到达几率是各个组成node的和
- For $h \sqsubseteq z$ ,   define $\pi^{\sigma}(h \rightarrow z)= \Pi_{h^{\prime} \cdot a \sqsubseteq z, h' \not\sqsubseteq h} \sigma_{P\left(h^{\prime}\right)} (h^{\prime}, a) $  ,  从h到z的几率, 是沿路各个node的概率积
- **best response** to $$\sigma_{-p}$$ ,  $$p$$'s strategy $$B R\left(\sigma_{-p}\right)$$, st $$u_{p}\left(B R\left(\sigma_{-p}\right), \sigma_{-p}\right)=\max _{\sigma_{p}^{\prime}} u_{p}\left(\sigma_{p}^{\prime}, \sigma_{-p}\right)$$ 
- **Nash equilibrium** $$\sigma^{*}$$ , a strategy profile, everyone plays a best response $$\forall p, u_{p}\left(\sigma_{p}^{*}, \sigma_{-p}^{*}\right)= \max _{\sigma_{p}^{\prime}} u_{p}\left(\sigma_{p}^{\prime}, \sigma_{-p}^{*}\right)$$
- **exploitability** $$e\left(\sigma_{p}\right)$$ , of a strategy $$\sigma_{p}$$ in a two-player zero-sum game is how much worse $$\sigma_{p}$$ does versus $$B R\left(\sigma_{p}\right)$$ compared to how a Nash equilibrium strategy $$\sigma_{p}^{*}$$ does against $$B R\left(\sigma_{p}^{*}\right) .$$   
  $$e\left(\sigma_{p}\right)=u_{p}\left(\sigma_{p}^{*}, B R\left(\sigma_{p}^{*}\right)\right)-u_{p}\left(\sigma_{p}, B R\left(\sigma_{p}\right)\right)$$  可利用性, 其他人用最强来针对玩家的策略.
- **total exploitability**   :  $\sum_{p \in P} e\left(\sigma_{p}\right)$



#### 2.1 Counterfactual Regret Minimization (CFR)

CFR is an iterative algorithm that **converges to a Nash equirlibrium** in any finite two-player zero-sum game with a theoretical convergence bound of $O\left(\frac{1}{\sqrt{T}}\right)$.  在实践中，CFR的收敛速度要快得多。近期的一些CFR形式在self-play以 $O\left(\frac{1}{T^{0.75}}\right)$ 速度收敛, 但在实践中收敛较慢, 本文不用. 

- $\sigma^{t}$ , strategy profile on iteration $t$ 

- **counter factual value** $v^{\sigma}(I)$ of player $p=P(I)$ at $I$   , is the expected payoff to $p$ when reaching $I$, weighted by the probability that $p$ would reached $I$ if she tried to do so that iteration. 
  $$
  v^{\sigma}(I)=\sum_{z \in Z_{I}} \pi_{-p}^{\sigma}(z[I]) \pi^{\sigma}(z[I] \rightarrow z) u_{p}(z) \tag{1}
  $$
  
- $v^{\sigma}(I, a)$ is the same except it assumes that player $p$ plays action $a$ at infoset $I$ with $100 \%$ probability

- **instantaneous regret** $r^{t}(I, a)$ , counterfactual value from playing $a$ vs. playing $\sigma$ on iteration $t$ 
  $$
  r^{t}(I, a)=v^{\sigma^{t}}(I, a)-v^{\sigma^{t}}(I) \tag{2}
  $$
  
- counterfactual regret for infoset $I$ action $a$ on iteration $T$ is
  $$
  R^{T}(I, a)=\sum_{t=1}^{T} r^{t}(I, a) \tag{3}
  $$
  
- $$R_{+}^{T}(I, a)=\max \left\{R^{T}(I, a), 0\right\}$$ and  $$R^{T}(I)= \max _{a}\left\{R^{T}(I, a)\right\}$$   

- **Total regret** for $p$ in the entire game is $$R_{p}^{T}=\max _{\sigma_{p}^{\prime}} \sum_{t=1}^{T}\left(u_{p}\left(\sigma_{p}^{\prime}, \sigma_{-p}^{t}\right)-u_{p}\left(\sigma_{p}^{t}, \sigma_{-p}^{t}\right)\right)$$



CFR通过对每个信息集应用任意一种regret最小化算法来确定迭代策略。通常情况下，使用 **regret matching (RM)**.



**regret matching** ,  玩家选择某个action的概率公式:
$$
\sigma^{t+1}(I, a)=\frac{R_{+}^{t}(I, a)}{\sum_{a^{\prime} \in A(I)} R_{+}^{t}\left(I, a^{\prime}\right)}  \tag{4}
$$
如果分母为0, 则随机选.  通常情况下，随机的时候每个动作的概率是相等的，但在本文中，我们选择了概率为1的反事实误差最大的动作，我们发现在经验上，这有助于RM更好地处理近似误差。 ?

收敛到纳什均衡的讨论: 

如果玩家执行 **regret matching** in $$I$$ on every iteration, then on iteration $$T$$ ,  $$R^{T}(I) \leq \Delta \sqrt{ \vert A(I) \vert } \sqrt{T}$$  .   
sum of counterfactual regret across all infosets upper bounds the total regret. Therefore if $$p$$ plays according to CFR on every iteration, then $$R_{p}^{T} \leq \sum_{I \in \mathcal{I}_{p}} R^{T}(I)$$ .  So, as $$T \rightarrow \infty, \frac{R_{p}^{T}}{T} \rightarrow 0$$ 

**average strategy**  $$\bar{\sigma}_{p}^{T}(I)$$ for an infoset $$I$$ on iteration $$T$$ is $$\bar{\sigma}_{p}^{T}(I)=\frac{\sum_{t=1}^{T}\left(\pi_{p}^{\sigma^{t}}(I) \sigma_{p}^{t}(I)\right)}{\sum_{t=1}^{T} \pi_{p}^{\sigma^{t}}(I)}$$ .

In two-player zero-sum games, if both players' average total regret satisfies $$\frac{R_{p}^{T}}{T} \leq \epsilon,$$ then average strategies $$\left\langle\bar{\sigma}_{1}^{T}, \bar{\sigma}_{2}^{T}\right\rangle$$ form a $$2 \epsilon$$ -Nash equilibrium. Thus, CFR constitutes an anytime algorithm for finding an $$\epsilon$$ -Nash equilibrium in two-player zero-sum games.




在实践中，在每次迭代时交替更新玩家的遗憾 收敛更快，而不是每次迭代时同时更新两个玩家的遗憾 . 这使得理论更加复杂。我们在本文中使用CFR的交替更新形式.



#### 2.2  Monte Carlo Counterfactual Regret Minimization

Vanilla CFR需要完全遍历游戏树，对large games不可行。一种方法是**Monte Carlo CFR (MCCFR)**，每次迭代时只遍历游戏树的一部分。

In **MCCFR**, 每次迭代遍历game tree node的子集 $Q^{t}$ ,  而 $Q^{t}$ 由某个分布 $\mathcal{Q}$ 采样.    
**Sampled regrets** $\tilde{r}^{t}$ are tracked rather than exact regrets.   
For infosets that are sampled at iteration $t$,   $\tilde{r}^{t}(I, a)$ is equal to $r^{t}(I, a)$ divided by the probability of having sampled $I ;$ for unsampled infosets $\tilde{r}^{t}(I, a)=0 .$    修正权重

MCCFR有很多变体, 这里采用**external-sampling** ,  因为简单又性能强大.     
In external-sampling MCCFR the game tree is traversed for one player at a time, alternating back and forth. 一次遍历一个玩家,来回交替,  当前正在遍历树的玩家称为 **traverser** ; 在迭代中 只更新 traverser 的regret.    
At infosets where the traverser acts, all actions are explored. At other infosets and chance nodes, only a single action is explored.  对当前玩家的动作是遍历, 其他的都是采样.

External-sampling MCCFR probabilistically **converges** to an equilibrium. For any $\rho \in(0,1]$, total regret is bounded by $R_{p}^{T} \leq\left(1+\frac{\sqrt{2}}{\sqrt{\rho}}\right)\left \vert \mathcal{I}_{p}\right  \vert    \Delta \sqrt{ \vert A \vert } \sqrt{T}$ with probability $1-\rho$ . 



### 3 Related Work

CFR 不是唯一能够求解大型不完全信息博弈的迭代算法。

**First-order methods** 一阶方法 converge to a Nash equilibrium in $O(1 / T)$  , 比CFR的理论约束要好.然而，在实践中，CFR的最快的变体比最好的一阶方法要快得多。此外，CFR对误差的鲁棒性更强，因此与函数拟合相结合时可能做得更好。

**Neural Fictitious Self Play（NFSP）**（Heinrich & Silver，2016）将深度学习函数拟合与Fictitious Play（Brown，1951）相结合，产生了一种用于（heads-up limit Texas hold'em）的AI，这是一种大型不完全信息游戏。然而，Fictitious Play的理论收敛保证比CFR弱，实践中收敛较慢。我们在本文中对我们的算法与NFSP进行了比较。无模型的策略梯度算法已经被证明，当参数调优得当时，可以最大限度地减少regret（Srinivasan，2018），并实现了与NFSP相当的性能。

过去的工作**Deepstack** 研究了在不完全信息游戏中使用深度学习来估计有限深度子游戏的值（Brown，2018）。然而，在子博弈本身内部使用了tabular CFR。  
**Large-scale function approximated CFR**  对 single-agent 的也被开发出来了（Jin，2017）。我们的算法是针对multi-agent设置的，与针对单代理设置提出的算法有很大区别。

之前的工作将回归树函数近似regression tree function approximation与CFR结合在一起（Waugh，2015）, **Regression CFR (RCFR)** 算法。该算法定义了游戏中的信息集的一些特征，并计算出权重来近似 tabular CFR产生的regret.  回归CFR在算法上类似于Deep CFR，但使用了类似于Abstraction的手工制作的特征，而不是学习特征。RCFR也使用了完全遍历游戏树（这在大型游戏中是不可行的），并且只在小游戏上进行过评估。因此，它最好被看作是第一个概念证明，即function approximation可以应用于CFR。





### 4  Description of the Deep Counterfactual Regret Minimization Algorithm

Deep CFR的目标是  使用DNN  泛化相似的信息集，近似CFR,   不需要对每个信息集 计算和积累regret.  
Goal: to approximate the behavior of CFR without calculating and accumulating regrets at each infoset, by generalizing across similar infosets using function approximation via deep neural net works.

在第 $t$次迭代上，Deep CFR对博弈树进行一定数量(常数K)的部分遍历，遍历路径根据external sampling MCCFR确定。 对遇到的每个infoset $I$ ,  将其作为神经网络的输入( 网络 $V: I \rightarrow \mathbf{R}^{ \vert A \vert }$, 参数 $\theta_{p}^{t-1}$ ),  输出 $V\left(I, a  \vert  \theta^{t-1}\right)$  , 在输出上是用**regret matching** 得到策略 $\sigma^{t}(I)$  .  我们的目标是让 $V\left(I, a  \vert  \theta^{t-1}\right)$ 与tabular CFR所产生的regret $R^{t-1}(I, a)$ 近似成正比。

value指regret.  当到达终端节点时，value 被向上传回。  
在chance and opponent信息集，采样动作的value被向上传回，不做任何改变。  
在traverser信息集，传回的值是所有动作值的加权平均值，其中动作a的权重为$\sigma^{t}(I, a)$。这就产生了这个迭代的各种动作的instantaneous regrets样本。  
样本被添加到内存$\mathcal{M}_{v, p}$ 中(p 代表 traverser player)，如果超过了容量，则使用reservoir sampling（Vitter，1985）。

external sampling 采样到的 instantaneous regrets有个良好特性 :

**Lemma 1**. For external sampling MCCFR, the sampled instantaneous regrets are an **unbiased estimator** of the **advantage**, i.e. the difference in expected payoff for playing $a$ vs $\sigma_{p}^{t}(I)$ at $I,$ assuming both players play $\sigma^{t}$ everywhere else. 


$$
\mathbb{E}_{Q \in \mathcal{Q}_{t}}\left[\tilde{r}_{p}^{\sigma^{t}}(I, a)  \vert  Z_{I} \cap Q \neq \emptyset\right]=\frac{v^{\sigma^{t}}(I, a)-v^{\sigma^{t}}(I)}{\pi_{-p}^{\sigma^{t}}(I)}
$$



最近在深度强化学习方面的工作表明，神经网络可以有效地预测和泛化具有大状态空间的环境中的advantages，并利用这些优势来学习好的策略. 

一旦玩家的K次遍历完成，一个新的网络就会从头开始训练，通过最小化MSE来更新参数: 预测的advantage $V_{p}\left(I, a  \vert  \theta^{t}\right)$ 和 内存保存的之前迭代中的instantaneous regrets采样 $t^{\prime} \leq t$  , $\tilde{r}^{t^{\prime}}(I, a)$ 。所有sampled instantaneous advantages $\tilde{r}^{t^{\prime}}(I, a)$ 的平均 与 total sampled regret $\tilde{R}^{t}(I, a)$成正比（across actions in an infoset），所以一旦一个样本被添加到内存中，除了 reservoir sampling，即使在下一次CFR迭代开始时，它也不会被移除。

对于价值和平均策略模型，可以使用任何满足Bergman发散的损失函数(Banerjee，2005)，例如平均误差平方损失。

尽管几乎所有的采样方案都可以接受，但只要样本的权重合适，external sampling具有方便的特性，在一个迭代中分配所有样本的权重相等，从而实现了我们期望的两个目标。此外，探索一个遍历者的所有动作有助于**减少方差**。然而，在具有非常大的分支因子的游戏中，外部取样可能不切实际，因此，在这些游戏中，可能需要不同的取样方案，例如outcome sampling.

除了value network之外，还有一个单独的 policy network $\Pi: I \rightarrow \mathbf{R}^{ \vert A \vert }$ , 近似运行结束时的平均策略，因为它是在所有迭代过程中的平均策略，收敛到纳什均衡。为了做到这一点，我们维护一个单独的内存$$\mathcal{M}_{\Pi}$$，里面是两个玩家的采样信息集的概率向量。每当属于玩家$p$的信息集$I$在对方玩家通过外部取样遍历游戏树时被遍历，信息集概率向量 $$\sigma^{t}(I)$$ 就会被添加到$$\mathcal{M}_{\Pi}$$ 并赋予权重 $t$ .  

如果深度CFR迭代的次数和每个价值网络模型的大小都很小，那么可以通过存储每个迭代的价值网络来避免训练最终的策略网络（Steinberger，2019）。在实际博弈过程中，随机抽取一个价值网络，玩家根据该网络的预测优势，进行CFR策略。这样可以消除最终平均策略网络的函数近似误差，但需要存储所有之前的价值网络。尽管如此，只存储先前价值网络的一个子集，仍然可以实现强性能和低**可利用性exploitability**（Jackson，2016）。

定理1指出，如果内存缓冲区足够大，那么很大的概率，Deep CFR 使得 average regret被一个常数所约束，这个常数与函数逼近误差的平方根成正比。



Theorem 1. Let $$T$$ denote the number of Deep CFR iterations, $$ \vert A \vert $$ the maximum number of actions at any infoset, and $$K$$ the number of traversals per iteration. Let $$\mathcal{L}_{V}^{t}$$ be the average MSE loss for $$V_{p}\left(I, a  \vert  \theta^{t}\right)$$ on a sample in $$\mathcal{M}_{V, p}$$ at iteration $$t,$$ and let $$\mathcal{L}_{V^*}^{t}$$ be the minimum loss achievable for any function $$V$$ . Let $$\mathcal{L}_{V}^{t}-\mathcal{L}_{V^{*}}^{t} \leq \epsilon_{\mathcal{L}}$$ . 

If the value memories are sufficiently large, then with probability $1-\rho$ total regret at time $T$ is bounded $b y$


$$
R_{p}^{T} \leq\left(1+\frac{\sqrt{2}}{\sqrt{\rho K}}\right) \Delta\left \vert \mathcal{I}_{p}\right \vert  \sqrt{ \vert A \vert } \sqrt{T}+4 T\left \vert \mathcal{I}_{p}\right \vert  \sqrt{ \vert A \vert  \Delta \epsilon_{\mathcal{L}}}
$$

with probability $1-\rho$



**Corollary 1.**  As $T \rightarrow \infty,$ average regret $\frac{R_{p}^{T}}{T}$ is bounded by

$$
4\left \vert \mathcal{I}_{p}\right \vert  \sqrt{ \vert A \vert  \Delta \epsilon_{\mathcal{L}}}
$$

with high probability.



我们没给出 使用linear weighting时Deep CFR的收敛极限，因为在蒙特卡罗案例中，线性CFR的收敛率没有显示出来。但是，图4显示了实际应用中收敛速度适中。





<img src="/img/2020-04-23-DeepCFR.assets/image-20200520043723085.png" alt="image-20200520043723085" style="zoom:50%;" />



<img src="/img/2020-04-23-DeepCFR.assets/image-20200520043523030.png" alt="image-20200520043523030" style="zoom:50%;" />





### 5 Experimental Setup

我们测量了Deep CFR（算法1）在（Head-up Flop Hold'em poker，FHP）中的近似均衡性能。FHP是一个有超过$10^{12}$个节点和超过$10^9$个信息集的大型游戏。相比之下，我们使用的网络有98,948个参数。FHP类似于heads-up limit Texas hold’em（HULH），但在第二轮下注后结束，而不是第四轮下注后结束，只发三张公共牌。我们还在HULH , 测量了相对于特定领域的抽象技术的性能，有超过$10^{17}$个节点和超过$10^{14}$个信息集。

在这两个博弈中，我们比较了NFSP的性能，NFSP是之前领先的不完全信息博弈求解算法，使用域无关的函数逼近法，以及为扑克领域设计的最先进的抽象技术。



#### 5.1 Network Architecture

<img src="/img/2020-04-23-DeepCFR.assets/image-20200520040900409.png" alt="image-20200520040900409" style="zoom:50%;" />



我们使用图5.1所示的神经网络体系结构，用于两个网络. value network $V$ that computes advantages  and network $Π$ that approximates the final average strategy。这个网络有7层和98,948个参数。信息集由牌组和历史下注记录组成。这些牌被表示为三个embedding的总和：rank embedding(1-13)、suit embedding(1-4)和card embedding(1-52)。这些embedding是对每组牌（hole, flop, turn, river）的总和，然后这些embedding被串联起来。在每N轮的每一轮下注中，最多可以有6个连续的动作，共6N 个下注位置。每个下注位置由一个二进制值编码，指定是否发生了下注，以及一个float下注大小。

Normalization (to zero mean and unit variance) is applied to the last-layer features. The network architecture was not highly tuned, but normalization and skip connections were used .

在价值网络中，输出向量代表输入信息集的每个行动的预测优势。在平均策略网络中，产出被解释为行动的概率分布的对数。



#### 5.2  Model training

我们给每个玩家的优势内存MV,p和策略内存MΠ分配了最大4000万个信息集。价值模型在每次CFR迭代时，从随机初始化开始，从头开始训练。我们使用batch size 10,000,  进行4000个minibatch（SGD）迭代，并使用Adam优化器进行参数更新，学习率为0.001，gradient norm clipping为1。对于HULH，我们使用32000个SGD迭代，batch size 20000。图4显示，在每次迭代时从头开始训练模型，而不是使用上一次迭代的权重，会带来更好的收敛性。



#### 5.3 Linear CFR

存在一些CFR的变体，它们的性能比vanilla CFR快得多。然而，这些更快的CFR的大多数变体并不能很好地处理近似误差approximation error（Tammelin等人，2015；Burch，2017；Brown & Sandholm，2019；Schmid等人，2019）。在本文中，我们使用Linear CFR（LCFR）（Brown & Sandholm，2019），这是一种比CFR更快的CFR的变体，在某些设置中是CFR最快的已知变体（特别是在报酬率分布较广的设置中），并且能很好地容忍近似误差。LCFR并不是必需的，而且似乎并没有导致更好的拟合性能，但在我们的实验中，LCFR确实会导致更快的收敛。
LCFR与CFR类似，除了迭代t是由t来加权。具体来说，我们对存储在优势存储器和策略存储器中的每一个条目都保持一个权重，等于这个条目加入时的t。 当每次迭代T训练$θ_p$时，我们将所有的批处理权重都重新调整为2/T，将加权误差最小化。



### 6 Experimental Results

<img src="/img/2020-04-23-DeepCFR.assets/image-20200520043825889.png" alt="image-20200520043825889" style="zoom: 33%;" />



图2比较了Deep CFR 与 不同大小的特定域抽象的性能。这些抽象使用external-sampling 的线性蒙特卡罗CFR（Lanctot等，2009；Brown & Sandholm，2019）进行求解，该算法在此设置中处于领先地位。40,000个聚类抽象是指将博弈中超过$10^9$个不同的决策被聚类成40,000个抽象的决策，其中同一bucket的情况被完全相同地处理。这种bucket是利用K-means聚类对特定领域的特征进行聚类。无损抽象**lossless abstraction** 只将策略上同构的情况聚类在一起（例如，flushes that differ only by suit），所以这个抽象的解映射到完整博弈中的解，不会出错。

成绩和可利用性是以每场比赛的 milli big blinds per game (mbb/g)来衡量的，这是衡量扑克牌的标准胜率。

从图中可以看出，Deep CFR的可利用性与使用360万个集群的抽象达到了相似的水平，但收敛速度大大加快。虽然Deep CFR在触及的节点方面效率更高，但神经网络推理和训练需要相当大的开销，而tabular CFR避免了这些开销。然而，Deep CFR不需要高级领域知识。我们展示了每步10,000次CFR遍历的Deep CFR性能。每步使用更多的遍历次数会降低样本效率，需要更多的神经网络训练时间，但需要更少的CFR步骤。

图2也比较了深度CFR和NFSP的性能，NFSP是一种在不完全信息博弈中学习近似纳什均衡的方法。NFSP近似于虚构的自我博弈，它被证明可以收敛到纳什均衡，但实际上它的收敛速度要比CFR慢得多。我们观察到Deep CFR达到了37 mbb/g，而NFSP收敛到47 mbb/g 。然而，这些方法的大部分时间都是在执行SGD步骤，所以在我们的实施中，我们看到与NFSP相比，Deep CFR在现实中时间上的改善不如NFSP在采样效率上的改善那么明显。











![image-20200520043924401](/img/2020-04-23-DeepCFR.assets/image-20200520043924401.png)

图3显示了使用不同的游戏遍历次数、网络SGD步数和模型大小的Deep CFR的性能。随着每次迭代的CFR遍历次数的减少，收敛速度变慢，但模型收敛到相同的最终可利用性。这可能是因为需要更多的迭代来收集足够多的数据来充分减少方差。另一方面，减少SGD步数并不改变收敛率，但会影响模型的拟合可利用性。这大概是因为随着每次迭代的训练步数的增加，模型的损失会减少（见定理1）。在FHP中，增加模型大小也会降低最终可利用性，直到一定的模型大小。







![image-20200520043956732](/img/2020-04-23-DeepCFR.assets/image-20200520043956732.png)

在图4中，我们考虑了深度CFR的某些组件的消融。在每次CFR迭代时从头开始重新训练遗憾模型，其可利用性要比在所有迭代中微调单一模型要低得多。我们怀疑这是因为当目标从一个迭代到另一个迭代时，单一模型会被卡在不良的局部最小值中。存储器**采样更新算法**的选择被证明是**至关重要的**；如果使用滑动窗口存储器，那么一旦存储器被填满，可利用性就开始增加，即使存储器足够大，可以容纳许多CFR迭代的样本。



<img src="/img/2020-04-23-DeepCFR.assets/image-20200520043850296.png" alt="image-20200520043850296" style="zoom:50%;" />

最后，我们测量了HULH中的一对一性能。我们将Deep CFR和NFSP与三种不同化的抽象的近似解（通过线性蒙特卡罗CFR求解）进行比较. 其结果如表1所示。作为比较，扑克AI Polaris在2007年的HULH人机竞赛中与人类职业选手的人机竞赛中使用的最大的抽象数大约包含$3 \cdot 10^{8}$ buckets。当应用variance- reduction 技术时，结果显示，专业的人类选手输给了2007年的Polaris AI约$52 \pm 10 \mathrm{mbb} / \mathrm{g}$，而2007年的Polaris AI则输给了约52 ǩ。/mathrm{g} Johanson，2016 )。相比之下，我们的ur Deep CFR agent loses to a $3.3 \cdot 10^{8}$ bucket abstraction by only $-11 \pm 2 \mathrm{mbb} / \mathrm{g}$ and beats NFSP by $43 \pm 2 \mathrm{mbb} / \mathrm{g}$ .





### 7  Conclusions

我们描述了一种在大型不完全信息博弈中寻找近似平衡的方法，通过将CFR算法与深度神经网络函数近似相结合，在大型不完全信息博弈中寻找近似平衡。该方法从理论上讲是ok的，在大型扑克游戏中实现了相对于特定领域的抽象技术的强大性能，而不依赖于高级领域知识。这是CFR的第一个在大型游戏中成功的non-tabular 变体。

深度CFR和其他用于不完全信息游戏的神经网络方法，为处理状态或动作空间对于表格方法来说太大、抽象不直接的大型游戏提供了一个有前途的方向。将Deep CFR扩展到更大的博弈中，可能需要更多的可扩展的采样策略，以及减少高方差的采样报酬的策略。最近的工作提出了更多可扩展的采样策略（Li等人，2018年）和减少方差技术（Schmid等人，2019年），都是有希望的方向。我们认为这些都是未来工作的重要领域。



 



## Reference

Deep Counterfactual Regret Minimization


















