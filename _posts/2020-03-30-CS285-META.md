---
layout:     post
title:      CS 285. Meta Reinforcement Learning
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

 

##  Meta Reinforcement Learning

Kate Rakelly



#### Questions we seek to answer

- **Motivation**: What problem is meta-RL trying to solve?
- **Context**: What is the **connection** to other problems in RL?
- **Solutions**: What are solution methods for meta-RL and their **limitations**? 
- **Open Problems**: What are the open problems in meta-RL?



#### Meta-learning problem statement

meta learning就是learning to learn。探索如何在training task里面找到一些共性，可以在一个新的，仅有少量数据的test task上面快速学习。



##### Meta-RL problem statement

- **Regular RL**: learn policy for single task ,  $$\mathcal{M}$$:  MDP

$$
\begin{aligned}
\theta^{\star} &=\arg \max _{\theta} E_{\pi_{\theta}(\tau)}[R(\tau)] \\
&=f_{\mathrm{RL}}(\mathcal{M})   
\end{aligned}
$$

- **Meta-RL**: learn adaptation rule ,   $$\mathcal{M}_i$$:   MDP for task i  
  
  $$
  \theta^{\star}= {\arg \max }_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}[R(\tau)] \quad  \leftarrow \text { Meta-training Outer loop} \\
  \text { where } \phi_{i}=f_{\theta}\left(\mathcal{M}_{i}\right)  \quad  \leftarrow \text { Adaptation Inner loop}
  $$
  
  在meta-RL中, 并不想从头对每个新的task进行训练.   使用新task很少的一些经验, 然后调整现有的这些策略使得能在新task上work well.

  Adaptation Inner loop 使用 adaptation procedure $$f_\theta$$ 来产出 adapted policy $$\phi$$ , 然后目标在new task上得高分.   这里参数$$\theta$$  主要是 adaptation rule的参数.

 <img src="/img/CS285.assets/image-20200329004906499.png" alt="image-20200329004906499" style="zoom: 33%;" />



#### Relation to goal-conditioned policies

<img src="/img/CS285.assets/image-20200329005043857.png" alt="image-20200329005043857" style="zoom:33%;" />

<img src="/img/CS285.assets/image-20200329005059313.png" alt="image-20200329005059313" style="zoom:33%;" />



- Meta-RL can be viewed as a **goal-conditioned** policy where the task information is inferred from ***experience***
- Task information could be about the **dynamics or reward functions** 
- Rewards are a strict generalization of goals.   任何的goal都可以表达为 reward function, 反之不成立

**Q: What is an example of a reward function that can’t be expressed as a goal state?**

A: E.g., seek while avoiding, action penalties.   如seek out state A while avoiding state B ;或者 move to B 但不能太快.



#### Adaptation

**What should the adaptation procedure do?**

-  **Explore**: Collect the most informative data
-  **Adapt**: Use that data to obtain the optimal policy



#### General meta-RL algorithm outline
while training:   
&emsp;1 sample task $$i,$$ collect data $$\mathcal{D}$$  
&emsp;2 adapt policy by computing $$\phi_{i}=f\left(\theta, \mathcal{D}_{i}\right)$$  
&emsp;3 collect data $$\mathcal{D}_{i}^{\prime}$$ with adapted policy $$\pi_{\phi}$$  
&emsp;4 update $$\theta$$ according to $$\mathcal{L}\left(\mathcal{D}_{i}^{\prime}, \phi_{i}\right)$$

1,2,3  Can do more than one round of adaptation  
4 In practice, compute update across a batch of tasks



Different algorithms:

- \-  Choice of function f
- \-  Choice of loss function L







### Solution #1: recurrence

Implement the policy as a recurrent network, train across a set of tasks. 
$$
\theta^{\star}= {\arg \max }_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}[R(\tau)] \quad  \leftarrow \text { Meta-training Outer loop: } {\arg \max }_{\theta} \text{ PG}\\
\text { where } \phi_{i}=f_{\theta}\left(\mathcal{M}_{i}\right)  \quad  \leftarrow \text { Adaptation Inner loop : } f_\theta \text{ RNN}
$$


<img src="/img/CS285.assets/image-20200329013101886.png" alt="image-20200329013101886" style="zoom:50%;" />

Persist the hidden state across episode boundaries for continued adaptation!

Duan et al. 2016, Wang et al. 2016. Heess et al. 2015. Fig adapted from Duan et al. 2016



while training:  
&emsp;&emsp;for $$i$$ in tasks:  
&emsp;&emsp;&emsp;initialize hidden state $$\mathbf{h}_{\mathbf{0}}=0$$  
&emsp;&emsp;&emsp;for $$t$$ in timesteps:  
&emsp;&emsp;&emsp;&emsp; 1 sample 1 transition $$\mathcal{D}_{i}=\mathcal{D}_{i} \cup\left\{\left(s_{t}, a_{t}, s_{t+1}, r_{t}\right)\right\}$$ from $$\pi_{h_{t}}$$  
&emsp;&emsp;&emsp;&emsp; 2 update policy hidden state $$\mathbf{h}_{\mathbf{t}+\mathbf{1}}=f_{\theta}\left(\mathbf{h}_{\mathbf{t}}, s_{t}, a_{t}, s_{t+1}, r_{t}\right)$$  
&emsp;&emsp;update policy parameters  $$\theta \leftarrow \theta-\nabla_{\theta} \sum_{i} \mathcal{L}_{i}\left(\mathcal{D}_{i}, \pi_{\mathrm{h}}\right)$$

 

- **Pro: general, expressive**
  - There exists an RNN that can compute any function

- **Con: not consistent**
  - What does it mean for adaptation to be “consistent”?

*Will converge to the optimal policy given enough data*



<img src="/img/CS285.assets/image-20200329020028426.png" alt="image-20200329020028426" style="zoom: 50%;" />



#### Wait, what if we just fine-tune?

<img src="/img/CS285.assets/image-20200329020133737.png" alt="image-20200329020133737" style="zoom:50%;" />

**is pretraining a *type* of meta-learning?**  
better features = faster learning of new task!

**Sample inefficient, prone to overfitting, and is particularly difficult in RL**



### Solution #2: optimization

Learn a parameter initialization from which fine-tuning for a new task works!


$$
\theta^{\star}= {\arg \max }_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}[R(\tau)] \quad  \leftarrow \text { Meta-training Outer loop: } {\arg \max }_{\theta} \text{ PG}\\
\text { where } \phi_{i}=f_{\theta}\left(\mathcal{M}_{i}\right)  \quad  \leftarrow \text { Adaptation Inner loop : } f_\theta \text{ PG}
$$
<img src="/img/CS285.assets/image-20200329020353543.png" alt="image-20200329020353543" style="zoom:50%;" />



while training:  
&emsp;&emsp;for $$i$$ in tasks:  
&emsp;&emsp;&emsp; 1 sample k episodes $$\mathcal{D}_{i}= \left\{\left(s, a, s', r\right)\right\}_{1:k}$$ from $$\pi_{\theta}$$  
&emsp;&emsp;&emsp; 2 compute adapted parameters  $$\phi_{i}=\theta-\alpha \nabla_{\theta} \mathcal{L}_{i}\left(\pi_{\theta}, \mathcal{D}_{i}\right)$$  
&emsp;&emsp;&emsp; 3 sample k episodes $$\mathcal{D}_{i}^{\prime}=\left\{\left(s, a, s^{\prime}, r\right)_{1: k}\right\}$$ from $$\pi_{\phi}$$  
&emsp;&emsp;update policy parameters  $$\theta \leftarrow \theta-\nabla_{\theta} \sum_{i} \mathcal{L}_{i}\left(\mathcal{D}_{i}^{\prime}, \pi_{\phi_{i}}\right)$$

注意这里是二阶导. Requires second order derivatives!

 

##### How exploration is learned automatically

<img src="/img/CS285.assets/image-20200329021058767.png" alt="image-20200329021058767" style="zoom:50%;" />

$$
\nabla_{\theta} J(\theta)= \mathbb{E}_{\mathcal{T} \sim \rho(\mathcal{T})} \left[ \mathbb{E}_{ \mathop{}^{\boldsymbol{\tau} \sim P_{\mathcal{T}}(\boldsymbol{\tau}  \vert  \theta)}_{\boldsymbol{\tau'} \sim P_{\mathcal{T}}(\boldsymbol{\tau'}  \vert  \theta')}}    \left[\nabla_{\theta} J_{\text {post }}\left(\boldsymbol{\tau}, \boldsymbol{\tau}^{\prime}\right)+\nabla_{\theta} J_{\mathrm{pre}}\left(\boldsymbol{\tau}, \boldsymbol{\tau}^{\prime}\right)\right] \right]
$$

$$
\nabla_{\theta} J_{\mathrm{post}}\left(\tau, \tau^{\prime}\right)=\underbrace{\nabla_{\theta^{\prime}} \log \pi_{\theta}\left(\tau^{\prime}\right) R\left(\tau^{\prime}\right)}_{\nabla_{\theta^{\prime}} J^{\text {outer }}} \underbrace{\left.\left(I+\alpha R(\tau) \nabla_{\theta}^{2} \log \pi_{\theta^{\prime}}(\tau)\right)\right)}_{\text {transformation from } \theta^{\prime} \text { to } \theta}
$$

$$
\nabla_{\theta} J_{\mathrm{pre}}^{I}\left(\boldsymbol{\tau}, \boldsymbol{\tau}^{\prime}\right)=\alpha \nabla_{\theta} \log \pi_{\theta}(\boldsymbol{\tau}) \left(\underbrace{\left(\nabla_{\theta} \log \pi_{\theta}\left(\boldsymbol{\tau}\right) R \left(\boldsymbol{\tau} \right)\right)^{\top}}_{\nabla_{\theta} J^\text { inner }} \underbrace{\left(\nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\boldsymbol{\tau}^{\prime}\right) R\left(\boldsymbol{\tau}^{\prime}\right)\right)}_{\nabla_{\theta^{\prime}} J^{\text {outer }}} \right)
$$

 大括号里面的 **View this as a “return” that encourages gradient alignment**



- Pro: consistent!
- Con: not as expressive

**Q: When could the optimization strategy be less expressive than the recurrent strategy?**

Example: when no rewards are collected, adaptation will not change the policy, even though this data gives information about which states to avoid



<img src="/img/CS285.assets/image-20200329023024998.png" alt="image-20200329023024998" style="zoom:50%;" />



<img src="/img/CS285.assets/image-20200329023042871.png" alt="image-20200329023042871" style="zoom:50%;" />




### Meta-RL on robotic systems

#### Meta-imitation learning

Test: perform task given single **robot demo**  
Training: run **behavior cloning** for adaptation

$$
\theta^{\star}= {\arg \max }_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}[R(\tau)] \quad  \leftarrow \text { Meta-training Outer loop: } {\arg \max }_{\theta} \text{ PG}\\
\text { where } \phi_{i}=f_{\theta}\left(\mathcal{M}_{i}\right)  \quad  \leftarrow \text { Adaptation Inner loop : } f_\theta \text{ Behavior cloning}
$$

$$
\phi_{i}=\theta-\alpha \nabla_{\theta} \sum_{t}\left \Vert \pi_{\theta}\left(o_{t}\right)-a_{t}^{*}\right \Vert ^{2}
$$

<img src="/img/CS285.assets/image-20200329023203804.png" alt="image-20200329023203804" style="zoom: 67%;" />



#### Meta-imitation learning from **human** demos

Test: perform task given single **human demo**   
Training: **learn a loss function** that adapts policy


$$
\theta^{\star}= {\arg \max }_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}[R(\tau)] \quad  \leftarrow \text { Meta-training Outer loop: } {\arg \max }_{\theta} \text{ PG}\\
\text { where } \phi_{i}=f_{\theta}\left(\mathcal{M}_{i}\right)  \quad  \leftarrow \text { Adaptation Inner loop : } f_\theta \text{ Learned loss}
$$

$$
\phi=\theta-\alpha \nabla_{\theta} \mathcal{L}_{\psi}\left(\theta, \mathbf{d}^{h}\right)
$$



<img src="/img/CS285.assets/image-20200329023346235.png" alt="image-20200329023346235" style="zoom: 67%;" />



<img src="/img/CS285.assets/image-20200329023432052.png" alt="image-20200329023432052" style="zoom:50%;" />

Supervised by **paired robot-human demos** only during meta-training!



#### Model-Based meta-RL

1. run base policy $$\pi_{0}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)$$ (e.g., random policy) to collect $$\mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_{i}\right\}$$
2. learn dynamics model $$f(\mathbf{s}, \mathbf{a})$$ to minimize $$\sum_{i}\left \Vert f\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{s}_{i}^{\prime}\right \Vert ^{2}$$
3. plan through $$f(\mathbf{s}, \mathbf{a})$$ to choose actions



<img src="/img/CS285.assets/image-20200329023658066.png" alt="image-20200329023658066" style="zoom: 33%;" />

What if the system dynamics change?  
 \- Low battery  
 \- Malfunction  
 \- Different terrain  
Re-train model? :(



$$
\theta^{\star}= {\arg \max }_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}[R(\tau)] \quad  \leftarrow \text { Meta-training Outer loop: } {\arg \max }_{\theta} \text{ MPC}\\
\text { where } \phi_{i}=f_{\theta}\left(\mathcal{M}_{i}\right)  \quad  \leftarrow \text { Adaptation Inner loop : } f_\theta \text{  Supervised model learning}
$$

<img src="/img/CS285.assets/image-20200329024007493.png" alt="image-20200329024007493" style="zoom:50%;" />



### Aside: POMDPs

<img src="/img/CS285.assets/image-20200329024051274.png" alt="image-20200329024051274" style="zoom: 25%;" />



#### The POMDP view of meta-RL

Two approaches to solve: 1) policy with memory (RNN)  2) explicit state estimation

<img src="/img/CS285.assets/image-20200329024151506.png" alt="image-20200329024151506" style="zoom: 33%;" />



#### Model belief over latent task variables

<img src="/img/CS285.assets/image-20200329024226632.png" alt="image-20200329024226632" style="zoom:50%;" />

<img src="/img/CS285.assets/image-20200329024255897.png" alt="image-20200329024255897" style="zoom:50%;" />



### Solution #3: task-belief states

$$
\theta^{\star}= {\arg \max }_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}[R(\tau)] \quad  \leftarrow \text { Meta-training Outer loop: }  \\
\text { where } \phi_{i}=f_{\theta}\left(\mathcal{M}_{i}\right)  \quad  \leftarrow \text { Adaptation Inner loop : } f_\theta \text{  Stochastic encoder}
$$



<img src="/img/CS285.assets/image-20200329041723257.png" alt="image-20200329041723257" style="zoom:50%;" />



##### posterior sampling in action



##### belief training objective

$$
\mathbb{E}_{\mathcal{T}}\left[\mathbb{E}_{\mathbf{z} \sim q_{\phi}\left(\mathbf{z}  \vert  \mathbf{c}^{\mathcal T}\right)}\left[R(\mathcal{T}, \mathbf{z})+\beta D_{\mathrm{KL}}\left(q_{\phi}\left(\mathbf{z}  \vert  \mathbf{c}^{\mathcal{T}}\right)  \Vert  p(\mathbf{z})\right)\right]\right]
$$

- $$R(\mathcal{T}, \mathbf{z})$$ : “Likelihood” term (Bellman error)
- $$q_{\phi}\left(\mathbf{z}  \vert  \mathbf{c}^{\mathcal{T}}\right)  \Vert  p(\mathbf{z})$$ : Variational approximations to posterior and prior 
- $$D_{\mathrm{KL}}\left(q_{\phi}\left(\mathbf{z}  \vert  \mathbf{c}^{\mathcal{T}}\right)  \Vert  p(\mathbf{z})\right)$$ :  “Regularization” term / information bottleneck



##### encoder design

<img src="/img/CS285.assets/image-20200329044923728.png" alt="image-20200329044923728" style="zoom:50%;" />



Don't need to know the order of transitions in order to identify the MDP (Markov property)  
Use a **permutation-invariant encoder** for simplicity and speed



### Aside: Soft Actor-Critic (SAC)

"Soft": Maximize rewards and entropy of the policy (higher entropy policies explore better)
$$
J(\pi)=\sum_{t=0}^{T} \mathbb{E}_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \rho_{\pi}}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\alpha \mathcal{H}\left(\pi\left(\cdot  \vert  \mathbf{s}_{t}\right)\right)\right]
$$
"Actor-Critic": Model both the actor (aka the policy $$)$$ and the critic (aka the Q-function)
$$
\begin{array}{l}
J_{Q}(\theta)=\mathbb{E}_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \sim \mathcal{D}}\left[\frac{1}{2}\left(Q_{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\hat{Q}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)^{2}\right] \\
J_{\pi}(\phi)=\mathbb{E}_{s_{t}, a_{t}}\left[Q_{\theta}\left(s_{t}, a_{t}\right)+\alpha \mathcal{H}\left(\pi_{\phi}\left(\cdot  \vert  s_{t}\right)\right)\right]
\end{array}
$$
Much more **sample efficient** than on-policy algs.



<img src="/img/CS285.assets/image-20200329045527099.png" alt="image-20200329045527099" style="zoom:33%;" />

SAC Haarnoja et al. 2018, Control as Inference Tutorial. Levine 2018, SAC BAIR Blog Post 2019



#### Soft Actor-Critic

<img src="/img/CS285.assets/image-20200329045639512.png" alt="image-20200329045639512" style="zoom:33%;" />



#### task-belief + SAC

$$
\theta^{\star}= {\arg \max }_{\theta} \sum_{i=1}^{n} E_{\pi_{\phi_{i}}(\tau)}[R(\tau)] \quad  \leftarrow \text { Meta-training Outer loop: } {\arg \max }_{\theta} \text{ SAC}\\
\text { where } \phi_{i}=f_{\theta}\left(\mathcal{M}_{i}\right)  \quad  \leftarrow \text { Adaptation Inner loop : } f_\theta \text{ Stochastic encoder}
$$



<img src="/img/CS285.assets/image-20200329045824934.png" alt="image-20200329045824934" style="zoom:50%;" />



Rakelly & Zhou et al. 2019



### Meta-RL experimental domains

<img src="/img/CS285.assets/image-20200329045914485.png" alt="image-20200329045914485" style="zoom:50%;" />

Simulated via MuJoCo (Todorov et al. 2012), tasks proposed by (Finn et al. 2017, Rothfuss et al. 2019)



<img src="/img/CS285.assets/image-20200329050014566.png" alt="image-20200329050014566" style="zoom:50%;" />

**20-100X more sample efficient!**

<img src="/img/CS285.assets/image-20200329050122787.png" alt="image-20200329050122787" style="zoom:50%;" />

ProMP (Rothfuss et al. 2019), MAML (Finn et al. 2017), RL2 (Duan et al. 2016)



#### two views of meta-RL

##### Mechanistic view

- Deep neural network model that can read in an entire dataset and make predictions for new datapoints
- Training this network uses a meta-dataset, which itself consists of many datasets, each for a different task

##### Probabilistic view

- Extract prior information from a set of (meta-training) tasks that allows efficient learning of new tasks
- Learning a new task uses this prior and (small) training set to infer most likely posterior parameters



#### Summary

<img src="/img/CS285.assets/image-20200329050433651.png" alt="image-20200329050433651" style="zoom:50%;" />



### Frontiers

#### Where do tasks come from?

*Idea: generate self-supervised tasks and use them during meta-training*
$$
\max \mathcal{H}[Z]-\mathcal{H}[Z  \vert  S]+\mathcal{H}[A  \vert  S, Z]
$$

- $$\mathcal{H}[Z  \vert  S]$$ : Separate skills visit different 
- $$\mathcal{H}[A  \vert  S, Z]$$ : Skills should be  high entropy states



**Limitations**  
Assumption that skills shouldn’t depend on action not always valid  
Distribution shift meta-train -> meta-test

<img src="/img/CS285.assets/image-20200329051119088.png" alt="image-20200329051119088" style="zoom:50%;" />

Eysenbach et al. 2018, Gupta et al. 2018



#### How to explore efficiently in a new task?

Learn exploration strategies better...

<img src="/img/CS285.assets/image-20200329051224564.png" alt="image-20200329051224564" style="zoom: 50%;" />



Bias exploration with extra information...

<img src="/img/CS285.assets/image-20200329051259851.png" alt="image-20200329051259851" style="zoom: 33%;" />

Gupta et al. 2018, Rakelly et al. 2019, Zhou et al. 2019



### Online meta-learning

Meta-training tasks are presented in a sequence rather than a batch

<img src="/img/CS285.assets/image-20200329051402194.png" alt="image-20200329051402194" style="zoom:50%;" />

Finn et al. 2019



### Summary

- Meta-RL finds an adaptation procedure that can quickly adapt the policy to a new task
- Three main solution classes: RNN, optimization, task-belief and several learning paradigms: model-free (on and off policy), model-based, imitation learning

- Connection to goal-conditioned RL and POMDPs
- Some open problems (there are more!): better exploration, defining task distributions, meta-learning online



### References

- **Recurrent meta-RL**
  - Learning to Reinforcement Learn, Wang et al. 2016
  - Fast Reinforcement Learning by Slow Reinforcement Learning, Duan et al. 2016 
  - Memory-Based Control with Recurrent Neural Networks, Heess et al. 2015

- **Optimization-based meta-RL**
  - Model-Agnostic Meta-Learning, Finn et al. 2017 Proximal Meta-Policy Search, Rothfuss et al. 2018
- **Optimization-based meta-RL + imitation learning**
  - One-Shot Visual Imitation Learning via Meta-Learning, Yu et al. 2017
  - One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning, Yu et al. 2018
- **Model-based meta-RL**
  - Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning, Nagabandi et al. 2019
- **Off-policy meta-RL**
  - Soft Actor-Critic, Haarnoja et al. 2018
  - Control as Inference, Levine 2018.

  - Efficient Off-Policy Meta-RL via Probabilistic Context Variables, Rakelly et al. 2019
- **Open Problems**
  - Diversity is All You Need: Learning Skills without a Reward Function, Eysenbach et al. 2018 
  - Unsupervised Meta-learning for RL, Gupta et al. 2018
  - Meta-Reinforcement Learning of Structured Exploration Strategies, Gupta et al. 2018 Watch, Try, 
  - Learn, Meta-Learning from Demonstrations and Reward, Zhou et al. 2019 Online Meta-Learning, Finn et al. 2019
- **Slides and Figures**
  - Some slides adapted from Meta-Learning Tutorial at ICML 2019, Finn and Levine 














