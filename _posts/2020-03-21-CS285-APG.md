---
layout:     post
title:      CS 285. Advanced Policy Gradients
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

## Advanced Policy Gradients

##### Recap: policy gradients

REINFORCE (Williams, 1992):

1. 运行策略$\pi_\theta(\mathbf{a} \vert \mathbf{s})$，抽取样本$$\{\tau^i\}$$
2. 估计梯度$$\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t} \vert \mathbf{s}_{i,t})\right)\left(\sum_{t=1}^Tr(\mathbf{s}_{i,t},\mathbf{a}_{i,t})\right)\right]$$
3. $\theta\leftarrow\theta+\alpha\nabla_\theta J$


$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t}) \underbrace{\hat {Q^\pi} _{i,t}}_{\text{reward to go} }
$$

can also use function approximation here, AC

#### Why does policy gradient work?

为什么PG算法能improve , 因为是梯度,可以上升
$$
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right) \hat{A}_{i, t}^{\pi}
$$
下面使用一个更框架的角度来看.  

PG 算法: 

1. Estimate $$\hat{A}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$ for current policy $\pi$
2. Use $$\hat{A}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$ to get improved policy $\pi^{\prime}$

policy iteration algorithm:   PG与PI的两个模块是比较类似的

1. evaluate $A^{\pi}(\mathbf{s}, \mathbf{a})$
2. set $\pi \leftarrow \pi^{\prime}$



#### Policy gradient as policy iteration

要比较新旧两个策略的优劣, 同时希望新策略能比旧策略尽可能的好. 先看看两者的差是啥.

下面证明, 第一步没啥问题, 从s0出发, 然后期望的分布切换为 新策略的轨迹,  因为新策略的轨迹, 其实的s0与旧策略是一样的, 状态s0的分布跟具体策略没关系.  然后在s0上做文章,展开, 纯数学计算, 然后得到结论. 

$$
\begin{aligned}
J\left(\theta^{\prime}\right)-J(\theta) &=J\left(\theta^{\prime}\right)-E_{\mathbf{s}_{0} \sim p\left(\mathbf{s}_{0}\right)}\left[V^{\pi_{\theta}}\left(\mathbf{s}_{0}\right)\right] \\
&=J\left(\theta^{\prime}\right)-E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[V^{\pi_{\theta}}\left(\mathbf{s}_{0}\right)\right] \\
&=J\left(\theta^{\prime}\right)-E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)-\sum_{t=1}^{\infty} \gamma^{t} V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right] \\
&=J\left(\theta^{\prime}\right)+E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right] \\
&=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]+E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right] \\
&=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma V^{\pi_{\theta}}\left(\mathbf{s}_{t+1}\right)-V^{\pi_{\theta}}\left(\mathbf{s}_{t}\right)\right)\right] \\
&=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t=0}^{\infty} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
\end{aligned}
$$

重要结论,  新策略只要在遇到的每个s下, 选最大的A, 就能让新策略比旧策略提升最大. 有点反直觉. 为什么是在新策略下轨迹的A , 而不是旧的,或者两者结合的一个轨迹分布. 

$$
J\left(\theta^{\prime}\right)-J(\theta)=E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
$$

这个公式并没有使得计算变的简单, 之前求$J(\theta')$ 需要在$p_{\theta'}(\tau)$上求期望 , 现在求A 仍然需要. 要求这个新公式的梯度也比较困难, 而这需要在新策略下做很多的sample.

现在转换角度, 如果都可以在$\theta$下采样, 那么就可以利用现有策略的很多sample, 很快比较很多新策略的优劣. 

所以要用到 importance sampling. 
$$
\begin{aligned}
E_{\tau \sim p_{\theta^{\prime}}(\tau)}\left[\sum_{t} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] &=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right] \\
&=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
\end{aligned}
$$

跟之前不一样, 还里没有 exponential in T 的问题 , 因为只有一步, 外面都是加起来的.  

##### Ignoring distribution mismatch? 

但这里外面还是有状态s服从$p_\theta$ 分布的问题.  搞个近似 用$p_\theta$代替$p_{\theta'}$ 看看会怎么样. 


$$
\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]  \approx \underbrace{\sum_{t} E_{\mathbf{s}_{t} \sim \color{red}{p_{\theta}} \left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]  }_{\bar A(\theta')}
$$

why do we want this to be true?  如果近似可行, 则等价以下问题

$J\left(\theta^{\prime}\right)-J(\theta) \approx \bar{A}\left(\theta^{\prime}\right) \quad \Rightarrow \quad \theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \bar{A}(\theta)$. 

则可以用  Use $$\hat{A}^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$ to get improved policy $\pi^{\prime}$

is it true? and when? 下面讨论近似的假设是不是成立.  如果两个策略很接近, 感觉近似就成立



##### Bounding the distribution change

- Claim: $$p_{\theta}\left(\mathbf{s}_{t}\right)$$ is close to $$p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)$$ when $\pi_{\theta}$ is close to $$\pi_{\theta^{\prime}}$$  
- 那究竟什么定义离得近close,  total variance divergence

- 先看简单情况, 确定性策略.  Simple case: assume $\pi_{\theta}$ is a **deterministic** policy $$\mathbf{a}_{t}=\pi_{\theta}\left(\mathbf{s}_{t}\right)$$ . 

- $\pi_{\theta^{\prime}}$ is close to $\pi_{\theta}$ if $$\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \neq \pi_{\theta}\left(\mathbf{s}_{t}\right) \vert \mathbf{s}_{t}\right) \leq \epsilon$$. 
- $$\left.p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)=(1-\epsilon)^{t} p_{\theta}\left(\mathbf{s}_{t}\right)+\left(1-(1-\epsilon)^{t}\right)\right) p_{\text {mistake }}\left(\mathbf{s}_{t}\right)$$  第一项是一直没出错的概率,完全按照策略走, 第二项是其他未知的分布 , 参考之前模仿学习走钢丝的那个例子
- 所以可以把 $p_{\theta'}$ 与 $p_\theta$ 不一致的地方, 称为$p_{\theta'}$ 出错, 不agree
- $$\left|p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right|=\left(1-(1-\epsilon)^{t}\right)\left|p_{\text {mistake }}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right| \leq 2\left(1-(1-\epsilon)^{t}\right) \leq 2 \epsilon t$$
  - useful identity: $(1-\epsilon)^{t} \geq 1-\epsilon t$ for $\epsilon \in[0,1]$
  - **not a great bound, but a bound!**

- 更一般的情况.   general case: assume $\pi_{\theta}$ is an **arbitrary** distribution 
- $\pi_{\theta^{\prime}}$ is close to $\pi_{\theta}$ if  $$\left \vert \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} \vert  \mathbf{s}_{t}\right)\right \vert  \leq \epsilon$$  for all $$\mathbf{s}_{t}$$ 
- 对于随机策略的情况, 怎么判断不一致以及策略本身的随机.   感觉像是猜拳, 一个出石头的几率是0.4, 一个出石头几率是0.3 , 则还是能看出来的
- 直观的例子, 伪随机数, 如果对同一个随机数种子, run $\theta$ 和 $\theta'$ , 两者一样的几率, 也可以bound by $\epsilon$. 

Useful lemma:  联系随机数的例子

- if $\left \vert p_{X}(x)-p_{Y}(x)\right \vert=\epsilon$,  exists $p(x, y)$  存在联合分布
- such that $p(x)=p_{X}(x)$ and $p(y)=p_{Y}(y)$ and $p(x=y)= \epsilon$ 
- $\Rightarrow p_{X}(x)$ "agrees" with $p_{Y}(y)$ with probability $\epsilon$ 
- $$\Rightarrow \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)$$ takes a different action than $$\pi_{\theta}\left(\mathbf{a}_{t} \vert\mathbf{s }_{t}\right)$$ with probability at most $\epsilon$

随机策略的结论与确定性策略的一样.

$$\left|p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right|=\left(1-(1-\epsilon)^{t}\right)\left|p_{\text {mistake }}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right| \leq 2\left(1-(1-\epsilon)^{t}\right) \leq 2 \epsilon t$$



#### Bounding the objective value

现在看, 如果两个策略很接近, 则目标回报会相差多少. 

- $\pi_{\theta^{\prime}}$ is close to $\pi_{\theta}$ if $$\left \vert \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)\right \vert \leq \epsilon$$ for all $$\mathbf{s}_{t}$$. 
- $$\left \vert p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right \vert \leq 2 \epsilon t$$ . 
- 假定 f(s) >= 0 , 然后就是 切换期望, 假定从$\theta'$ 到 $\theta$ ,发生了最坏的情况发生, 回报尽可能的下跌.

$$
 \begin{aligned}
E_{p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[f\left(\mathbf{s}_{t}\right)\right]=\sum_{\mathbf{s}_{t}} p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right) f\left(\mathbf{s}_{t}\right) & \geq \sum_{\mathbf{s}_{t}} p_{\theta}\left(\mathbf{s}_{t}\right) f\left(\mathbf{s}_{t}\right)-\left|p_{\theta}\left(\mathbf{s}_{t}\right)-p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)\right| \max _{\mathbf{s}_{t}} f\left(\mathbf{s}_{t}\right) \\
& \geq E_{p_{\theta}\left(\mathbf{s}_{t}\right)}\left[f\left(\mathbf{s}_{t}\right)\right]-2 \epsilon t \max _{\mathbf{s}_{t}} f\left(\mathbf{s}_{t}\right)
\end{aligned}  
$$

-  将importance sampling 的部分代入上面的 f(s)

$$
\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right] \geq \\
\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta }\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right] -\sum_{t} 2 \epsilon t C
$$

-  $C$ 的数量级为 : $O(T r_{\max})$  or $O( \frac{r_{\max}}{1-\gamma})$   看是不是episodic , 最大可能得到的reward
- 上面 $\sum_{t} 2 \epsilon t C$ 的数量级是 quadratic in horizon , 时间t的二次方 , 或者  $\frac{1}{1-\gamma}$的二次方 
- tip, 在强化学习分析中, 对infinity任务, 如果看到$\frac{1}{1-\gamma}$, 当作horizon, 时间t
- 虽然这个bound很宽松, 但足够推导一些方法. 



#### Where are we at so far? 

$$
\begin{aligned}
&\theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \overbrace{   \sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)} \left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right] }^{\bar A(\theta')}\\
& \text { such that }\left|\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right| \leq \epsilon
\end{aligned}
$$


for small enough $\epsilon,$ this is guaranteed to improve $J\left(\theta^{\prime}\right)-J(\theta)$

所以 $\theta'$ 要使得 后面的一串$\bar A(\theta')$, argmax ,   因为已经有了当前策略的很多sample,所以可以计算$\bar A(\theta')$的梯度,  后面只有一处出现$\theta'$, 所以很好计算, 可以使用 automatic differentiation. 但是有限制constraint. 

同时又必须离 $\theta$ 很近. 才能保证 improve > 一个小的负数



#### A more convenient bound

之前虽然bound找到了, 但是 total variance divergence 只在理论分析上好用, 在算法中很难用. 

- Claim: $$p_{\theta}\left(\mathbf{s}_{t}\right)$$ is close to $$p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)$$ when $\pi_{\theta}$ is close to $$\pi_{\theta^{\prime}}$$  
- $\pi_{\theta^{\prime}}$ is close to $\pi_{\theta}$ if $$\left \vert \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)\right \vert \leq \epsilon$$ for all $$\mathbf{s}_{t}$$. 
- $$\left \vert p_{\theta^{\prime}}\left(\mathbf{s}_{t}\right)-p_{\theta}\left(\mathbf{s}_{t}\right)\right \vert \leq 2 \epsilon t$$ . 

**KL divergence** has some very convenient properties that make it much easier to approximate!

$$
D_{\mathrm{KL}}\left(p_{1}(x) \| p_{2}(x)\right)=E_{x \sim p_{1}(x)}\left[\log \frac{p_{1}(x)}{p_{2}(x)}\right]
$$

- a more convenient bound: $$\left \vert \pi_{\theta^{\prime}}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)-\pi_{\theta}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right \vert  \leq \sqrt{\frac{1}{2} D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)  \Vert  \pi_{\theta}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right)}$$
- 
  $$\Rightarrow D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)  \Vert  \pi_{\theta}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right)$$ bounds state marginal difference 



#### How do we optimize the objective?

$$
\begin{aligned}
& \theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)} \left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right] \\
& \text { such that }D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)  \Vert  \pi_{\theta}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)\right) \leq \epsilon
\end{aligned}
$$


for small enough $\epsilon,$ this is guaranteed to improve $J\left(\theta^{\prime}\right)-J(\theta)$



##### How do we enforce the constraint?

optimize the surrogate objective with $p(\theta)$ . 

an objective and a constraint.   拉格朗日法. 


$$
\mathcal{L}\left(\theta^{\prime}, \lambda\right)=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} A^{\pi_{\theta}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\right]\right]-\lambda\left(D_{\mathbf{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \| \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)-\epsilon\right)
$$

1. Maximize $\mathcal{L}\left(\theta^{\prime}, \lambda\right)$ with respect to $\theta^{\prime}$    -- **can do this incompletely (for a few grad steps)**
2. $$\lambda \leftarrow \lambda+\alpha\left(D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right) \Vert \pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)\right)-\epsilon\right)$$. 
  

- Intuition: raise $\lambda$ if constraint violated too much, else lower it.   如果约束条件KL散度太大, 增加$\lambda$ ,否则缩小
- an instance of **dual gradient descent** 
- 以上是一个可行的算法



#### How (else) do we optimize the objective?

<img src="/img/CS285.assets/image-20200322053736055.png" alt="image-20200322053736055" style="zoom:50%;" />

trust region

$$
\begin{aligned}
&\theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \nabla_{\theta} \bar{A}(\theta)^{T}\left(\theta^{\prime}-\theta\right)\\
&\text { such that } D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right) \Vert \pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)\right) \leq \epsilon
\end{aligned}
$$

不去优化整条曲线, 只在小范围内, 优化一阶导数.  **Use first order Taylor approximation for objective (a.k.a., linearization)**



##### How do we optimize the objective?

$$
\nabla_{\theta^{\prime}} \bar{A}\left(\theta^{\prime}\right)=\sum_{t} E_{\mathrm{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
$$

下面, 求grad A at  $\theta$  ,  **exactly the normal policy gradient!** 就是PG
$$
\nabla_{\theta} \bar{A}(\theta)=\sum_{t} E_{\mathrm{s}_{t} \sim p_{\theta}\left(\mathrm{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathrm{s}_{t}\right)}\left[\frac{ {\pi}_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)} \gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]
$$

$$
\nabla_{\theta} \bar{A}(\theta)=\sum_{t} E_{\mathbf{s}_{t} \sim p_{\theta}\left(\mathbf{s}_{t}\right)}\left[E_{\mathbf{a}_{t} \sim \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\left[\gamma^{t} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) A^{\pi_{\theta}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]\right]=\nabla_{\theta} J(\theta)
$$

那么让$\theta'$ 沿着 $\theta$ 梯度的方向一直提升, 尽可能的更新 $\theta$,直到 KL散度的约束边界.

##### Can we just use the gradient then?

$\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$    ;         $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)$$ .
some parameters change probabilities a lot more than others!

那能否直接使用PG呢, 不行. 因为对于$\theta$直接沿着PG方向走一步, 会 violate the constraint for kind of a subtle reason .  对于神经网络表达的策略函数$$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{s}_{t}\right)$$,  $\theta$对$\pi$的影响很复杂,  很可能改动一点点, 策略发生很大的变化, 也可能改动很大, 策略没啥变化.  所以走定长, 很容易破坏约束. 



下面讨论另外一个不相关的约束条件的情况, 二次欧式距离的约束, 主要是为了体现二次的优势

<img src="/img/CS285.assets/image-20200322062123109.png" alt="image-20200322062123109" style="zoom:33%;" />


Claim: gradient ascent does this:
- $$\theta^{\prime} \leftarrow \arg \max _{\theta^{\prime}} \nabla_{\theta} J(\theta)^{T}\left(\theta^{\prime}-\theta\right)$$   ,   such that $\left\|\theta-\theta^{\prime}\right\|^{2} \leq \epsilon$  

- $$\theta^{\prime}=\theta+\sqrt{\frac{\epsilon}{\left\|\nabla_{\theta} J(\theta)\right\|^{2}}} \nabla_{\theta} J(\theta)$$ . 

显然上面的约束就是 $\theta'$在$\theta$为圆心的一个圆内,  那么沿着梯度方向, 并且走到圆的边缘,就是这次可以improve并且符合约束的最大的点 .  该点的坐标就是上式. 



那么是否可以转变约束条件.  变成这种很好理解计算的二次型.   一个方案就是 二阶泰勒展开. 

<img src="/img/CS285.assets/image-20200322171432469.png" alt="image-20200322171432469" style="zoom:33%;" />

KL散度的泰勒二阶近似公式如下: 

$$
D_{\mathrm{KL}}\left(\pi_{\theta^{\prime}} \| \pi_{\theta}\right)   \approx \frac{1}{2}\left(\theta^{\prime}-\theta\right)^{T} \mathbf{F}\left(\theta^{\prime}-\theta\right)
$$

$\mathbf{F}$ : **Fisher-information matrix** , can estimate with samples 

$$
\mathbf{F}=E_{\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(\mathbf{a} | \mathbf{s}) \nabla_{\theta} \log \pi_{\theta}(\mathbf{a} | \mathbf{s})^{T}\right]
$$

转为二次型, 形状大概就是一个椭圆. 

代入上的二次型的梯度公式:   得到 **natural gradient**  
$$
\theta ' = \theta + \alpha \mathbf F^{-1}\nabla_{\theta} J(\theta)
$$

$$
\alpha=\sqrt{\frac{2 \epsilon}{\nabla_{\theta} J(\theta)^{T} \mathbf{F} \nabla_{\theta} J(\theta)}}
$$

直觉解释:  要 normalize 规范J的影响. 如果J 太大, 则可以限制步长不会太大

为什么要叫 **natural gradient**  ,  是gradient ascent 在概率空间 probability space.



#### Is this even a problem in practice?

看看怎么用 natural gradient 来解决实际问题

<img src="/img/CS285.assets/image-20200322201456194.png" alt="image-20200322201456194" style="zoom:50%;" />一个例子, s,a都是连续的.  目标点星星在s=0 的点,  a就是选择朝左朝右走, 大小代表走的距离, 正负代表方向.   下面的 奖励函数, 是两个惩罚项, 一个是惩罚离中心太远, 然后就是惩罚动作幅度太大. 策略函数是个正太分布, 有两个超参数 , 两个超参拼成向量$\theta$.  策略的取值,就是以a的各种取值作为坐标轴,  ks当前位置为中心点的一个正态分布. 
$$
\begin{array}{l}
r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=-\mathbf{s}_{t}^{2}-\mathbf{a}_{t}^{2} \\
\log \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=-\frac{1}{2 \sigma^{2}}\left(k \mathbf{s}_{t}-\mathbf{a}_{t}\right)^{2}+\text { const } \quad \theta=(k, \sigma)
\end{array}
$$
假设起点就是图上左边的位置, 那么刚开始之后, 策略是比较随机的, 左右乱动, 通过不断调整参数k, 使得能一直向右, 使得走到原点.  这里的参数$\sigma$ , 应该是越来越小的, 变成0以后, 策略就变成了一个确定性的最优策略.  所以$\sigma$的梯度是一直向下的. 

<img src="/img/CS285.assets/image-20200322213341645.png" alt="image-20200322213341645" style="zoom: 33%;" />

上图左边, 就是二维参数的梯度, 最优点就是 $k=-1, \sigma = 0$ .  而且因为sigma是二次的,所以当sigma接近0的时候, 梯度还是指向下的. 因为是-3次方的, 越接近0, 梯度值越大.  这是一个问题. 



<img src="/img/CS285.assets/image-20200322225921850.png" alt="image-20200322225921850" style="zoom: 33%;" />

有点类似这个情况, 要优化一个ill-conditioned function. 完全按照梯度, 会走zigzag.  

所以上面的sigma在接近0的时候, 相当于在一个深谷里面, 但又不能取小于0 ,所以只能慢慢挪, 要挪到目标点会特别慢.   这是高斯策略的一个大问题. 神经网络也是问题. 

该问题的 , natural gradient 去掉了这些 ill-condition 



#### Practical methods and notes

- **Natural policy gradient**
  -  $$\theta ' = \theta + \alpha \mathbf F^{-1}\nabla_{\theta} J(\theta)$$,  $\alpha$是手工选择,或者放Adam 自动选择
  - Generally a good choice to **stabilize policy gradient training**
  - Peters, Schaal. Reinforcement learning of motor skills with policy gradients.
- Practical implementation: requires **efficient Fisher-vector products**, a bit non-trivial to do without computing the full matrix
  - See: Schulman et al. Trust region policy optimization 
- **Trust region policy optimization** 
  - natural PG 的一个变体,  $$\alpha=\sqrt{\frac{2 \epsilon}{\nabla_{\theta} J(\theta)^{T} \mathbf{F} \nabla_{\theta} J(\theta)}}$$
  - 然后选择 $\epsilon$ . 

- Just use the IS objective directly.  用拉格朗日法来优化
  - Use regularization to stay close to old policy 
  - See: **Proximal policy optimization**





### Review

- Policy gradient = policy iteration
- Optimize advantage under new policy state distribution
- Using old policy state distribution optimizes a bound, *if* the policies are close enough
- Results in *constrained* optimization problem
- First order approximation to objective = gradient ascent
- Regular gradient ascent has the wrong constraint, use **natural gradient**
- Practical algorithms
  - Natural policy gradient
  - Trust region policy optimization










