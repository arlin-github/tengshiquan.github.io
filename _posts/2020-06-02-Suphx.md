---
layout:     post
title:      Suphx
subtitle:   Suphx &#58; Mastering Mahjong with Deep Reinforcement Learning
date:       /img/2020-06-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-dice.jpg"
catalog: true
tags:
    - AI
    - Imperfect Information
    - 麻将 
    - MCTS

---



# Suphx

from 微软亚洲研究院.  目前天凤平台段位最高的AI. 打的是日本规则.

思路:  CNN(resnet), 牌谱ML +  self-play  RL

运行时,  pretrained model + MCTS online train 





![img](/img/2020-06-02-Suphx.assets/d5c7ae6afd5d40889c44c92b7144cc43.jpeg)









### Introduction

Mahjong : multi-player imperfect-information zero-sum.



首先，麻将有复杂的计分规则。每一局麻将都包含多个回合，而最终排名（奖励）是由这些回合的累积得分决定的。一轮比赛的失利并不总是意味着玩家该轮比赛的发挥不佳（例如，如果玩家在前几轮比赛中占据了很大的优势，那么他可能会战术性地输掉最后一轮比赛，以确保游戏的排名第一），因此我们不能直接将轮次得分作为学习的反馈信号。此外，麻将有大量可能的听牌。这些听牌之间可以有很大的不同，不同的听牌会导致该轮的胡牌分数不同。这样的计分规则比以前研究的游戏包括象棋、围棋等要复杂得多。职业棋手需要认真选择形成什么样的牌型，以换取本轮的赢牌概率和赢牌分数。

其次，在麻将中，每个玩家手中最多有13张牌，其他玩家是不可见的，而王山中有14张牌，在整个游戏过程中，所有玩家都是不可见的，而牌山中有70张牌，一旦被玩家抽出和丢弃，这些牌就会变得可见。因此，平均每一个信息集（玩家的一个决策点），就有超过**10^48个隐藏状态**。如此庞大的隐藏信息集，使得麻将成为比之前研究的德州扑克等不完全信息游戏难度更大的游戏。麻将玩家很难只根据自己的私牌来决定哪个动作是好的，因为一个动作的好坏高度依赖于其他玩家的私牌和每个人都看不见的牌山。因此，人工智能也很难将奖励信号与观察到的信息联系起来。

第三，麻将的游戏规则复杂: (1) 有不同类型的动作，包括立直, 吃，碰，杠等(2) 正常的游戏顺序可以被吃、碰、杠、抢杠、点铳中断。因为每个玩家最多可以拥有13张牌，所以很难预测这些中断，以至于我们甚至无法构建一个普通的game tree; 即使我们构建了一个游戏树，树的节点在玩家的两个连续动作之间也会有大量的路径。因此, 这阻止了先前技术的直接应用，比如MCTS 和CFR 。





Suphx采用CNN作为模型。首先通过对人类职业玩家的日志进行监督学习来训练网络，然后通过自我游戏强化学习（RL）来提升策略网络。使用 policy gradient 来进行自我游戏RL，并引入几种技术来解决上述挑战。
1. 全局奖励预测，训练一个预测器来预测游戏的最终奖励（在未来几轮之后）。这个预测器提供了有效的学习信号，从而可以进行策略网络的训练。此外，我们还设计了look-ahead特征，以编码不同胡牌的可能性以及他们的分数，为我们RL的决策提供支持。   **相当于一个value network**
2. Oracle guiding, 引入了一个Oracle代理，它可以看到完美的信息，包括其他玩家的私人牌和王山牌。由于（不公平的）完美信息获取，这个Oracle代理是一个超强的麻将AI。在我们的RL训练过程中，我们逐渐放弃Oracle的完美信息，最后将其转换为一个只接受可观察信息作为输入的普通代理。在Oracle的帮助下，我们的普通代理比只利用可观察信息的标准RL训练提高得更快。 **该思路比较好, 提升效率**
3. 由于麻将复杂的玩法规则导致游戏树不规则，无法应用蒙特卡洛树搜索技术，我们引入了参数化蒙特卡洛策略自适应 **parametric Monte-Carlo policy adaptation（pMCPA）**来提高我们代理的运行时性能，当回合进行到一定程度，可观察到的信息较多时，pMCPA会逐渐修改和适应离线训练的策略，以适应特定的回合。(如四个玩家丢弃的牌)  .   **关键: 离线训练的策略还是需要 online的MCTS来修正, 特别是靠近残局**




三。由于麻将游戏规则复杂，导致游戏树不规则，阻碍了蒙特卡罗树搜索技术的应用，我们引
入参数蒙特卡罗策略自适应(pMCPA) 的方法来提高代理的运行时性能。pMCPA逐渐修改和
适应线下训练的策略，以适应在线上游戏阶段的特定回合以及在游戏时有更多的信息可观察
(如四个玩家丢弃的牌)。



### Overview of Suphx

<img src="/img/2020-06-02-Suphx.assets/image-20200616033643695.png" alt="image-20200616033643695" style="zoom:50%;" />

#### Decision Flow

- 如果这不是游戏的最后一轮，则Ron或者自摸;
- 如果这是游戏的最后一轮，  如果Ron或者自摸后，全场得分是四名选手中最低的，那就
  放弃; 否则，Ron或者自摸。 继续挣扎..





麻将中,玩家可以action的点, 见下图,  可见都是在进模型之前都是用规则判断的. 并没有用模型尝试去学习规则.  比如 , 判断能吃后, 让吃模型判断要不要吃; 而不是让模型判断能不能吃本身.

1. 摸牌
2. 别人出牌后



![image-20200616032724092](/img/2020-06-02-Suphx.assets/image-20200616032724092.png)





### Features and Model Structures



![image-20200616043952015](/img/2020-06-02-Suphx.assets/image-20200616043952015.png)



输入编码:  34*N



![image-20200616044109914](/img/2020-06-02-Suphx.assets/image-20200616044109914.png)



由于日麻中有34种牌，因此使用多个34×1通道表示一个状态。 如图3所示，我们使用四个通道对玩家的私牌进行编码。副露、宝牌和弃牌的顺序被编码到其他通道中。 分类要素被编码为多通道，每个通道要么全为0要么全为1。 数字特征被分割成多个buckets，每个bucket都使用一个通道编码，每个通道都是用0或1表示。    **非手牌的特征编码是一长条的1或者0**

 

**手工推理特征. 这块非常重要.**  除了直接观察到的信息外，我们还设计了一些前瞻功能和胡牌的概率/得点。在日本的麻将中，一副胡牌包含**4张面子和1张对子**。有**89种顺子和34种对子**，也就是大量不同的可能赢牌组合。此外，根据复杂的计分规则，这些手牌会影响终局顺位。因此，为了降低计算复杂度，我们在提取**前瞻特征**时做了一些简化处理。(1) 用深度优先搜索来寻找可能的胡牌。(2) 忽略对手的行为，只考虑自己的draw和discard。通过这些简化，我们得到了100多个前瞻特征，每个特征对应一个34维向量。例如，一个特征代表了一张指定的出牌是否能三向听胡12000分。在Suphx中，除了输入层和输出层的尺寸（表2）外，所有的模型（即弃牌/立直/吃/碰/杠模型）都使用类似的网络结构（图4和图5）。discard模型有34个输出神经元对应于34张牌，立直/吃/碰/杠模型只有两个输出神经元对应是否采取某种行动。除了状态信息和前瞻特征外，吃/碰/杠模型的输入中还包含了牌的信息(**action in**)。注意，我们的模型中**没有池化层**，因为通道的每一列都有它的语义，池化会导致信息丢失。

![image-20200616114000982](/img/2020-06-02-Suphx.assets/image-20200616114000982.png)





![image-20200616114051549](/img/2020-06-02-Suphx.assets/image-20200616114051549.png)



### Learning Algorithm

Suphx的学习包含三个主要步骤。  ML + RL + perfect + online MCTS

首先，我们通过监督学习训练Suphx的五种模型，使用从天凤平台上收集到的顶级人类玩家的（状态、动作）进行训练。其次，我们通过self-play 强化学习（RL）对监督模型进行改进。我们采用了流行的policy gradient算法（3.1节），并引入了全局奖励预测（3.2节）和oracle guiding（3.3节），以处理麻将的特殊挑战。第三，在在线游戏过程中，我们采用了运行时策略适应（3.4节），利用对当前回合的新观察到的的数据，去获得更好的表现。



#### 3.1 Distributed Reinforcement Learning with Entropy Regularization

Suphx的训练是基于分布式强化学习。具体来说，我们采用了策略梯度，并利用重要性采样来处理异步分布式训练所导致的滞后问题。

$$
\mathcal{L}(\theta)=\underset{s, a \sim \pi_{\theta^{\prime}}}{\mathrm{E}}\left[\frac{\pi_{\theta}(a | s)}{\pi_{\theta^{\prime}}(a | s)} A^{\pi_{\theta}}(s, a)\right]
$$

where $\theta^{\prime}$ is (the parameters of ) an old policy generating trajectories for training, $\theta$ is the latest policy to update, and $A^{\pi_{\theta}}(s, a)$ is the advantage of action $a$ at state $s$ with respect to policy $\pi_{\theta}$.



我们发现RL训练对策略的熵很敏感。如果熵太小，RL训练收敛速度快，不会显著改善策略；如果熵太大，RL训练变得不稳定，学习到的策略方差大。因此， 对RL训练过程中的策略熵进行正则化处理，具体方法如下。

$$
\nabla_{\theta} J\left(\pi_{\theta}\right)=\underset{s, a \sim \pi_{\theta^{\prime}}}{\mathrm{E}}\left[\frac{\pi_{\theta}(s, a)}{\pi_{\theta^{\prime}}(s, a)} \nabla_{\theta} \log \pi_{\theta}(a | s) A^{\pi_{\theta}}(s, a)\right]+\alpha \nabla_{\theta} H\left(\pi_{\theta}\right) 
$$

这里 $H\left(\pi_{\theta}\right)$ 是策略的熵,  $\alpha>0 $ is a trade-off coefficient.  

为了确保稳定的exploration，如果我们的政策熵在最近一段时间内小于或大于目标H_target，我们会动态调整α以增加/减少熵项

$$
  \alpha \leftarrow \alpha+\beta\left(H_{\text {target }}-\bar{H}\left(\pi_{\theta}\right)\right) 
$$

$\bar{H}\left(\pi_{\theta}\right)$ 是最近一段时间trajectories的empirical entropy.   



Suphx使用的分布式RL系统如图6所示。该系统由多个self-play workers组成，每个包含一组基于CPU的麻将模拟器和一组基于GPU的推理引擎来进行学习。策略$π_θ$的更新与学习过程是解耦的。

在训练过程中，麻将模拟器随机初始化一个游戏，RL代理作为自家和其他三个对手打牌。当这四个玩家中的任何一个需要采取动作时，模拟器会将当前状态（用特征向量表示）发送到GPU推理引擎，然后GPU推理引擎返回一个动作给模拟器。GPU推理引擎会定期从参数服务器上更新策略。



<img src="/img/2020-06-02-Suphx.assets/image-20200616115555138.png" alt="image-20200616115555138" style="zoom:50%;" />



#### 3.2 Global Reward Prediction

每局游戏都包含多个回合，如天凤是8-12个回合（东一到西四）. 每轮结束时，玩家会获得分数，8-12轮后，玩家会获得游戏奖励。不过，无论是回合分数还是游戏奖励都不是RL训练的好信号。

- 由于同一游戏中的多轮游戏共享同一游戏奖励，因此，将游戏奖励作为反馈信号，无法区分玩得好的轮次和玩得差的轮次。因此，人们应该更好地分别衡量每个回合的表现。
- 虽然回合得分是针对每一个单独的回合计算的，但未必能反映出行动的好坏，尤其是对于顶级职业选手来说。例如，在比赛的最后一两轮，排名第一的棋手在累计回合分数上领先很多，通常会变得比较保守，可能会故意让排名第三或排名第四的棋手赢下这一轮，这样就可以安全地保持总排名第一。也就是说，一轮负分不一定意味着政策不好：有时可能反映出某些战术，从而对应相当好的政策。



因此，为了给RL训练提供有效的信号，我们需要将最终的游戏奖励（全局奖励）适当地归属于每一轮游戏。为此，我们引入了一个全局奖励predictor $Φ$，该predictor给定本轮游戏和本轮游戏之前所有轮次的信息，预测最终的游戏奖励。在Suphx中，奖励预测器$Φ$是一个循环神经网络，更具体地说，是一个两层门控循环单元（GRU），后面是两个全连接层，如图7所示。



<img src="/img/2020-06-02-Suphx.assets/image-20200616115631094.png" alt="image-20200616115631094" style="zoom: 33%;" />



训练数据来自天风顶级玩家日志, 通过MSE loss训练.
$$
\min \frac{1}{N} \sum_{i=1}^{N} \frac{1}{K_{i}} \sum_{j=1}^{K_{i}}\left(\Phi\left(x_{i}^{1}, \cdots, x_{i}^{j}\right)-R_{i}\right)^{2}
$$
where $N$ denotes the number of games in the training data, $R_{i}$ denotes the **final**
**game reward** of the $i$ -th game, $K_{i}$ denotes the number of rounds in the $i$ -th game,
$x_{i}^{k}$ denotes the feature vector of the $k$ -th round in the $i$ -th game, including the score of this round, the current accumulated round score, the dealer position, the counters of repeat dealer and Riichi bets. 

对self-play, 使用 $\Phi\left(x^{k}\right)-\Phi\left(x^{k-1}\right)$ 作为 第k轮的 reward.    这个有点类似 critic网络的作用.

这个网络对游戏前期的终局得分的预测应该是不太准的, 但越到后期越准. 



#### 3.3 Oracle Guiding

麻将中有丰富的隐藏信息。 如果无法访问此类隐藏信息，则很难采取良好的action。 这是麻将游戏很难的根本原因。 在这种情况下，尽管代理可以通过强化学习来学习策略，但是学习可能会非常缓慢。 为了加快RL训练的速度，我们引入了一个oracle代理，该代理可以查看有关状态的所有完美信息.

这里有个问题, 就是Oracle是怎么训练出来的, 应该是ML训练后的模型, 再加上完美信息开始RL训练得到.

通过（不公平的）访问完美信息，RL培训之后，Oracle代理将很容易成为麻将大师水平。 **挑战是如何利用oracle代理来指导和加速我们普通代理的培训。** 根据我们的研究，简单的知识提炼效果不好：普通代理难以模仿训练有素的Oracle代理的行为。 因此，我们需要一种更聪明的方法来指导我们的普通代理与Oracle一起使用。

Suphx中，首先使用包括完美特征在内的所有特征 通过强化学习训练Oracle代理。然后，逐渐剔除完美的特征，使Oracle代理最终转换为普通代理。


$$
\mathcal{L}(\theta)=\underset{s, a \sim \pi_{\theta^{\prime}}}{\mathrm{E}}\left[\frac{\pi_{\theta}\left(a |\left[x_{n}(s), \delta_{t} x_{o}(s)\right]\right)}{\pi_{\theta^{\prime}}\left(a |\left[x_{n}(s), \delta_{t} x_{o}(s)\right]\right.} A^{\pi_{\theta}}\left(\left[x_{n}(s), \delta_{t} x_{o}(s)\right], a\right)\right]
$$

$x_{n}(s)$ normal features ,   $x_{o}(s)$ perfect features,  $\delta_{t}$ 是第t次迭代的dropout矩阵, 元素为Bernoulli 随机变量 , $P\left(\delta_{t}(i, j)=1\right)=\gamma_{t}$.   $\gamma_{t}$ 从 1 衰减到 0. 当 $\gamma_{t}=0,$ 所有的完美特征都会衰减掉. 

当 $\gamma_{t}$ 变为零后，继续对普通代理进行一定次数的迭代训练。在持续训练的过程中，我们采用了两个技巧。第一，将学习率衰减到十分之一。第二，如果重要性权重大于预先定义的阈值，我们拒绝一些状态动作对。根据我们的实验，如果没有这些技巧，持续训练是**不稳定的**，也不能带来进一步的改进。



#### 3.4 Parametric Monte-Carlo Policy Adaptation

当一个顶尖的人类玩家的初始手牌不同时，策略将非常不同。如果能够在运行时调整离线训练的策略，我们可以建立一个更强大的麻将代理。
MCTS用于提高运行时的性能。不幸的是，麻将的play顺序并不固定，很难建立一个规则的游戏树。MCTS不能直接应用于麻将。在这项工作中，设计了一种新的方法，pMCPA.

当一个回合开始，初始的手牌被发到我们的代理手中时，我们将离线训练的策略调整为匹配初始手牌，如下：

1. 模拟：去掉自己的手牌, 从剩下的牌中随机抽取三个对手的手牌和牌山里的牌，然后使用离线训练的策略来rollout。 以这种方式生成了K条轨迹。
2. 适应：使用推出轨迹执行梯度更新以微调离线策略。
3. 推理：在本轮比赛中，使用微调的策略与其他玩家对抗。



$h$ 玩家手牌, $\theta_{o}$  off-line 参数,  $\theta_{a}$ 新策略参数. 

$$
\theta_{a}=\arg \max _{\theta} \sum_{\tau \sim_{\theta_{o}} \mathcal{T}(h)} R(\tau) \frac{p(\tau ; \theta)}{p\left(\tau ; \theta_{o}\right)}
$$

$\mathcal{T}(h)$ 是 $h$ 之后的轨迹,  $p(\tau ; \theta)$ 是策略$\theta$ 生成 trajectory $\tau$ 的概率.

根据研究，模拟轨迹的数量K不需要很大，pMCPA 不需要收集这一轮的所有状态的统计数据。由于pMCPA是一种参数化的方法，所以更新的策略（使用K个模拟量）可以导致那些在模拟中没有访问过的状态的更新。这样的运行时适应策略可以帮助我们将从有限的模拟中获得的知识泛化到未见过的状态。

请注意，策略调整是针对每个回合独立执行的。 也就是说，在本轮调整代理策略之后，对于下一轮，将再次从离线策略中重新启动。



### 4 Offline Evaluation

#### 4.1 Supervised Learning

在Suphx中，首先通过SL分别训练了五个模型。 每个训练样本都是从人类职业玩家那里收集的状态-动作对，状态是输入，动作是监督学习的标签。 例如，对于出牌模型的训练，样本的输入是状态的所有可观察信息（和look-head特征），而标签是人类玩家出的牌。

训练准确率以及data size见表:

![image-20200616173352487](/img/2020-06-02-Suphx.assets/image-20200616173352487.png)

出牌是34分类, 所以需要更多的样本, 准确率也低. 后面的准确率是其他研究者的工作.



#### 4.2 Reinforcement Learning

为了体现Suphx中每个RL组件的价值，我们训练了几个麻将代理：

- SL：监督学习代理。
- SL-weak：SL代理的训练不足版本
- RL-basic：强化学习代理的基本版本。出牌模型用SL出牌模型初始化，然后通过策略梯度方法进行提升，通过轮次分数作为奖励和熵正则化。 Riichi，Chow，Pong和Kong模型与SL模型相同
- RL-1：RL-basic + 全局奖励预测。通过日志监督学习来训练奖励预测网络。
- RL-2：Oracle指导 + RL-1。

注意，在RL-1和RL-2中，我们也仅使用RL训练了出牌模型，而其他四个模型与SL代理的模型相同。

初始手牌具有较大的随机性，会极大地影响输赢。为了减少初始手牌造成的方差，在off-line评测过程中，随机生成了100万局游戏。每个代理与3个SL-weak代理对弈。在这样的设置中，对一个代理的评估需要20个Tesla K80 GPU运行两天。评估指标，按照天凤的规则计算一个等级。为了减少排名的方差，对每个代理，从100万局游戏中随机抽取800K局，进行1000次的随机抽样。

注意，为了公平比较，每个RL代理都是用150万局游戏训练的。每个代理的训练耗费了44个GPU（4个Titan XP用于参数服务器，40个Tesla K80用于self-play）和两天的时间。

 

<img src="/img/2020-06-02-Suphx.assets/image-20200616181048741.png" alt="image-20200616181048741" style="zoom:50%;" />



通过全局奖励预测网络将游戏奖励分配到每个回合，训练有素的代理可以更好地最大化最终游戏奖励，而不是回合得分。例如，在图9中，代理（南）在最后一轮的游戏中，取得了较大的领先优势。根据目前四位棋手的分数，赢得本轮比赛仅获得少量奖励，而输掉本轮比赛则将受到重罚。因此，代理选择最安全的牌进行弃牌，最终获得本场比赛的一位。相比之下，RL-basic弃掉另一张牌来赢下这一局，这就带来了失去整个游戏第一名的巨大风险。



<img src="/img/2020-06-02-Suphx.assets/image-20200616181657607.png" alt="image-20200616181657607" style="zoom: 33%;" />

 当前玩家应该出六饼, 跟打, 比较安全.



#### 4.3 Evaluation of Run-Time Policy Adaptation

除了测试对离线RL训练的增强，我们还测试了运行时策略的适应性。实验环境描述如下。
当一个回合开始，我们的代理摸牌，

1. 数据生成。固定代理的手牌，模拟100K的轨迹。在每个轨迹中，随机生成其他三个玩家的手牌和牌山. 用四个代理的copy来rollout完成轨迹。
2. 策略适应。在这100K的轨迹上, 使用策略梯度法，对训练好的策略进行fine-tune和更新
3. 测试调整后的策略。我方agent使用更新后的策略，固定自己的手牌, 在另一个10K的测试集测试。

注意，运行时策略适配是非常耗时的，因为roll-outs和在线学习。因此，在现阶段，我们只在几百个初始回合中测试了这个技术。adapted RL-2版本与非adapted版本相比，胜率为66%，这说明了运行时策略适应的优势。 



<img src="/img/2020-06-02-Suphx.assets/image-20200616184115222.png" alt="image-20200616184115222" style="zoom:33%;" />

当前玩家, 如果以不高的牌型胡牌, 则排第四; 所以要打的激进.



### 5 Online Evaluation

在天风平台评估. 

Suphx玩了5000+游戏，获得了最高十段，安定8.74段的成绩. 天凤上唯一一个取得十段的AI。



与其他AI的比较:

- Bakuuchi,  from 东京大学，基于蒙特卡洛模拟和对手建模。no RL

- NAGA, from Dwango Media Village,  CNNs,   no RL

  

<img src="/img/2020-06-02-Suphx.assets/image-20200616194023382.png" alt="image-20200616194023382" style="zoom:50%;" />













通过全局奖励预测器将游戏奖励分配给每个回合，训练有素的代理商可以更好地最大化最终游戏奖励，而不是回合得分。 例如，在图9中，我们的经纪人（南方球员）在比赛的最后一轮中拥有领先优势，并且拥有良好的手感。 根据当前这四名球员的累计得分，赢得本轮比赛仅获得少量奖励，而输掉本轮比赛则将受到重罚。 因此，我们的经纪人没有采取积极的行动来赢得本轮比赛，而是采取保守的态度，选择最安全的牌来放弃，最终获得本场比赛的第一名/排名。 相反，RL-basic放弃了另一张牌来赢得回合，这带来了失去整个游戏第一名的巨大风险。



- Suphx的防守能力很强，放炮率很低。

- Suphx的得最后一名概率很低，这是获得高稳定的关键。

Suphx已经形成了自己的打法，得到了人类顶尖棋手的认可。例如，Suphx非常善于保留安全牌，喜欢half-flush. 



### 6 Conclusion and Discussions

作者提出的提升方向

1. 全局奖励预测。在目前的系统中，奖励预测器的输入信息有限。显然，更多的信息会带来更好的奖励信号。例如，如果由于我们初始手牌的运气好，一局很容易赢，那么赢了这一局并不能体现我们策略的优越性，不应该得到太多的奖励；相反，赢了一局困难的牌，应该得到更多的奖励。也就是说，在设计奖励信号时，应该考虑到游戏难度。我们正在研究如何利用prefect信息（例如，通过比较不同玩家的私人初始手牌）来衡量回合/游戏的难度，然后提升奖励预测器。
2. oracle guiding, 除此之外，还可以采用其他方法来利用完美信息。例如，我们可以同时训练一个oracle代理和一个普通代理，让oracle代理将其知识提炼给普通代理，同时约束这两个代理之间的距离。根据我们的初步实验，这种方法的效果也相当好。再举一个例子，我们可以考虑设计一个oracle批评者，它可以提供更有效的 state 级别的即时反馈（代替 回合 级别的反馈），以加速基于完美信息的策略函数训练。
3. run-time policy adaptation, 现在是在开局的时候rollout.  其实可以在每一手牌之后都rollout. 可以进一步提升性能.








## Reference

Suphx: Mastering Mahjong with Deep Reinforcement Learning

https://arxiv.org/abs/2003.13590













