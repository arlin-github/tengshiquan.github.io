---
layout:     post
title:      NFSP
subtitle:   Deep Reinforcement Learning from Self-Play in Imperfect-Information Games
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-dice.jpg"
catalog: true
tags:
    - AI
    - Imperfect Information
    - NFSP

---



# NFSP 算法

## Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

2016  Johannes Heinrich, David Silver



Self-play + DRL + Imperfect

NFSP = FSP + NN



后面直接机翻, 不改了



### Abstract

许多现实世界中的应用可以被描述为不完全信息的大规模博弈。为了处理这些具有挑战性的领域，之前的工作主要集中在手工抽象abstraction的领域中计算纳什均衡。在这篇论文中，我们介绍了第一个可扩展的端到端学习近似纳什均衡的方法，而不需要事先的领域知识。我们的方法将虚拟的自我博弈与深度强化学习相结合。当应用于Leduc poker时，Neural Fictitious Self-Play（NFSP）接近了一个纳什均衡，而常见的强化学习方法则出现了分歧。在Limit Texas Hold'em这种真实世界规模的扑克博弈中，NFSP学习的策略接近于最先进的、基于重要领域专业知识的超人类算法的性能。



### 1 Introduction

虽然许多机器学习方法已经在经典的完美信息博弈中实现了近乎最优的解，但这些方法在不完美信息博弈中却无法收敛。另一方面，许多用于寻找纳什均衡的博弈理论方法缺乏学习抽象模式并将其推广到新的情况的能力。这导致对大型博弈的**可扩展性scalability 有限**，除非利用人类的专家知识、启发式算法或建模将领域抽象到可管理的规模。然而，获取人类的专业知识往往需要昂贵的资源和时间。此外，人类很容易被骗到非理性的决策或假设。这就促使算法end-to-end学习有用的策略。



**虚拟博弈Fictitious play**(Brown, 1951)是在**正则normal(单步)博弈**中学习纳什均衡的一种流行方法。虚拟博弈者对对手的平均行为选择最佳反应（即最佳反策略）。Fictitious Self-Play（FSP）（Heinrich等，2015）将这种方法扩展到**展开型（多步）博弈**中。在本文中，我们介绍了NFSP，一种深度强化学习方法，用于学习不完全信息博弈的近似纳什均衡。NFSP结合了FSP和神经网络函数近似。一个NFSP代理由两个神经网络组成。第一个网络是通过强化学习从对战经验中训练出来的。这个网络根据其他代理的历史行为，学习一个近似的**最佳反应**。第二种网络是通过对代理自身的经验的监督学习来训练的。这个网络学习了一个模型，该模型是对代理自身历史 **平均策略** 进行学习。代理根据其平均策略和最佳响应策略的混合策略行动。



我们在双人零和扑克博弈中对我们的方法进行了验证。在这个领域中，目前的博弈理论方法使用牌力的启发式方法将博弈抽象为可操作的大小。我们的方法不依赖于工程上的这种抽象，也不依赖于任何其他先验的领域知识。NFSP代理利用深度强化学习，直接从他们在博弈中与其他代理交互的经验中学习。当应用于Leduc扑克时，NFSP接近了一个纳什均衡，而普通的强化学习方法则出现了分歧。我们还将NFSP应用于有限押注德州扑克（Limited Texas Hold'em，LHE），直接从原始输入中学习。NFSP学习的策略接近于最先进的、基于手工抽象的打败人类的方法的性能。



### 2 Background

#### 2.1 Reinforcement Learning

#### 2.2 Extensive-Form Games

这里加上了state.

- **information states** 信息集
- **behavioural strategy**   
- **perfect recall**,  player's current information state $s_{t}^{i}$ implies knowledge of the sequence of his information states and actions, $s_{1}^{i}, a_{1}^{i}, s_{2}^{i}, a_{2}^{i}, \ldots, s_{t}^{i}$ that led to this information state. 当前state的信息包含玩家相关的所有历史信息.
- **realization-probability** , $x_{\pi^{i}}\left(s_{t}^{i}\right)=\prod_{k=1}^{t-1} \pi^{i}\left(s_{k}^{i}, a_{k}^{i}\right),$ determines the probability that player $i$ 's behavioural strategy, $\pi^{i},$ contributes to realizing his information state $s_{t}^{i}$ . 玩家的策略对到达某个state的贡献,  到达概率.
- **strategy profile** $\pi=\left(\pi^{1}, \ldots, \pi^{n}\right)$ 
- **best response** 
- $\epsilon$ -best response
- **Nash equilibrium** , strategy profile of best responses
- $\epsilon$ -Nash equilibrium

Nash equilibrium ,  a **fixed point** of rational self-play learning.



#### 2.3 Fictitious Self-Play

**虚拟博弈Fictitious play** 是博弈论里的自我博弈学习模型。虚拟玩家对 对手的平均行为 选择最佳反应。虚拟玩家的平均策略在某些类别的博弈中会趋近于纳什均衡，例如双人零和博弈和多人potential势博弈。Leslie和Collins(2006)引入了generalised weakened fictitious play。它具有与普通虚拟博弈类似的收敛保证，但允许近似的最佳反应和扰动平均策略更新，因此特别适用于机器学习。

虚拟博弈通常以正则形式定义，对于展开博弈而言，其效率较低。Heinrich等（2015）引入了**Full-Width Extensive-Form Fictitious Play，(XFP)**，使虚拟博弈者能够以展开形式更新策略，从而实现线性的时间和空间复杂度。一个关键是，对于正则形式策略的凸组合，$\hat{\sigma}=\lambda_{1} \hat{\pi}_{1}+\lambda_{2} \hat{\pi}_{2}$ ， 我们可以得到一个 realization-equivalent behavioural strategy $\sigma,$ by setting it to be proportional to the respective convex combination of realization-probabilities
$$
\sigma(s, a) \propto \lambda_{1} x_{\pi_{1}}(s) \pi_{1}(s, a)+\lambda_{2} x_{\pi_{2}}(s) \pi_{2}(s, a) \quad \forall s, a  \tag{1} 
$$
where $\lambda_{1} x_{\pi_{1}}(s)+\lambda_{2} x_{\pi_{2}}(s)$ is the normalizing constant for the strategy at information state $s$. 

方程 (1) 一个这种凸组合策略的采样方法. 

Heinrich et al. (2015) 引入了 **Fictitious Self-Play (FSP)**, 是一类基于样本和机器学习的近似XFP的算法。FSP分别用强化学习和有监督学习取代了最佳响应计算和平均策略更新。特别是，FSP代理在自我博弈中生成经验数据集。每个代理将自己的经验 $\left(s_{t}, a_{t}, r_{t+1}, s_{t+1}\right)$ 存在$\mathcal{M}_{R L}$中。代理对自己行为的经验$\left(s_{t}, a_{t}\right)$ ， 存在单独的$\mathcal{M}_{S L}$ ，用于监督学习。

自我博弈采样的方式是，an agent's reinforcement learning memory approximates data of an MDP defined by the other players' average strategy profile. 因此，通过强化学习对MDP的近似解可以得到一个近似的最佳反应。



### 3 Neural Fictitious Self-Play

NFSP = FSP + NN. 

在算法1中，所有的博弈者都由独立的NFSP代理控制，这些代理从同时对弈中学习，即自我博弈。一个NFSP代理与同伴的代理进行交互，并将自己的博弈经验和自己的最佳反应行为记忆在两个记忆中，即$\mathcal{M}_{R L}$ and $\mathcal{M}_{S L}$.    NFSP将这些记忆作为两个不同的数据集，分别适合于深度强化学习和有监督分类。代理训练一个神经网络$Q\left(s, a | \theta^{Q}\right)$, 在数据集$\mathcal{M}_{R L}$ 上利用off-policy RL 训练, 预测动作值。得到的网络定义了代理的近似最佳响应策略，$\beta=\epsilon-greedy(Q)$. 代理训练一个独立的神经网络$\Pi\left(s, a | \theta^{\mathrm{I}}\right)$ ,  数据集 $\mathcal{M}_{S L}$进行监督分类来仿自己过去的最佳反应行为。这个网络将状态映射到行动概率，并定义了代理的平均策略，$\pi=\Pi$ 。在博弈过程中，代理从$\beta$ and $\pi$ 的混合策略中选择自己的动作。NFSP还利用了两项技术创新，以确保所产生的算法的稳定性，以及并行化。首先，它使用了 reservoir sampling，以避免由于从有限内存中采样而产生的windowing artifacts。其次，它使用了anticipatory dynamics（Shamma和Arslan，2005），使每个代理既能对自己的最佳反应行为进行采样，又能更有效地跟踪对手的行为变化

![image-20200517234059063](/img/2020-04-20-NFSP.assets/image-20200517234059063.png)



Fictitious play通常会记录玩家在博弈中选择的normal型的平均最佳反应策略, $\hat{\pi}_{T}^{i}=\frac{1}{T} \sum_{t=1}^{T} \hat{\beta}_{t}^{i} $. Heinrich提出，根据方程1，利用抽样和机器学习来生成数据，并根据方程1学习正则形式的策略的凸组合。例如，我们可以通过对博弈的整个episodes进行抽样，使用 $\beta_{t}^{i}, t=1, \ldots, T$ , 在凸组合中按其权重的比例，$\frac{1}{T}$，生成一组extensive-form 数据。NFSP代理使用reservoir sampling 来记忆他们的历史最佳反应的经验。该代理的监督学习 $\mathcal{M}_{S L}$ 是一个蓄水池，它只有在遵循其近似的最佳响应策略时，才会将经验添加到该蓄水池中，即$\beta=\epsilon$ -greedy $(Q)$.  一个NFSP代理定期训练它的平均策略网络 $\Pi\left(s, a | \theta^{\mathrm{I}}\right)$，例如，通过优化过去所采取的行动的对数概率log-probability，来匹配存储在其SL memory 中的平均行为。

如果我们希望所有的代理在相互博弈时同时学习，有个难题。原则上，每个代理可以发挥其平均策略，$\pi$ , 并通过off-policy的Q-learning来学习一个最佳响应，即 评估并最大化它的行动值，$Q^{i}(s, a) \approx \mathbb{E}_{\beta^{i}, \pi^{-i}}\left[G_{t}^{i} | S_{t}=s, A_{t}=a\right]$, Q是最佳响应策略 $\beta^{i}$  对上 其他人的平均策略组合 $\pi^{-i}$.  然而，在这种情况下，代理将不会产生任何关于自己的最佳响应行为 $\beta$ 的经验，这是训练平均策略网络$\Pi\left(s, a | \theta^{\mathrm{T}}\right)$ 所需要的，即近似于代理过去的最佳响应的平均策略。为了解决这个问题，我们建议使用an approximation of anticipatory dynamics of continuous-time dynamic fictitious play。在这种虚拟博弈的变体中，博弈者选择对对手的平均normal策略的短期short-term预测的最佳反应，$\hat{\pi}_{t}^{-i}+\eta \frac{d}{d t} \hat{\pi}_{t}^{-i}$ , 其中，$ \eta \in \mathbb{R}$ 称为预期anticipatory参数。作者表明，对于适当的，game-dependent的选择，可以提高在均衡点 虚拟博弈的稳定性。NFSP使用$\hat{\beta}_{t+1}^{i}-\hat{\pi}_{t}^{i} \approx \frac{d}{d t} \hat{\pi}_{t}^{\mathrm{a}}$ 作为导数的离散时间近似值，在这些预测性动态中使用。请注意， $\Delta \hat{\pi}_{t}^{i} \propto \hat{\beta}_{t+1}^{i}-\hat{\pi}_{t}^{i}$是常见的离散时间虚拟博弈的正则形式更新方向。

NFSP代理从混合策略 $\sigma \equiv(1-\eta) \hat{\pi}+\eta \hat{\beta}$ 中选择它们的行动。这使得每个代理可以计算出一个近似的最佳反应，即 $\beta^{i}$ , 对应对手的 期望平均策略组合 $\sigma^{-i} \equiv \hat{\pi}^{-i}+\eta\left(\hat{\beta}^{-i}-\hat{\pi}^{-i}\right)$  ,  通过迭代来评估和最大化它的action值, $Q^{i}(s, a) \approx \mathbb{E}_{\beta^{i}, \sigma^{-i}}\left[G_{t}^{i} | S_{t}=s, A_{t}=a\right] $。此外，由于每个代理的最佳响应策略现在是按anticipatory参数的比例采样的，所以他们现在可以根据这个经验训练他们的平均策略网络。

 

### 4 Experiments

通过 可利用性exploitability 来衡量. 

In a two-player zero-sum game, the exploitability of a strategy profile is defined as the expected average payoff that a best response profile achieves against it. An exploitability of 2δ yields at least a δ-Nash equilibrium.



#### 4.1 Leduc Hold’em

研究了Leduc Hold'em中NFSP向纳什均衡的收敛。

目标之一是尽量减少对先验知识的依赖。因此，我们试图定义一种与领域无关的信息状态编码, k-of-n encoding. LHE有一副52张牌，第二轮有三张牌被揭晓。因此，这一轮的编码是一个长度为52的向量，三个元素设为1，其余的元素设为0。 

在Limittle Hold'em扑克游戏中，玩家通常有三个动作可供选择，即 fold，call，raise。 注意，根据上下文，call和raise可以分别称为check和bet。下注的上限是每轮的加注次数是固定的。我们可以用4个维度的张量来表示下注历史，{ player, round, number of raises, action taken } ,  E.g. heads-up LHE contains 2 players, 4 rounds, 0 to 4 raises per round and 3 actions. Thus we can represent a LHE betting history as a 2 × 4 × 5 × 3 tensor. In a heads-up game we do not need to encode the fold action. 可以将4维张量扁平化为一个长度为80的向量。与4轮的牌输入相加，我们将LHE的信息状态编码为一个长度为288的向量。

同样，Leduc Hold'em的信息状态可以编码为长度为30的向量 , 6 cards with 3 duplicates, 2 rounds, 0 to 2 raises per round and 3 actions.



对于Leduc Hold'em中的学习，我们手动调整了NFSP, 为含1个隐藏层64个relu, 的DNN。然后，我们用相同的参数对各种网络架构重复实验。特别是，我们将MRL和MSL的记忆大小分别设置为200k和2m。MRL作为一个圆形缓冲器来发挥作用, 包含最近经验的窗口。而MSL则是通过reservoir采样更新。强化学习率和监督学习率分别设置为0.1和0.005，均采用SGD不含动量.  每一个代理在游戏中每128步，每个网络执行2次随机梯度更新，batchsize为128。DQN算法的目标网络每300次更新后，更新参数。NFSP的anticipatory参数被设置为η=0.1。ε-greedy策略的探索开始于0.06，然后衰减到0，与迭代次数的倒数平方根成正比。

图1a显示了不同网络架构下NFSP接近纳什均衡的情况。我们观察到，NFSP的性能随着网络规模的增大而单调增加。NFSP的可利用性达到了0.06，而full-widthXFP通常在1000 full-width 迭代后就能达到。
为了研究NFSP的各种组件的相关性，如reservoir sampling和anticipatory dynamics，我们进行了一个实验。图1b显示，这些修改导致了性能下降。特别是，使用一个固定大小的滑动窗口来存储代理自身行为的经验，导致了发散。NFSP的性能在0.5的高anticipatory参数下达到了峰值，这很可能违反了game-dependent stability conditions of dynamic fictitious play。最后，使用指数平均reservoir sampling进行监督学习记忆更新导致了噪声性能。



![image-20200518014114608](/img/2020-04-20-NFSP.assets/image-20200518014114608.png)



#### 4.2 Comparison to DQN

DQN之类的RL, 稳定性方面的经验只限于 single-agent, perfect (or near-perfect) information MDPs. 

DQN 是deterministic, greedy strategy,  在 single-agent domains 可以最优. 而不完全信息博弈一般需要随机策略来实现最优行为。

人们想知道，DQN的平均行为average behaviour是否会收敛到纳什均衡。为了测试这个问题，我们对DQN增加了监督学习memory，并训练一个神经网络来估计其平均策略。与NFSP不同的是，平均策略不会以任何方式影响代理的行为；它是被动地观察DQN代理，以估计其随着时间的演化。我们通过使用NFSP实现DQN的这种变体，anticipatory参数为η = 1。用以下参数的所有组合来训练DQN。学习率{0.2，0.1，0.05}，$\epsilon$开始值{0.06，0.12}，强化学习memory{2m reservoir, 2m sliding window}。其他参数被固定为与NFSP相同的值；注意，这些参数只影响被动观察过程。然后，我们选择了DQN的最佳表现结果，并与上一节实验中的NFSP的表现进行了比较。DQN在学习率为0.1、探索起始值为0.12、滑动窗口内存大小为2m的情况下，取得了最佳表现。



从图1c可以看出，DQN的确定性策略具有很强的可利用性，这也是意料之中的，因为不完全信息博弈通常需要随机策略。DQN的平均行为也没有接近纳什均衡。原则上，DQN也会根据同伴代理产生的历史经验学习一个最佳反应。那么，为什么它的表现比NFSP差呢？问题在于，DQN代理完全根据ε-greedy策略产生自我博弈经验。这些经验在时间上都是高度相关的，而且高度集中在狭窄的状态分布上。与此相反，NFSP代理使用一个越来越缓慢变化的（预期）平均策略来产生自我游戏经验。因此，他们的经验变化更平稳，从而产生更稳定的数据分布，从而使神经网络更稳定。注意，这个问题并不限于DQN；其他常见的强化学习方法在扑克游戏中表现出类似的停滞不前。



#### 4.3 Limit Texas Hold’em

通过尝试9种配置，对NFSP进行了手动校准。在以下参数下，我们取得了最佳性能。  
fully connected with four hidden layers of 1024, 512, 1024 and 512 neurons with relu.

MRL和MSL的内存大小分别设置为600k和30m。MSL，以最小概率0.25替换MSL中的条目。使用没有动量的vanilla SGD进行强化学习和监督学习，学习率分别设置为0.1和0.01。每个代理在游戏中每256步，每个网络进行2次随机梯度更新，每256步进行2次小批量大小的随机梯度更新。目标网络每1000次更新后，对目标网络进行reload。NFSP的预测参数被设置为η=0.1。ε-greedy策略的探索从0.08开始，衰减到0，比Leduc Hold'em中更慢。除了NFSP的主要的、平均策略剖面外，我们还评估了最佳响应策略和贪婪平均策略，这两种策略分别决定性地选择了预测行动值或概率最大化的行动。



![image-20200518014140913](/img/2020-04-20-NFSP.assets/image-20200518014140913.png)



#### 4.4 Approximations to fictitious play

NFSP approximates fictitious play, both by using a neural network function approximator to represent strategies, and by averaging those strategies via gradient descent machine learning.

在附录中，我们提供了一些实验来说明这两种近似方法的效果，与策略的精确表示和完美平均化程序相比，这两种近似方法的效果。





### 5 Related work

对人类专家知识的依赖可能是昂贵的，容易产生人为的偏差，而且如果这种知识是次优的，就会有局限性。然而，许多应用于棋局的方法都依赖于人类专家知识。深蓝棋使用了人类设计的评价功能来进行国际象棋的评价。在计算机围棋中，从人类专家对弈的数据中训练了深度神经网络。在计算机扑克中，目前的对局理论方法使用了牌力启发式算法，将对局抽象为能处理的大小。最近将其中的一种方法与函数近似相结合。然而，他们的full-width算法必须在每次迭代时隐含地推理所有的信息状态，这在大规模博弈中是非常昂贵的。相比之下，NFSP专注于基于样本的强化学习设置，在这种情况下，博弈的状态不需要详尽地枚举，学习者甚至可能没有博弈的动态模型。

许多成功的应用都是依靠本地搜索。本地搜索有效地在运行时规划决策 ，例如，通过Monte Carlo simulation或limited-depth backward induction等方式。然而，常见的基于模拟的局部搜索算法已经被证明是有发散的,  当应用于不完全信息扑克游戏时. 此外。在不完全信息博弈中，即使是博弈论方法，在不完全信息博弈中的局部规划时，一般也不能实现不可利用的行为。局部搜索的另一个问题是，如果没有注入先验知识来指导搜索，那么在运行时的成本可能会很高。这就提出了如何获得这些先验知识的问题。本作中，我们在运行时不进行任何本地搜索评估我们的代理。如果开发出不完全信息博弈的局部搜索方法，由NFSP训练的策略可以成为指导搜索的有希望的选择。

纳什均衡是理性代理在自我博弈中唯一可以收敛的策略组合。TD-Gammon是一个世界级的双陆棋代理，其主要成分是一个由self-play强化学习训练出来的神经网络。虽然它的算法基于TD学习，但在两个人的零和完美信息博弈中是完善的，但在不完美信息的游戏中一般不会收敛。DQN将时差学习与experience repla和深度神经网络函数逼近相结合。它在大多数雅达利游戏中实现了人类级别的性能， 然而，这些雅达利游戏被设置为单代理模式，潜在的对手固定并由雅达利模拟器控制。我们的实验表明，在Leduc Hold'em中，DQN agent无法接近纳什均衡. 



在这项工作中，我们重点研究的是不完全信息的双人零和博弈。然而，在合作性的势函数博弈中，虚拟博弈也可以保证收敛到纳什均衡（Monderer 和 Shapley, 1996）。因此，可以想象，NFSP也可以成功地应用于这些博弈。此外，最近在连续动作强化学习方面的发展，可以使NFSP应用于连续动作博弈，而目前的博弈理论方法无法直接处理这些问题。 



### 6 Conclusion

引入了NFSP，第一个端到端的深度强化学习方法，用于学习不完全信息博弈的近似纳什均衡。与以往的博弈论方法不同，NFSP是可扩展的，无需先验领域知识。此外，NFSP是第一个已知的可以收敛到自博弈中的近似纳什均衡的深度强化学习方法。我们的实验表明，NFSP在小型扑克游戏中可靠地收敛了近似纳什均衡，而DQN的贪婪策略和平均策略则没有。NFSP在一个现实中的不完全信息博弈中从头开始学习,  可以与超过人类的程序竞争，而不使用显式先验知识。





### A Robustness of XFP

为了了解函数逼近与FSP的交互作用，我们进行了一些简单的实验，模拟全宽算法XFP中的逼近和采样误差。首先，我们探讨当XFP中使用的完美平均法被更接近梯度下降的增量平均法所取代时，会发生什么。其次，我们探讨了当XFP中使用的精确表查询被近似误差的epsilon误差取代时，会发生什么情况。

![image-20200518041222900](/img/2020-04-20-NFSP.assets/image-20200518041222900.png)

图3a显示了XFP在默认、1/T和恒定步幅下的策略更新性能。我们看到，在较小的步幅下，XFP的初始性能得到了改善，但初始性能较低。对于恒定步长的步长，其性能似乎趋于平稳，而不是偏离。在储层采样时，我们可以实现1/T的有效步长。然而，结果表明，指数平均的水库采样也可以是一个可行的选择，因为对过去的记忆进行指数平均，大约相当于使用恒定步长。



步长为1的XFP相当于全宽迭代最佳响应算法。虽然这种算法在有限完美信息的双人零和博弈中收敛到纳什均衡，但结果表明，在信息不完全的情况下，一般情况下不是这样。Yakovenko等人（2016）介绍的Poker-CNN算法存储了一小部分过去的策略，并根据这些策略迭代计算新的策略。替换该集中的策略类似于用大的步长更新平均策略。这可能会导致类似的问题，如图3a所示。



我们的NFSP代理在策略中加入了随机探索，并使用噪声随机梯度更新来学习动作值，这决定了它们的近似最佳响应。因此，我们研究了添加随机噪声对最佳响应计算的影响，XFP通过动态编程来执行最佳响应计算。在每一个反向归纳步骤中，我们传回一个概率ε的均匀随机动作值，否则传回最佳动作值。图3b显示了随着噪声的增加，性能呈单调递减。然而，性能保持稳定，并且在所有的噪声水平上都在不断提高。










## Reference

Deep Reinforcement Learning from Self-Play in Imperfect-Information Games

















