---
layout:     post
title:      Neuro-Dynamic Programming&#58;  An Overview
subtitle:   Note on "Neuro-Dynamic Programming&#58;  An Overview" (2000)
date:       2020-01-06 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-sunset.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - Dynamic Programming
    - NDP

---

# Neuro-Dynamic Programming: An Overview

以前RL论文的很多公式,术语表示不太一样,记录一下   

Neuro-dynamic programming (NDP) =  neural network + DP

in **state i**  , choose **decision u**, move to **state j** with given **probability** $p_{ij}(u)$  , incur a **cost** $g(i, u, j)$. 

**optimal cost**  starting from state j,   denoted by $J^∗(j)$ ,    

Bellman’s equation: 

$$
J^*(i)  = \min_u E \{g(i,u,j) + J^*(j) \vert i, u \},  \quad \forall i
$$

这里用了cost, 而不是gain ,所以是取min.   如果在每个状态都能取到最小值, 则是找到了最优策略.

DP可以 off-line 计算.    即在实际系统开始运行之前。

由于 Bellman’s “curse of dimensionality” ,  DP对很多问题, 在计算上不可行, 需要寻找次优解 suboptimal.



##### Cost Approximations in Dynamic Programming

NDP方法是以最优成本函数$J^*$的近似评估为中心的次优方法，通过使用神经网络或模拟。 

NDP methods are **suboptimal methods** that **center** around the **approximate evaluation** of the **optimal cost** function  $J^\ast$, possibly through the use of neural networks and/or simulation. 

Replace the optimal cost $J^{*}(j)$ with a suitable **approximation** $\tilde{J}(j, r)$,  where $r$ is a vector of **parameters**, and use at state i the (suboptimal) **control** $\tilde{\mu}(i)$ that attains the minimum in the (approximate) righthand side of Bellman's equation

$$
\tilde{\mu}(i)=\arg \min _{u} E \{g(i, u, j)+\tilde{J}(j, r) | i, u \}
$$

$\tilde{J}$  **scoring function**,   $\tilde{J}(j, r)$  score of state $j .$ 


对很多问题, 计算一些状态下的所有u的最小值, 计算量太大.  引入Q

$$
Q(i, u)=E\{g(i, u, j)+J^{*}(j) | i, u\}
$$

**Q-factor** corresponding to $(i, u) .$  replace $Q(i, u)$ with a suitable approximation $Q(i, u, r),$ $r$ parameters. 

$$
\tilde{\mu}(i)=\arg \min _{u}  \tilde{Q}(i,u,r)
$$

$\tilde{J}$用较少维度的参数来描述状态空间很大的问题,  叫作 **compact representations**.  紧凑表达

用表格描述的$J^\ast$ 叫作 **lookup table representation** . 


##### Approximation Architectures

近似方法的一个主要问题就是怎么选择模型去拟合.  **selection of architecture**.  $\tilde J(\cdot, r)$ $\tilde Q(\cdot,\cdot, r)$



Cost approximation enhanced through **feature extraction**, a process that maps the state i into some vector f(i), called the **feature vector** . Feature vectors summarize, in a **heuristic** sense, what are considered to be important <u>characteristics</u> of the state, and they are very useful in incorporating the designer’s <u>prior knowledge</u> or intuition about the problem and about the structure of the optimal controller.

Feature vectors are particularly useful when they can capture the “dominant nonlinearities” in the optimal cost function $J^∗$.  如果特征向量能表达主要的非线性.  By this we mean that $J^∗(i)$ can be approximated well by a “relatively smooth” function $ \hat J(f(i))$; 将输入由state变为特征向量, 目标函数变成近乎线性或低阶多项式. --特征工程.

获得更复杂的近似值的一种简单方法是将状态空间划分为几个子集subsets，并在每个子集中构造一个单独的成本函数近似值。 例如，通过在分区的每个子集中使用线性或二次多项式逼近，可以在整个状态空间上构造分段线性或分段二次逼近。 这里一个重要的问题是选择用于划分状态空间的方法。 可以使用常规分区（例如，网格分区 grid partitions），但是它们经常导致大量的子集和非常耗时的计算。分区的每个子集都应包含“相似”状态，以使最优成本在子集状态上的变化相对平滑，并且可以用平滑函数来近似。 一个有趣的可能性是使用特征作为分区的基础。特别地，可以使用特征空间的或多或少规则的离散化，这引起了原始状态空间的可能不规则的划分。 这样，不规则分区的每个子集都包含具有“相似特征”的状态。 -- 状态空间维度 切换为 特征维度,再划分



##### Simulation and Training

对 DP , 不存在真值 $(i,J^∗(i))$用来拟合.  唯一的可能性是通过模拟 simulation 来评估（精确或近似）给定（次优）策略的成本函数，并尝试根据模拟结果迭代地改进这些策略。这就产生了分析和计算上的困难. 使用仿真来评估近似最佳成本函数是一个新思想，这使本文的方法与DP中的早期近似方法有所不同。  --  采样,  训练模型

使用仿真具有另一个主要优点：它允许本文的方法用于难以建模但易于仿真的系统；就是说，在无法使用显式模型且只能实时或通过软件仿真器观察系统的问题中。对于这样的问题，传统的DP技术是不适用的，估计转移概率来建立一个详细的数学模型往往是繁琐或不可能的。

仿真还有第三个潜在优势：它可以隐式识别系统的“最重要”或“最具代表性”状态。似乎合理的是，如果这些状态是模拟过程中最常访问的状态，则评分函数将趋于更好地逼近这些状态的最优成本，并且获得的次优策略将表现得更好。



##### Neuro-Dynamic Programming

NDP = NN + DP

**reinforcement learning** : the methods allow systems to “learn how to make good decisions by observing their own behavior, and use built-in mechanisms for improving their actions through a reinforcement mechanism.”

mathematical: simulation and iterative improve quality of approximation

DP: value and  policy iteration

policy iteration 和 value iteration 这两个DP算法是 NDP的起点.  

PI 步骤:  given policy (在每个状态下选动作的规则) ,  近似评估该策略的cost,  least-squares-fitting $\tilde J(\cdot, r)$ 与 遵循该策略得到的许多仿真trajectory的结果,  通过最小化bellman方程得到一个新策略. 方程里的最优被替换为近似函数.   不断重复.

PI generates a sequence of policies,    这些策略最终会在最佳策略附近震荡。与最优解的偏差取决于多种因素，其中主要是模型对各种策略的cost的拟合能力.

上面的PI在改动模型参数前, 计算了许多样本轨迹.  另外一个NDP方法更频繁的更新参数, 因为它产生样本轨迹.

$$
\left(i_{0}, i_{1}, \dots, i_{k}, i_{k+1}, \dots,\right)
$$

这些轨迹对应一个固定策略, 或者一个贪婪策略. 贪婪策略利用最小化下式来选

$$
E\{g(i, u, j)+\tilde{J}(j, r) | i, u\}
$$

**temporal difference**

$$
d_{k}=g\left(i_{k}, u_{k}, i_{k+1}\right)+\tilde{J}\left(i_{k+1}, r\right)-\tilde{J}\left(i_{k}, r\right)
$$

表示 expected期望的cost估计值 $\tilde{J}\left(i_{k}, r\right)$  与 predicted预测的cost估计值 $g\left(i_{k}, u_{k}, i_{k+1}\right)+\tilde{J}\left(i_{k+1}, r\right)$ 的差值. 


如果cost的拟合是准确的，则根据Bellman方程，平均td为0.  因此, td values can be used to make incremental adjustments to r so as to bring about an approximate equality (on the average) between expected and predicted cost estimates along the simulated trajectories.

根据这个观点, Sutton提出 $TD(\lambda)$ . TD(1), policy iteration and least-squares parameter estimation;  TD(0), value iteration and stochastic approximation.

Q-learning, a stochastic approximation-like method that iterates on the Q-factors.

TD(λ) 和 Q-learning 的收敛性, 比compact representations要不明确.



一种更简单的 NDP方法称为 **rollout**，它是通过一些合理的次优策略（称为基本策略 base policy）的cost来估算optimal 成本。取决于上下文，基本策略的成本可以通过分析来计算，或者更普遍地通过仿真来计算。在该方法的变体中，通过使用一些模型来拟合基本策略的cost. 可以将这种方法视为（可能是近似的）策略迭代方法的一个步骤。rollout 特别易于实现，well-suited for on-line replanning, in situations where the problem parameters change over time。The rollout ap- proach may also be combined with rolling horizon approaximations, and in some variations is related to model predictive control, and receding horizon control。尽管不像前面提到的近似策略迭代和TD方法那样雄心勃勃，但rollout在各种研究和应用程序中的表现都出乎意料地出色，通常比基本策略有了显着的改进。



NDP的实际应用在计算上非常密集，并且经常需要大量的反复试验。幸运的是，所有计算和实验都可以离线完成。off-line. 离线获得近似值后，就可以使用它来足够快地生成决策以便实时使用。在提到的机器学习文献中，传统强化学习通常被视为“在线”on-line方法， the cost approximation is improved as the system operates in real time.  作者不这么认为.  对大型和复杂系统的应用程序, 这样的系统需要大量的训练数据。随着系统的运行，通常无法以足够的数量获得这些数据。即使可以，相应的处理要求通常也太大，无法实时有效使用. 



