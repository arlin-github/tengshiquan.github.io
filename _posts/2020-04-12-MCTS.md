---
layout:     post
title:      MCTS 蒙特卡洛树总结
subtitle:   A Survey of Monte Carlo Tree Search Methods
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-AlphaGo.jpg"
catalog: true
tags:
    - AI
    - MCTS
    - Reinforcement Learning

---



# MCTS



## mcts.ai

翻译自 mcts.ai , 很简洁的概括了MCTS 

MCTS 全称 Monte Carlo Tree Search，是一种人工智能问题中做出最优决策的方法，典型的是**组合博弈(combinatorial games)**中的行动(move)规划。它结合了随机**模拟(simulation)**的通用性和**树搜索(tree search)**的准确性。

MCTS 受到快速关注主要是由计算机围棋程序的成功以及其潜在的在众多难题上的应用所致。超越博弈游戏本身，MCTS 理论上可以被用在以 {状态 state，行动 action} 形式定义和用模拟进行预测输出结果的任何领域。



#### Basic Algorithm 基本算法

基本的 MCTS 算法非常简单：根据模拟的输出结果，逐个节点构造搜索树。其过程可以分为下面的若干步：

![img](/img/2020-04-12-MCTS.assets//mcts-algorithm-1a.png)



1. 选择 Selection：从根节点 R 开始，递归选择**最优子节点** 直到达到叶子节点 L。
2. 扩展 Expansion：如果 L 不是一个终止terminal节点（也就是，不会导致游戏结束）那么就创建一个或者更多的子节点，选择其中一个 C。（一般是随机的选）
3. 模拟 Simulation：从 C 开始运行一个模拟，直到游戏结束获得结果。
4. 反向传播 Backpropagation：用模拟的结果输出更新当前行动序列。

每个节点并需包含两个重要的信息：一个是根据模拟结果估计的值 和 该节点已经被访问的次数。

按照最简单和最节约内存的实现，MCTS 将在每个迭代过程中增加一个子节点。不过，要注意其实根据不同的应用, 在每个迭代过程中增加超过一个子节点也可能是有益的。



#### Node Selection 节点选择

##### Bandits 和 UCB

在树向下遍历时, 节点的选择通过选择最大化某个量来实现，这其实类似于 Multiarmed bandit problem。可以使用 **Upper Confidence Bounds（UCB）**公式计算：

$$
v_i + C \times \sqrt{ \frac{\ln N}{n_i}}
$$

其中 $v_i$ 是节点估计的值，$n_i$ 是节点被访问的次数，而 N 则是其父节点已经被访问的总次数。C 是超参

##### Exploitation 和 Exploration

UCB公式平衡了对已知奖励的利用和对相对未访问过的节点的探索.   
奖励估计是基于随机模拟的，所以节点必须经过多次访问后，这些估计才会变得可靠；MCTS的估计通常在搜索开始时是不可靠的，但在足够的时间内会收敛到更可靠的估计，而在无限的时间内则会收敛到完美的估计。



##### MCTS 和 UCT

Kocsis 和 Szepervari 在 2006 年首先构建了一个完备的 MCTS 算法，通过扩展 UCB 到 minimax 树搜索，并将其命名为 **Upper Confidence Bounds for Trees（UCT）**方法。这是目前绝大多数MCTS实现中使用的算法。

UCT 可以被描述为 MCTS 的一个特例：**UCT = MCTS + UCB** 



### 优点

MCTS 提供了比传统树搜索更好的方法。

##### Aheuristic 启发式

MCTS 不要domain knowledge。只需游戏基本规则, 就可以进行有效工作；这意味着 MCTS 通用性好.   

##### Asymmetric 非对称

MCTS 生成的树是非对称的。这个算法会更频繁地访问更加有趣的节点，并聚焦其搜索时间在相关的树的部分。



![img](/img/2020-04-12-MCTS.assets//mcts-tree-4.png)



这使得 MCTS 更加适合那些有着更大的分支因子的博弈游戏，比如说 19X19 的围棋。这么大的**组合空间combinatorial spaces**会给标准的基于深度或者宽度的搜索方法带来问题，所以 MCTS 的自适应性意味着它将（最终）找到那些看起来是最优的move，并将搜索工作集中在那里。

##### Anytime 任何时间

算法可以在任何时间终止，并返回当前的最佳估计。当前构造出来的搜索树可以被丢弃或者供后续重用。

##### Elegant  简洁

算法实现非常方便 

```python
def UCT(rootstate, itermax, verbose = False): # 查找当前状态下， 下一步哪个最好
    
    rootnode = Node(state = rootstate)

    for i in range(itermax): # 遍历步数
        node = rootnode
        state = rootstate.Clone()

        # Select   本节点已经没有下一步行动并且本节点有子节点， 选一个最好的子节点 切换node
        while node.untriedMoves == [] and node.childNodes != []: 
            # node is fully expanded and non-terminal
            node = node.UCTSelectChild()
            state.DoMove(node.move)

        # Expand  如果node可以行动， 随机找一个行动作为子节点   切换node
        if node.untriedMoves != []: # if we can expand (i.e. state/node is non-terminal)
            m = random.choice(node.untriedMoves) 
            state.DoMove(m)
            node = node.AddChild(m,state) # add child and descend tree

        # 下面随机的走到游戏结束， 然后 打分
        # Rollout - this can often be made orders of magnitude quicker using a state.GetRandomMove() function
        while state.GetMoves() != []: # while state is non-terminal
            state.DoMove(random.choice(state.GetMoves()))

        # Backpropagate  回溯打分
        while node != None: # backpropagate from the expanded node  back to the root node
            #node.visits += 1
          #node.wins += result
            node.Update(state.GetResult(node.playerJustMoved)) # state is terminal. Update node with result from POV of node.playerJustMoved
            node = node.parentNode

    return sorted(rootnode.childNodes, key = lambda c: c.visits)[-1].move 
   #return the move that was most visited
```

```python
    def UCTSelectChild(self):
        """
        UCT 算法（Upper Confidence Bound Apply to Tree），即上限置信区间算法
        UCT 算法与传统搜索技术的最大区别在于不同的分支可以有不同的搜索深度
        UCT 算法在不同的深度获取评估值． 对于最有“希望”求解问题的分支， UCT搜索深度可以很深（ 远大于 d），而对于“ 希望” 不大的分支， 其搜索深度可以很浅（ 远小于 d） 。
         当最有“希望”求解问题的分支数量远少于“ 希望” 不大的分支数量时，UCT 就可以把搜索资源有效地用于最有“希望”求解问题的分支， 从而获得比传统搜索算法更深的有效深度 d′。
         这个具有神奇力量的“希望”是由树内选择策略计算的．

        Use the UCB1 formula to select a child node. Often a constant UCTK is applied so we have
            lambda c: c.wins/c.visits + UCTK * sqrt(2*log(self.visits)/c.visits to vary the amount of
            exploration versus exploitation.
        """
        s = sorted(self.childNodes, key = lambda c: c.wins/c.visits + sqrt(2*log(self.visits)/c.visits))[-1]
        return s
```



#### Drawbacks 缺点

MCTS 有很少的缺点，不过这些缺点也可能是非常关键的影响因素。

##### Playing Strength  行为能力

MCTS算法在其基本形式下，即使是中等复杂度的游戏，也无法在合理的时间内找到合理的move。这主要是由于组合move空间过于庞大，而且关键节点的访问次数可能不够多，无法给出可靠的估计。

##### 速度

MCTS搜索可能需要多次迭代才能收敛到一个好的解决方案，这对于较难优化的通用应用来说，可能是个问题。  
幸运的是，使用一些技术可以显著提高算法的性能。



#### Improvements 提升

很多种 MCTS 强化的技术已经出现了。这些基本上可以归纳为领域知识或者领域独立两大类。

##### Domain Knowledge  领域知识

可以利用当前游戏特有的领域知识来在树上过滤掉不靠谱的move，或者在模拟中产生更类似于人类对手之间的playouts。这意味着博弈结果将比随机模拟更真实，节点需要更少的迭代就能产生真实的奖励值。  
领域知识可以带来显著的改进，但代价是速度和通用性的损失。

##### Domain Independent 领域独立

领域独立强化能够应用到所有的问题领域中。这些一般用在树的结构（如 AMAF），还有一些用在模拟（如选择倾向于胜利的行动）。领域独立强化并不和特定的领域绑定，具有一般性，这也是当前研究的重心所在。

 







------

 

## A Survey of Monte Carlo Tree Search Methods

2012

MCTS的总结, 主要是在博弈类游戏中的应用. 



### INTRODUCTION

MCTS : a method for **finding optimal decisions** in a given domain by **taking random samples** in the **decision space** and **building a search tree** according to the results.

attractions:

1. a **statistical** anytime algorithm for which **more computing** power generally leads to better performance.
2. can be used with little or **no domain knowledge**, and has succeeded on difficult problems where other techniques have failed.



#### Overview

A great benefit of MCTS is that the values of intermediate states do not have to be evaluated, as for depth-limited minimax search, which greatly reduces the amount of domain knowledge required. Only the value of the terminal state at the end of each simulation is required.

虽然基本算法被证明对各种问题都是有效的，但通常情况下，只有当这个基本算法被调整到适合手头的领域时，MCTS的全部优点才会被实现。大量MCTS研究的主旨是确定最适合每个给定情况的变化和增强，并了解如何更广泛地使用某一领域的增强。



#### Importance

success in various AI game playing algorithms, particularly imperfect information games.    
success in computer Go  
预期未来十年是研究的关注点;  但现在DL+RL是主流了.



### BACKGROUND

#### Decision Theory 决策理论

***Decision theory 决策理论***  combines **probability theory 概率论** with **utility theory 效用论** to provide a formal and complete framework for decisions made under uncertainty .  

##### Markov Decision Processes (MDPs)

##### Partially Observable Markov Decision Processes(POMDP)



#### Game Theory 博弈论

Game theory extends **decision theory** to situations in which multiple agents interact.  
A **game** can be defined as a set of established rules that allows the interaction of one or more players to produce specified outcomes.  

The combination of players’ strategies forms a **Nash equilibrium** if no player can benefit by unilaterally switching strategies . 如果没有玩家可以通过单方面的改变策略来获益，那么玩家的策略组合就形成了一个纳什均衡。这样的均衡总是存在的，但是对于真实的博弈来说，计算这样的均衡一般是很难的。



##### Combinatorial Games 组合博弈

Games 按以下属性进行分类

- *Zero-sum*:  是否所有玩家的奖励总和为零,  零和游戏
- *Information*: Whether the state of the game is fully or partially observable to the players.
- *Determinism*:  是否有随机因素. (also known as completeness, i.e. uncertainty over rewards).
- *Sequential*: Whether actions are applied sequentially or simultaneously. 即时或者回合
- *Discrete*: Whether actions are discrete or applied in real-time.



Games with **two players** that are **zero-sum**, **perfect information**, **deterministic**, **discrete** and **sequential** are described as ***combinatorial games***   两人回合制对弈   
单人纸牌游戏可以视为谜题设计者和解谜者之间的组合博弈 .  
有两个以上玩家的游戏不是组合博弈, 因为可能出现的社会性联盟。



##### AI in Real Game

现实世界的游戏通常涉及到延迟奖励，只有在游戏的终端状态下获得的奖励才能准确地描述每个玩家的表现。因此，游戏通常被建模为决策树: 

1. **Minimax**, 试图将对手在每个状态下的**最大收益最小化**，是传统的双人组合博弈的搜索方法。  
   通常会过早地停止搜索，用一个值函数来估计博弈的结果，通常用启发式$α-β$剪枝来优化。  
   $\text{max}^n$算法类似于minimax，用于非零和游戏和/或有两个以上玩家的游戏。
2. **Expectimax** , 将Minimax扩展到随机博弈。chance node的值是其子节点的概率加权后的总和，否则搜索结果与$\text{max}^n$相同。由于概率节点的影响，剪枝策略更难。
3. **Miximax**,  与single-player expectimax类似, 主要用于 imperfect information博弈. 使用预先定义的对手的策略, 当作chance nodes





#### Monte Carlo Methods 蒙特卡洛方法

蒙特卡洛方法起源于统计物理学，它被用于获得难解的积分的近似值，此后被广泛地应用于包括博弈研究在内的各个领域。

Q-value of an action: 所有与(s,a)相关的gain的均值. 

$$
Q(s, a)=\frac{1}{N(s, a)} \sum_{i=1}^{N(s)} \mathbb{I}_{i}(s, a) z_{i}
$$

在蒙特卡洛方法中，给定状态的动作被统一采样 uniformly sampled 的蒙特卡洛方法被描述为**flat Monte Carlo**。Ginsberg[97]和Sheppard[199]证明了**flat Monte Carlo**的威力，他们分别用这种方法实现了桥牌和拼字游戏的世界冠军水平。  
然而可以很简单地构造出 flat Monte Carlo 失败的退化情况，因为它不允许对手模型的存在。

Altho ̈fer 描述了平面蒙特卡洛在非紧密non-tight 情况下的懒惰性*laziness* [5]。他还描述了可能发生的意想不到的*basin*行为[6]，这可能被用来帮助寻找给定问题的最优UCT搜索参数。

利用已有estimate来选action, 加权重: 通过biasing action selection based on past experience，可以提高博弈论估计的可靠性。利用迄今为止收集到的estimates，将行动选择偏向那些具有higher intermediate reward的action是合理的。



#### Bandit-Based Methods   MAB 问题

**exploitation-exploration** dilemma

**K-armed bandit**  defined by random variables $X_{i, n}$ for $1 \leq i \leq K$ and $n \geq 1,$ where $i$ indicates the arm of the bandit.   
X是独立同分布的, 分布未知, 期望$\mu_{i} $未知.   

##### Regret

The policy should aim to **minimise the player's regret**  after $n$ plays:

$$
R_{N}=\mu^{\star} n-\mu_{j} \sum_{j=1}^{K} \mathbb{E}\left[T_{j}(n)\right]
$$

$\mu^{\star}$ 最大期望reward ,   $\mathbb{E}\left[T_{j}(n)\right]$ 表示 arm j 在前n次play中, 被选中的期望次数.    
即 regret 就是没有选中最佳机器造成的loss. 

**UCB 的出发点**

必须强调的是，策略在任何时候对所有arm都是非零概率，以确保不会因暂时reward比较大的sub-optimal arm而错过optimal。因此，对到目前为止所观察到的奖励设置一个能确保这一点的**upper confidence bound** 是非常重要的。        

一篇具有开创性的论文表明，对一大类奖励分布，不存在regret增长慢于$O(\ln n)$的策略。  
如果regret的增长速度within a constant factor of this rate，那么这个策略被认为是解决了exploration-exploitation问题。Lai和Robbins提出的政策利用了 置信上界指数  *upper confidence indices*，允许策略 to estimate the expected reward of a specific bandit once its index is computed. 

然而，这些指数很难计算，Agrawal[2]提出了一些策略，在这些策略中，指数可以表示为bandit到目前为止获得的总奖励的简单函数。Auer等人[13]随后提出了Agrawal的index-based policy的变体，该变体有 a finite-time regret logarithmically bound for arbitrary reward distributions with bounded support。

接下来介绍其中的一个变体，即UCB1。

 

In a seminal paper, Lai and Robbins [124] showed there exists no policy with a regret that grows slower than $O(\ln n)$ for a large class of reward distributions. A policy is subsequently deemed to resolve the exploration-exploitation problem if the growth of regret is within a constant factor of this rate. The policies proposed by Lai and Robbins made use of upper confidence

##### Upper Confidence Bounds (UCB)

UCB1 , 最简单的UCB算法. which has an expected logarithmic growth of regret uniformly over $n$ (not just asymptotically) without any prior knowledge regarding the reward distributions (which have to have their support in [0,1] ).  策略选择标准:

$$
\mathrm{UCB} 1=\overline{X}_{j}+\sqrt{\frac{2 \ln n}{n_{j}}}
$$

exploitation term + exploration term    
The exploration term is related to the size of the one-sided confidence interval for the average reward within which the true expected reward falls with overwhelming probability. 探索项与平均reward的one-sided置信区间的大小有关，在这个区间内，真实的预期reward以压倒性的概率落在该区间内。



### MCTS

MCTS基于两个基本概念：一个行动的真实值可以用随机模拟来近似；这些值可以被有效地用来调整策略，使之趋向于best-first strategy。  
该算法在之前对该树的探索结果的指导下，逐步建立了一个部分博弈树partial game tree。该树被用来估计棋子的值，随着树的建立，这些估计值会越来越准确。



#### Algorithm

基本算法涉及到迭代建立一个搜索树，直到达到某种预定义的计算预算为止--通常是时间、内存或迭代限制，这时搜索就会停止，并返回性能最好的根操作。



![image-20200417145235240](/img/2020-04-12-MCTS.assets//image-20200417145235240.png)

1. **Selection**: Starting at the root node, a **child selection policy** is recursively applied to descend through the tree until the **most urgent** expandable node is reached. A node is *expandable* if it represents a non-terminal state and has unvisited (i.e. unexpanded) children.
2. **Expansion**: One (or more) child nodes are added to expand the tree, according to the available actions.
3. **Simulation**: A simulation is run from the new node(s) according to the default policy to produce an outcome.
4. **Backpropagation**: The simulation result is "backed up" (i.e. backpropagated) through the selected nodes to update their statistics.



<img src="/img/2020-04-12-MCTS.assets//image-20200417145319907.png" alt="image-20200417145319907" style="zoom: 50%;" />



two distinct policies:

1. **Tree Policy**: Select or create a leaf node from the nodes already contained within the search tree (**selection and expansion**). 
2. **Default Policy**: Play out the domain from a given non-terminal state to produce a value estimate (**simulation**).  默认用均匀随机.



一旦search被中断或达到计算预算，search就会终止，通过某种机制选择根节点的一个动作。 四个标准

1. *Max child*: Select the root child with the highest reward.
2. *Robust child*: Select the most visited root child.
3. *Max-Robust child*: Select the root child with both the highest visit count and the highest reward. 如没有,继续搜索直到有;  效果最好
4. *Secure child*: Select the child which maximises a lower confidence bound.



#### Development 发展历程

**蒙特卡洛方法Monte Carlo methods** 已被广泛地应用于随机性和部分可观察性的博弈中，但它们同样可以应用于完全信息的确定性博弈。在大量的模拟博弈之后，从**当前状态**开始，一直到博弈结束，选择胜率最高的初始动作来推进博弈。在大多数情况下，行动被统一随机抽样（或带有某种游戏特有的启发式偏好），但没有game-theoretic guarantees博弈论理论上的保证。换句话说，即使迭代过程执行了很长时间，最后选择的action也不一定是最优的。

尽管缺乏博弈理论上最优的保证，**蒙特卡洛模拟 Monte Carlo simulations**  的精确性往往可以通过根据游戏事件的累积奖励来选择行动来改善。这可以通过跟踪树中访问的状态来实现。2006年，Coulom[70]提出了一种结合了**蒙特卡罗评估和树搜索**的新方法 **Monte Carlo evaluations with tree search**。

MCTS的突破也出现在2006年，主要归功于Kocsis和Szepesva ́ri提出的**选择性机制**，他们的目的是设计一种蒙特卡洛搜索算法Monte Carlo search，如果过早停止，错误概率较小，并且在足够的时间内收敛到博弈论最优值. 这可以通过尽快降低节点值的估计误差来实现。为了做到这一点，算法必须在利用当前最有希望的行动与探索以后可能会有更多的替代方案之间取得平衡。这种利用-探索的两难困境可以通过MAB问题的UCB1方法解决.



演化的重要事件:

1. 1990 Monte Carlo simulations can be used to **evaluate value** of state 
2. 2002 propose **UCB1** for multi-armed bandit, laying the theoretical foundation for UCT.
3. 2006 Monte Carlo evaluations for tree-based search, coining the term **Monte Carlo tree search**. 
4. 2006 associate UCB with tree-based search to give the **UCT** algorithm.
5. 2006 apply UCT to computer Go with remarkable success, with their program MOGO.
6. 2006 escribe MCTS as a broader framework for game AI [52] and general domains [54].





#### Upper Confidence Bounds for Trees (UCT)

##### The UCT algorithm

The goal of MCTS is to approximate the (true) game-theoretic value of the actions that may be taken from the current state . MCTS的目标是近似 当前状态下的行动的博弈理论上的真值。

目标通过迭代建立一个**部分搜索树 partial search tree**来实现的。树的构建方式取决于树中的节点是如何选择的。MCTS的成功，特别是在Go中，主要是由于这种**树策略 tree policy**。特别是Kocsis和Szepesva ́ri[119]，[120]提出了使用**UCB1**作为树策略。在把子节点的选择看作是一个**MAB问题**，子节点的值是由蒙特卡罗模拟近似的预期报酬，因此这些报酬对应于未知分布的随机变量。

UCB1具有一些好的特性：它非常简单、高效，并且保证within a constant factor of the best possible bound on the growth of regret.


$$
U C T=\overline{X}_{j}+2 C_{p} \sqrt{\frac{2 \ln n}{n_{j}}}
$$

$C_p>0$  is a constant. 用于调整exploration  
如果几个action的UCT值一样大,broken randomly .   
todo The values of $X_{i, t}$ and thus of $\bar{X}_j$ are understood to be within [0,1] (this holds true for both the UCB1 and the UCT proofs).   
$n_j=0$ yields a UCT value of $\infty$,    确保在任何子节点被进一步扩展之前，至少考虑过一次节点的所有子节点。 *iterated local search*

如果rewards in the range $[0,1]$, 当 $C_{p}=1 / \sqrt{2}$ 满足 Hoeffding ineqality .    
如果reward超过该范围, 则$C_p$ 可能需要不同的值. 



<img src="/img/2020-04-12-MCTS.assets//WX20200418-024549@2x.png" alt="WX20200418-024549@2x" style="zoom:33%;" />



算法3显示了一种替代性的、更有效的backup方法，用于交替下棋的双人零和博弈。这类似于minimax search的negamax变体，其中奖励值在树的每一层都被否定negated，以获得对方玩家的奖励.



<img src="/img/2020-04-12-MCTS.assets//WX20200418-024605@2x.png" alt="WX20200418-024605@2x" style="zoom:33%;" />



##### Convergence to Minimax

Kocsis和Szepesva ́ri的主要贡献是证明了在non-stationary reward distributions的情况下，UCB1的regret约束仍然有效，并证明了MCTS与UCT在不同域中的工作原理。  
然后证明，随着模拟的博弈次数增加到无穷大，树根处的失败概率（即选择次优行动的概率）以多项式的速度收敛到零。这个证明意味着，给定足够的时间（和内存），UCT 允许 MCTS 收敛到minimax tree，因此是最优的。



#### Characteristics 特点

##### Aheuristic 启发式

AlphaGo zero 已经完全不用启发了, 但游戏游戏还是需要的, 特别是逻辑相关的.

MCTS最显著的优点之一是不需要特定领域的知识，使其很容易适用于任何可以用树来建模的领域。尽管**从博弈论上讲，full-depth minimax是最优的**，但depth-limited minimax的策略的质量在很大程度上取决于用于evaluate叶子节点的启发式heuristic算法。在象棋这样的棋局中，经过几十年的研究，可靠的启发式方法已经出现了，而在象棋这样的棋局中，minimax的表现非常好。然而，在像围棋这样的情况下，分支系数要大上几个数量级，有用的启发式方法也更难制定，因此minimax的性能明显下降。
对MCTS, 使用特定领域的知识往往可以实现性能的显著提高。现在，所有性能最好的MCTS-based的围棋程序都使用了特定领域的信息，通常以pattern的形式。
在使用domain-specific知识进行biasing move selection时，需要考虑一些权衡：uniform random选择move的优点之一是速度快，可以在给定的时间内进行许多模拟。特定领域的知识通常会大幅减少模拟的数量，但也可能减少模拟结果的方差, 即探索的多样性减少。



##### Anytime

MCTS立即对每个游戏的结果进行反推，这确保了算法的每一次迭代后，所有的值都是最新的。这使得算法可以在任何时候都能从root返回一个动作；同时允许算法运行额外的迭代，通常可以改善结果。
? 使用iterative deepening可以近似一个anytime版本的minimax。但是，由于每次迭代时都会在树上增加一整层，所以progress的粒度要粗很多。



##### Asymmetric 非对称

出现的树形是非对称的, 甚至可以用来更好地理解博弈本身。

<img src="/img/2020-04-12-MCTS.assets//image-20200418224234280.png" alt="image-20200418224234280" style="zoom: 33%;" />





#### Comparison with Other Algorithms

当面对一个问题时， 如果博弈树的大小不大，而且没有可靠的启发式，那么minimax是不适合的，但MCTS是适用。另一方面，如果特定领域的知识是现成的，那么这两种算法都可能是可行的方法。
然而，正如Ramanujan等人[164]所指出的那样，MCTS方法对于象棋这样的棋类，并不像围棋这样的棋类那样成功。他们考虑了一类synthetic空间，在这些空间中，UCT明显优于minimax模型。特别是，该模型产生了有界树bounded trees，每个状态都有一个最优的动作；次优的选择用固定的加法成本来惩罚。在这个领域中，UCT明显优于minimax，而且性能上的差距随着树的深度而增大。
Ramanujan等人[162]认为，UCT在具有许多陷阱状态*trap states*（在少量的棋步内导致损失的状态）的域中表现很差，而iterative deepening minimax表现相对较好。陷阱状态在国际象棋中很常见，但在围棋中相对不常见，这可能在一定程度上解释了算法在这些棋局中的相对性能。



下面介绍各种变体.





## Reference

mcts.ai  https://www.cs.swarthmore.edu/~bryce/cs63/s16/reading/mcts.html

A Survey of Monte Carlo Tree Search Methods  http://ccg.doc.gold.ac.uk/wp-content/uploads/2016/10/browne_tciaig12_1.pdf

中文翻译概要 https://blog.csdn.net/bowean/article/details/78808584



















