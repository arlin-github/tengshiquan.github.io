---
layout:     post
title:      CS 285. Optimal Control and Planning
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

## Optimal Control and Planning

之前都是 model-free ,  现在开始 model-based 的 .  这课是 model 已知.

model 分 确定性, 随机, env 分 open, close.  close的情况更像之前强化学习.



plan 算法: 随机优化,  CEM, MCTS 

close 算法 : 针对线性系统 : LQR;    非线性  iLQR, DDP

这课很多都是控制理论的内容. 



1. Introduction to model-based reinforcement learning
2. What if we know the dynamics? How can we make decisions?
3. Stochastic optimization methods
4. **Monte Carlo tree search (MCTS)**
5. Trajectory optimization

Goals : Understand how we can perform planning with known dynamics models in discrete and continuous spaces



##### Recap: model-free reinforcement learning



$$
\underbrace{p_{\theta}\left(\mathbf{s}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{s}_{T}, \mathbf{a}_{T}\right)}_{\pi_{\theta}(\tau)} = p\left(\mathbf{s}_{1}\right) \prod_{t=1}^{T} \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}\right) 
\\
\theta^{\star}=\arg \max _{\theta} E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t}  r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]
$$

系统状态转移概率p矩阵: assume this is unknown don’t even attempt to learn it 



#### What if we knew the transition dynamics?

- Often we do know the dynamics

  1. Games (e.g., Atari games, chess, Go) 游戏
  2. Easily modeled systems (e.g., navigating a car) 方便建模
  3. Simulated environments (e.g., simulated robots, video games) 仿真环境

- Often we can learn the dynamics
  1. **System identification** – fit unknown parameters of a known model 系统识别
  2. Learning – fit a general-purpose **model** to observed transition data 直接学习转移

Does knowing the dynamics **make things easier**? Often, yes!



### Model-based reinforcement learning

1. Model-based reinforcement learning: learn the transition dynamics, then figure out how to choose actions
2. Today: how can we **make decisions** if we *know* the dynamics?
   1. How can we **choose actions under perfect knowledge** of the system dynamics?
   2. **Optimal control, trajectory optimization, planning**

3. Next week: how can we learn *unknown* dynamics?

4. How can we then also learn policies? (*e.g. by imitating optimal control*)



#### The objective

$$
\min_{\mathbf{a}_1,\ldots,\mathbf{a}_T} \sum_{t=1}^T c(\mathbf{s}_t,\mathbf{a}_t) \mbox{ s.t. } \mathbf{s}_t=f(\mathbf{s}_{t-1},\mathbf{a}_{t-1})
$$

f 就是转移函数.  同时也是这个系统的 constraint.  所以也就是一个最优化问题. 

 

##### deterministic case 

在确定性环境下，我们上面做的就是一个最优控制了.  如果说已经解决了这个最优化, 那么只要得到一个初始状态, 就能给出最优策略action的sequence.



##### The stochastic open-loop case

probability distribution:  给定一个$s_1$, controller 给出一串action sequence, 然后出现各个状态的概率. 

$$
p_\theta(\mathbf{s}_1,\ldots,\mathbf{s}_T|\mathbf{a}_1,\ldots,\mathbf{a}_T)=p(\mathbf{s}_1)\prod_{t=1}^Tp(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t) \\
\mathbf{a}_1,\ldots,\mathbf{a}_T=\arg\max_{\mathbf{a}_1,\ldots,\mathbf{a}_T}\mathbf{E}\left[\sum_{t=1}^Tr(\mathbf{s}_t,\mathbf{a}_t) |\mathbf{a}_1,\ldots,\mathbf{a}_T\right]
$$

也可以写成一个随机版本的最优化问题, 怎么取一个action sequence, 得到最大回报. 



why is this **sub optimal**?  但某些随机问题, 给出了action sequence后, 后面出现的状态并不完全由action控制. 不一定出现的每个状态都能遇到最好的action.



##### what is this “loop”?

<img src="/img/CS285.assets/image-20200323014402818.png" alt="image-20200323014402818" style="zoom:50%;" />

- open-loop ,就是controller得到s1, 然后给出一个**plan**: a1,a2...., 单向的
- close-loop   是给出一个策略$\pi$



区别在于开环系统在一开始就把所有决策单向传递给环境，因此不接受反馈；闭环系统则每次只传递单次行动，并接受下一个状态作为反馈。

open , close 用于描述 how controller  work.   



##### The stochastic closed-loop case

<img src="/img/CS285.assets/image-20200324015643702.png" alt="image-20200324015643702" style="zoom: 33%;" />
$$
p(\mathbf{s}_1,\mathbf{a}_1,\ldots,\mathbf{s}_T,\mathbf{a}_T)=p(\mathbf{s}_1)\prod_{t=1}^T\pi(\mathbf{a}_t|\mathbf{s}_t)p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)  \\
\pi=\arg\max_\pi\mathbf{E}_{\tau\sim p(\tau)}\left[\sum_{t=1}^Tr(\mathbf{s}_t,\mathbf{a}_t)\right]
$$

close-loop 因为需要一直与环境交互, 所以就需要一个策略. 

优化这想 closely policies 就与 强化学习的目标一样. 

如何构建策略:

1. neural net
2. **Time-varying linear** $$\mathbf{K}_t\mathbf{s}_t+\mathbf{k}_t$$.  没用到 learning. 反馈状态的线性组合加上一些偏移



#### open-loop planning

control 最优化的一般抽象形式:  abstract away optimal control/planning:

$$
\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}=\arg \max _{\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}} J\left(\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}\right) \quad \mathbf{A}=\arg \max _{\mathbf{A}} J(\mathbf{A})
$$

$\mathbf A$是简洁的写法, 表示action sequence.  其中$$J\left(\mathbf{a}_{1}, \ldots, \mathbf{a}_{T}\right)$$是某种函数，并不关心具体是什么, 下面就是求 argmax ,  可以用很多最优化方法,  gradient ascent , **L-BFGS** ...



##### Stochastic optimization 随机优化

如果action维度很低, 并且 plan on a short  horizon 时序很短, 可以用一个简单方法:

simplest method: guess & check,   “**random shooting method**”

1. pick $$\mathbf{A}_{1}, \ldots, \mathbf{A}_{N}$$ from some distribution (e.g., uniform)
2. choose $$\mathbf{A}_{i}$$ based on  $$\arg \max _{i} J\left(\mathbf{A}_{i}\right)$$



##### Cross-entropy method (CEM) 交叉熵法

这个方法比随机好一些.

改良之前**random shooting method**的方法, 之前是随机抽取, 一次好,一次坏的在高纬空间中要找到一系列的不错的决策序列可能性极低. 所以在之后的采样中 , 尽量提高一些好的action的几率.    

在之前的方法中，我们需要从some distribution中选取决策序列，但是关键问题是，我们使用哪一种分布？

下图假设, $\mathbf A$ 是一维的.

![Mar-23-2020 15-50-33](/img/CS285.assets/Mar-23-2020 15-50-33.gif)

1. 第一次随机采样了几个动作, 得到几个 J(A)
  
2. 觉得好的应该在这个区间内, 用个分布拟合下

3. 再次sample, 就不再使用随机了, 使用上次的信息. 

4. 发现 J(A) 的分布更加清楚, 最大值更明显

5. 继续 直到发现最优解.

这里的sample, 并不是说用action与env交互, 而是从分布中 sampling  action sequence 然后用model 来evaluate sequence 的 total reward . 



cross-entropy method with continuous-valued inputs:
1. sample $$\mathbf{A}_{1}, \ldots, \mathbf{A}_{N}$$ from $$p(\mathbf{A})$$
2. evaluate $$J\left(\mathbf{A}_{1}\right), \ldots, J\left(\mathbf{A}_{N}\right)$$
3. pick the elites  $$\mathbf{A}_{i_{1}}, \ldots, \mathbf{A}_{i_{M}}$$ with the highest value, where $M < N$ , top 10% works well
4. refit $p(\mathbf{A})$ to the elites  $$\mathbf{A}_{i_{1}}, \ldots, \mathbf{A}_{i_{M}}$$ , goto 1. 



- typically use **Gaussian distribution** for $$p(\mathbf{A})$$  最常用的还是高斯分布

- see also: CMA-ES (sort of like CEM with momentum)
- CEM 对30到50维这样的问题效果不错。



What’s the upside?  上面方法的优点

1. Very fast if parallelized   可并行, 非常快
2. Extremely simple  实现简单

What’s the problem?  问题

1. Very harsh **dimensionality limit**  维度灾难
2. **Only open-loop planning**  只能应对 open-loop, 即只给action sequence plan , 可能陷入局部最优





#### Discrete case: Monte Carlo tree search (MCTS)

MCTS 方法在**离散问题**中非常通用.     一个思考, 也能应用到连续空间的. 

<img src="/img/CS285.assets/image-20200323105025473.png" alt="image-20200323105025473" style="zoom: 33%;" />

- how to approximate value without full tree?

对一个游戏做所有状态的树搜索不可行, 因为是指数级的。这时引入MCTS, 到达一定层数开始, 树就不再展开：把此时的节点作为叶子来看待，使用一些启发式策略（也可以是随机策略）走到底来评估这些叶子节点的好坏。 

rollout 这个方法的基本思想是, 当前已经进入了一个优势很大的局面，那后面随便一般的启发式策略也足够用了。因此，在实际中，大家做MCTS的时候通常选择的就是随机策略 . 但MCTS的问题是, 对一些任务, 特别是中间没reward的, 如果前面的走的不好, 后面是无法找到正确解的.  前面的估值由后面total value传导backup回来, 所以前期的估值也不准.  所以前面有很大一段时间的random乱跑.

- can't search all paths - where to search first?

选子节点是个 **Exploit & Explore** 问题. 从根节点开始, 如何选择要各层展开expand的节点.  可以弄个权值.

intuition: choose nodes with **best reward**, but **also prefer rarely visited** nodes.  选优选少.

对深度进行了限制，这棵搜索树的节点扩展还是指数级别的。因此，我们不能指望搜索所有的路径。MCTS的最核心想法还是搜索最“有前途”的节点（**选择最大价值的节点**），然后加入一些小小的修正，来补偿那些访问比较少的部分（**也倾向于很少被访问到的节点**）。



##### generic MCTS sketch

是个递归. 

1. find a leaf $s_{l}$ using **TreePolicy** $\left(s_{1}\right)$
2. evaluate the leaf using **DefaultPolicy** $\left(s_{l}\right)$
3. update all values in tree between $s_{1}$ and $s_{l}$  , goto 1  loop 若干次
4. take best action from $s_{1}$



对每个node ,需要记下 Q以及访问次数N , Q是每次访问得到的Gain的sum, 所以Q/N就是平均的Gain.

这里这里的s是 time step.   即, 从s1走a1以后, 叶子节点都是s2, 不论遇到的是啥局面. 

![Mar-23-2020 15-35-19](/img/CS285.assets/Mar-23-2020 15-35-19-4949410.gif)



**UCT TreePolicy** $\left(s_{t}\right)$

- if $s_{t}$ not fully expanded, choose new $a_{t}$ else choose child with best Score( $s_{t+1}$ ) 

- UCT给节点打分,  如果当前没有完全展开, 则选新的, 负责选最高分的.  

- 每个节点的就是Q/N + 访问频率  .  自己访问过多,降权, 父节点访问很多, 自己升权.  

- C是一个超参. 

$$
\operatorname{Score}\left(s_{t}\right)=\frac{Q\left(s_{t}\right)}{N\left(s_{t}\right)}+2 C \sqrt{\frac{2 \ln N\left(s_{t-1}\right)}{N\left(s_{t}\right)}}
$$

Browne, Powley, Whitehouse, Lucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, Colton. (2012). **A Survey of Monte Carlo Tree Search Methods**.





#### Case study: imitation learning from MCTS

<img src="/img/CS285.assets/image-20200323165042953.png" alt="image-20200323165042953" style="zoom:50%;" />

Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning



DAgger , 然后把3 human打标改为 MCTS
1. $$\operatorname{train} \pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ from human data $$\mathcal{D}=\left\{\mathbf{o}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{o}_{N}, \mathbf{a}_{N}\right\}$$
2. run $$\pi_{\theta}\left(\mathbf{u}_{t} \vert  \mathbf{o}_{t}\right)$$ to get dataset $$\mathcal{D}_{\pi}=\left\{\mathbf{o}_{1}, \ldots, \mathbf{o}_{M}\right\}$$
3. Choose actions for states in $\mathcal{D}_{\pi}$ using MCTS
4. Aggregate: $\mathcal{D} \leftarrow \mathcal{D} \cup \mathcal{D}_{\pi}$



Why train a policy? 

- In this case, MCTS is too slow for real-time play

- Other reasons – perception, generalization, etc.: more on this later





### use derivatives

- 下面开始上微积分, 使用导数, 来求 control optimization. 

- 之前的优化目标, 最小化cost , 是个有约束的优化问题

$$
\min_{\mathbf{u}_1,\ldots,\mathbf{u}_T}\sum_{t=1}^Tc(\mathbf{x}_t,\mathbf{u}_t)~\text{s.t.}~\mathbf{x}_t=f(\mathbf{x}_{t-1},\mathbf{u}_{t-1})
$$

- 展开这个公式, 变成了无约束的优化问题

$$
\min_{\mathbf{u}_1,\ldots,\mathbf{u}_T}c(\mathbf{x}_1,\mathbf{u}_1)+c(f(\mathbf{x}_1,\mathbf{u}_1),\mathbf{u}_2)+\ldots+c(f(\ldots),\mathbf{u}_T)
$$

- usual story: differentiate via backpropagation and optimize! 通过梯度反向传播BP来优化
- need $$\frac{d f}{d \mathbf{x}_{t}}, \frac{d f}{d \mathbf{u}_{t}}, \frac{d c}{d \mathbf{x}_{t}}, \frac{d c}{d \mathbf{u}_{t}}$$   , df/du 是个矩阵
- in practice, it really helps to use a $2^{\text {nd }}$ order method!  实践中,使用二阶导数方法比较好
- 如果使用一阶导数, 因为看展开的公式, 最后一项,超级长, 如果梯度BP,则很多项相乘, 又是个exponential 问题.



#### Shooting methods vs collocation

不重要, 了解

- shooting method: **optimize over actions only** , 是个无约束优化
  - optimizing over actions and the states are the consequences of those actions 
  - choose actions, run dynamics, 类似于把actions都shoot outward, 看看哪些state hit
  - ![Mar-23-2020 17-44-47](/img/CS285.assets/Mar-23-2020 17-44-47.gif) 
  - 轨迹波动起伏特别大

$$
\min _{\mathbf{u}_{1}, \ldots, \mathbf{u}_{T}} c\left(\mathbf{x}_{1}, \mathbf{u}_{1}\right)+c\left(f\left(\mathbf{x}_{1}, \mathbf{u}_{1}\right), \mathbf{u}_{2}\right)+\cdots+c\left(f(f(\ldots) \ldots), \mathbf{u}_{T}\right)
$$

- collocation method: optimize over actions and states, with constraints

  - action 与 state 都是 variable
  - 改变了第一个action, 不会整个轨迹都波动, 稳定性高
  - ![Mar-23-2020 17-53-46](/img/CS285.assets/Mar-23-2020 17-53-46.gif)

  $$
  \min_{\mathbf{u}_1,\ldots,\mathbf{u}_T}\sum_{t=1}^Tc(\mathbf{x}_t,\mathbf{u}_t)~\text{s.t.}~\mathbf{x}_t=f(\mathbf{x}_{t-1},\mathbf{u}_{t-1})
  $$

  

### Linear case: LQR  Linear Quadratic Regulator

线性二次型调节器,  针对线性系统.   线性指 系统的动态函数f 是线性的.

LQR可得到状态线性反馈的最优控制规律，易于构成闭环最优控制。

思路:  线性二次近似,  然后动态规划 !





下面使用一种简化的setting.  非常易于计算. 

$$
\min _{\mathbf{u}_{1}, \ldots, \mathbf{u}_{T}} c\left(\mathbf{x}_{1}, \mathbf{u}_{1}\right)+c\left(f\left(\mathbf{x}_{1}, \mathbf{u}_{1}\right), \mathbf{u}_{2}\right)+\cdots+c\left(f(f(\ldots) \ldots), \mathbf{u}_{T}\right)
$$

- f() 表示为 线性 linear   ,  F是矩阵, f是常量
  
  $$
  f(\mathbf{x}_t,\mathbf{u}_t)=\mathbf{F}_t\left[\begin{array}{l}\mathbf{x}_t\\\mathbf{u}_t\end{array}\right]+\mathbf{f}_t
  $$

- Cost 表示为二次型  quadratic 
  
  $$
  c(\mathbf{x}_t,\mathbf{u}_t)=\frac{1}{2}\left[\begin{array}{l}\mathbf{x}_t\\\mathbf{u}_t\end{array}\right]^\top\mathbf{C}_t\left[\begin{array}{l}\mathbf{x}_t\\\mathbf{u}_t\end{array}\right]+\left[\begin{array}{l}\mathbf{x}_t\\\mathbf{u}_t\end{array}\right]^\top\mathbf{c}_t
  $$



下面使用 动态规划 来反向计算整个优化目标函数. 

- 最后一项, only term that depends on $$\mathbf{u}_{T}$$;  $$\mathbf{X}_{T}$$  则依赖于其他优化变量  

$$
c (\overbrace{ f(f(\ldots) \ldots)}^{\mathbf{X}_{T}(\text { unknown })}, \mathbf{u}_{T} )
$$

- Base case : solve for $$\mathbf{u}_{T}$$ only ,    先treat  $$\mathbf{X}_{T}$$ 未知,  solve  $$\mathbf{u}_{T}$$ in term of  $$\mathbf{X}_{T}$$ ,  然后 recurse backwards through time to remove the dependance on the X.
- Q函数表示的是 cost-to-go = 常量 + cost term

$$
Q(\mathbf{x}_T,\mathbf{u}_T)=\text{const}+\frac{1}{2}\left[\begin{array}{l}\mathbf{x}_T\\\mathbf{u}_T\end{array}\right]^\top\mathbf{C}_T\left[\begin{array}{l}\mathbf{x}_T\\\mathbf{u}_T\end{array}\right]+\left[\begin{array}{l}\mathbf{x}_T\\\mathbf{u}_T\end{array}\right]^\top\mathbf{c}_T
$$

- 下面将Cost 矩阵 分为 X, u 两部分.  其中矩阵是对称的. 

$$
\mathbf{C}_T=\left[\begin{array}{l}\mathbf{C}_{\mathbf{x}_T,\mathbf{x}_T}&\mathbf{C}_{\mathbf{x}_T,\mathbf{u}_T}\\\mathbf{C}_{\mathbf{u}_T,\mathbf{x}_T}&\mathbf{C}_{\mathbf{u}_T,\mathbf{u}_T}\end{array}\right] \quad \quad \mathbf{c}_T=\left[\begin{array}{l}\mathbf{c}_{\mathbf{x}_T}\\\mathbf{c}_{\mathbf{u}_T}\end{array}\right]
$$

- 下面计算梯度: 求极值点

  $$
  \nabla_{\mathbf{u}_T}Q(\mathbf{x}_T,\mathbf{u}_T)=\mathbf{C}_{\mathbf{u}_T,\mathbf{x}_T}\mathbf{x}_T+\mathbf{C}_{\mathbf{u}_T,\mathbf{u}_T}\mathbf{u}_T+\mathbf{c}_{\mathbf{u}_T}^\top=0
  \\
  \mathbf{u}_{T}=-\mathbf{C}_{\mathbf{u}_{T}, \mathbf{u}_{T}}^{-1}\left(\mathbf{C}_{\mathbf{u}_{T}, \mathbf{x}_{T}} \mathbf{x}_{T}+\mathbf{c}_{\mathbf{u}_{T}}\right)
  $$

- 下面为了表达简洁,  可见**最优的action是state的一个线性函数**. 

  $$
  \quad \mathbf{u}_{T}=\mathbf{K}_{T} \mathbf{x}_{T}+\mathbf{k}_{T} \\
  \mathbf{K}_T=-\mathbf{C}_{\mathbf{u}_T,\mathbf{u}_T}^{-1}\mathbf{C}_{\mathbf{u}_T,\mathbf{x}_T} \\
  \mathbf{k}_T=-\mathbf{C}_{\mathbf{u}_T,\mathbf{u}_T}^{-1}\mathbf{c}_{\mathbf{u}_T}
  $$

- 因为 $$\mathbf{u}_{T}$$ 完全由 $$\mathbf{x}_{T}$$决定, 所以遇到u的时候, 直接替换成x. 将u的公式代入Q, 即把 Q(x,u)公式变成V(x)公式.  

$$
V\left(\mathbf{x}_{T}\right)=\operatorname{const}+\frac{1}{2}\left[\begin{array}{c}
\mathbf{x}_{T} \\
\mathbf{K}_{T} \mathbf{x}_{T}+\mathbf{k}_{T}
\end{array}\right]^{T} \mathbf{C}_{T}\left[\begin{array}{c}
\mathbf{x}_{T} \\
\mathbf{K}_{T} \mathbf{x}_{T}+\mathbf{k}_{T}
\end{array}\right]+\left[\begin{array}{c}
\mathbf{x}_{T} \\
\mathbf{K}_{T} \mathbf{x}_{T}+\mathbf{k}_{T}
\end{array}\right]^{T} \mathbf{c}_{T}
$$

$$
V\left(\mathbf{x}_{T}\right)=\frac{1}{2} \mathbf{x}_{T}^{T} \mathbf{C}_{\mathbf{x}_{T}, \mathbf{x}_{T}} \mathbf{x}_{T}+\frac{1}{2} \mathbf{x}_{T}^{T} \mathbf{C}_{\mathbf{x}_{T}, \mathbf{u}_{T}} \mathbf{K}_{T} \mathbf{x}_{T}+\frac{1}{2} \mathbf{x}_{T}^{T} \mathbf{K}_{T}^{T} \mathbf{C}_{\mathbf{u}_{T}, \mathbf{x}_{T}} \mathbf{x}_{T}+\frac{1}{2} \mathbf{x}_{T}^{T} \mathbf{K}_{T}^{T} \mathbf{C}_{\mathbf{u}_{T}, \mathbf{u}_{T}} \mathbf{K}_{T} \mathbf{x}_{T}+ \\
\mathbf{x}_{T}^{T} \mathbf{K}_{T}^{T} \mathbf{C}_{\mathbf{u}_{T}, \mathbf{u}_{T}} \mathbf{k}_{T}+\frac{1}{2} \mathbf{x}_{T}^{T} \mathbf{C}_{\mathbf{x}_{T}, \mathbf{u}_{T}} \mathbf{k}_{T}+\mathbf{x}_{T}^{T} \mathbf{c}_{\mathbf{x}_{T}}+\mathbf{x}_{T}^{T} \mathbf{K}_{T}^{T} \mathbf{c}_{\mathbf{u}_{T}}+\text { const }
$$

- 展开整理后, 发现只有二次项,一次项以及常数. 

  $$
  V(\mathbf{x}_T)=\text{const}+\frac{1}{2}\mathbf{x}_T^\top\mathbf{V}_T\mathbf{x}_T+\mathbf{x}_T^\top\mathbf{v}_T \\
  \mathbf{V}_T=\mathbf{C}_{\mathbf{x}_T,\mathbf{x}_T}+\mathbf{C}_{\mathbf{x}_T,\mathbf{u}_T}\mathbf{K}_T+\mathbf{K}_T^\top\mathbf{C}_{\mathbf{u}_T,\mathbf{x}_T}+\mathbf{K}_T^\top\mathbf{C}_{\mathbf{u}_T,\mathbf{u}_T}\mathbf{K}_T 
  \\
  \mathbf{v}_T=\mathbf{c}_{\mathbf{x}_T}+\mathbf{C}_{\mathbf{x}_T,\mathbf{u}_T}\mathbf{k}_T+\mathbf{K}_T^\top\mathbf{c}_{\mathbf{u}_T}+\mathbf{K}_T^\top\mathbf{C}_{\mathbf{u}_T,\mathbf{u}_T}\mathbf{k}_T
  $$
  
- 开始反向递归, 求倒数第二项.  Solve for $$\mathbf{u}_{T-1}$$ in terms of $$\mathbf{x}_{T-1}$$ ; 注意
  $$\mathbf{u}_{T-1}$$ affects $$\mathbf{x}_{T} !$$   由下式可以看出 X_T 由u,x两个变量影响

$$
f(\mathbf{x}_{T-1},\mathbf{u}_{T-1})=\mathbf{x}_T=\mathbf{F}_{T-1}\left[\begin{array}{l}\mathbf{x}_{T-1}\\\mathbf{u}_{T-1}\end{array}\right]+\mathbf{f}_{T-1}
$$

- 再由 Q = cost +  V  , 注意这里的 V(X_T) 是下个状态的.
  $$
  \begin{aligned}
  &Q\left(\mathbf{x}_{T-1}, \mathbf{u}_{T-1}\right)=\mathrm{const}+\frac{1}{2}\left[\begin{array}{c}
  \mathbf{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]^{T} \mathbf{C}_{T-1}\left[\begin{array}{c}
  \mathbf{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]+\left[\begin{array}{c}
  \mathbf{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]^{T} \mathbf{c}_{T-1}+ V\left( f\left(\mathbf{x}_{T-1}, \mathbf{u}_{T-1}\right)\right)\\
   & \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad    \overbrace{ V\left(\mathrm{x}_{T}\right)=\mathrm{const}+\frac{1}{2} \mathrm{x}_{T}^{T} \mathbf{V}_{T} \mathbf{x}_{T}+\mathbf{x}_{T}^{T} \mathbf{v}_{T}}
  \end{aligned}
  $$
  
- 求将转移公式代入 $V(X_T)$  , 公式不完整..
  
  $$
  V\left(\mathrm{x}_{T}\right)=\text { const }+\frac{1}{2}\left[\begin{array}{c}
  \mathrm{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]^{T} \mathbf{F}_{T-1}^{T} \mathbf{V}_{T} \mathbf{F}_{T-1}\left[\begin{array}{c}
  \mathbf{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]+\left[\begin{array}{c}
  \mathbf{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]^{T} \mathbf{F}_{T-1}^{T} \mathbf{V}_{T} \mathbf{f}_{T-1}+\left[\begin{array}{c}
  \mathbf{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]^{T} \mathbf{F}_{T-1}^{T} ...
  $$

- 整理得到, 递推公式:

  $$
  \begin{aligned}
  &Q\left(\mathbf{x}_{T-1}, \mathbf{u}_{T-1}\right)=\text { const }+\frac{1}{2}\left[\begin{array}{c}
  \mathbf{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]^{T} \mathbf{Q}_{T-1}\left[\begin{array}{c}
  \mathbf{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]+\left[\begin{array}{c}
  \mathbf{x}_{T-1} \\
  \mathbf{u}_{T-1}
  \end{array}\right]^{T} \mathbf{q}_{T-1}\\
  &\mathbf{Q}_{T-1}=\mathbf{C}_{T-1}+\mathbf{F}_{T-1}^{T} \mathbf{V}_{T} \mathbf{F}_{T-1}\\
  &\mathbf{q}_{T-1}=\mathbf{c}_{T-1}+\mathbf{F}_{T-1}^{T} \mathbf{V}_{T} \mathbf{f}_{T-1}+\mathbf{F}_{T-1}^{T} \mathbf{v}_{T}
  \end{aligned}
  $$

- 求导求极值:

  $$
  \begin{array}{l}
  \nabla_{u_{T-1}} Q\left(\mathbf{x}_{T-1}, \mathbf{u}_{T-1}\right)=\mathbf{Q}_{\mathbf{u}_{T-1}, \mathbf{X}_{T-1}} \mathbf{x}_{T-1}+\mathbf{Q}_{\mathbf{u}_{T-1}, \mathbf{u}_{T-1}} \mathbf{u}_{T-1}+\mathbf{q}_{\mathbf{u}_{T-1}}^{T}=0 \\
  \mathbf{u}_{T-1}=\mathbf{K}_{T-1} \mathbf{x}_{T-1}+\mathbf{k}_{T-1} \quad \mathbf{K}_{T-1}=-\mathbf{Q}_{\mathbf{u}_{T-1}, \mathbf{u}_{T-1}}^{-1} \mathbf{Q}_{\mathbf{u}_{T-1}, \mathbf{x}_{T-1}} \quad \quad \mathbf{k}_{T-1}=-\mathbf{Q}_{\mathbf{u}_{T-1}, \mathbf{u}_{T-1}}^{-1} \mathbf{q}_{\mathbf{u}_{T-1}}
  \end{array}
  $$



具体算法:

##### Backward recursion

- for $t=T$ to 1:
  - $$\mathbf{Q}_{t}=\mathbf{C}_{t}+\mathbf{F}_{t}^{T} \mathbf{V}_{t+1} \mathbf{F}_{t}$$ .
  - $$\mathbf{q}_{t}=\mathbf{c}_{t}+\mathbf{F}_{t}^{T} \mathbf{V}_{t+1} \mathbf{f}_{t}+\mathbf{F}_{t}^{T} \mathbf{v}_{t+1}$$ .
  - $$Q\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)= \text{const}+\frac{1}{2}\left[\begin{array}{l}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]^{T} \mathbf{Q}_{t}\left[\begin{array}{l}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]+\left[\begin{array}{l}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]^{T} \mathbf{q}_{t}$$ . 
  - $$\mathbf{u}_{t} \leftarrow \arg \min _{\mathbf{u}_{t}} Q\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathbf{K}_{t} \mathbf{x}_{t}+\mathbf{k}_{t}$$.
  - $$\mathbf{K}_{t}=-\mathbf{Q}_{\mathbf{u}_{t}, \mathbf{u}_{t}}^{-1} \mathbf{Q}_{\mathbf{u}_{t}, \mathbf{x}_{t}}$$ .
  - $$\mathbf{k}_{t}=-\mathbf{Q}_{\mathbf{u}_{t}, \mathbf{u}_{t}}^{-1} \mathbf{q}_{\mathbf{u}_{t}}$$.
  - $$\mathbf{V}_{t}=\mathbf{Q}_{\mathbf{x}_{t}, \mathbf{x}_{t}}+\mathbf{Q}_{\mathbf{x}_{t}, \mathbf{u}_{t}} \mathbf{K}_{t}+\mathbf{K}_{t}^{T} \mathbf{Q}_{\mathbf{u}_{t}, \mathbf{x}_{t}}+\mathbf{K}_{t}^{T} \mathbf{Q}_{\mathbf{u}_{t}, \mathbf{u}_{t}} \mathbf{K}_{t}$$.
  - $$\mathbf{v}_{t}=\mathbf{q}_{\mathbf{x}_{t}}+\mathbf{Q}_{\mathbf{x}_{t}, \mathbf{u}_{t}} \mathbf{k}_{t}+\mathbf{K}_{t}^{T} \mathbf{Q}_{\mathbf{u}_{t}}+\mathbf{K}_{t}^{T} \mathbf{Q}_{\mathbf{u}_{t}, \mathbf{u}_{t}} \mathbf{k}_{t}$$.
  - $$V\left(\mathbf{x}_{t}\right)= \text{const}+\frac{1}{2} \mathbf{x}_{t}^{T} \mathbf{V}_{t} \mathbf{x}_{t}+\mathbf{x}_{t}^{T} \mathbf{v}_{t}$$ .

 思路就是:  反向递推,  通过Q极值点得到 u与x 的关系, 然后代入消去u_t, 再求V, 再用V带入上一时间步. 

<img src="/img/CS285.assets/image-20200324013332894.png" alt="image-20200324013332894" style="zoom:50%;" />

一路倒推回来以后, 我们是知道第一个状态x_1的,再正向代入回去得到最优策略.

##### Forward recursion

- for $t=1$ to $T:$
  - $$\mathbf{u}_{t}=\mathbf{K}_{t} \mathbf{x}_{t}+\mathbf{k}_{t} $$.
  - $$\mathbf{x}_{t+1}=f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)$$ . 



上面的Q与V 与之前公式含义类似

- Q: total cost from now until end if we take $$\mathbf{u}_{t}$$ from state $$\mathbf{x}_{t}$$ 
- V: total cost from now until end from state $$\mathbf{x}_{t}$$
- $$V\left(\mathbf{x}_{t}\right)=\min _{\mathbf{u}_{t}} Q\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)$$ .





#### Stochastic dynamics

使用LQR来解决随机环境任务. 

- $$f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathbf{F}_{t}\left[\begin{array}{c}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]+\mathbf{f}_{t}$$ .
- $$\mathbf{x}_{t+1} \sim p\left(\mathbf{x}_{t+1} \vert \mathbf{x}_{t}, \mathbf{u}_{t}\right)$$. 
- 常见方法使用高斯分布来建模 , 相当于是在原来的确定性过程中加入了一些（单峰）高斯噪音，噪音协方差阵可以是时变的
- $$p\left(\mathbf{x}_{t+1} \vert  \mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathcal{N}\left(\mathbf{F}_{t}\left[\begin{array}{c}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]+\mathbf{f}_{t}, \Sigma_{t}\right)$$. 
- Solution: choose actions according to $$\mathbf{u}_{t}=\mathbf{K}_{t} \mathbf{x}_{t}+\mathbf{k}_{t}$$ .
- $$\mathbf{x}_{t} \sim p\left(\mathbf{x}_{t}\right)$$,  no longer deterministic, but $$p\left(\mathbf{x}_{t}\right)$$ is Gaussia
- no change to algorithm! can ignore $\Sigma_{t}$ due to symmetry of Gaussians (checking this is left as an exercise; hint: the expectation of a quadratic under a Gaussian has an analytic solution) 在这个问题中，我们可以忽略高斯噪音的影响, 其原因主要是高斯分布具有对称性（解析地看，高斯随机变量的二次型的期望有解析解）



#### Nonlinear case: DDP/iterative LQR

现在讨论非线性系统, 这个才是主要的. 使用泰勒展开来近似. 

- Linear-quadratic assumptions:

  $$f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathbf{F}_{t}\left[\begin{array}{c}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]+\mathbf{f}_{t}$$.
  $$c\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=\frac{1}{2}\left[\begin{array}{c}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]^{T} \mathbf{C}_{t}\left[\begin{array}{c}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]+\left[\begin{array}{c}\mathbf{x}_{t} \\ \mathbf{u}_{t}\end{array}\right]^{T} \mathbf{c}_{t}$$.
  
- Can we approximate a nonlinear system as a linear-quadratic system? 

  $$f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right) \approx f\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)+\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}} f\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)\left[\begin{array}{c}\mathbf{x}_{t}-\hat{\mathbf{x}}_{t} \\ \mathbf{u}_{t}-\hat{\mathbf{u}}_{t}\end{array}\right]$$
  $$c\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right) \approx c\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)+\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}} c\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)\left[\begin{array}{c}\mathbf{x}_{t}-\hat{\mathbf{x}}_{t} \\ \mathbf{u}_{t}-\hat{\mathbf{u}}_{t}\end{array}\right]+\frac{1}{2}\left[\begin{array}{c}\mathbf{x}_{t}-\hat{\mathbf{x}}_{t} \\ \mathbf{u}_{t}-\hat{\mathbf{u}}_{t}\end{array}\right]^{T} \nabla_{\mathbf{x}_{t} \mathbf{u}_{t}}^{2} c\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)\left[\begin{array}{c}\mathbf{x}_{t}-\hat{\mathbf{x}}_{t} \\ \mathbf{u}_{t}-\hat{\mathbf{u}}_{t}\end{array}\right]$$

- 近似得到

  $$
  \bar{f}(\delta\mathbf{x}_t,\delta\mathbf{u}_t)=\mathbf{F}_t\left[\begin{array}{l}\delta\mathbf{x}_t\\\delta\mathbf{u}_t\end{array}\right] \quad , \quad
  \bar{c}(\delta\mathbf{x}_t,\delta\mathbf{u}_t)=\frac{1}{2}\left[\begin{array}{l}\delta\mathbf{x}_t\\\delta\mathbf{u}_t\end{array}\right]^\top\mathbf{C}_t\left[\begin{array}{l}\delta\mathbf{x}_t\\\delta\mathbf{u}_t\end{array}\right]+\left[\begin{array}{l}\delta\mathbf{x}_t\\\delta\mathbf{u}_t\end{array}\right]^\top\mathbf{c}_t
  $$
  
   其中, $$\mathbf{F}_t=\nabla_{\mathbf{x}_t,\mathbf{u}_t}f(\hat{\mathbf{x}}_t,\hat{\mathbf{u}}_t) , \mathbf{C}_t=\nabla^2_{\mathbf{x}_t,\mathbf{u}_t}c(\hat{\mathbf{x}}_t,\hat{\mathbf{u}}_t),   \mathbf{c}_t=\nabla_{\mathbf{x}_t,\mathbf{u}_t}c(\hat{\mathbf{x}}_t,\hat{\mathbf{u}}_t)$$

- 定义
  $$
  \begin{aligned}
  &\delta \mathbf{x}_{t}=\mathbf{x}_{t}-\hat{\mathbf{x}}_{t}\\
  &\delta \mathbf{u}_{t}=\mathbf{u}_{t}-\hat{\mathbf{u}}_{t}
  \end{aligned}
  $$

Now we can run LQR with dynamics $\bar{f},$ cost $\bar{c},$ state $$\delta \mathbf{x}_{t},$$ and action $$\delta \mathbf{u}_{t}$$ 



**Iterative LQR** (simplified pseudocode)

- until convergence:
  - $$\mathbf{F}_{t}=\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}} f\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)$$.
  - $$\mathbf{c}_{t}=\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}} c\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right) $$.
  - $$\mathbf{C}_{t}=\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}}^{2} c\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)$$
  - Run LQR backward pass on state $$\delta \mathbf{x}_{t}=\mathbf{x}_{t}-\hat{\mathbf{x}}_{t}$$ and action $$\delta \mathbf{u}_{t}=\mathbf{u}_{t}-\hat{\mathbf{u}}_{t}$$
  - Run forward pass with real nonlinear dynamics and $$\mathbf{u}_{t}=\mathbf{K}_{t}\left(\mathbf{x}_{t}-\hat{\mathbf{x}}_{t}\right)+\mathbf{k}_{t}+\hat{\mathbf{u}}_{t}$$
  - Update $$\hat{\mathbf{x}}_{t}$$ and $$\hat{\mathbf{u}}_{t}$$ based on states and actions in forward pass





#### 跟牛顿法类比: Compare to Newton's method for computing $\min _{\mathbf{x}} g(\mathbf{x}):$

LQR 类似于 对该类问题 高效的使用牛顿法, 但不需要计算 Hessian 矩阵 on the size of whole trajectories.  只求一个 action space 维度大小的逆矩阵.  整个算法复杂度与时间是线性的.

- until convergence:

$$
\begin{array}{l}
\mathbf{g}=\nabla_{\mathbf{x}} g(\hat{\mathbf{x}}) \\
\mathbf{H}=\nabla_{\mathbf{x}}^{2} g(\hat{\mathbf{x}}) \\
\hat{\mathbf{x}} \leftarrow \arg \min _{\mathbf{x}} \frac{1}{2}(\mathbf{x}-\hat{\mathbf{x}})^{T} \mathbf{H}(\mathbf{x}-\hat{\mathbf{x}})+\mathbf{g}^{T}(\mathbf{x}-\hat{\mathbf{x}})
\end{array}
$$

- Iterative LQR (iLQR) is the same idea: locally approximate a complex nonlinear function via Taylor expansion
- In fact, iLQR is an approximation of Newton's method for solving $$\min _{\mathbf{u}_{1}, \ldots, \mathbf{u}_{T}} c\left(\mathbf{x}_{1}, \mathbf{u}_{1}\right)+c\left(f\left(\mathbf{x}_{1}, \mathbf{u}_{1}\right), \mathbf{u}_{2}\right)+\cdots+c\left(f(f(\ldots) \ldots), \mathbf{u}_{T}\right)$$





下面引入 f 函数的二阶信息

To get Newton's method, need to use **second order** dynamics approximation:
$$
f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right) \approx f\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)+\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}} f\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right)\left[\begin{array}{c}\delta \mathbf{x}_{t} \\ \delta \mathbf{u}_{t}\end{array}\right]+\frac{1}{2}\left(\nabla_{\mathbf{x}_{t}, \mathbf{u}_{t}}^{2} f\left(\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}\right) \cdot\left[\begin{array}{c}\delta \mathbf{x}_{t} \\ \delta \mathbf{u}_{t}\end{array}\right]\right)\left[\begin{array}{l}\delta \mathbf{x}_{t} \\ \delta \mathbf{u}_{t}\end{array}\right]
$$

该方法, 不推荐 ,因为一般的任务,都是 cost函数简单,  状态转移函数f复杂 , 二阶求解更复杂. 

##### differential dynamic programming (DDP) 差分动态规划





#### Additional reading

1. Mayne, Jacobson. (1970). Differential dynamic programming. 
   - Original differential dynamic programming algorithm.

2. Tassa, Erez, Todorov. (2012). Synthesis and Stabilization of Complex Behaviors through Online Trajectory Optimization. 
   - Practical guide for implementing non-linear iterative LQR.

3. Levine, Abbeel. (2014). Learning Neural Network Policies with Guided Policy Search under Unknown Dynamics.
   - Probabilistic formulation and trust region alternative to deterministic line search.

