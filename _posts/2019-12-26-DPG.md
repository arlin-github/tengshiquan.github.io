---
layout:     post
title:      Deterministic Policy Gradient Algorithms
subtitle:   Note on "Deterministic Policy Gradient Algorithms"
date:       2019-12-26 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-dpg.jpg"
catalog: true
tags:
    - ai
    - pg
    - dpg
---



# Note on Deterministic Policy Gradient Algorithms

论文笔记



### Introduction

策略梯度PG 算法被广泛用于 连续动作空间continuous action spaces 的问题. 基本思想是以参数化的概率函数 $\pi_\theta(a \vert s) = \mathbb{P}[a \vert s; \theta]$ 来表示一个策略, 依据参数$\theta$, 在state下选择action. 然后沿着提升总体reward的方向来调整参数.

DPG(deterministic policy gradient) 用一种确定性的策略形式 $a=\mu_\theta(s)$.  那DPG能否按照PG的一样方式来调整参数呢?  在这之前, 普遍认为不存在 DPG算法. 然而, DPG存在, 并且有简单的model-free的方式, 沿着 action-value 函数的梯度.   并且, DPG是 SPG(stochastic policy gradient) 在policy方差趋近于0时的极限情况.

从实践角度看,  SPG涉及s与a,   DPG只涉及s.  于是, 计算SPG需要更多的sample, 特别是 action spaces 很大时. 

要遍历整个(state, action)的空间, SPG通常是必要的.  为了保证DPG可以遍历足够多, 需要引入off-policy算法. 基本思想是 按照随机策略来选择action(为了保证探索性exploration), 再通过DPG来学习(利用DPG的高效性).  作者用DPG 推导出一个  off-policy actor- critic 算法, 用可导函数拟合action value,  然后沿着该函数的导数方向来调整参数. also introduce a notion of compatible function approximation for deterministic policy gradients, to ensure that the approximation does not bias the policy gradient. 

benchmark 结果显示

1. DPG在高维任务中比SPG有优势
2. 计算成本在 action 维度 以及 参数数量 上都是线性的
3. 某些问题, no functionality to inject noise into the controller, SPG不适用, DPG可能可以



### Background

#### Preliminaries

在MDP过程中，要寻找是一个能使累计reward最大化的策略，目标函数定义如下

$$
\begin{align*} J(\pi_\theta) &= \int_\mathcal{S} \rho^\pi(s) \int_\mathcal{A} \pi_\theta (s,a)r(s,a) \mathrm{d}a \mathrm{d}s \\ &= \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [r(s,a)] \end{align*} \tag{1}
$$



#### Stochastic Policy Gradient Theorem

策略梯度的基本思想就是沿着 $\nabla_\theta J(\pi_\theta)$ 方向调整参数：

$$
\begin{align*} \nabla_\theta J(\pi_\theta) &= \int_\mathcal{S} \rho^\pi(s) \int_\mathcal{A} \nabla_\theta \pi_\theta (s,a) Q^\pi(s,a) \mathrm{d}a \mathrm{d}s \\ &= \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(a\vert s) Q^\pi(s,a)] \end{align*} \tag{2}
$$

尽管state的分布$\rho^\pi(s)$取决于policy参数,  policy参数的梯度却不依赖state的概率分布函数的梯度. 

所有的梯度算法都涉及到一个问题, 就是求 action-value的估值函数; 最简单的方式是使用 sample return G作为Q(s,a)的估值,  由此引出 REINFORCE 算法



#### Stochastic Actor-Critic Algorithms

actor-critic 建立在pg原理之上. actor沿着上面公式2的梯度 调整参数. 因为$Q^\pi(s,a)$未知, 所以使用参数w的函数$Q^w(s,a)$来代替.  critic使用诸如TD(temporal-difference)等policy evalution估值算法来估计estimate $Q^w(s,a) \approx Q^\pi(s,a)$

通常, 使用$Q^w(s,a)$来拟合$Q^\pi(s,a)$ 通过会带来bias. 然而, 如果该近似函数满足以下条件则是无偏的:

1. $Q^w(s,a) = \nabla_\theta \log \pi_\theta(a \vert s)^\top w$ 
2. 按照最小化 mse $\epsilon^2(w) = E_{s \sim \rho^\pi, a \sim \pi_\theta} \Big[(Q^w(s,a) - Q^\pi(s,a)) \Big]^2$

即Q是 随机策略的特征$\nabla_\theta \log \pi_\theta(a \vert s)$的线性函数, 并且w是 这些特征线性回归$Q^\pi(s,a)$的解

$$
\nabla_\theta J(\pi_\theta)  = \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(a\vert s) Q^w(s,a)]\tag{3}
$$

通常条件2会被放宽, 为了利用更高效的td算法来policy evalution. 

如果两个条件都满足, 则该算法可以说没有用上critic, 更多的像REINFORCE算法. (critic一定要上td)



#### Off-Policy Actor-Critic

用不同的行为策略 $\beta(s,a) \neq \pi_\theta(s,a)$ 来采样获得轨迹trajectories, 再off-policy 来估计policy gradient 很有用.

在off-policy环境下, 评估目标函数改为

$$
\begin{align*} J_\beta (\pi_\theta) &= \int_\mathcal{S} \rho^\beta(s) V^\pi(s) \mathrm{d}s \\ &= \int_\mathcal{S} \int_\mathcal{A} \rho^\beta(s) \pi_\theta (s,a)Q^\pi(s,a) \mathrm{d}a \mathrm{d}s \end{align*}
$$

求梯度并取近似,  得到 off-policy policy-gradient  

$$
\begin{align*}
\nabla_\theta J_\beta(\pi_\theta) & \approx \int_\mathcal{S} \int_\mathcal{A} \rho^\beta(s) \nabla_\theta \pi_\theta (s,a) Q^\pi(s,a) \mathrm{d}a \mathrm{d}s \tag{4} \\
&= \mathbb{E}_{s \sim \rho^\beta, a \sim \beta} \left[ \frac{\pi_\theta(a \vert s)}{\beta_\theta(a \vert s)} \nabla_\theta \log \pi_\theta(a \vert s) Q^\pi(s,a) \right]  \tag{5}
\end{align*}
$$

取近似,是因为丢掉了action-value梯度$\nabla_\theta Q^\pi(s,a)$ 这一项.  Degris et al. (2012b) 认为这是个很好的近似, since it can preserve the set of local optima to which gradient ascent converges. 

Off-Policy Actor-Critic (OffPAC)算法, 使用行为策略$\beta(s,a)$来生成trajectories. critic 使用gradient TD learning , off-policy 从这些trajectories   估计 $V^v(s) \approx V^\pi(s)$ ,  actor 利用trajectories (off-policy)沿着 公式5的stochastic gradient ascent 方向调整参数$\theta$.  公式5里的 $Q^\pi(s,a)$ 是未知的,  用 TD-error $\delta_t=r_{t+1} + \gamma V^v(s_{t+1})-V^v(s_t)$ 代替. 因为是用 $\beta$ 来采样, 所以 actor 和 critic都用importance sampling ratio $\frac{π_θ(a\vert s)}{β_θ(a\vert s)}$. 



### Gradients of Deterministic Policies

现在考虑怎么将PG拓展到 deterministic policies . 

#### Action-Value Gradients

大多数mdoel-free算法都是基于GPI(generalised policy iteration), 轮流进行 policy evaluation 和  policy improvement. Policy evaluation 通过 MC或TD来估计 action-value function $Q^\pi(s,a)$ . Policy improvement 按照估计好的 action-value 函数来更新policy. 最常用的是 greedy maximisation(或soft maximisation) , $\mu^{k+1}(s) = \arg \max_a Q^{\mu^k}(s,a)$

对continuous action spaces问题, 贪婪的 policy improvement 是有问题的, 需要在每一步计算全局的最大值.  有一种简单的替代方式,  沿着Q的梯度方向, 而不是 全局最大值Q的梯度方向. 特别的, 对每个s, policy参数 $θ^{k+1}$ 沿着$\nabla_θ Q^{μ^k}(s,μ_θ(s))$ 梯度来更新. 每个state的梯度可以按照分布distribution平均一下:

$$
\theta^{k+1} = \theta^k + \alpha  \mathbb{E}_{s \sim \rho^{\mu^k}} \left[ \nabla_θ Q^{μ^k}(s,μ_θ(s)) \right] \tag{6}
$$

按照链式法则, 改为先求a的导数再求$\theta$的导数:

$$
\theta^{k+1} = \theta^k + \alpha  \mathbb{E}_{s \sim \rho^{\mu^k}} \bigg[ \nabla_θ \mu_\theta(s) \nabla_a Q^{\mu^k} (s,a)\Big\vert_{a=\mu_\theta(s)} \bigg] \tag{7}
$$

 $\nabla_θ \mu_\theta(s)$ 是一个 雅克比矩阵.  

当改变policy时, 会访问不同的state, 造成 $\rho^\mu$改变.  在不考虑state分布会改变的情况下, 上面方法不能保证improvement.

然而, 下面的理论显示, 像SPG, 没有必要计算 state分布的梯度, 上面的更新是准确的沿着 目标函数的梯度. 



#### Deterministic Policy Gradient Theorem









## References

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, et al.. Deterministic Policy Gradient Algorithms. 



