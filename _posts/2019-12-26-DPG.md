---
layout:     post
title:      Deterministic Policy Gradient Algorithms
subtitle:   Note on "Deterministic Policy Gradient Algorithms"
date:       2019-12-26 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-dpg.jpg"
catalog: true
tags:
    - ai
    - pg
    - dpg
---



# Note on "Deterministic Policy Gradient Algorithms"

论文笔记

个人理解: 

1. DPG是SPG的一种特例, 毕竟什么都可以放进概率论. 但是这种特例是对问题求解的一种简化, 效率上会更好.
2. $a=\mu_\theta(s)$ 直接给出a(离散的是下标, 连续的是值) , 而不是具体的概率, 是一种简化; 但同时也限定了,对问题的解必定是有局限的. 比如某个s下, a1,a2一样好, 但这种情况只会给出a1
3. SPG对s,a的两个积分来求期望,  DPG 只对s的积分求期望, 但同时对 a 对 $\theta$ 都求导
4. 论文思路, PG > AC > deterministic Q gradient > DPG > DAC on-policy > DAC off-policy 





### Introduction

策略梯度PG 算法被广泛用于 连续动作空间continuous action spaces 的问题. 基本思想是以参数化的概率函数 $\pi_\theta(a \vert s) = \mathbb{P}[a \vert s; \theta]$ 来表示一个策略, 依据参数$\theta$, 在state下选择action. 然后沿着提升总体reward的方向来调整参数.

DPG(deterministic policy gradient) 用一种确定性的策略形式 $a=\mu_\theta(s)$.  

[^_^]: 直接给出选哪个a而不是给出所有a的几率, 所以其实是一种简化

那DPG能否按照PG的一样方式来调整参数呢?  在这之前, 普遍认为不存在 DPG算法. 然而, DPG存在, 并且有简单的model-free的方式, 沿着 action-value 函数的梯度.   并且, DPG是 SPG(stochastic policy gradient) 在policy方差趋近于0时的极限情况.

从实践角度看,  SPG涉及s与a,   DPG只涉及s.  于是, 计算SPG需要更多的sample, 特别是 action spaces 很大时. 

要遍历整个(state, action)的空间, SPG通常是必要的.  为了保证DPG可以遍历足够多, 需要引入off-policy算法. 基本思想是 按照随机策略来选择action(为了保证探索性exploration), 再通过DPG来学习(利用DPG的高效性).  作者用DPG 推导出一个  off-policy actor- critic 算法, 用可导函数拟合action value,  然后沿着该函数的导数方向来调整参数. also introduce a notion of compatible function approximation for deterministic policy gradients, to ensure that the approximation does not bias the policy gradient. 

benchmark 结果显示

1. DPG在高维任务中比SPG有优势
2. 计算成本在 action 维度 以及 参数数量 上都是线性的
3. 某些问题, no functionality to inject noise into the controller, SPG不适用, DPG可能可以



> **确定性策略的优点：需要采样的数据少，算法效率高**
>
> 然而，确定性策略的动作是确定的，所以如果确定性策略梯度存在的话，策略梯度的求解不需要在动作空间进行采样积分。因此，相比于随机策略方法，确定性策略需要的样本数据要小。尤其是对那些动作空间很大的智能体，比如多关节机器人等，其动作空间维数很大。如果用随机策略，需要在这些动作空间中进行大量的采样。
>
> 通常来说，确定性策略方法的效率比随机策略的效率高十倍，这也是确定性策略方法最主要的优点。
>
> 相比于确定性策略，随机策略也有它自身的优点：**随机策略将探索和改进集成到一个策略中**
>
> 强化学习领域中的各路大神在过去十几年中乐忠于发展随机策略搜索方法也是有原因的。其中最重要的原因是随机策略本身自带探索，通过探索产生各种各样的数据，有好的数据，也有坏的数据，强化学习算法通过在这些好的数据中学到知识从而改进当前的策略。



### Background

#### Preliminaries

在MDP过程中，要寻找是一个能使累计reward最大化的策略，目标函数定义如下

$$
\begin{align*} J(\pi_\theta) &= \int_\mathcal{S} \rho^\pi(s) \int_\mathcal{A} \pi_\theta (s,a)r(s,a) \mathrm{d}a \mathrm{d}s \\ &= \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [r(s,a)] \end{align*} \tag{1}
$$



#### Stochastic Policy Gradient Theorem

策略梯度的基本思想就是沿着 $\nabla_\theta J(\pi_\theta)$ 方向调整参数：

$$
\begin{align*} \nabla_\theta J(\pi_\theta) &= \int_\mathcal{S} \rho^\pi(s) \int_\mathcal{A} \nabla_\theta \pi_\theta (s,a) Q^\pi(s,a) \mathrm{d}a \mathrm{d}s \\ &= \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(a\vert s) Q^\pi(s,a)] \end{align*} \tag{2}
$$

> 策略梯度公式是关于状态和动作的期望，在求期望时，需要对状态分布和动作分布进行求积分。这就要求在状态空间和动作空间采集大量的样本，这样求均值才能近似期望。

尽管state的分布$\rho^\pi(s)$取决于policy参数,  policy参数的梯度却不依赖state的概率分布函数的梯度. 

所有的梯度算法都涉及到一个问题, 就是求 action-value的估值函数; 最简单的方式是使用 sample return G作为Q(s,a)的估值,  由此引出 REINFORCE 算法



#### Stochastic Actor-Critic Algorithms

actor-critic 建立在pg原理之上. actor沿着上面公式2的梯度 调整参数. 因为$Q^\pi(s,a)$未知, 所以使用参数w的函数$Q^w(s,a)$来代替.  critic使用诸如TD(temporal-difference)等policy evalution估值算法来估计estimate $Q^w(s,a) \approx Q^\pi(s,a)$

通常, 使用$Q^w(s,a)$来拟合$Q^\pi(s,a)$ 通过会带来bias. 然而, 如果该近似函数是compatible, 即满足以下条件:

1. $Q^w(s,a) = \nabla_\theta \log \pi_\theta(a \vert s)^\top w$ 
2. 按照最小化 mse $\epsilon^2(w) = E_{s \sim \rho^\pi, a \sim \pi_\theta} \Big[(Q^w(s,a) - Q^\pi(s,a)) \Big]^2$

则是无偏的. 即Q是 随机策略的特征$\nabla_\theta \log \pi_\theta(a \vert s)$的线性函数, 并且w是 这些特征线性回归$Q^\pi(s,a)$的解

$$
\nabla_\theta J(\pi_\theta)  = \mathbb{E}_{s \sim \rho^\pi, a \sim \pi_\theta} [\nabla_\theta \log \pi_\theta(a\vert s) Q^w(s,a)]\tag{3}
$$

通常条件2会被放宽, 为了利用更高效的td算法来policy evalution. 

如果两个条件都满足, 则该算法可以说没有用上critic, 更多的像REINFORCE算法. (critic一定要上td)



#### Off-Policy Actor-Critic

用不同的行为策略 $\beta(s,a) \neq \pi_\theta(s,a)$ 来采样获得轨迹trajectories, 再off-policy 来估计policy gradient 很有用.

在off-policy环境下, 评估目标函数改为

$$
\begin{align*} J_\beta (\pi_\theta) &= \int_\mathcal{S} \rho^\beta(s) V^\pi(s) \mathrm{d}s \\ &= \int_\mathcal{S} \int_\mathcal{A} \rho^\beta(s) \pi_\theta (s,a)Q^\pi(s,a) \mathrm{d}a \mathrm{d}s \end{align*}
$$

求梯度并取近似,  得到 off-policy policy-gradient  

$$
\begin{align*}
\nabla_\theta J_\beta(\pi_\theta) & \approx \int_\mathcal{S} \int_\mathcal{A} \rho^\beta(s) \nabla_\theta \pi_\theta (s,a) Q^\pi(s,a) \mathrm{d}a \mathrm{d}s \tag{4} \\
&= \mathbb{E}_{s \sim \rho^\beta, a \sim \beta} \left[ \frac{\pi_\theta(a \vert s)}{\beta_\theta(a \vert s)} \nabla_\theta \log \pi_\theta(a \vert s) Q^\pi(s,a) \right]  \tag{5}
\end{align*}
$$

取近似,是因为丢掉了action-value梯度$\nabla_\theta Q^\pi(s,a)$ 这一项.  Degris et al. (2012b) 认为这是个很好的近似, since it can preserve the set of local optima to which gradient ascent converges. 

Off-Policy Actor-Critic (OffPAC)算法, 使用行为策略$\beta(s,a)$来生成trajectories. critic 使用gradient TD learning , off-policy 从这些trajectories 来拟合 $V^v(s) \approx V^\pi(s)$ ,  actor 利用trajectories (off-policy)沿着 公式5的stochastic gradient ascent 方向调整参数$\theta$.  公式5里的 $Q^\pi(s,a)$ 是未知的,  可用 TD-error $\delta_t=r_{t+1} + \gamma V^v(s_{t+1})-V^v(s_t)$ 代替. 因为是用 $\beta$ 来采样, 所以 actor 和 critic都用importance sampling ratio $\frac{π_θ(a\vert s)}{β_θ(a\vert s)}$. 



### Gradients of Deterministic Policies

现在考虑怎么将PG拓展到 deterministic policies . 

#### Action-Value Gradients

大多数mdoel-free算法都是基于GPI(generalised policy iteration), 轮流进行 policy evaluation 和  policy improvement. Policy evaluation 通过 MC或TD来估计 action-value function $Q^\pi(s,a)$ . Policy improvement 按照估计好的 action-value 函数来更新policy. 最常用的是 greedy maximisation(或soft maximisation) , $\mu^{k+1}(s) = \arg \max_a Q^{\mu^k}(s,a)$

对continuous action spaces问题, 贪婪的 policy improvement 是有问题的, 需要在每一步计算全局的Q最大值.  有一种简单的替代方式,  沿着Q的梯度方向, 而不是 全局最大值Q的梯度方向. 特别的, 对每个s, policy参数 $θ^{k+1}$ 沿着$\nabla_θ Q^{μ^k}(s,μ_θ(s))$ 梯度来更新. 每个state的梯度可以按照分布distribution平均一下:

$$
\theta^{k+1} = \theta^k + \alpha  \mathbb{E}_{s \sim \rho^{\mu^k}} \left[ \nabla_θ Q^{μ^k}(s,μ_θ(s)) \right] \tag{6}
$$

按照链式法则, 改为先求a的导数再求$\theta$的导数:

$$
\theta^{k+1} = \theta^k + \alpha  \mathbb{E}_{s \sim \rho^{\mu^k}} \bigg[ \nabla_θ \mu_\theta(s) \nabla_a Q^{\mu^k} (s,a)\Big\vert_{a=\mu_\theta(s)} \bigg] \tag{7}
$$

 $\nabla_θ \mu_\theta(s)$ 是一个 雅克比矩阵.  

当改变policy时, 会访问不同的state, 造成 $\rho^\mu$改变.  在不考虑state分布会改变的情况下, 上面方法不能保证improvement.

然而, 下面的理论显示, 像SPG, 没有必要计算 state分布的梯度, 上面的更新是准确的沿着 目标函数的梯度. 



#### Deterministic Policy Gradient Theorem

现在正式考虑deterministic policy $\mu_\theta : \mathcal S \to \mathcal A$ , 定义目标函数:

$$
\begin{align*}
J(\mu_\theta) &= \int_\mathcal{S} \rho^\mu(s) r(s,\mu_\theta(s)) \mathrm{d}s \\
&= \mathbb{E}_{s \sim \rho^\mu}[r(s,\mu_\theta(s))]  \tag{8}
\end{align*}
$$

Theorem 1 (Deterministic Policy Gradient Theorem)

$$
\begin{align*}
\nabla_\theta J(\mu_\theta) &= \int_\mathcal{S} \rho^\mu(s) \nabla_θ \mu_\theta(s) \nabla_a Q^{\mu} (s,a)|_{a=\mu_\theta(s)} \mathrm{d}s \\
&= \mathbb{E}_{s \sim \rho^\mu} \left[ \nabla_θ \mu_\theta(s) \nabla_a Q^{\mu} (s,a)|_{a=\mu_\theta(s)} \right]  \tag{9}
\end{align*}
$$

> 少了对动作的积分，多了回报函数对动作的导数。

#### Limit of the Stochastic Policy Gradient

DPG 公式乍一看并不像 SPG的公式2，但实际上对很多随机策略,  DPG 是SPG的一种极限。现在用 DPG$\mu_\theta : \mathcal S \to \mathcal A$  以及方差参数 $\sigma$来参数化一个SPG.   则$\sigma=0$, SPG等价于DPG, $\pi_{\mu_\theta, \sigma} \equiv \mu_\theta$.  下面证明当$\sigma \to 0$, SPG收敛到DPG. 

Theorem 2. 两边的梯度也相等

$$
\lim_{\sigma \rightarrow 0} \nabla_\theta J(\pi_{\mu_\theta, \sigma}) = \nabla_\theta J(\mu_\theta)
$$



### Deterministic Actor-Critic Algorithms

下面用DPG来推导 on-policy 以及 off-policy actor-critic 算法.

先看最简单的case: on-policy, Sarsa critic , 再看一个简单的Q-learning critic的case 来阐述核心思想.这两个case在实践中都有不收敛的问题, 因为近似函数会引入bias 以及 off-policy本身的不稳定性. 



> 确定性策略算法中，给定状态s和策略参数时，动作是固定的。也就是说，当初试状态已知时，用确定性策略所产生的轨迹永远都是固定的，智能体无法探索其他的轨迹或访问其他的状态，从这个层面来说，智能体无法学习。我们知道，强化学习算法是通过智能体与环境交互来学习的。这里的交互是指探索性交互，即智能体会尝试很多动作，然后在这些动作中学到好的动作。
>
> **确定性策略无法探索环境，那么如何学习呢？**
>
> 1. 带噪声的确定性策略, 通过epsilon 探索
> 2. off-policy.



#### On-Policy Deterministic Actor-Critic

通常, 按照DPG来执行的行为无法保证足够的探索性, 所以会陷入局部最优解.  尽管如此, 首个算法是 on-policy actor-critic算法, 通过确定性策略来学习.  如果一个环境里有足够的噪音来保证探索性, 那么就算是DPG算法,也可能是有用的. 

像随机actor-critic一样, 确定性actor-critic也包含两部分: critic评估action value , actor沿着 action value 函数的梯度来提升. 不过actor是按照公式9来调整参数.   critic 使用$Q^w(s,a)$来代替 true action-value $Q^\mu(s)$, 使用一个合适的policy evaluation算法. 例如下面, 使用sarsa: 

$$
\begin{align*}
\delta_t &= r_t + \gamma Q^w(s_{t+1},a_{t+1}) - Q^w(s_t,a_t) \tag{11} \\
w_{t+1} &= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t,a_t) \tag{12} \\
\theta_{t+1} &= \theta_t + \alpha_\theta  \nabla_θ \mu_\theta(s) \nabla_a Q^w (s_t,a_t)\vert_{a=\mu_\theta(s)} \tag{13} 
 \end{align*}
$$

#### Off-Policy Deterministic Actor-Critic

目标函数:
$$
\begin{align*}
J_\beta (\mu_\theta) &= \int_\mathcal{S} \rho^\beta(s) V^\mu(s) \mathrm{d}s \\
&= \int_\mathcal{S} \rho^\beta(s) Q^\mu(s,\mu_\theta(s)) \mathrm{d}s  \tag{14}
\end{align*}
$$

$$
\begin{align*}
\nabla_\theta J_\beta(\mu_\theta) &\approx \int_\mathcal{S} \rho^\beta(s) \nabla_θ \mu_\theta(a\vert s) Q^{\mu} (s,a) \mathrm{d}s \\
&= \mathbb{E}_{s \sim \rho^\beta} \left[ \nabla_θ \mu_\theta(s) \nabla_a Q^{\mu} (s,a) \vert_{a=\mu_\theta(s)} \right]  \tag{15}
\end{align*}
$$

该公式给出了 off-policy deterministic policy gradient. 像随机策略的情况一样, 这里省略了一项: $\nabla_\theta Q^{\mu_\theta}(s,a)$. 

下面给出 off-policy deterministic actor- critic (OPDAC) 算法: 

$$
\begin{align*}
\delta_t &= r_t + \gamma Q^w(s_{t+1},\mu_\theta(s_{t+1})) - Q^w(s_t,a_t) \tag{16} \\
w_{t+1} &= w_t + \alpha_w \delta_t \nabla_w Q^w(s_t,a_t) \tag{17}\\
\theta_{t+1} &= \theta_t + \alpha_\theta  \nabla_θ \mu_\theta(s_t) \nabla_a Q^w (s_t,a_t)\vert_{a=\mu_\theta(s)} \tag{18}
 \end{align*}
$$

> 第一行和第二行是利用值函数逼近的方法更新值函数参数，第三行是利用确定性策略梯度的方法更新策略参数。

注意, stochastic off-policy actor-critic算法通常给actor以及critic都使用 importance sampling . 然而, 由于DPG去掉了actions上的积分, 所以可以避免actor上的importance sampling, 同时因为使用了Q-learning, 也去掉了critic的importance sampling. 

> 确定性策略梯度求解时少了重要性权重，这是因为重要性采样是用简单的概率分布区估计复杂的概率分布，而确定性策略的动作是确定值不是概率分布，另外确定性策略的值函数评估用的是Qlearning的方法，即用TD(0)来估计动作值函数并忽略重要性权重。
>

#### Compatible Function Approximation

作者找到一类近似函数是 compatible的, 可以保持true gradient. 即, 找到一个critic $Q^w(s,a)$, 使得, $\nabla_a Q^\mu(s, a)$ 可以被 $\nabla_a Q^w(s, a)$ 替代, 而不影响 确定性策略的梯度 . 

Theorem 3. 近似函数$Q^w(s,a)$ compatible with 一个确定性策略$\mu_\theta(s)$, $\nabla_\theta J_\beta(\theta) = \mathbb{E} \left[ \nabla_θ \mu_\theta(s) \nabla_a Q^w (s,a) \vert_{a=\mu_\theta(s)} \right]$ , 如果满足下面两个条件:

1. $\nabla_a Q^w (s,a) \vert_{a=\mu_\theta(s)} = \nabla_\theta \mu_\theta(s) ^\top w$
2. 调整w最小化 mse,  $MSE(\theta, w)= \mathbb E [\epsilon(s;\theta, w)^\top\epsilon(s;\theta, w)]$,  $\epsilon(s;\theta, w) = \nabla_a Q^w (s,a) \vert_{a=\mu_\theta(s)} - \nabla_a Q^\mu (s,a) \vert_{a=\mu_\theta(s)}$



**compatible off-policy deterministic actor-critic (COPDAC)**

两部分: critic线性近似函数, 从特征$\phi(s,a)= a^\top \nabla_\theta \mu_\theta(s)$ 估计action-value. 可以通过off-policy,从$\beta(a\vert s)$的samples来学习, 比如用Q-learning. actor, 沿着 critic action-value函数的梯度更新. 

COPDAC-Q 使用简单的 Q-learning critic

$$
\begin{align*}
\delta_t &= r_t + \gamma Q^w(s_{t+1},\mu_\theta(s_{t+1})) - Q^w(s_t,a_t) \tag{19} \\
\theta_{t+1} &= \theta_t + \alpha_\theta  \nabla_θ \mu_\theta(s_t) (\nabla_\theta \mu_\theta(s_t)^\top w_t) \tag{20} \\
w_{t+1} &= w_t + \alpha_w \delta_t \phi(s_t,a_t) \tag{21} \\
v_{t+1} &= v_t + \alpha_v \delta_t \phi(s_t) \tag{22} 
 \end{align*}
$$



### Experiments



### Discussion and Related Work

SPG 更难estimate, 因为policy gradient在靠近均值时改变地更快. 实际上, 随机Gaussian策略的方差正比于$1/\sigma^2$, 会慢慢变成无穷大, 当策略逐渐变得deterministic. 

可以把 deterministic actor-critic 算法类比于Q-learning.  Q-learning learns a deterministic greedy policy, off-policy, while executing a noisy version of the greedy policy (soft greedy) .  COPDAC-Q 也是这样. 



### Conclusion

DPG 可以比 SPG 更高效的estimate gradient ,  避免了对action space的积分. 




## References

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, et al.. Deterministic Policy Gradient Algorithms. 

https://zhuanlan.zhihu.com/p/26441204

