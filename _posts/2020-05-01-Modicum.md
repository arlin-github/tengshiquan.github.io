---
layout:     post
title:      Modicum
subtitle:   Depth-Limited Solving for Imperfect-Information Games
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-dice.jpg"
catalog: true
tags:
    - AI
    - Imperfect Information
    - Game Theory 
    - MCCFR

---



# Modicum

## Depth-Limited Solving for Imperfect-Information Games



#### Why are imperfect-information games hard?

- Because an optimal strategy for a **subgame** cannot be determined from that subgame alone
- Because states don't have **well-defined** values



这篇论文主要讨论第二个问题:



![image-20200604104908839](/img/2020-05-01-Modicum.assets/image-20200604104908839.png)

在perfect 游戏中, 由于subgame 规模太大, 可以使用 depth limited  reasoning.    
**该方法在imperfect不可行.  state values are not well defined.**



![image-20200604105657326](/img/2020-05-01-Modicum.assets/image-20200604105657326.png)

一个收益不对称的 石头剪子布游戏, 剪刀的收益是其他出法的两倍,  均衡策略是 0.4 0.4 0.2;   右边是P1 的depth limited version的解法,depth=1,   P1过后的每个state的ev都是0,因为假设P2玩的是均衡策略, 显然只靠右边的信息, 信息不足, 是无法得到最优解的.

In the RPS+ example, the core problem is that we incorrectly assumed P2 would always play a fixed strategy. If indeed P2 were to always play Rock, Paper, and Scissors with probability ⟨0.4, 0.4, 0.2⟩, then P1 could choose any **arbitrary** strategy and receive an expected value of 0. However, by assuming P2 is playing a fixed strategy, P1 may not find a strategy that is **robust to P2 adapting**. In reality, P2’s optimal strategy depends on the probability that P1 chooses Rock, Paper, and Scissors. 





![image-20200604112506609](/img/2020-05-01-Modicum.assets/image-20200604112506609.png)

In general, in imperfect-information games a player’s optimal strategy at a decision point depends on the player’s belief distribution over states as well as the strategy of all other agents beyond that decision point.

然而现实中, P2的策略可以基于P1的策略来改动.  就是 depth-limited外的策略, 可能是针对 depth-limited内的策略的.  如果我们的p1的策略是 0.8 0.1 0.1 ,  则p2的策略就会变成纯策略, paper.

解决方法:

1. 定义函数,  $\pi$ ,  是在state上所有策略的值函数 ,    代价高昂

2. DeepStack approach: **condition state values on believed state distribution of each player** Problem: still very expensive (DeepStack used 1.5 million core hours and could not beat prior top Als)

   Problem: does not (currently) scale. In Texas Hold'em, input is  floats. In five-card draw, input is  billion floats. In Stratego, input is     infoset里面的node数.



![image-20200604142501486](/img/2020-05-01-Modicum.assets/image-20200604142501486.png)

Modicum里面的做法,  不再假设在depth-limit 处只有一个value,   允许对手选择多个不同的值; 在迭代的过程中,生成这些值 ,  开始的时候,假设在depth-limit外 P2 执行 预计算的均衡策略,  

![image-20200604144933792](/img/2020-05-01-Modicum.assets/image-20200604144933792.png)

然后解决这个 depth-limit subgame, 得到P1的策略是 1/3

下面计算P2的BR, P2会选择 rock

![image-20200604145218168](/img/2020-05-01-Modicum.assets/image-20200604145218168.png)

![image-20200604145355688](/img/2020-05-01-Modicum.assets/image-20200604145355688.png)

把P2的BR 加到 depth-limit 的策略集合的,  再次 解决subgame得到P1的策略??  .   这个时候, P2可以选择 Br, 也可以选择之前的 均衡策略. 

![image-20200604145652093](/img/2020-05-01-Modicum.assets/image-20200604145652093.png)

再次计算P1的subgame ; 然后算P2的 BR

![image-20200604160028091](/img/2020-05-01-Modicum.assets/image-20200604160028091.png)

再加到 depth-limit处

![image-20200604160115476](/img/2020-05-01-Modicum.assets/image-20200604160115476.png)



![image-20200604160311774](/img/2020-05-01-Modicum.assets/image-20200604160311774.png)

所以P2可以选的policy会扩张的很快.





![image-20200604160424482](/img/2020-05-01-Modicum.assets/image-20200604160424482.png)





该方法的好处是 节省资源. 

![image-20200604160459183](/img/2020-05-01-Modicum.assets/image-20200604160459183.png)



modicum 比之前的AI使用的资源少的多, 也可以达到 超人类的水平.  

虽然比不上Libratus ,但使用资源少的多.

<img src="/img/2020-05-01-Modicum.assets/image-20200604164556940.png" alt="image-20200604164556940" style="zoom:50%;" />



<img src="/img/2020-05-01-Modicum.assets/image-20200604165759846.png" alt="image-20200604165759846" style="zoom:50%;" />



<img src="/img/2020-05-01-Modicum.assets/image-20200604180246229.png" alt="image-20200604180246229" style="zoom:50%;" />











## Reference

Depth-Limited Solving for Imperfect-Information Games

https://www.youtube.com/watch?v=S4-g3dPT2gY













