---
layout:     post
title:      CS 285. Policy Gradients
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---



## Policy Gradients

##### Review1

- Evaluating the RL objective  : Generate samples
- Evaluating the policy gradient : Generate samples
  - **Log-gradient trick**
- Understanding the policy gradient : Formalization of **trial-and-error**
- Partial observability :  Works just fine
- What is wrong with policy gradient?

##### Review2

- The **high variance** of policy gradient
- Exploiting **causality** : Future doesn’t affect the past
- **Baselines  : Unbiased**!
- Analyzing variance
  - Can derive optimal baselines

##### Review3

- Policy gradient is **on-policy**
- Can derive off-policy variant
  - Use **importance sampling**
  - **Exponential scaling in T** 
  - Can ignore state portion(approximation)
- Can implement with automatic differentiation – need to know what to backpropagate
- Practical considerations: batch size, learning rates, optimizers



对于PG, policy 可以用一个神经网络来做, representation能力可以很强. 

##### Evaluating the objective

$$
\theta^*=\arg\max_\theta  \underbrace{\mathbf{E}_{\tau\sim p_\theta(\tau)}\left[\sum_tr(\mathbf{s}_t,\mathbf{a}_t)\right]}_{J(\theta)}
$$

$$
J(\theta)=\mathbf{E}_{\tau\sim p_\theta(\tau)}\left[\sum_tr(\mathbf{s}_t,\mathbf{a}_t)\right] 
\approx \frac{1}{N}\sum_i\sum_tr(\mathbf{s}_{i,t},\mathbf{a}_{i,t})
$$

约等于是因为 蒙特卡洛方法抽样近似,  i : 1~N





##### Direct policy differentiation

- 这里的积分用期望公式的角度去看待 , 按照某个策略走得到的$\tau$分布, 再求期望.
- $r(\tau)$ 是与策略无关的, 因为具体的$\tau$ 已经生成, 只与env有关, 然后 状态转移概率也只与env相关.

$$
J(\theta)  = E_{\tau \sim \pi_\theta(\tau)}\underbrace{[r(\tau)]}_{\sum_{t=1}^Tr(s_t,a_t)} = \int \pi_\theta(\tau)r(\tau)\mathrm{d}\tau
$$

$$
\nabla_\theta J(\theta)=\int \nabla_\theta \pi_\theta(\tau)r(\tau)\mathrm{d}\tau = \int \pi_\theta(\tau)\nabla_\theta \log \pi_\theta(\tau)r(\tau)\mathrm{d}\tau\\ =\mathbf{E}_{\tau\sim p_\theta(\tau)}\left[ \nabla_\theta \log \pi_\theta(\tau)r(\tau) \right]
$$

- 上面推导利用了一个 log-gradient trick:

$$
\pi_\theta(\tau)\nabla_\theta \log \pi_\theta(\tau)= \pi_\theta(\tau)\frac{\nabla_\theta \pi_\theta(\tau)}{\pi_\theta(\tau)}=\nabla_\theta \pi_\theta(\tau)
$$

- 推导: 

$$
\underbrace{\pi_\theta(\mathbf{s}_1,\mathbf{a}_1,\ldots,\mathbf{s}_T,\mathbf{a}_T)}_{\pi_\theta(\tau)}= {p(\mathbf{s}_1)\prod_{t=1}^T\pi_\theta(\mathbf{a}_t\vert\mathbf{s}_t)p(\mathbf{s}_{t+1}\vert\mathbf{s}_t,\mathbf{a}_t)}  \\
\text{log of both sides} \\
\Rightarrow \log \pi_\theta(\tau)=\log p(\mathbf{s}_1)+\sum_{t=1}^T[\log \pi_\theta(\mathbf{a}_t|\mathbf{s}_t)+\log p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)]
$$

$$
\begin{aligned}
\nabla_\theta J(\theta) &=\mathbf{E}_{\tau\sim \pi_\theta(\tau)}[\nabla_\theta \log \pi_\theta(\tau)r(\tau)] \quad \text{代入上面的 log, 这里} r(\tau) 看成常量 \\ 
&= \mathbf{E}_{\tau\sim \pi_\theta(\tau)} \left[\nabla_\theta \left( {\color{red}{ \underline{\log p(\mathbf{s}_1)}}}+\sum_{t=1}^T[\log \pi_\theta(\mathbf{a}_t|\mathbf{s}_t)+ {\color{red}{ \underline{\log p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)}}} ] \right)  r(\tau) \right] \quad \text{红色部分梯度为0}\\  
&=   \mathbf{E}_{\tau\sim p_\theta(\tau)}\left[\left(\sum_{t=1}^T\nabla_\theta\log \pi_\theta(\mathbf{a}_t|\mathbf{s}_t)\right)\left(\sum_{t=1}^Tr(\mathbf{s}_t,\mathbf{a}_t)\right)\right]
\end{aligned}
$$

- 初始分布和转移概率本身都与参数$\theta$ 并不相关;  $r(\tau)$ 也只与env相关, 将$r(\tau)$展开, 这部分已经完全在梯度之外了, 可以用因果律



##### Evaluating the policy gradient  评估策略梯度

使用蒙特卡洛估计法来评估 , 就得到  REINFORCE 算法

$$
\begin{aligned}
J(\theta)&=E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \right] \approx  \frac{1}{N} \sum_{i} \sum_{t} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right) \\
\nabla_{\theta} J(\theta)&=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right] \\
\nabla_{\theta} J(\theta) &\approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)
 
\end{aligned}
$$


最简单的策略梯度法REINFORCE (Williams, 1992):

1. 运行策略$\pi_\theta(\mathbf{a} \vert \mathbf{s})$，抽取样本$$\{\tau^i\}$$
2. 估计梯度$$\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t} \vert \mathbf{s}_{i,t})\right)\left(\sum_{t=1}^Tr(\mathbf{s}_{i,t},\mathbf{a}_{i,t})\right)\right]$$
3.  $\theta\leftarrow\theta+\alpha\nabla_\theta J$



#### Comparison to maximum likelihood

<img src="/img/CS285.assets/image-20200317155857540.png" alt="image-20200317155857540" style="zoom: 33%;" />

- policy gradient:

$$
\nabla_{\theta} J(\theta)  \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)
$$

- maximum likelihood:

$$
\nabla_\theta J_\mathrm{ML}(\theta)\approx\frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right) 
$$

- “trial and error” : maximum likelihood 是提升所有的样本中出现的action的可能性, 模仿学习中, 只要出现了(s,a),就说明人类在s下选择了a,都是正向的. 这时loss函数只考虑策略与样本的概率距离, 当然也可以设定没选的其他a都是-1;  PG的每一步算上了总体的回报, 好的action提升的可能性更高. 

- 对模仿学习中的监督学习,  两者的一部分很相似:   **Score Function**   $\nabla_{\theta} \log \pi_{\theta}\left(\mathbf{s} , \mathbf{a} \right) $

- 一个trick, 可以用交叉熵来实现PG.



补充: **Gaussian Policy**: Continuous Action

- In continuous action spaces, a Gaussian policy is natural 
- Mean is a linear combination of state features   $\mu(s) = \phi(s)^T \theta$ 
- Nonlinear extensions: replace $\phi(s)$ with a deep neural network with trainable weights w
- Variance may be fixed $\sigma^2$, or can also parameterized 
- Policy is Gaussian $a \sim \mathcal{N}\left(\mu(s), \sigma^{2}\right)$ 
- The score function is

$$
\nabla_{\theta} \log \pi_{\theta}(s, a)=\frac{(a-\mu(s)) \phi(s)}{\sigma^{2}}
$$

#### Partial observability 

- 如果观测不完全，即只有部分观测信息
- 在策略梯度法中, 没有利用Markov性,  所以PG算法可以直接使用
- 任何系统都可以转化为Markovian System, 只要定义过去所有的观察序列作为state

$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{o}_{i,t})\right)\left(\sum_{t=1}^Tr(\mathbf{s}_{i,t},\mathbf{a}_{i,t})\right)\right]
$$

- Env 会给出observation 以及reward value, not reward  function,  这些value depends on states. 如果value depends on observation, 则可以等效的假定reward是随机的. 因为observation是state的随机consequence. 



#### What is wrong with the policy gradient?

PG的一个重要问题就是整体reward的取值影响收敛速度.  即reward函数的设计非常关键.

例子, 下面假设一个env, 是一维的, 越靠左边的trajectory越不好, 实线的曲线代表当前策略,  黄色的3个sample的reward都是正的, 将当前策略移到了中间虚线, 绿色的3个sample,一负两正, 将当前策略优化到了右边的虚线位置.  更差的情况, 如果一个好的action得到的总回报是0, 则这个sample不会对策略进行任何的改进.

<img src="/img/CS285.assets/image-20200317224936146.png" alt="image-20200317224936146" style="zoom:50%;" />

**High variance 高方差**: 从不同的sample里面得到非常不同的gradient. 说明gradient非常noisy. 如果用modest数量的sample, 不会straight to optimum, 会得到一个zigzag的路径, 如果是较大的学习率, 可能永远达不到optimum. 实践中最大的问题.



##### Reducing variance

- 对于一个变量的方差, 如果这个变量本身小的话, 则显然方差也会小. 所以要尽量减少这个变量变化的幅度. 

- **causality**  利用因果性来减少取值.  policy at time t' cannot affect reward at time t when t< t' . 

- 之前是看总体reward, 一个$\tau$ 的每个action都乘以 $r(\tau)$ , 是一个sum

$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{o}_{i,t})\right)\left(\sum_{t=1}^Tr(\mathbf{s}_{i,t},\mathbf{a}_{i,t})\right)\right]
$$

- 然后把这个sum拿到每步里面去. 

$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})\left(\sum_{t'=1}^Tr(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)\right]
$$

- 再利用因果性, 每个action只是在产生作用后才有影响, 所以内部的sum从t开始, 这里减小了reward的波动幅度, 值跟之前的总体reward不相等, 

$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t}) \underbrace{\left(\sum_{\color{red}{t'= t}}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)}_{\text{reward to go: } \hat Q_{i,t}}
$$

- **Reward to go**: 这个sum这时可以替换为Q,   而这个公式也是一般PG代码中的实现, 这个trick是标配. only helps.



##### Baselines

- 这个trick 也是 never hurts, only helps. 减去一个baseline, 可以减少方差

- 一个极端反例: 好的action  reward 1000000+1, 坏的action reward 1000000-1, 这样好的坏的都有类似幅度的提升, 如果一开始坏的随机到的多, 基本上没法再学到好的了

- 核心思想:   让 better than average more likely ,  worse than average less likely. 

- 直觉上就是用所有回报的平均值作为baseline.   not best, but pretty good

  $$
  b=\frac{1}{N}\sum_{i=1}^Nr(\tau_i) \\
  \nabla_\theta J(\theta)\approx \frac{1}{N} \sum_{i=t}^N \Big[\nabla_\theta \log \pi_\theta(\tau) \big(r(\tau)-b \big)\Big]
  $$
  
-  证明,  subtracting a baseline is *unbiased* in expectation!   只要这个baseline是const,  有没有baseline的期望与之前一样.   能提取到最前面的前提是, baseline需要对batch中的每个$\tau$ 取值都一样即可.

  $$
  E\left[\nabla_{\theta} \log \pi_{\theta}(\tau) b\right]=\int \pi_{\theta}(\tau) \nabla_{\theta} \log \pi_{\theta}(\tau) b d \tau=\int \nabla_{\theta} \pi_{\theta}(\tau) b d \tau=b \nabla_{\theta} \int \pi_{\theta}(\tau) d \tau=b \nabla_{\theta} 1=0
  $$
  

##### Analyzing variance

下面分析为什么baseline会减少方差. 以及求出 理论上最优的baseline.

- 方差定义:  $$\operatorname{Var}[x]=E\left[x^{2}\right]-E[x]^{2} $$

- $$\nabla_{\theta} J(\theta)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau)(r(\tau)-b)\right]$$ 

- $$
  \operatorname{Var}=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\nabla_{\theta} \log \pi_{\theta}(\tau)(r(\tau)-b)\right)^{2}\right]- \underbrace{ E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau)(r(\tau)-b)\right]^{2} }_{\text { this bit is just } E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau)\right]   }
  $$

-  let $$g(\tau):=\nabla_\theta \log \pi_\theta(\tau)$$

- $$
  \frac{d \operatorname{Var}}{d b}=\frac{d}{d b} E\left[g(\tau)^{2}(r(\tau)-b)^{2}\right]=\frac{d}{d b}\left(E\left[g(\tau)^{2} r(\tau)^{2}\right]-2 E\left[g(\tau)^{2} r(\tau) b\right]+b^{2} E\left[g(\tau)^{2}\right]\right)\\
   
  =-2 E\left[g(\tau)^{2} r(\tau)\right]+2 b E\left[g(\tau)^{2}\right]=0
  $$

- 得出最优b , just expected reward, but weighted by gradient magnitudes!
  $$
  b=\frac{E\left[g(\tau)^{2} r(\tau)\right]}{E\left[g(\tau)^{2}\right]}
  $$
  
- 实践中, 用平均回报即可 $\nabla_\theta J(\theta) = \mathbb E_{\pi_\theta} [\nabla_\theta \log\pi_\theta(s,a) \color{red} {A^{π_θ}(s,a)}]$





#### Policy gradient is on-policy 

- PG是**on-policy** ,  求期望的时候必须是从当前的分布上采样才能有无偏性，REINFORCE算法必须在当前策略上采样很多,才评估当前策略, 这就要求每次梯度更新之后就根据新分布全部重新采样
- 在DRL中, 神经网络每次BP, 在每个gradient step可能只更新一点点，但也要求把之前的样本全都扔了然后重新采样，对数据的利用率是非常低的。



#### Off-policy learning & importance sampling

- 还是要求去估计最新的期望，但是可以考虑用其他分布去估计它。提高sample利用率.

- IS 是 somewhat  off-policy , 一般被认为 off-policy

- **重要性抽样** (importance sampling):

  $$
  \mathbf{E}_{x\sim p(x)}[f(x)]=\int p(x)f(x)\mathrm{d}x=\int q(x)\frac{p(x)}{q(x)}f(x)\mathrm{d}x=\mathbf{E}_{x\sim q(x)}\left[\frac{p(x)}{q(x)}f(x)\right]
  $$
  
- 将 IS 代入 
  $$
  J(\theta)=\mathbf{E}_{\tau\sim \bar{\pi}(\tau)}\left[\frac{\pi_\theta(\tau)}{\bar{\pi}(\tau)}r(\tau)\right]
  $$

- $$
  \frac{\pi_\theta(\tau)}{\bar{\pi}(\tau)}=\frac{p(\mathbf{s}_1)\prod_{t=1}^T\pi_\theta(\mathbf{a}_t|\mathbf{s}_t)p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)}{p(\mathbf{s}_1)\prod_{t=1}^T\bar{\pi}(\mathbf{a}_t|\mathbf{s}_t)p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)}=\frac{\prod_{t=1}^T\pi_\theta(\mathbf{a}_t|\mathbf{s}_t)}{\prod_{t=1}^T\bar{\pi}(\mathbf{a}_t|\mathbf{s}_t)}
  $$
  
- estimate the value of some new parameters $\theta^{\prime}$

- $$
  J\left(\theta^{\prime}\right)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\pi_{\theta^{\prime}}(\tau)}{\pi_{\theta}(\tau)} r(\tau)\right]
  $$

  只有分子依赖参数$\theta^{\prime}$

- $$
  \nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\nabla_{\theta^{\prime}} \pi_{\theta^{\prime}}(\tau)}{\pi_{\theta}(\tau)} r(\tau)\right]=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\pi_{\theta^{\prime}}(\tau)}{\pi_{\theta} (\tau)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\tau) r(\tau)\right]
  $$

- 上面公式是 off-policy,  如果要该公式用作on-policy, 则 estimate locally , at $ \theta=\theta^{\prime}$: 则回到了之前的PG公式. 也是一个推导的角度. on-policy是off-policy的特例.
  $$
   \nabla_{\theta} J(\theta)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau)\right]
  $$

下面重点看 off-policy PG


$$
\begin{aligned}
\nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) &=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\pi_{\theta^{\prime}}(\tau)}{\pi_{\theta}(\tau)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\tau) r(\tau)\right] \quad \text { when } \theta \neq \theta^{\prime} \\
&=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\prod_{t=1}^{T} \frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\right)\left(\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right] \text { what about causality? } \\
&=E_{\tau \sim \pi_{\theta}(\tau)} \left [\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\left(\prod_{t^{\prime}=1}^{t} \frac{\pi_{\theta^{\prime}} (\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}} )}{\pi_{\theta} (\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}} )}\right) \left(\sum_{t^{\prime}=t}^{T} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)\left(\color{red}{  \prod_{t^{\prime \prime} = t}^{t'} \frac{\pi_{\theta^{\prime}} (\mathbf{a}_{t^{\prime\prime}} | \mathbf{s}_{t^{\prime \prime}} )}{\pi_{\theta}\left(\mathbf{a}_{t^{''}} | \mathbf{s}_{t^{''}}\right)} } \right)\right)\right]
\end{aligned}
$$

- 一个问题是, 都是很多概率相乘, T个,指数级,  会得到超级大或者小的数, 会有精度问题, 并且造成方差很大.
- 可以引入因果性, 一定程度上缓解这个问题. 
- 做一个近似, 不再是原来的PG算法, 前提是, off-policy采样的策略是old $\theta$,而不是其他不相干的策略. 将上式最后一部分近似成1, 可以得到  policy iteration algorithm,  然后仍然能improve策略.



##### A first-order approximation for IS

$$
\nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\left( \underline{ \prod_{t^{\prime}=1}^{t} \frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}\right)}{\pi_{\theta}\left(\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}\right)} } \right) \left(\sum_{t^{\prime}=t}^{T} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)\right)\right]
$$

- 画线部分仍然是指数级 **exponential in T** ,  改写成下面两种情况

- on-policy policy gradient:  $$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \vert \mathbf{s}_{i, t}\right) \hat{Q}_{i, t}$$

- off-policy policy gradient:   连乘拿到log外, 求导只是对$log\pi$的 , 但$\pi(s,a)$仍然是麻烦的,是个边际分布, 无法确切知道每个t所有的(s,a)的分布, 所以再改成条件概率

  $$
  \nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \frac{\pi_{\theta^{\prime}}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)}{\pi_{\theta}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right) \hat{Q}_{i, t} \\
  =\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \color{red}{ \frac{\pi_{\theta^{\prime}}\left(\mathbf{s}_{i, t}\right)}{\pi_{\theta}\left(\mathbf{s}_{i, t}\right)} }\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)}{\pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right) \hat{Q}_{i, t}
  $$
  
  当两个策略很接近的时候, 可以忽略掉红色部分. 前提, old $\theta$ 不能太old, 两个策略必须很接近. 



####  Policy gradient with automatic differentiation

具体实现部分. 利用交叉熵*Q



#### Policy gradient in practice

Practical considerations: batch size, learning rates, optimizers

- Remember that the gradient has **high variance**
  - This isn’t the same as supervised learning!
  - Gradients will be really noisy!
- Consider using much **larger batches**
- Tweaking learning rates is very hard
  - Adaptive step size rules like **ADAM** can be OK-ish
  - We’ll learn about policy gradient-specific learning rate adjustment methods later!



#### Example

##### policy gradient with importance sampling

<img src="/img/CS285.assets/image-20200319170113214.png" alt="image-20200319170113214" style="zoom: 67%;" />

- Incorporate example demonstrations using importance sampling
- Neural network policies

##### trust region policy optimization TRPO

- Natural gradient with automatic step adjustment

- Discrete and continuous actions

- Code available (see Duan et al. ‘16)



### Policy gradients suggested readings

- Classic papers
  - Williams (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning: introduces **REINFORCE** algorithm

  - Baxter & Bartlett (2001). Infinite-horizon policy-gradient estimation: temporally decomposed policy gradient (not the first paper on this! see actor-critic section later)
  - Peters & Schaal (2008). Reinforcement learning of motor skills with policy gradients: very accessible overview of optimal baselines and natural gradient

- Deep reinforcement learning policy gradient papers
  - Levine & Koltun (2013). **Guided policy search**: deep RL with importance sampled policy gradient (unrelated to later discussion of guided policy search)
  - Schulman, L., Moritz, Jordan, Abbeel (2015). **Trust region policy optimization**: deep RL with natural policy gradient and adaptive step size
  - Schulman, Wolski, Dhariwal, Radford, Klimov (2017). **Proximal policy optimization** algorithms: deep RL with importance sampled policy gradient

















