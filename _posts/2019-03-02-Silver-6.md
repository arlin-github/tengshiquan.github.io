---
layout:     post
title:      UCL Course on RL,  Policy Gradient
subtitle:   David Silver 的课程笔记6
date:       2019-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---

 

# Policy Gradient

- **不通过求V，Q， 直接优化策略**  
- **之前的函数逼近的方式， 是用NN来拟合 值函数 V或者Q ,  目标函数是方差最小** 
- **PG是直接拟合  $\pi_\theta(s)$ 概率函数 ， 然后再考虑怎么样让某些情况下的action几率提升, 通过梯度** 



**Deriving Policy Gradients**. from  http://karpathy.github.io/2016/05/31/rl/


$$
\begin{align}
\nabla_{\theta} E_x[f(x)] &= \nabla_{\theta} \sum_x p(x) f(x) & \text{definition of expectation} \\
& = \sum_x \nabla_{\theta} p(x) f(x) & \text{swap sum and gradient} \\
& = \sum_x p(x) \frac{\nabla_{\theta} p(x)}{p(x)} f(x) & \text{both multiply and divide by } p(x) \\
& = \sum_x p(x) \nabla_{\theta} \log p(x) f(x) & \text{use the fact that } \nabla_{\theta} \log(z) = \frac{1}{z} \nabla_{\theta} z \\
& = E_x[f(x) \nabla_{\theta} \log p(x) ] & \text{definition of expectation}
\end{align}
$$



- 策略是关于$\theta$的函数, 总体的Gain是一个关于的策略参数$\theta$的函数
- CS294教程里面的公式, 简洁

$$
J(\theta)  = E_{\tau \sim \pi_\theta(\tau)}\underbrace{[r(\tau)]}_{\sum_{t=1}^Tr(s_t,a_t)} = \int \pi_\theta(\tau)r(\tau)\mathrm{d}\tau
$$

$$
\nabla_\theta J(\theta)=\int \nabla_\theta \pi_\theta(\tau)r(\tau)\mathrm{d}\tau
 = \int \pi_\theta(\tau)\nabla_\theta \log \pi_\theta(\tau)r(\tau)\mathrm{d}\tau
\\ =\mathbf{E}_{\tau\sim \pi_\theta(\tau)}\left[ \nabla_\theta \log \pi_\theta(\tau)r(\tau) \right]
$$



- 数学技巧  $log'f = \frac{1}{f} f'  \to  f' = f* log'f$





1. Introduction
2. Finite Difference Policy Gradient
3. Monte-Carlo Policy Gradient
4. Actor-Critic Policy Gradient 



### Policy-Based Reinforcement Learning

- directly parametrise the policy

$$
\pi_\theta(s,a) = \mathbb P[a|s,\theta]
$$

- focus again on model-free reinforcement learning

类型

- Value Based
   - Learnt Value Function 
   - **Implicit policy**(e.g. ε-greedy)     隐式策略,  一般都是某种greedy
- Policy Based
   - No Value Function 
   - Learnt Policy 
- Actor-Critic 
  - Learnt Value Function 
  - Learnt Policy 



####  Advantages of Policy-Based RL

- Advantages:
  - **Better convergence** properties  收敛性更好
  - Effective in high-dimensional or **continuous** action spaces   连续空间
  - Can learn **stochastic** policies     可以学  随机策略

- Disadvantages:
  - Typically converge to a **local** rather than global optimum 
  - Evaluating a policy is typically **inefficient** and **high variance**   高方差



#### Example: Aliased Gridworld

- **Aliased** : The agent cannot differentiate the  Aliased states   有些状态agent无法区分, pomdp
- 对gridworkld, 比如我们用某一个格子的某个方向是否有墙挡住这些特征来描述格子状态
- 又比如我们可以用“某格子在北面有墙，同时向东移步”来作为状态行为空间的特征
- **Under aliasing, an optimal deterministic policy may get stuck**
- **Value-based RL** learns a **near-deterministic policy**  e.g. greedy or ε-greedy  , near确定性策略
- **Policy-based RL** can learn the **optimal stochastic policy**  随机的重要性



### Policy Search

##### Policy Objective Functions

- Goal: given policy $π_θ(s,a)$ with parameters θ, find best θ
- But how do we **measure** the quality of a policy $π_θ$?    看V，抽样看G

- In **episodic** environments , use **start value** ;   评估策略的优劣，还是要看v的期望
  
  $$
  J_1(\theta) = V^{\pi_\theta}(s_1) = \mathbb E_{\pi_\theta} [v_1]
  $$

- In **continuing** environments, use **average value**   ;  平均Value
  
  $$
  J_{avV}(\theta) = \sum_s d^{\pi_\theta}(s)V^{\pi_\theta}(s)
  $$

  or  **average reward per time-step**   ;  看平均回报，先看s的分布，再看该s的R
  
  $$
  J_{avR}(\theta) = \sum_s d^{\pi_\theta}(s) \sum_a  \pi_\theta(s,a)\mathcal R_s^a
  $$
  
  - $d^{\pi_\theta}(s)$ : **stationary distribution** of Markov chain for $π_θ$

- 对连续的情况， 两种的公式求得的策略会是一样的



#### Policy Optimisation

- **Policy based reinforcement learning is an optimisation problem:  Find θ that maximises $J(θ)$** 

- Some approaches do not use gradient 
  - **Hill climbing**
  - Simplex / amoeba / Nelder Mead 
  - **Genetic algorithms** 

- Greater **efficiency** often possible using **gradient**  用梯度更高效
  - **Gradient descent** 

  - **Conjugate gradient** 
  - Quasi-newton 

- focus on **gradient descent**, many extensions possible
- And on methods that exploit **sequential structure**



### Policy Gradient

#### Finite Difference Policy Gradient

- Let $J(θ)$ be any policy objective function. 
- Policy gradient algorithms search for a **local maximum** in $J(θ)$ by ascending the gradient of the policy, w.r.t. parameters θ

$$
\Delta \theta = \alpha\nabla_\theta J(\theta)
$$

$$
\nabla_\theta J(\theta)=\begin{pmatrix}
\frac{\partial J(\theta)}{\partial \theta_1}  \\
\vdots\\
\frac{\partial J(\theta)}{\partial \theta_n}
\end{pmatrix}
$$



#### Computing Gradients By Finite Differences

- 有限差分法

- 这个方法相当于手工去调梯度； 把各个维度的参数当作超参数来调整，看单独调某个维度的参数，看是不是有效果，在小规模问题上，调起来比较快

- To evaluate policy gradient  $\pi_\theta(s,a)$

- For each dimension k ∈ [1, n] :  对每一个分量θk, 用下式计算梯度

  - Estimate kth partial derivative of objective function w.r.t. $θ$

  - By perturbing θ by small amount ε in kth dimension
    
    $$
    \frac{\partial{J(\theta)}}{\partial{\theta_k}} \approx \frac{J(\theta+ \epsilon u_k) - J(\theta)}{\epsilon}
    $$
    
  - where uk is unit vector with 1 in kth component, 0 elsewhere

- Uses n evaluations to compute policy gradient in n dimensions
- Simple, noisy, inefficient - but sometimes effective
- Works for arbitrary policies, even if policy is not differentiable
- 无需知道梯度函数本身，梯度函数不可导也无所谓，但效率很低

##### Training AIBO to Walk by Finite Difference Policy Gradient 



### Monte-Carlo Policy Gradient

#### Score Function

- 这里借用了**Likelihood ratios（**似然比、似然系数）这个概念。
- 不要求在所有a上都可导， 只要在取a的时候可导就行
- now compute the policy gradient *analytically*   该策略函数是我们设计的, 需要是可导的
- Assume policy $π_θ$ is **differentiable** whenever it is non-zero

- **Likelihood ratios** exploit the following identity

$$
\nabla_\theta \pi_\theta(s,a) = \pi_\theta(s,a) \frac{\nabla_\theta \pi_\theta(s,a)}{\pi_\theta(s,a)}
\\ = \pi_\theta(s,a) \nabla_\theta \log{ \pi_\theta(s,a)}
$$

- **score function** is $\nabla_\theta \log{ \pi_\theta(s,a)}$   该术语常出现于统计机器学习当中
- 使用这个技巧，最大的好处是计算期望比较简单，因为我们是按照当前的策略来走
- 梯度函数表示 梯度方向是让该动作出现的几率更高的方向



下面举了两个例子来解释 score function

#### Softmax Policy

- 对于离散动作的问题;  $\phi(s,a)$ 表示特征向量，由env决定,  这里是action in的方式, 然后**网络参数只有$\theta$这一个矢量**, 输出节点是1个,  然后求结果需要正向执行action次, 然后再softmax一下 
- 离散领域最简单的PG算法了, 一个线性再加softmax , 然后再计算loss BP
- 一般的softmax  action out的模式是 , state的状态向量x,  然后输出节点是 action个

- Weight actions using linear combination of **features** $\phi(s,a)^⊤θ$

- Probability of action is proportional to exponentiated weight

$$
\pi_\theta(s,a) \propto e^{\phi(s,a)^\top\theta}
$$

- score function is  注意上式并不是等于, 而是正比于;

- 下式左边是矢量,  等式右边第一项是 state的特征向量 

$$
\nabla_\theta \log{ \pi_\theta(s,a)} = \phi(s,a) - \mathbb E_{\pi_\theta}[\phi(s,\cdot)]
$$

- 该公式说明，  feature we took  minus  average feature ,  梯度的方向就是 输入特征 - 平均特征 

- 减号前面 s下执行特定a 该维度特征的多少， 可以说是该特征对选择a的影响程度，减号后面是该特征在s下一般的均值；减号表示， how much more of the feature 比平常多采用的程度 

- 推导，$\phi(s)$是个矩阵， $\phi(s,a)$是个向量  , 这里用的是 action in的方式来做的
- softmax : $a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}}$    对于普通的**softmax分类**， 输出是个K维向量，m个样本  n维度  K类 ，所以训练的参数 n乘以k 的矩阵,   $z_k$表示线性组合
- 在RL中，softmax输出 num(a)维的向量， 值最大的那个是被推荐的a 
- ML需要训练一个参数矩阵，对一个情况，输入的参数是一样的, 即state ,  相当于是 action out； 而这个问题， 对s下不同的a，有不同的输入特征， 但只训练一个参数向量!! action in 的方式

- 关于RL的softmaxpolicy 与 softmax回归的区别：
  - 首先是训练的输入不一样， RL这边输入是 各种(s,a)下面的特征向量， ML是 x向量；
  - 训练的目标不一样， ML这边是训练一个参数矩阵， 本质上是要让label的那个分类得到最大化的线性组合的值，其他的分类的值要尽可能的小，目的就只是模型拟合； RL这边只是训练一个 参数向量，该参数向量配合的是在s下，取哪个a的特征的多少的问题； 关键的是，RL的目标是R的期望最大化，所以模型参数拟合只是一个方面，最终是通过调整参数来调整策略，优化s下选的a，来达到改进策略的目的.

$$
\mathbb E_{\pi_\theta}[\phi(s,\cdot)] = \sum_i \Big (\pi(a_i)  \phi(s,a_i)\Big )  = \sum_i \Big(\frac{e^{z_i}}{\sum_j e^{z_j}} \phi(s, a_i) \Big)
$$

$$
\log(\pi(s,a)) = \log(e^{\phi(s,a)^\top\theta}) - \log(\sum_i e^{\phi(s,a_i)^\top\theta})
\\ \nabla_{\theta} = \phi(s, a) - \frac{ \sum_i \left(e^{\phi(s,a_i)\theta_i}  \phi(s,a_i)  \right) }{\sum_j e^{\phi(s,a_i)^\top\theta}}  \quad 下面的sum下标换成j，反正是整体
\\ = \phi(s, a) - \sum_i (\pi(a_i)  \phi(s,a_i) )
$$

- scorefunction 的作用， 如何调整各个r的系数，来获得最高的期望



##### Gaussian Policy

- 对于连续行为问题， 会对某个动作有较高的高峰。。。
- In continuous action spaces, a Gaussian policy is natural  对于连续动作空间, 使用高斯分布比较自然
- Mean is a linear combination of state features $μ(s) = \phi(s)^⊤θ$   使用高斯策略时，通常对于均值, 是s的线性组合
- Variance may be fixed $σ^2$, or can also parametrised 方差可以是固定值，也可以用参数化表示
- Policy is Gaussian, $a ∼ \mathcal N (μ(s), σ^2)$  行为对应于一个具体的数值，从高斯分布中随机采样产生

- 下面公式中,  x就是a,  是个连续值

$$
f(x)=\displaystyle\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},\ -\infty<x<\infty
$$

- score function is

$$
\nabla_\theta \log{ \pi_\theta(s,a)} = \frac{(a-\mu(s))\phi(s)}{\sigma^2}
$$





### Policy Gradient Theorem

#### One-Step MDPs

- Consider a simple class of one-step MDPs  简单情况，整个MDP只有一个状态、行为、即时奖励。
  - **Starting in state** $s ∼ d(s)$   这里s是起始状态, 是有分布的
  - Terminating after one time-step with reward $r = \mathcal R_{s,a}$ 

- Use **likelihood ratios** to compute the policy gradient 

- 目标函数， 获得reward的期望 ， 这里小r是变量, R是采样值

$$
\begin{aligned}
J(\theta) &= \mathbb E_{\pi_\theta} [r]
				\\&= \sum_{s\in \mathcal S}d(s)\sum_{a\in \mathcal A}\pi_\theta(s,a)\mathcal R_{s,a}
\\ \nabla_\theta J(\theta) &= \sum_{s\in \mathcal S}d(s)\sum_{a\in \mathcal A}\pi_\theta(s,a) \nabla_\theta \log{ \pi_\theta(s,a)} \mathcal R_{s,a}
\\ &= \mathbb E_{\pi_\theta}[\nabla_\theta \log{ \pi_\theta(s,a)} r]
\end{aligned}
$$


#### Policy Gradient Theorem

- 将之前的单步情况，推广到多步； 可以认为,当前t，策略$\pi$ 出a之后都是一步，用Q_t 即可，当前这一步a要对之后的Q负责,  同时, 也用上了 backup 

- The policy gradient theorem **generalises** the likelihood ratio approach to **multi-step MDPs** 
- Replaces instantaneous reward r with long-term value $Q^π(s,a)$    
- 唯一要变动的就是把即时奖励值换成目标的Q值 ，主要就是把 r换成Q 
- $Q^{\pi_\theta}$ 是一个在某个s,a下执行目前策略$\pi_\theta$ 的期望, 是 $G_t$ 的期望,  只不过 G对应的是s, Q对应的是(s,a) ;

- 然后$Q^{\pi_\theta}$ 还是 $\theta$ 的函数,  sutton的书上有证明, 该部分偏导最终会转换到$\pi_\theta$部分, 还提到了state distribution,  主要思路是通过 $Q^π(s,a) = r + \sum_a Q^π(s',a)$ , 然后r肯定是只与env相关的

> 一般来说，乘积的期望不等于期望的乘积，除非变量相互独立。如果x和y相互独立，则$E(xy)=E(x)E(y)$

- 对于三种目标函数都是通用的, 直接考虑episode的环境就行,    **下式中 s 按定义应该是起始状态**

$$
J(\theta) = \color{red} {\mathbb E_{\pi_\theta}[Q^{\pi_\theta}(s,a)]}
$$

- For any differentiable policy $π_θ(s, a)$, for any of the policy objective functions $J = J_1$, $J_{avR}$ , or $\frac{1}{1−γ}J_{avV}$ ,the policy gradient is

$$
\nabla_\theta J(\theta) = \color{red} {\mathbb E_{\pi_\theta}[\nabla_\theta \log{ \pi_\theta(s,a)} Q^{\pi_\theta}(s,a)]}
$$



- **sutton版本的公式**, 更严谨 ,  K是episode的长度, $\mu(s)$是所有状态s 的百分比

$$
\begin{aligned}
\nabla J(\theta) =& \nabla v_\pi(s_0)
\\ = & \sum_s \bigg( \sum_{k=0}^\infty Pr(s_0 \to s,k,\pi) \bigg) \sum_a \nabla \pi(a|s) q_\pi(s,a)
\\ = & \sum_{s'} \eta(s') \sum_s \mu(s)  \sum_a \nabla \pi(a|s) q_\pi(s,a)
\\ = & K \sum_s  \left ( \mu(s) \bigg ( \sum_a \nabla \pi(a|s) q_\pi(s,a) \bigg )\right )
\end{aligned}
$$

- 这个公式显示  起始状态$s_0$的v的梯度是 , 所有状态s的加权$\pi q$梯度和

- 注意 两边都是梯度才成立,   $ J(\theta) =  v_\pi(s_0) \neq   K \sum_s  \left ( \mu(s) \bigg ( \sum_a   \pi(a \vert s) q_\pi(s,a) \bigg )\right )$

- 同时, **REINFORCE**

$$
\nabla J(\theta)  \propto   \sum_s \mu(s)  \sum_a  q_\pi(s,a) \nabla \pi(a|s,\theta)
 \\ = \mathbb E_\pi \bigg[  \sum_a q_\pi(S_t,a) \nabla\pi(a|S_t, \theta)  \bigg]
$$

- all  actions 方式, 这种目前研究不多

$$
\theta_{t+1} \doteq \theta_t + \alpha \sum_a \hat q(S_t,a,\mathbf w) \nabla \pi(a|S_t,\theta)
$$

- 经典 **reinforce** 算法 , 下式中, 也是利用 log' f 这个trick

$$
\theta_{t+1} \doteq \theta_t + \alpha G_t \frac{\nabla \pi(A_t|S_t,\theta_t)}{\pi(A_t|S_t,\theta_t)}
$$



###  <mark>Monte-Carlo Policy Gradient ( REINFORCE )</mark>

- 这个算法如果是 **action-in** 的, 网络的输出只有一个节点, 最后一个节点的激活函数可以用sigmoid, 输出的是个概率值.
- 也有采用 action-out 的方式的做法, 最后一层是softmax,  然后使用交叉熵loss, 或者 直接找出最大的prob, 然后求logProb,最后sum, 再反向传播,    这两种方式, 得到的梯度是一样的.
- 这里的$v_t$ 就是在当前t采样得到的G
- Update parameters by **stochastic gradient ascent**
- Using policy gradient theorem
- Using return $v_t$ as an **unbiased sample** of $Q^{π_θ}(s_t,a_t)$  ; 用$v_t$当作Q的一个无偏采样,这里$v_t$是$r_t$的sum

$$
\Delta \theta_t = \alpha \nabla_\theta \log{\pi_\theta(s_t,a_t)v_t}
$$



<img src="/img/2019-03-02-Silver.assets/image-20200701180352962.png" alt="image-20200701180352962" style="zoom: 33%;" />

- 先按照episode,  收集sample;   然后对每个step，更新了一次参数; 这里$v_t$就是$G_t$

  

##### 代码的实现trick  利用cross-entropy

- 机器学习中，对多分类问题，loss 函数 可以使用 cross entropy ，$Cost = - \mathbf Y log \mathbf H $  ， Y为onehot

- 即 $H_\theta(X)  \leftrightarrow Y$ , 给model fit( X, Y ), 则会对参数 $\theta$ 按照Cost 进行梯度下降 
- 对于 MC的PG,  利用model表示策略， 即输出向量 $\pi(s_t)$ = H ， 维度就是a的空间大小 

$$
J(\theta) = \mathbb E_{\pi_\theta}[{ \pi_\theta(s,a)} Q^{\pi_\theta}(s,a)]
\\ \nabla_\theta J(\theta) =   {\mathbb E_{\pi_\theta}[\nabla_\theta \log{ \pi_\theta(s,a)} Q^{\pi_\theta}(s,a)]}
$$

- 需要对参数进行 $\nabla_\theta \log{ \pi_\theta(s,a)} Q^{\pi_\theta}(s,a)$ 的梯度上升，  套用机器学习的模式，只要让 Y 为 Q(s,a) 即可

- **即onehot的Y再乘以一个该a维度上的Q即可** 核心是这个Q要学的好 

  



##### 关于数据标准化

- 标准化（Z-Score），或者去除均值和方差缩放
- 公式为：(X-mean)/std  计算时对每个属性/每列分别进行。 
- 做了以后， 收敛会变快 ; 这个待验证



##### Puck World Example

<img src="/img/2019-03-02-Silver.assets/image-20190224221450208.png"  style="zoom: 80%;" />

MCPG  很慢





### Actor-Critic Policy Gradient

- 之前的Q是**采样**来的，现在用 TD来评估
- 也可以用神经网络来近似拟合 ; 所以会有两个网络，策略网络值网络
- REINFORCE使用了G 作为v(s)的估计，它虽然是无偏的，但噪声比较大，方差较高。如果我们能够相对准确地估计状态价值，用它来指导策略更新，那么是不是会有更好的学习效果呢？这就是Actor-Critic策略梯度的主要思想,  下面用TD评估Q



#### Reducing Variance Using a Critic

- Monte-Carlo policy gradient still has high variance
- use a **critic** to estimate the action-value function, $Q_w(s,a) \approx Q^{\pi_\theta}(s,a)$ 

- Actor-critic algorithms maintain **two** sets of parameters 
  - **Critic** Updates **action-value function parameters w** 
  - **Actor** Updates **policy parameters θ**, in direction suggested by critic

- Actor-critic algorithms follow an **approximate policy gradient**

$$
\nabla_\theta J(\theta) \approx \mathbb E_{\pi_\theta}[\nabla_\theta \log{\pi_\theta(s,a)Q_w(s,a)}]
\\ \Delta \theta  = \alpha \nabla_\theta \log{\pi_\theta(s,a)Q_w(s,a)}
$$



#### Estimating the Action-Value Function

- The **critic** is solving a familiar problem: **policy evaluation** 
- How good is policy $π_θ$ for current parameters θ?
- This problem was explored in previous two lectures, e.g. 
  - Monte-Carlo policy evaluation 
  - Temporal-Difference learning 
  - TD(λ) 
- Could also use e.g. least-squares policy evaluation



#### Action-Value Actor-Critic

- 线性Q函数的近似的Actor-Critic算法

- Simple actor-critic algorithm based on action-value critic
- Using **linear value fn** approx. $Q_w (s, a) = \phi(s, a)^⊤w$
  - Critic Updates w by **linear TD(0)**
  - Actor Updates θ by **policy gradient**

<img src="/img/2019-03-02-Silver.assets/image-20200701181359855.png" alt="image-20200701181359855" style="zoom: 33%;" />


- 有两个学习率 alpha beta，这是一个在线实时算法，针对每一步进行更新，不需要等到Episode结束。
- 在Policy Based的算法中，策略是随机的, 探索性是接根据参数θ得到的。参数更新时有一个学习率α， 控制了策略更新的平滑度。
- **PG 与 epsilon greedy** 的关系 : 如果pg学习的步长特别长，那就是贪婪的情况了，就像之前TD - MC 之间的鸿沟，一般取中间比较好， 这个也类似. 



#### Compatible Function Approximation

##### Bias in Actor-Critic Algorithms

- Approximating the policy gradient introduces **bias**    V,Q近似以后引入bias
- A **biased policy gradient** may not find the right solution
- Luckily, if we choose value function approximation carefully,  Then we can avoid introducing any bias , i.e. We can still follow the **exact** policy gradient    
  如果小心设计近似函数，是可以避免引入偏倚的，这样我们相当于遵循了准确的策略梯度。



##### Compatible Function Approximation Theorem

- If the following two conditions are satisfied: 

  - Value function approximator is **compatible** to the policy 两个梯度要完全相同
  
  $$
  ∇_wQ_w(s,a) = ∇_θ \logπ_θ(s,a)
  $$
  
  - Value function parameters w minimise the mean-squared error 
  
  $$
  \varepsilon=\mathbb{E}_{\pi_{\theta}}\left[\left(Q^{\pi_{\theta}}(s, a)-Q_{w}(s, a)\right)^{2}\right]
  $$
  
- Then the policy gradient is **exact**,

$$
∇_θJ(θ) =\mathbb E_{π_θ} [∇_θ \log π_θ(s, a) Q_w (s, a)]
$$



##### Proof of Compatible Function Approximation Theorem

- If w is chosen to minimise mean-squared error, gradient of ε w.r.t. w must be zero,
- 均方差最小，取得极值，得证

$$
\begin{aligned}
\nabla_w \varepsilon &= 0 
\\ \mathbb E_{\pi_\theta} [(Q^\theta(s,a)-Q_w(s,a))\nabla_wQ_w(s,a) ] &= 0
\\ \mathbb E_{\pi_\theta} [(Q^\theta(s,a)-Q_w(s,a))\nabla_\theta \log\pi_\theta(s,a) ] &= 0
\\ \mathbb E_{\pi_\theta} [Q^\theta(s,a)\nabla_\theta \log\pi_\theta(s,a)] &=  \mathbb E_{\pi_\theta} [Q_w(s,a)\nabla_\theta \log\pi_\theta(s,a)]
\end{aligned}
$$
- So $Q_w (s, a)$ can be substituted directly into the policy gradient,

$$
\nabla_\theta J(\theta) = \mathbb E_{\pi_\theta} [\nabla_\theta \log\pi_\theta(s,a)Q_w(s,a)]
$$



### Advantage Function Critic

#### Reducing Variance Using a Baseline

- 其基本思想是从策略梯度里抽出一个基准函数B(s)，**要求**这一函数**仅与状态有关，与行为a无关**，因而不改变梯度本身。 
- B(s)的特点是能在不改变行为价值期望的同时降低其Variance。

- We subtract a **baseline function** B(s) from the policy gradient
- This can **reduce variance**, without changing expectation

- 下面的公式, 梯度符号是紧跟着pi的, B(s)不涉及求导

$$
\begin{aligned}
\mathbb E_{\pi_\theta}[\nabla_\theta\log_{\pi_\theta}(s,a)B(s)] &= 
\sum_{s \in \mathcal S}d^{\pi_\theta}(s)\sum_a \nabla_\theta\pi_\theta(s,a)B(s)
\\ &= \sum_{s \in \mathcal S}d^{\pi_\theta}(s)B(s)\nabla_\theta \sum_{a \in \mathcal A} \pi_\theta(s,a)
\\ &= 0
\end{aligned}
$$

- **A good baseline is the state value function** $B(s) = V^{π_θ}(s)$
- rewrite the policy gradient using the **advantage function** $A^{π_θ}(s,a)$

$$
A^{π_θ}(s,a) = Q^{π_θ}(s,a) - V^{π_θ}(s)
\\ \nabla_\theta J(\theta) = \color{red} {\mathbb E_{\pi_\theta} [\nabla_\theta \log\pi_\theta(s,a)A^{π_θ}(s,a)]}
$$

- 意义在于，当个体采取行为a离开s状态时，究竟比该状态s总体平均价值要好多少？



#### Estimating the Advantage Function

- **advantage function** can significantly reduce variance of policy gradient ; advantage function显著减低方差
- **critic** should really estimate the advantage function 

- For example, by estimating both $V ^{π_θ} (s)$ and $Q ^{π_θ}  (s, a)$ 
- 需要两个近似函数也就是两套参数，一套用来近似状态价值函数，一套用来近似行为价值函数，以便计算advantage函数.  不过实际操作时，通过TD-error 来更新 
- Using two function approximators and two parameter vectors, 

$$
V_v(s) ≈ V^{π_θ}(s) \\ Q_w(s,a) ≈ Q^{π_θ}(s,a) \\
A(s, a) = Q_w (s, a) − V_v (s)
$$

- And updating both value functions by e.g. TD learning

- For the **true value function** $V ^{π_θ} (s)$, the **TD error** $δ^{π_θ}$ 

$$
δ^{π_θ} =r+γV^{π_θ}(s′)−V^{π_θ}(s) 
$$

- is an **unbiased estimate** of the **advantage function**

$$
\begin{aligned}
\mathbb{E}_{\pi_{\theta}}\left[\delta^{\pi_{\theta}} \mid s, a\right] &=\mathbb{E}_{\pi_{\theta}}\left[r+\gamma V^{\pi_{\theta}}\left(s^{\prime}\right) \mid s, a\right]-V^{\pi_{\theta}}(s) \\
&=Q^{\pi_{\theta}}(s, a)-V^{\pi_{\theta}}(s) \\
&=A^{\pi_{\theta}}(s, a)
\end{aligned}
$$

- use the **TD error** to compute the policy gradient，只用一套参数

$$
∇_θJ(θ) = \color{red}{\mathbb E_{π_θ} [∇_θ \log π_θ(s, a) δ^{π_θ} ]  }
$$

- In practice we can use an **approximate TD error** 

$$
δ_v =r+γV_v(s′)−V_v(s)
$$

- This approach only requires one set of critic parameters v 
- 到这里，   **PG FA TD的配合**   





### Eligibility Traces

#### Critics at Different Time-Scales

- 通过计算不同时间范围内（步长）的TD 误差来更新状态价值函数Vθ(s)，此时的Critic过程可以根据时间范围的的长短（步长的多少）来分
- TD 方面再引入 Eligibility  ,ETs
- Critic can estimate value function $V_θ(s)$ from many targets at sdifferent time-scales From last lecture...  这里应该都是线性拟合

- For MC, the target is the return vt

  $$
  ∆θ = α(\color{red}{v_t} − V_θ(s))\phi(s)
  $$

- For TD(0), the target is the TD target $r + γV (s′)$ 
  $$
  ∆θ = α(\color{red}{r + γV(s′)} − V_θ(s))\phi(s)
  $$

- For forward-view TD(λ), the target is the λ-return $v_t^λ$ 
  $$
  ∆θ = α(\color{red}{v_t^λ} − Vθ(s))\phi(s)
  $$

- For backward-view TD(λ), we use eligibility traces 

$$
δ_t =r_{t+1}+γV(s_{t+1})−V(s_t) 
\\ e_t =γλe_{t−1}+\phi(s_t)
\\∆θ = αδ_t e_t
$$



#### Actors at Different Time-Scales

- 选策略的时候，看的远近

- The policy gradient can also be estimated at many time-scales 

$$
∇_θJ(θ) = \mathbb E_{π_θ} [∇_θ \log π_θ(s, a) \color{red}{ A^{π_θ} (s, a)}] 
$$

- Monte-Carlo policy gradient uses error from complete return 

$$
∆θ=α(\color{red}{v_t} −V_v(s_t))∇_θ\logπ_θ(s_t,a_t)
$$

- Actor-critic policy gradient uses the one-step TD error 

$$
∆θ = α(\color{red}{r +γV_v(s_{t+1})}−V_v(s_t))∇_θ \logπ_θ(s_t,a_t) 
$$



#### Policy Gradient with Eligibility Traces

- Just like forward-view TD(λ), we can mix over time-scales

$$
∆θ=α(\color{red}{v_t^\lambda} −V_v(s_t))∇_θ\logπ_θ(s_t,a_t)
$$

- where $v_t^λ − V_v (s_t)$ is a biased estimate of advantage fn

- Like backward-view TD(λ), we can also use eligibility traces
  - By equivalence with TD(λ), substituting $\phi(s) = ∇_θ \logπ_θ(s,a)$

$$
δ_t =r_{t+1}+γV(s_{t+1})−V(s_t) 
\\ e_{t+1} =γλe_{t}+∇_θ \logπ_θ(s,a)
\\∆θ = αδ_t e_t
$$

- This update can be applied online, to incomplete sequences





### Natural Policy Gradient

#### Alternative Policy Gradient Directions

- Gradient ascent algorithms can follow any ascent direction   
- A good ascent direction can significantly **speed convergence**    好的梯度方向可以加速收敛
- a policy can often be reparametrised without changing action probabilities   ,  一个策略可以重新配参数, 然后输出的策略一样
- For example, increasing score of all actions in a softmax policy 
- The vanilla gradient is sensitive to these reparametrisations    原生梯度对此很敏感



#### Natural Policy Gradients

<img src="/img/2019-03-02-Silver.assets/image-20200701191444896.png" alt="image-20200701191444896" style="zoom: 33%;" />

- natural policy gradient is **parametrisation** **independent**
- finds **ascent direction** that is closest to vanilla gradient,  
  when changing policy by a small, fixed amount

$$
\nabla_\theta^{nat} \pi_\theta(s,a) = G_\theta^{-1}\nabla_\theta \pi_\theta(s,a)
$$

- where $G_θ$ is the **Fisher information matrix**

$$
G_\theta = \mathbb E_\theta \Big[\nabla_\theta \log\pi_\theta(s,a) \nabla_\theta \log\pi_\theta(s,a)^\top \Big]
$$


#### Natural Actor-Critic

- Using compatible function approximation,

$$
\nabla_w A_w(s,a) = \nabla_\theta \log \pi_\theta(s,a)
$$

- So the natural policy gradient simplifies,

$$
\begin{aligned}
\nabla_\theta J(\theta) &= \mathbb E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) A^{\pi_\theta}(s,a)]
\\ &= \mathbb E_{\pi_\theta}[\nabla_\theta \log \pi_\theta(s,a) \nabla_\theta \log \pi_\theta(s,a)^\top w]
\\ &= G_\theta w
\\ \color{red}{ \nabla_\theta^{nat}J(\theta) } & \color{red}{= w}
\end{aligned}
$$

- i.e. update actor parameters in direction of critic parameters



### Summary of Policy Gradient Algorithms

- policy gradient has many equivalent forms

$$
\begin{aligned}
\nabla_\theta J(\theta) &= \mathbb E_{\pi_\theta}[\nabla_\theta\log \pi_\theta(s,a) \color{red}{v_t}]  &\text{REINFORCE}
\\ &= \mathbb E_{\pi_\theta}[\nabla_\theta\log \pi_\theta(s,a) \color{red}{Q^w(s,a)}]  &\text{Q Actor-Critic}
\\ &= \mathbb E_{\pi_\theta}[\nabla_\theta\log \pi_\theta(s,a) \color{red}{A^w(s,a)}]  &\text{Advantage Actor-Critic}
\\ &= \mathbb E_{\pi_\theta}[\nabla_\theta\log \pi_\theta(s,a) \color{red}{\delta}]  &\text{TD Actor-Critic}
\\ &= \mathbb E_{\pi_\theta}[\nabla_\theta\log \pi_\theta(s,a) \color{red}{\delta e}]  &\text{TD(λ) Actor-Critic}
\\ G_\theta^{-1} \nabla_\theta J(\theta) &= w &\text{Natural Actor-Critic}
\end{aligned}
$$

- Each leads a stochastic gradient ascent algorithm 

- Critic uses **policy evaluation** (e.g. MC or TD learning) to estimate $Q^π(s, a)$, $A^π(s, a)$ or $V ^π(s)$ 





