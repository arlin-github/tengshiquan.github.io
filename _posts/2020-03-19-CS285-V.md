---
layout:     post
title:      CS 285. Value Function Methods
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

## Value Function Methods

其他教程都是 Value (DP->FA) -> PG -> AC ,  这边是 PG -> AC -> Value -> Q-learn , 可以看出侧重点.  Silver的课程在这块比较清晰.   

PG 强调方差小,求稳定;   线性  value function 可以用泛函证明收敛. $\gamma < 1$ 起到压缩权重的作用.  



### Lecture

1. What if we just use a critic, without an actor?
2. Extracting a policy from a value function
3. The Q-learning algorithm
4. Extensions: continuous actions, improvements

##### Goals:

- Understand how value functions give rise to policies
- Understand the Q-learning algorithm
- Understand practical considerations for Q-learning



##### omit policy gradient completely

$A^\pi(\mathbf{s}_t,\mathbf{a}_t)$ :  当前action 比按策略出的action 好多少

下面在$\pi$ 基础上构建**deterministic**策略 ,  at least as good as $\pi$ , 一般都会 better ; 这里用Q,A都行
$$
\pi^{\prime}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\left\{\begin{array}{ll}
1 & \text { if } \mathbf{a}_{t}=\arg \max _{\mathbf{a}_{t}} A^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \\
0 & \text { otherwise }
\end{array}\right.
$$
**如果有个fully observed MDP, 则总存在一个 deterministic policy 至少与最优策略一样好 , 也就是说随机策略不会有啥特别的好处.  当然在学习的时候, 有随机的话, 可以learn fast , explore fast.**



### Policy iteration

**Policy iteration** algorithm:

1. **policy evaluation** : evaluate $A^\pi(\mathbf s, \mathbf a)$
2. **policy improvement** : set $\pi \leftarrow \pi'$  上面的公式 ,  通过这个 $\mathbf{a}_{t}=\arg \max _{\mathbf{a}_{t}}$ 改进策略.

所以跟之前一样需要  evaluate $V^\pi(s)$ .



这个图比较清楚.

<img src="/img/CS285.assets/image-20190215165532474.png" alt="image-20200320145748201" style="zoom: 50%;" />



#### Dynamic programming

- Let's assume we know $p(s'\vert s,a)$, and $\mathbf s$ and $\mathbf a$ are both discrete (and small).  Small 是只规模小, 可以tabular 表示, 放内存里. 
- **bootstrapped update** :    **backup**  $$V^\pi(\mathbf{s})\leftarrow\mathbf{E}_{\mathbf{a}\sim\pi(\mathbf{a}\vert \mathbf{s})}[r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}' \vert \mathbf{s},\mathbf{a})}[V^\pi(\mathbf{s}')]]$$.    $V^\pi(\mathbf{s}')$ just use the current estimate here.
- 又因为策略是 deterministic , $\pi(\mathbf{s})=\mathbf{a}$
- Simpilified ,  化简得到evaluation 公式 : $$V^\pi(\mathbf{s})\leftarrow r(\mathbf{s},\pi(\mathbf{s}))+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}'\vert \mathbf{s},\pi(\mathbf{s}))}[V^\pi(\mathbf{s}')]$$



#### Value iteration

##### Even simpler dynamic programming

- 将 evaluation 与 improve 结合成一步 : 

$$
\pi^{\prime}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\left\{\begin{array}{ll}1 & \text { if } \mathbf{a}_{t}= \color{red}{ \arg \max _{\mathbf{a}_{t}} A^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) }\\0 & \text { otherwise }\end{array}\right.
$$

-  $A^\pi(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V^\pi(\mathbf{s}')] - V^\pi(\mathbf{s}) $.
-  $Q^\pi(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V^\pi(\mathbf{s}')]$.  Q用起来更方便
-  Advantage 取最大值的a , Q也取最大值.  即 $$\arg\max_{\mathbf{a}_t}A^\pi(\mathbf{s}_t,\mathbf{a}_t)=\arg\max_{\mathbf{a}_t}Q^\pi(\mathbf{s}_t,\mathbf{a}_t)$$



<img src="/img/CS285.assets/image-20200401175004434.png" alt="image-20200401175004434" style="zoom:50%;" />

$\arg \max _{\mathbf{a} } Q \left(\mathbf{s} , \mathbf{a} \right) \to \text{policy}$    



<img src="/img/CS285.assets/image-20200319231053012.png" alt="image-20200319231053012" style="zoom:33%;" />

<img src="/img/CS285.assets/image-20190203193449022.png" alt="image-20200320145748201" style="zoom: 50%;" />



**Value iteration 是 policy iteration中 policy evaluation 只执行一次, 没有到收敛就进行improve的特殊情况**

skip the policy and compute values directly!  可以把下面算法的两行换成一行. 

**Value iteration algorithm**:

1. Set  $Q(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V(\mathbf{s}')]$
2. Set  $V(\mathbf{s})=\max_\mathbf{a}Q(\mathbf{s},\mathbf{a})$

 

#### Fitted value iteration

- **curse of dimensionality**
- 上拟合函数,  Function Approximation, 进入 深度学习
- 使用 loss $\mathcal{L}(\phi)=\frac{1}{2}\left\Vert V_\phi(\mathbf{s})-\max_\mathbf{a}Q^\pi(\mathbf{s},\mathbf{a})\right\Vert^2$  来做回归

**Fitted Value** Iteration algorithm：

1. set $$\mathbf{y}_i\leftarrow\max_{\mathbf{a}_i}(r(\mathbf{s}_i,\mathbf{a}_i)+\gamma\mathbf{E}[V_\phi(\mathbf{s}_i')])$$.       更新待fit的目标, 这里包含了max, 其实是improve
2. set $\phi\leftarrow\arg\min_\phi\frac{1}{2}\sum_i\left\Vert V_\phi(\mathbf{s}_i)-\mathbf{y}_i\right\Vert^2$.        fit 最新的目标, 更新网络参数,如果用tabular就没这步了

这两个步骤不太好放入Policy iteration框架里面去



#### if don’t know the transition dynamics

- 对于无法知道系统转移概率的情况, 显然就无法执行max (r+V(s'))操作了
- 对大多数复杂的env, 没有model, 都是无法知道或者遍历的
- 这时 fit Q value



**Policy iteration**:  类似**SARSA** , on-policy ,不过 $\pi'$是个完全greedy

1. evaluate $Q^\pi(\mathbf s, \mathbf a)$ ,  fit with samples
2. set $\pi \leftarrow \pi'$                 imrove            

Policy evaluation:    用 Q 代替原来  V

$$
V^\pi(\mathbf{s})\leftarrow r(\mathbf{s},\pi(\mathbf{s}))+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}'|\mathbf{s},\pi(\mathbf{s}))}[V^\pi(\mathbf{s}')] \\
\Rightarrow 
Q^\pi(\mathbf{s},\mathbf{a})\leftarrow r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}'|\mathbf{s},\mathbf{a})}[Q^\pi(\mathbf{s}',\pi(\mathbf{s}'))]
$$



#####  the “max” trick

之前通过 max V 操作 来improve
$$
\begin{array}{ll}
\text { policy iteration: }&\Longrightarrow & \text { fitted value iteration algorithm: } \\
\text { 1. evaluate } V^{\pi}(\mathbf{s}) && 1 . \text { set } \mathbf{y}_{i} \leftarrow \max _{\mathbf{a}_{i}}\left(r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma E\left[V_{\phi}\left(\mathbf{s}_{i}^{\prime}\right)\right]\right) \\
\text { 2. set } \pi \leftarrow \pi^{\prime} & & \text { 2. set } \phi \leftarrow \arg \min _{\phi} \frac{1}{2} \sum_{i}\left\|V_{\phi}\left(\mathbf{s}_{i}\right)-\mathbf{y}_{i}\right\|^{2}
\end{array}
$$

将max应用到 Qfunction上.   这个时候没有V

**fitted Q** iteration algorithm:   

1. set $$\mathbf{y}_{i} \leftarrow r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma E\left[V_{\phi}\left(\mathbf{s}_{i}^{\prime}\right)\right]$$  <==   approxiate $$E\left[V\left(\mathbf{s}_{i}^{\prime}\right)\right] \approx \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$
2. set $$\phi \leftarrow \arg \min _{\phi} \frac{1}{2} \sum_{i}\left\|Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right\|^{2} \quad$$ doesn't require simulation of actions!

- +works even for **off-policy** samples (unlike actor-critic) 
- +**only one network, no high-variance policy gradient**
+ \- **no convergence guarantees** for non-linear function approximation (more on this later)



##### Fitted Q-iteration  Q-Learning

<img src="/img/CS285.assets/image-20200401230145459.png" alt="image-20200401230145459" style="zoom:50%;" />

full fitted Q-iteration algorithm:    
1. collect dataset $$\left\{\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)\right\}$$ **using some policy**  这里没明确用什么policy采样, 所以**off-policy**
2. loop K 
   1.  set $$\mathbf{y}_{i} \leftarrow r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}_{i}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$
   2. set $$\phi \leftarrow \arg \min _{\phi} \frac{1}{2} \sum_{i}\left\|Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right\|^{2}$$



#### Review

- Value-based methods
  - Don’t learn a policy explicitly
  - Just learn value or Q-function

- If we have value function, we have a policy

- Fitted Q-iteration



#### why off-policy

- 为什么Q-Learning (Fitted Q-iteration) 是 off-policy ?
- Off-policy : 采样的时候, 并不假定用的是啥policy, 就是任何policy与env交互的data都留下来做sample data.  
- 显然这对某些问题是比较有效的, 提高sample的使用率.  experience replay
- 因为 $max_{a'}$ 的 a' 都不是假定来自某一个特定分布的data.   不care a' 是来自正在running的policy,  因为是取max, 所以要在那个时间点考虑所有的action 而不仅仅来自当前policy . 
- (s,a) 之后, max选择的a' 与 当前running policy $\pi$ 即 采样的policy 无关
- 所以 evaluate 的policy公式中带max , 直接定为 off-policy



#### What is fitted Q-iteration optimizing?

Q-learning 对 tabular 是保证收敛的,  如果是非线性 function  approximator, 则不保证改进

**full fitted Q-iteration algorithm**:   

1. collect dataset $$\left\{\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)\right\}$$ using some policy
2. loop K 
   1.  set $$\mathbf{y}_{i} \leftarrow r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}_{i}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$  <== this max improves the policy (tabular case)
   2.  set $$\phi \leftarrow \arg \min _{\phi} \frac{1}{2} \sum_{i}\left\|Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right\|^{2}$$ <==  Q值 - y目标 =  **bellman error**



**bellman error** : $$\mathcal{E}=\frac{1}{2}\mathbf{E}_{(\mathbf{s},\mathbf{a})\sim\beta}\left[\left (Q_\phi(\mathbf{s},\mathbf{a})-\left[r(\mathbf{s},\mathbf{a})+\gamma\max_{\mathbf{a}'}Q_\phi(\mathbf{s}',\mathbf{a}')\right] \right)^2\right]$$

- 所以QLearning 算法中的回归部分的优化, 就是在缩小 bellman error.
- tabular的情况保证 error能收敛到0 ,  非线性的不保证
- if $\mathcal{E}=0$, then   $$Q^*(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\max_{\mathbf{a}'}Q^*(\mathbf{s}',\mathbf{a}')$$    
  这时, 得到 **optimal Q-function**,  对应 optimal policy $$\pi^*(\mathbf{a}_t\vert \mathbf{s}_t)=I\left(\mathbf{a}_t=\arg\max_{\mathbf{a}_t}Q^*(\mathbf{s}_t,\mathbf{a}_t)\right)$$



#### Online Q-learning algorithms

- **On-line : step-by-step**   

- **Off-line: episode-by-episode**



**online** Q iteration algorithm: 一步一更新.   标准的Q-learning

1. take some action $$\mathbf{a}_{i}$$ and observe $$\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)$$  ,    这里还是 off policy的, so many choices here!
2. $$\mathbf{y}_{i}=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$ .
3. $$\phi \leftarrow \phi-\alpha \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)\left(Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right)$$ .  goto 1.

这里与上面 full fitted Q-iteration algorithm的主要区别就是,  step1中只采样一个sample, step2, 3 只做一次, 而不是K次loop.



#### Exploration with Q-learning

在silver的教程中, **Greedy in the Limit with Infinite Exploration (GLIE)** .  $\epsilon-greedy$ 是 sarsa ,Q-learning work的关键.   sarsa公式本身只evaluate当前policy, 要靠 greedy部分improve; Q-learning要靠$\epsilon$ 来 探索.  

- **epsilon-greedy** :  实践中好用
  $$
  \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\left\{\begin{array}{ll}
  1-\epsilon \quad \quad  \text { if } \mathbf{a}_{t}=\arg \max _{\mathbf{a}_{t}} Q_{\phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) & &  \\
  \epsilon /(|\mathcal{A}|-1)  \quad  \text { otherwise }
  \end{array}\right.
  $$

- **Boltzmann exploration**:  
  $$
  \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \propto \exp \left(Q_{\phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)
  $$
  假定Q函数多多少少知道一些哪个行动好，但是有噪音。这样Q比较大的行动有显著较高的概率被选到，同时保持了其他选项的可能性。



##### Review

- Value-based methods
  - Don’t learn a policy explicitly 
  - Just learn value or Q-function

- If we have value function, we have a policy

- Fitted Q-iteration
  - Batch mode, off-policy method

- Q-learning
  - Online analogue of fitted Q- iteration

 

### Value function learning theory

下面研究收敛性, 是否收敛, 收敛到什么.      用函数算子来证明. 

#### tabular value function learning

先考虑 value function 是 tabular 的情况. 

<img src="/img/CS285.assets/image-20200402020807797.png" alt="image-20200402020807797" style="zoom: 33%;" />

Value iteration algorithm:  
	1 Set  $$Q(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V(\mathbf{s}')]$$  
	2 Set  $$V(\mathbf{s})=\max_\mathbf{a}Q(\mathbf{s},\mathbf{a})$$   
这两步可以合并为一步, 然后定义一个函数算子. 



- 定义算子 **bellman backup operator** $$\mathcal{B}$$  :   $$\mathcal{B}V=\max_\mathbf{a} (r_\mathbf{a}+\gamma\mathcal{T}_\mathbf{a}V)$$  张量的乘法  
  这个算子里面有backup ,也有max的操作  
- V 是tabular , 所以是个vector, 大小是state个数  
  reward 矩阵: $$r_\mathbf{a}$$ :  stacked vector of rewards at all states for action a, 大小s 
  状态转移矩阵: $$\mathcal{T}_{\mathbf{a},i,j}=p(\mathbf{s}'=i \vert \mathbf{s}=j,\mathbf{a})$$ , 固定下标a, $\mathcal{T}_{\mathbf{a}}$ 是个 s*s 大小的矩阵  
  所以 左边是 向量, 右边也是向量. 
- $$V^{\star}$$ is a **fixed point** of $$\mathcal{B}$$.   
  $$V^{\star}(\mathbf{s})=\max _{\mathbf{a}} r(\mathbf{s}, \mathbf{a})+\gamma E\left[V^{\star}\left(\mathbf{s}^{\prime}\right)\right],$$ so $$V^{\star}=\mathcal{B} V^{\star}$$  
  always exists, is always unique, always corresponds to the optimal policy,  练习题是证明这个.

那算法收敛么, 能收敛到这个最优解么.

- 可以证明,  如果是$$\mathcal{B}$$ 是个**压缩映射**, 就能收敛到最优解.  value iteration reaches $$V^{\star}$$ because $$\mathcal{B}$$ is a **contraction**  

- **contraction**: for any $$V$$ and $$\bar{V},$$ we have $$\Vert\mathcal{B} V-\mathcal{B} \bar{V}\Vert_{\infty} \leq \gamma\Vert V-\bar{V} \Vert_{\infty}$$  
  压缩的权重 factor是$\gamma$ .   gap always gets smaller by $$\gamma !$$ (with respect to $$\infty$$ -norm)
  
  - $$ \Vert x \Vert_\infty=max_i \vert x_i \vert  $$. 代表着, 两个向量之间的所有维度的最大距离.
  - 教授这里说了一个问题, 他不知道明确答案, 就是undiscount问题如何讨论收敛性.  比如 infinite 问题, 使用平均回报来做目标, 怎么能收敛到最优解. 
  
- if we choose $$V^{\star}$$ as $$\bar{V}$$ ?   $$\mathcal{B} V^{\star}=V^{\star} $$ !  将最优函数V*作为目标.  代入压缩映射的公式.

- $$\left\Vert B V-V^{\star}\right\Vert_{\infty} \leq \gamma\left\Vert V-V^{\star}\right\Vert_{\infty}$$ .   该式 V经过$$\mathcal{B}$$ 的一次映射以后, 距离V*更近了一点.所以只要一直policy iteration下去就会收敛到最优.   
  有点像优化问题, 去找到极值点. 或者看成SGD.

  <img src="/img/CS285.assets/image-20200402024057759.png" alt="image-20200402024057759" style="zoom:33%;" />

#### Non-tabular value function learning

- value iteration algorithm (using $\mathcal{B}$ ):
  loop:  $ V \leftarrow \mathcal{B} V$
- fitted value iteration algorithm (using $\mathcal{B}$ and $\Pi$ ):
  loop:  $ V \leftarrow \Pi \mathcal{B} V$      复合映射, 注意次序.



Fitted Value Iteration：

1. set $$\mathbf{y}_i\leftarrow\max_{\mathbf{a}_i}(r(\mathbf{s}_i,\mathbf{a}_i)+\gamma\mathbf{E}[V_\phi(\mathbf{s}_i')])$$. 
2. set $\phi\leftarrow\arg\min_\phi\frac{1}{2}\sum_i\left\Vert V_\phi(\mathbf{s}_i)-\mathbf{y}_i\right\Vert^2$. 



第一步 value iteration 可以定义为 **bellman backup operator** $$\mathcal{B}$$  , 跟tabular一样.

下面考虑第二步的算子 , 第二步 拟合值函数迭代需要做一个回归，它的本质是从神经网络能表示的所有函数的簇$\Omega$里面，找到一个函数$V'$ , 使得 $$V'\leftarrow\arg\min_{V'\in\Omega}\frac{1}{2}\sum\Vert V'(\mathbf{s})-(\mathcal{B}V)(\mathbf{s})\Vert^2$$   

<img src="/img/CS285.assets/image-20200320033132068.png" alt="image-20200320033132068" style="zoom:33%;" />  
上图表示,  神经网络能表达的函数集合就是蓝色的线, (简单表示为一维),  在线上能与BV最近的点就是欧氏距离L2 最小的点, 垂直的, 所以函数拟合所能找到的最优拟合点, 就是找到了使得L2loss最小的函数  
其实就是拟合网络本身的局限性,表达上限.  以上都是假设所有优化都是convex 凸优化的情况. 为了简化,否则情况更糟.

定义 最优拟合函数算子 $$\Pi: \Pi V=\arg \min _{V^{\prime} \in \Omega} \frac{1}{2} \sum\left\Vert V^{\prime}(\mathbf{s})-V(\mathbf{s})\right\Vert^{2}$$   
$\Pi$ 是 V函数在 $\Omega$ 上的 L2 norm 距离最小点的映射 projection.

<img src="/img/CS285.assets/image-20200402033646011.png" alt="image-20200402033646011" style="zoom:50%;" />

该projection算子也是个压缩映射.  在原来空间里的两个函数, 被映射到$\Omega$  函数空间里的两个点, 这个两个点的距离比之前更加近.  如上图. 



两个都是压缩映射

- $\mathcal{B}$ is a contraction w.r.t. $\infty$ -norm ("max" norm) $$\|\mathcal{B} V-\mathcal{B} \bar{V}\|_{\infty} \leq \gamma\|V-\bar{V}\|_{\infty}$$. 
- $\Pi$ is a contraction w.r.t. $\ell_{2}$ -norm (Euclidean distance) $\|\Pi V-\Pi \bar{V}\|^{2} \leq\|V-\bar{V}\|^{2}$

但两者合并起来, 并不是压缩映射. 关键在于 作用的范数维度不一样. $\Pi\mathcal{B}$  not a contraction.  
<img src="/img/CS285.assets/image-20200320042748933.png" alt="image-20200320042748933" style="zoom:33%;" />  
如图, V'实际上离最优比V还要远. 

Conclusions:

- value iteration converges (tabular case) 
- fitted value iteration does **not** converge not in general
   often not in practice



##### fitted Q-iteration

同理, 定义算子     重要!  

- $\mathcal{B}$:  $\mathcal{B}Q=r+\gamma\mathcal{T}\max_\mathbf{a}Q$
- $\Pi $ : $\Pi Q=\arg\min_{Q'\in\Omega}\frac{1}{2}\sum\Vert Q'(\mathbf{s},\mathbf{a})-Q(\mathbf{s},\mathbf{a})\Vert^2$ 
- $Q\leftarrow\Pi\mathcal{B}Q$ . 不收敛



##### Q-learning

online Q iteration algorithm: 一步一更新.   标准的Q-learning

1. take some action $$\mathbf{a}_{i}$$ and observe $$\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)$$ 
2. $$\mathbf{y}_{i}=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$ .
3. $$\phi \leftarrow \phi-\alpha \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)\left(Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right)$$ .  这里看起来像普通的 gradient decent

核心问题:  **no gradient through target value** .    

- 一般的GD算法, 是能收敛到一个局部最优解的. 但Q-learning 不是普通的GD.   
- 在于上式的 拟合目标y并不是一个常数, 该值depends on 网络的参数, 是个网络参数的函数  
- 对一般的GD,  loss都是越来越小.  但对Qlearning,  随着policy的越来越好, reward也越来越大, y也越来越大, loss也可能越来越大. 在Q-learning这种现象比较常见. 

- 而且y是包含max操作的, 不可微 not differentiable.  如果用手段来 soften 这个max操作, 能得到一个糟糕的算法,虽然收敛.  bellman residual minimization
- 所以, Q函数方法的梯度并不是目标函数的梯度，因此与策略梯度法并不同，它并不是梯度下降法。



##### AC

同样不收敛

An aside regarding terminology: 

1. $V^{\pi}$: value function for policy $\pi$ , this is what the **critic** does
2. $$V^{*}$$: value function for optimal policy $\pi^{*}$ , this is what **value iteration** does



#### Review

- Value iteration theory
  - Linear operator for backup
  - Linear operator for projection 
  - Backup is contraction
  - Value iteration converges

- Convergence with function approximation
  - Projection is also a contraction
  - Projection + backup is **not** a contraction
  - Fitted value iteration does not in general converge
- Implications for Q-learning
  - Q-learning, fitted Q-iteration, etc. does not converge with function approximation
- But we can make it work in practice! 
  - Sometimes – tune in next time





















