---
layout:     post
title:      CS 285. Value Function Methods
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

## Value Function Methods

其他教程都是 Value (DP->FA) -> PG -> AC ,  这边是 PG -> AC -> Value  , 可以看出侧重点.  Silver的课程在这块比较清晰.   

PG 强调方差小,求稳定;    value强调收敛性, 找最优



##### omit policy gradient completely

$A^\pi(\mathbf{s}_t,\mathbf{a}_t)$ :  a 比按策略出的action 好多少

下面在$\pi$ 基础上构建deterministic策略 ,  at least as good as $\pi$ , 一般都会 better
$$
\pi^{\prime}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\left\{\begin{array}{ll}
1 & \text { if } \mathbf{a}_{t}=\arg \max _{\mathbf{a}_{t}} A^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \\
0 & \text { otherwise }
\end{array}\right.
$$
如果有个fully observed MDP, 则总存在一个 deterministic policy 至少与最优策略一样好 , 也就是说随机策略不会有啥特别的好处.  当然在学习的时候, 有随机的话, 可以learn fast , explore fast.



#### Policy iteration

**Policy iteration** algorithm:

1. evaluate $A^\pi(\mathbf s, \mathbf a)$
2. set $\pi \leftarrow \pi'$  上面的公式

所以跟之前一样需要  evaluate $V^\pi(s)$ .



##### Dynamic programming

- Let's assume we know $p(s'\vert s,a)$, and $\mathbf s$ and $\mathbf a$ are both discrete (and small).  Small 是只规模小, 可以tabular 表示, 放内存里. 
- bootstrapped update : $$V^\pi(\mathbf{s})\leftarrow\mathbf{E}_{\mathbf{a}\sim\pi(\mathbf{a}\vert \mathbf{s})}[r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}' \vert \mathbf{s},\mathbf{a})}[V^\pi(\mathbf{s}')]]$$. 
- 又因为策略是 deterministic , $\pi(\mathbf{s})=\mathbf{a}$
- Simpilified ,  化简得到evaluation 公式 : $$V^\pi(\mathbf{s})\leftarrow r(\mathbf{s},\pi(\mathbf{s}))+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}'\vert \mathbf{s},\pi(\mathbf{s}))}[V^\pi(\mathbf{s}')]$$



##### Even simpler dynamic programming

- 将 evaluation 与 improve 结合成一步 : 

$$
\pi^{\prime}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\left\{\begin{array}{ll}1 & \text { if } \mathbf{a}_{t}= \color{red}{ \arg \max _{\mathbf{a}_{t}} A^{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) }\\0 & \text { otherwise }\end{array}\right.
$$

-  Advantage 取最大值的a , Q也取最大值.  即 $$\arg\max_{\mathbf{a}_t}A^\pi(\mathbf{s}_t,\mathbf{a}_t)=\arg\max_{\mathbf{a}_t}Q^\pi(\mathbf{s}_t,\mathbf{a}_t)$$
- skip the policy and compute values directly!



<img src="/img/CS285.assets/image-20200319231053012.png" alt="image-20200319231053012" style="zoom:33%;" />

**Value iteration algorithm**:

1. Set  $Q(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V(\mathbf{s}')]$
2. Set  $V(\mathbf{s})=\max_\mathbf{a}Q(\mathbf{s},\mathbf{a})$



<img src="/img/CS285.assets/image-20200319190558092.png" alt="image-20200319190558092" style="zoom: 33%;" />



##### Fitted value iteration

- **curse of dimensionality**
- 上拟合函数,  Function Approximation, 进入 深度学习
- 使用 loss $\mathcal{L}(\phi)=\frac{1}{2}\left\Vert V_\phi(\mathbf{s})-\max_\mathbf{a}Q^\pi(\mathbf{s},\mathbf{a})\right\Vert^2$  来做回归

**Fitted Value Iteration**：

1. set $$\mathbf{y}_i\leftarrow\max_{\mathbf{a}_i}(r(\mathbf{s}_i,\mathbf{a}_i)+\gamma\mathbf{E}[V_\phi(\mathbf{s}_i')])$$. 
2. set $\phi\leftarrow\arg\min_\phi\frac{1}{2}\sum_i\left\Vert V_\phi(\mathbf{s}_i)-\mathbf{y}_i\right\Vert^2$. 



##### if don’t know the transition dynamics

- 对于无法知道系统转移概率的情况, 显然就无法执行max (r+V(s'))操作了
- 对大多数复杂的env, 没有model, 都是无法知道或者遍历的
- 这时 fit Q value



**Policy iteration**:  **SARSA**

1. evaluate $Q^\pi(\mathbf s, \mathbf a)$ ,  fit with samples
2. set $\pi \leftarrow \pi'$  上面的公式

Policy evaluation:   代替原来 evaluate V

$$
V^\pi(\mathbf{s})\leftarrow r(\mathbf{s},\pi(\mathbf{s}))+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}'|\mathbf{s},\pi(\mathbf{s}))}[V^\pi(\mathbf{s}')] \\
\Rightarrow 
Q^\pi(\mathbf{s},\mathbf{a})\leftarrow r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}_{\mathbf{s}'\sim p(\mathbf{s}'|\mathbf{s},\mathbf{a})}[Q^\pi(\mathbf{s}',\pi(\mathbf{s}'))]
$$



#####  the “max” trick

之前通过 max V 操作 来improve
$$
\begin{array}{ll}
\text { policy iteration: }&\Longrightarrow & \text { fitted value iteration algorithm: } \\
\text { 1. evaluate } V^{\pi}(\mathbf{s}) && 1 . \text { set } \mathbf{y}_{i} \leftarrow \max _{\mathbf{a}_{i}}\left(r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma E\left[V_{\phi}\left(\mathbf{s}_{i}^{\prime}\right)\right]\right) \\
\text { 2. set } \pi \leftarrow \pi^{\prime} & & \text { 2. set } \phi \leftarrow \arg \min _{\phi} \frac{1}{2} \sum_{i}\left\|V_{\phi}\left(\mathbf{s}_{i}\right)-\mathbf{y}_{i}\right\|^{2}
\end{array}
$$

将max应用到Qfunction上.   这个时候没有V

fitted Q iteration algorithm:
1. set $$\mathbf{y}_{i} \leftarrow r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma E\left[V_{\phi}\left(\mathbf{s}_{i}^{\prime}\right)\right]$$  <==   approxiate $$E\left[V\left(\mathbf{s}_{i}^{\prime}\right)\right] \approx \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$
2. set $$\phi \leftarrow \arg \min _{\phi} \frac{1}{2} \sum_{i}\left\|Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right\|^{2} \quad$$ doesn't require simulation of actions!

- +works even for off-policy samples (unlike actor-critic) 
- +only one network, no high-variance policy gradient
+ \- no convergence guarantees for non-linear function approximation (more on this later)



##### Fitted Q-iteration  Q-Learning

full fitted Q-iteration algorithm:  
1. collect dataset $$\left\{\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)\right\}$$ **using some policy**  这里没明确用什么policy采样, 所以off-policy
2. loop K 
  1.  set $$\mathbf{y}_{i} \leftarrow r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}_{i}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$
  2. set $$\phi \leftarrow \arg \min _{\phi} \frac{1}{2} \sum_{i}\left\|Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right\|^{2}$$



#### Review

- Value-based methods
  - Don’t learn a policy explicitly
  - Just learn value or Q-function

- If we have value function, we have a policy

- Fitted Q-iteration



#### off-policy

- 为什么Q-Learning (Fitted Q-iteration) 是 off-policy ?
- Off-policy : 采样的时候, 并不假定用的是啥policy, 就是任何policy与env交互的data都留下来做sample data.  
- 显然这对某些问题是比较有效的, 提高sample的使用率.  experience replay
- 因为 $max_{a'}$ 的 a' 都不是假定来自某一个特定分布的data.   不care a' 是来自正在running的policy,  要在那个时间点考虑所有的action 而不仅仅来自当前policy . 
- (s,a) 之后, 选择的a' 与 当前running policy $\pi$ 即 采样的policy 无关
- 所以 evaluate 的policy公式中带max , 直接定为 off-policy



#### What is fitted Q-iteration optimizing?

Q-learning 对 tabular 是保证收敛的,  如果是非线性 function  approximator, 则不保证改进

full fitted Q-iteration algorithm:   

1. collect dataset $$\left\{\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)\right\}$$ using some policy
2. loop K 
   1.  set $$\mathbf{y}_{i} \leftarrow r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}_{i}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$  <== this max improves the policy (tabular case)
   2.  set $$\phi \leftarrow \arg \min _{\phi} \frac{1}{2} \sum_{i}\left\|Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right\|^{2}$$ <==  Q值 - y目标 =  **bellman error**



**bellman error** : $$\mathcal{E}=\frac{1}{2}\mathbf{E}_{(\mathbf{s},\mathbf{a})\sim\beta}\left[\left (Q_\phi(\mathbf{s},\mathbf{a})-\left[r(\mathbf{s},\mathbf{a})+\gamma\max_{\mathbf{a}'}Q_\phi(\mathbf{s}',\mathbf{a}')\right] \right)^2\right]$$

- 所以QLearning 算法中的回归部分的优化, 就是在缩小 bellman error.
- tabular的情况保证 error能收敛到0 ,  非线性的不保证
- if $\mathcal{E}=0$, then   $$Q^*(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\max_{\mathbf{a}'}Q^*(\mathbf{s}',\mathbf{a}')$$  
- 这时, 得到 optimal Q-function,  对应 optimal policy $$\pi^*(\mathbf{a}_t\vert \mathbf{s}_t)=I\left(\mathbf{a}_t=\arg\max_{\mathbf{a}_t}Q^*(\mathbf{s}_t,\mathbf{a}_t)\right)$$



#### Online Q-learning algorithms

- On-line : step-by-step   

- Off-line: episode-by-episode



online Q iteration algorithm: 一步一更新.   标准的Q-learning

1. take some action $$\mathbf{a}_{i}$$ and observe $$\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)$$  , off policy, so many choices here!

2. $$\mathbf{y}_{i}=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$ .
3. $$\phi \leftarrow \phi-\alpha \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)\left(Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right)$$ . 



#### Exploration with Q-learning

- **epsilon-greedy** :  实践中好用
  $$
  \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)=\left\{\begin{array}{ll}
  1-\epsilon \quad \quad  \text { if } \mathbf{a}_{t}=\arg \max _{\mathbf{a}_{t}} Q_{\phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) & &  \\
  \epsilon /(|\mathcal{A}|-1)  \quad  \text { otherwise }
  \end{array}\right.
  $$

- **Boltzmann exploration**:  
  $$
  \pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \propto \exp \left(Q_{\phi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)
  $$
  假定Q函数多多少少知道一些哪个行动好，但是有噪音。这样Q比较大的行动有显著较高的概率被选到，同时保持了其他选项的可能性。



##### Review

- Value-based methods
  - Don’t learn a policy explicitly 
  - Just learn value or Q-function

- If we have value function, we have a policy

- Fitted Q-iteration
  - Batch mode, off-policy method

- Q-learning
  - Online analogue of fitted Q- iteration

 

### Value function learning theory

下面研究收敛性, 是否收敛, 收敛到什么.      用函数算子来证明. 

Value iteration algorithm:

1. Set  $$Q(\mathbf{s},\mathbf{a})=r(\mathbf{s},\mathbf{a})+\gamma\mathbf{E}[V(\mathbf{s}')]$$
2. Set  $$V(\mathbf{s})=\max_\mathbf{a}Q(\mathbf{s},\mathbf{a})$$



- 定义算子 **bellman backup operator** $$\mathcal{B}$$  :   $$\mathcal{B}V=\max_\mathbf{a}r_\mathbf{a}+\gamma\mathcal{T}_\mathbf{a}V$$  张量的乘法
- 这个算子里面有backup ,也有max的操作
- $$r_\mathbf{a}$$ :  stacked vector of rewards at all states for action a , 也可以看成 S*a的matrix
- 状态转移矩阵 $$\mathcal{T}_{\mathbf{a},i,j}=p(\mathbf{s}'=i \vert \mathbf{s}=j,\mathbf{a})$$ 
- $$V^{\star}$$ is a fixed point of $$\mathcal{B}$$ $$V^{\star}(\mathbf{s})=\max _{\mathbf{a}} r(\mathbf{s}, \mathbf{a})+\gamma E\left[V^{\star}\left(\mathbf{s}^{\prime}\right)\right],$$ so $$V^{\star}=\mathcal{B} V^{\star}$$
  always exists, is always unique, always corresponds to the optimal policy
- value iteration reaches $$V^{\star}$$ because $$\mathcal{B}$$ is a **contraction** 压缩映射
-  $$ \Vert x \Vert_\infty=max_i \vert x_i \vert  $$.
- **contraction**: for any $$V$$ and $$\bar{V},$$ we have $$\Vert\mathcal{B} V-\mathcal{B} \bar{V}\Vert_{\infty} \leq \gamma\Vert V-\bar{V} \Vert_{\infty}$$
- gap always gets smaller by $$\gamma !$$ (with respect to $$\infty$$ -norm)
-  if we choose $$V^{\star}$$ as $$\bar{V}$$ ?   $$\mathcal{B} V^{\star}=V^{\star} $$ !
- $$\left\Vert B V-V^{\star}\right\Vert_{\infty} \leq \gamma\left\Vert V-V^{\star}\right\Vert_{\infty}$$ .



##### Non-tabular value function learning

Fitted Value Iteration：

1. set $$\mathbf{y}_i\leftarrow\max_{\mathbf{a}_i}(r(\mathbf{s}_i,\mathbf{a}_i)+\gamma\mathbf{E}[V_\phi(\mathbf{s}_i')])$$. 
2. set $\phi\leftarrow\arg\min_\phi\frac{1}{2}\sum_i\left\Vert V_\phi(\mathbf{s}_i)-\mathbf{y}_i\right\Vert^2$. 



- value iteration 第一步是 backup , 利用最新的r以及bellman来迭代V
- 下面考虑第二步的算子 , 第二步 拟合值函数迭代需要做一个回归，它的本质是从神经网络能表示的所有函数的簇$\Omega$里面，找到一个函数$V'$ , 使得 $$V'\leftarrow\arg\min_{V'\in\Omega}\frac{1}{2}\sum\Vert V'(\mathbf{s})-(\mathcal{B}V)(\mathbf{s})\Vert^2$$  
- <img src="/img/CS285.assets/image-20200320033132068.png" alt="image-20200320033132068" style="zoom:33%;" />
- 上图表示,  神经网络能表达的函数集合就是蓝色的线, (简单表示为一维),  在线上能与BV最近的点就是欧氏距离L2 最小的点, 垂直的, 所以函数拟合所能找到的最优拟合点. 其实就是拟合网络本身的局限性,表达上限.  以上都是假设所有优化都是convex 凸优化的情况. 为了简化,否则情况更糟.
- define new operator 最优拟合函数算子 $$\Pi: \Pi V=\arg \min _{V^{\prime} \in \Omega} \frac{1}{2} \sum\left\Vert V^{\prime}(\mathbf{s})-V(\mathbf{s})\right\Vert^{2}$$ 
- $\Pi$ 是 V函数在 $\Omega$ 上的 L2 norm 距离最小点的映射 projection.

- Fitted Value Iteration 等价于 $V\leftarrow\Pi\mathcal{B}V$ 这样的复合映射 

两个都是压缩映射

- $\mathcal{B}$ is a contraction w.r.t. $\infty$ -norm ("max" norm) $\|\mathcal{B} V-\mathcal{B} \bar{V}\|_{\infty} \leq \gamma\|V-\bar{V}\|_{\infty}$
- $\Pi$ is a contraction w.r.t. $\ell_{2}$ -norm (Euclidean distance) $\|\Pi V-\Pi \bar{V}\|^{2} \leq\|V-\bar{V}\|^{2}$

但两者合并起来, 并不是压缩映射. 关键在于 作用的范数维度不一样. $\Pi\mathcal{B}$  not a contraction.

<img src="/img/CS285.assets/image-20200320042748933.png" alt="image-20200320042748933" style="zoom:33%;" />

​	如图, V'实际上离最优比V还要远. 

Conclusions:

- value iteration converges (tabular case) 
- fitted value iteration does **not** converge not in general
   often not in practice



##### fitted Q-iteration

同理, 定义算子

- $\mathcal{B}$:  $\mathcal{B}Q=r+\gamma\mathcal{T}\max_\mathbf{a}Q$
- $\Pi $ : $\Pi Q=\arg\min_{Q'\in\Omega}\frac{1}{2}\sum\Vert Q'(\mathbf{s},\mathbf{a})-Q(\mathbf{s},\mathbf{a})\Vert^2$ 
- $Q\leftarrow\Pi\mathcal{B}Q$ . 不收敛



##### Q-learning

online Q iteration algorithm: 一步一更新.   标准的Q-learning

1. take some action $$\mathbf{a}_{i}$$ and observe $$\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)$$ 
2. $$\mathbf{y}_{i}=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)$$ .
3. $$\phi \leftarrow \phi-\alpha \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)\left(Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right)$$ .  这里看起来像普通的 gradient decent

- no gradient through target value :   一般的GD算法, 是能收敛到一个局部最优解的. 但Q-learning的GD不是普通的GD. 

- 问题在于上式的 拟合目标y并不是一个常数, 该值depends on 网络的参数, 是个网络参数的函数, 所以反向传播的并不是bellman error 函数的梯度. 

- 而且y是包含max操作的, 不可微 not differentiable.  如果用手段来 soften 这个max操作, 能得到一个糟糕的算法,虽然收敛.  bellman residual minimization
- Q函数方法的梯度并不是目标函数的梯度，因此与策略梯度法并不同，它并不是梯度下降法。



##### AC

同样不收敛



#### Review

- Value iteration theory
  - Linear operator for backup
  - Linear operator for projection 
  - Backup is contraction
  - Value iteration converges

- Convergence with function approximation
  - Projection is also a contraction
  - Projection + backup is **not** a contraction
  - Fitted value iteration does not in general converge
- Implications for Q-learning
  - Q-learning, fitted Q-iteration, etc. does not converge with function approximation
- But we can make it work in practice! 
  - Sometimes – tune in next time





















