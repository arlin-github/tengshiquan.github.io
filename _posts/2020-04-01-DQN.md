---
layout:     post
title:      DQN
subtitle:   DQN from Deepmind
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - DeepMind
    - Reinforcement Learning
    - DQN

---

 

# DQN

 DQN在Atari游戏上的成功是 AI复兴的一个重大标志性事件.  这里稍微梳理一下DQN相关工作的进展. 

经典论文很多人都做了翻译整理, 但还是值得细细研读, 主要是思路跟实验细节. 这里只记录我的笔记. 

experience replay + target network + error clipping



## Playing Atari with Deep Reinforcement Learning

2013  第一个提出  CNN + Q-learning ,  learn from high-dimensional sensory input (raw pixels)  
RL之前, 依赖手工特征+线性函数.   hand-crafted features combined with linear value functions or policy representations. 

RL 引入DL 的问题:

1. no hand-labelled training data. reward  sparse, noisy and delayed
2. <mark>**correlated data**</mark> : sample sequences not iid
3. <mark>**non-stationary** : **distribution shift**</mark>

本文证明了CNN 可以克服这些挑战,  Q-learning with SGD.    
<mark>**experience replay**</mark> , smooths the training distribution over many past behaviors.  **(s, a, r, s')** 

input: visual input (210 × 160 RGB video at 60Hz) ,  7个游戏，6个表现超过了之前所有的RL算法，3个超过了人类高手. 



#### Background

- environment $\mathcal{E}$ 

- optimal action-value function :   $$
  Q^{*}(s, a)=\max _{\pi} \mathbb{E}\left[R_{t} | s_{t}=s, a_{t}=a, \pi\right]
  $$

- **Bellman equation**: 直觉解释, 如果知道了下步的最大Q值, 则最佳策略就是选取该a' 使得回报的期望最大.

$$
Q^{*}(s, a)=\mathbb{E}_{s^{\prime} \sim \mathcal{E}}\left[r+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right) | s, a\right]
$$

- value iteration:   converge to the optimal action- value function $Q_i \to Q^*$  as  $i \to \infty$ 

- basic approach is totally impractical, because the action-value function is estimated separately for each sequence, without any generalisation.  tabular 不现实, 无泛化.  必须拟合. 

- Q-network:  $Q\left(s, a ; \theta\right) \approx Q^{*}(s, a) $ .   
  at each iteration i , Loss:

$$
L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(\cdot)}\left[\left(y_{i}-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]
$$

- <mark>**behaviour distribution**</mark> : $\rho(s,a)$  a probability distribution over sequences s and actions a
- Note that the **targets depend on the network weights**; this is in contrast with the targets used for supervised learning, which are fixed before learning begins.  显然在minibatch的时候, max用的是上次版本的Q函数. 但这次没提到target network

$$
\nabla_{\theta_{1}} L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(s) ; s^{\prime} \sim \mathcal{E}}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i-1}\right)-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{1}} Q\left(s, a ; \theta_{i}\right)\right]
$$
- model-free ;  **off-policy** : learns  greedy strategy $a=\max _{a} Q(s, a ; \theta),$  following behaviour distribution that ensures adequate exploration of the state space. $\epsilon$ -greedy strategy. 

#### Related Work

- **TD-gammon**, learnt entirely by reinforcement learning and self-play, approximated the value function using a multi-layer perceptron with one hidden layer.   
  approximated  V (s) rather than Q(s, a), and learnt on-policy directly from the self-play games

- Q-Learning(Watkins, 1989)  diverge(except tabular). 

- 最近, DNN estimate env; RBM estimate Value func or policy ; gradient TD 解决Q-learning divergence.  
  这些方法,converge when evaluating a fixed policy with a nonlinear function approximator ; or when learning a control policy with linear function approximation using a restricted variant of Q-learning. 

   not yet been extended to nonlinear control.

- 之前跟本文最接近的: **neural fitted Q-learning (NFQ)** . BP使用的Rprop, 但是是batch update.  作者用SGD, have a low constant cost per iteration and scale to large data-sets.   
  NFQ用于纯视觉输入, 但先用了 deep autoencoders to learn a low dimensional representation, 然后再上NFQ. 作者直接DQN, 能学到特征.

- experience replay + Q-learning 也有, 但也只搞的是low-dimensional输入.

#### Deep Reinforcement Learning

- start point是 TD-Gammon, 当时挺强 ;  那么RL 加上 现代DNN 技术,能搞点事情.
- 与之前不同, 引入 **experience replay** 
- Q-function inputs:  fixed length representation of histories produced by  function $\phi$.
- deep Q-learning 比 online Q-learning 优点:
  1. data efficiency,   sample 可复用
  2. correlated data 数据相关性问题,  randomizing breaks correlations, reduces the variance of the updates.
  3. sample与当前参数相关.  learning on-policy the current parameters determine the next data sample that the parameters are trained on , 可能造成局部最优或者不收敛. By using experience replay the behavior distribution is averaged over many of its previous states, **smoothing out learning** and avoiding oscillations or divergence in the parameters.
- 选择Q-learning的动机 :  when learning by **experience replay**, it is necessary to learn **off-policy** (because our current parameters are different to those used to generate the sample), which motivates the choice of Q-learning.
- 不同sample的重要性问题. In practice,  only stores the last N experience tuples in the replay memory, and **samples uniformly** at random.  有点问题, 忽视了重要的 transition , 如果时间久了都会被最新的覆盖掉.  **prioritized sweeping**

![image-20200406023628549](/img/2020-04-01-DQN.assets/image-20200406023628549.png)

一个点就是, 在这个版本里面, 是一个minibatch, 用上一版本的Q函数算一批Q(s'), 然后执行一次GD step. 对每个时间t, 都会有一次GD.  上面有个Q* ,就是Q

##### Preprocessing and Model Architecture

- 原始 210 × 160 pixel images with a 128 color palette =>  gray-scale and down-sampling it to a 110×84 image => cropping an 84 × 84 region roughly captures the playing area ,  GPU的CNN需要方形.
- action in 需要每个action都计算 forward pass一次.  这里用 action out, 对所有可能的action输出只FP一次  
  大概有 4 and 18 个action
- Medel: Deep Q-Networks (DQN)
  - input: 84 × 84 × 4,  image produced by $\phi$
  - first hidden layer convolves 16 8 × 8 filters with stride 4 ,  rectifier nonlinearity
  - second hidden layer convolves 32 4 × 4 filters with stride 2,  rectifier nonlinearity
  - final hidden layer is fully-connected and consists of 256 rectifier units
  - The output layer is a fully-connected linear layer with a single output for each valid action



#### Experiments

- 几个游戏, 分数统一.  统一了学习率. Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be −1, leaving 0 rewards unchanged. Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to **use the same learning rate across multiple games**. At the same time, it could affect the performance of our agent since it cannot differentiate between rewards of different magnitude.
- RMSProp minibatches 32.  
  线性衰减 ε-greedy with ε annealed **linearly** from 1 to 0.1 over the first million frames, and fixed at 0.1 thereafter  
- frame-skipping,  agent sees and selects actions on every $k^{th}$ frame instead of every frame, and its last action is repeated on skipped frames. 因为env跑一个action比agent选一个action快的多, 所以相当于多训练了k倍而不增加训练时间. 主要的问题还是状态太类似了, 在图片上. 

##### Training and Stability

RL评估很麻烦.  more stable, metric is the policy’s estimated action-value function Q. Q函数一直都是稳增的. 说明, 虽然没有收敛保证 , 也能稳定学习. 

![image-20200406032546859](/img/2020-04-01-DQN.assets/image-20200406032546859.png)

##### Visualizing the Value Function

![image-20200406033144689](/img/2020-04-01-DQN.assets/image-20200406033144689.png)

##### Main Evaluation

![image-20200406034526266](/img/2020-04-01-DQN.assets/image-20200406034526266.png)

 



## Human-level control through deep reinforcement learning

2015 ,  明确提出 <mark>**target network**</mark> 

RL task : must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. 

deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning.   绕过了人工提取特征以及简单的low-dimensional情况.

cnn: deep convolutional network, which uses **hierarchical layers** of **tiled convolutional filters** to mimic the effects of receptive fields, thereby exploiting the **local spatial correlations** present in images, and building in robustness to natural transformations such as changes of viewpoint or scale.


$$
Q^{*}(s, a)=\max _{\pi} \mathbb{E}\left[r_{t}+\gamma r_{t+1}+\gamma^{2} r_{t+2}+\ldots | s_{t}=s, a_{t}=a, \pi\right]
$$


- Q-learning(nonlinear func approximator) : unstable or diverge. 
  1. correlations in sequence
  2. small updates to Q may significantly change the policy, therefore change the data distribution
  3. correlations between Q and target $r+\gamma \max_{a'}Q(s',a')$

- address these instabilities with a novel variant of Q-learning, 2 key ideas
  1. experience replay: removing correlations in the observation sequence and smoothing over changes in the data distribution
  2. target network: iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.


$$
L_{i}\left(\theta_{i}\right)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right) \sim U(D)}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]
$$

网络结构:   
![image-20200406114430548](/img/2020-04-01-DQN.assets/image-20200406114430548.png)



![image-20200406122739925](/img/2020-04-01-DQN.assets/image-20200406122739925.png)



使用 t-SNE 查看图像的相关性.  t-SNE算法倾向于将 DQN representation of perceptually similar states dqn感觉相近的图像映射到附近的点.  t-SNE还把回报相似但视觉上不太相似的点放一块.  说明NN可以学到 representation that support adaptive behaviour .  Furthermore, the representations learned by DQN are able to generalize to data generated from policies other than its own. DQN学到的特征表达, 可以覆盖自己的数据以及人类玩家的数据. 就是说, DQN学到的策略, 而不完全是大数据拟合.   
图中, 深红色是V高的,深蓝色是V低的.

![image-20200406122839173](/img/2020-04-01-DQN.assets/image-20200406122839173.png)

DQN 算法在游戏中可以通用.  在某些游戏中，DQN能够发现一个相对长期的策略. 然而,更多的时间延伸的策略仍然是挑战. games demanding more temporally extended planning strategies still constitute a major challenge for all existing agents including DQN. 

谈了下, 受生物技术的启发.  experience replay 和 prioritized sweeping

#### METHODS

##### Preprocessing

- 雅达利机能限制, 显示sprite数目有限, 所以有的是隔帧显示, 这里对每一个像素的颜色值在当前帧和前一帧上取最大值, 消除了闪烁的问题.   

- 处理最近4帧作为输入. function $\phi$  applies this preprocessing to the m most recent frames and stacks them to produce the input to the Q-function, in which m = 4. 

##### Code availability

https://sites.google.com/a/deepmind.com/dqn

##### Model architecture

acton out ,  4 - 18 个action

跟之前的稍微有点不一样.   之前两层cnn, 现在三层cnn

- input: 84 * 84 * 4 image
- 32 filters of 8 * 8 with stride 4 , rectifier nonlinearity
- 64 filters of 4 * 4 with stride 2, rectifier nonlinearity
- 64 filters of 3 * 3 with stride 1 followed by a rectifier
- final hidden layer is fully-connected and consists of 512 rectifier units
-  output layer is a fully-connected linear layer with a single output for each valid action

##### Training details

- 基本与之前处理的方式一样.   分数截取到-1,1, 使用相同学习率.   
  RMSProp minibatch 32;    
  ε-greedy with e annealed linearly from 1.0 to 0.1 over the first million frames, and fixed at 0.1 thereafter.

- 总共训练了5000万帧数据, buffer是100万大小;  trained for a total of 50 million frames (that is, around 38 days of game experience in total) and used a replay memory of 1 million most recent frames.

- frame-skipping, k=4

- 超参的选择, 根据几个游戏来调的, 然后固定下来, 其他所有游戏都用.

##### Evaluation procedure

ained agents were evaluated by playing each game 30 times for up to 5 min each time with different initial random conditions(‘no-op’ 通过不操作造成开局的不一样) and an ε-greedy policy with ε=0.05.

##### Algorithm

- 多了target network 部分.

- 训练 Q-network, 但target由  $$r+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right)$$ 替换为 $$y=r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)$$, using parameters $$\theta_{i}^{-}$$ from some previous iteration. 用之前的Q网络产生target.   
  则 loss 改为 $L_{i}\left(\theta_{i}\right)$ that changes at each iteration $i$

$$
\begin{aligned}
L_{i}\left(\theta_{i}\right) &=\mathbb{E}_{s, a, r}\left[\left(\mathbb{E}_{s'}[y | s, a]-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right] \\
&=\mathbb{E}_{s, a, r, s'}\left[\left(y-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]+\mathbb{E}_{s, a, r}\left[\mathbb{V}_{s'}[y]\right]
\end{aligned}
$$

- 上式, 利用 $E(X^2) - (E(X))^2 = D(X)$

- 现在target在学习开始后不会变动, 所以是 **welldefined** optimization problems. 上式最后一项是target的方差 variance, does not depend on the parameters $\theta_{i}$ currently optimizing, and may therefore be ignored.   
  求梯度:

$$
\nabla_{\theta_{i}} L\left(\theta_{i}\right)=\mathbb{E}_{\varepsilon, a, r, s'}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{i}} Q\left(s, a ; \theta_{i}\right)\right]
$$

- 再SGD.

- 经典Q-learning可以用上式这个框架表示, 只需在每个time step 更新weights, 然后期望改用单个sample, 令$\theta_{i}^{-} = \theta_{i-1}$

- model-free; off-policy: learn greedy follow soft-greedy



##### Training algorithm for deep Q-networks

- buffer  size , fixed

解决Q-learning suitable for training large neural networks without diverging.

1. **experience replay**
   - agent's experiences at each time-step, $$e_{t}=\left(s_{t}, a_{t}, r_{t} ,s_{t+1}\right)$$, in a data set $$D_{t}=\left\{e_{1}, \ldots, e_{t}\right\}$$ pooled over many episodes , $\left(s, a, r, s^{\prime}\right) \sim U(D)$ drawn at random from the pool 
   - 较 standard online Q-learning 优点, 跟上篇一样
2. **target  network**
   - more stable compared to standard online Q-learning, where an update that increases $Q\left(s_{t}, a_{t}\right)$ often also increases $Q\left(s_{t}+1, a\right)$ for all $a$ and hence also increases the target $y_{i}$, possibly leading to oscillations or divergence of the policy.
   - Generating the targets using an older set of parameters adds a **delay** between the time an update to $Q$ is made and the time the update affects the targets $y_{j},$ making divergence or oscillations much more unlikely.
3. <mark>**error clipping**</mark>
   - clip the **error term** from the update $$r+\gamma \max _{a^{\prime}} Q   \left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ; \theta_{i}\right)$$ to be [-1 , 1].
   - **相当于绝对值函数的导数**.  absolute value loss function $\vert x \vert$ has a derivative of -1 for all negative values of $x$ and a derivative of 1 for all positive values of $x,$ clipping the squared error to be between -1 and 1 corresponds to using an absolute value loss function for errors outside of the (-1,1) interval. 
   - error clipping further improved the stability of the algorithm.



![image-20200407020008740](/img/2020-04-01-DQN.assets/image-20200407020008740.png)





![image-20200407031916677](/img/2020-04-01-DQN.assets/image-20200407031916677.png)

上图说明, Q值预估以及DQN给出的动作概率, 都是很合理的.



#### List of hyperparameters and their values

![image-20200407032158313](/img/2020-04-01-DQN.assets/image-20200407032158313.png)



#### The effects of replay and separating the target Q-network

![image-20200407032324187](/img/2020-04-01-DQN.assets/image-20200407032324187.png)



说明, experience  replay 比 target network 要重要的多!



 




## Reference

Playing atari with deep reinforcement learning. In *NIPS Deep Learning Workshop*. 2013.

Human-level control through deep reinforcement learning. *Nature*, URL http://dx.doi.org/10.1038/nature14236.

 























