---
layout:     post
title:      DQN 相关论文笔记
subtitle:   DQN from Deepmind
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - DeepMind
    - Reinforcement Learning
    - DQN

---

 

# DQN 相关论文笔记

 DQN在Atari游戏上的成功是 AI复兴的一个重大标志性事件.  这里稍微梳理一下DQN相关工作的进展. 

经典论文很多人都做了翻译整理, 但还是值得细细研读, 这里只记录我的笔记. 



## Playing Atari with Deep Reinforcement Learning

2013  第一个提出  CNN + Q-learning ,  learn from high-dimensional sensory input (raw pixels)  
RL之前, 依赖手工特征+线性函数.   hand-crafted features combined with linear value functions or policy representations. 

RL 引入DL 的问题:

1. no hand-labelled training data. reward  sparse, noisy and delayed
2. correlated data : sample sequences not iid
3. non-stationary : distribution shift

本文证明了CNN 可以克服这些挑战,  Q-learning with SGD.    
**experience replay** , smooths the training distribution over many past behaviors.

input: visual input (210 × 160 RGB video at 60Hz) ,  7个游戏，6个表现超过了之前所有的RL算法，3个超过了人类高手. 

#### Background

- environment $\mathcal{E}$ 

- optimal action-value function :   $$
  Q^{*}(s, a)=\max _{\pi} \mathbb{E}\left[R_{t} | s_{t}=s, a_{t}=a, \pi\right]
  $$

- **Bellman equation**: 直觉解释, 如果知道了下步的最大Q值, 则最佳策略就是选取该a' 使得回报的期望最大.

$$
Q^{*}(s, a)=\mathbb{E}_{s^{\prime} \sim \mathcal{E}}\left[r+\gamma \max _{a^{\prime}} Q^{*}\left(s^{\prime}, a^{\prime}\right) | s, a\right]
$$

- value iteration:   converge to the optimal action- value function $Q_i \to Q^*$  as  $i \to \infty$ 

- basic approach is totally impractical, because the action-value function is estimated separately for each sequence, without any generalisation.  tabular 不现实, 无泛化.  必须拟合. 

- Q-network:  $Q\left(s, a ; \theta\right) \approx Q^{*}(s, a) $ .   
  at each iteration i , Loss:

$$
L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(\cdot)}\left[\left(y_{i}-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]
$$

- **behaviour distribution** : $\rho(s,a)$  a probability distribution over sequences s and actions a
- Note that the **targets depend on the network weights**; this is in contrast with the targets used for supervised learning, which are fixed before learning begins.

$$
\nabla_{\theta_{1}} L_{i}\left(\theta_{i}\right)=\mathbb{E}_{s, a \sim \rho(s) ; s^{\prime} \sim \mathcal{E}}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i-1}\right)-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{1}} Q\left(s, a ; \theta_{i}\right)\right]
$$
- model-free ;  **off-policy** : learns  greedy strategy $a=\max _{a} Q(s, a ; \theta),$  following behaviour distribution that ensures adequate exploration of the state space. $\epsilon$ -greedy strategy. 

#### Related Work

- **TD-gammon**, learnt entirely by reinforcement learning and self-play, approximated the value function using a multi-layer perceptron with one hidden layer.   
  approximated  V (s) rather than Q(s, a), and learnt on-policy directly from the self-play games

- Q-Learning diverge(except tabular). 

- 最近, DNN estimate env; RBM estimate Value func or policy ; gradient TD 解决Q-learning divergence.  
  这些方法,converge when evaluating a fixed policy with a nonlinear function approximator ; or when learning a control policy with linear function approximation using a restricted variant of Q-learning. 

   not yet been extended to nonlinear control.

- 之前跟本文最接近的: **neural fitted Q-learning (NFQ)** . BP使用的Rprop, 但是是batch update.  作者用SGD, have a low constant cost per iteration and scale to large data-sets.   
  NFQ用于纯视觉输入, 但先用了 deep autoencoders to learn a low dimensional representation, 然后再上NFQ. 作者直接DQN, 能学到特征.

- experience replay + Q-learning 也有, 但也只搞的是low-dimensional输入.

#### Deep Reinforcement Learning

- 起点 TD-Gammon,  加上 最新的DNN技术
- experience replay 
- Q-function inputs:  fixed length representation of histories produced by  function $\phi$.
- deep Q-learning 比 online Q-learning 优点:
  1. data efficiency,   sample 可复用
  2. correlated data 解决相关性问题, reduces the variance of the updates. 
  3. sample与当前参数相关.  learning on-policy the current parameters determine the next data sample that the parameters are trained on , 可能造成局部最优或者不收敛. By using experience replay the behavior distribution is averaged over many of its previous states, **smoothing out learning** and avoiding oscillations or divergence in the parameters.
- 不同sample的重要性问题. In practice,  only stores the last N experience tuples in the replay memory, and **samples uniformly** at random.  有点问题, 忽视了重要的 transition , 如果时间久了都会被最新的覆盖掉.  **prioritized sweeping**

![image-20200406023628549](/img/2020-04-01-DQN.assets/image-20200406023628549.png)

##### Preprocessing and Model Architecture

- 原始 210 × 160 pixel images with a 128 color palette =>  gray-scale and down-sampling it to a 110×84 image => cropping an 84 × 84 region roughly captures the playing area ,  GPU的CNN需要方形.
- action in 需要每个action都计算 forward pass一次.  这里用 action out, 对所有可能的action输出只FP一次  
   4 and 18 个action
- input: 84 × 84 × 4,  first hidden layer convolves 16 8 × 8 filters with stride 4 , second hidden layer convolves 32 4 × 4 filters with stride 2, final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully- connected linear layer with a single output for each valid action.   =>  Deep Q-Networks (DQN)



#### Experiments

- 几个游戏, 分数统一.  统一了学习率. Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be −1, leaving 0 rewards unchanged. Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to **use the same learning rate across multiple games**. At the same time, it could affect the performance of our agent since it cannot differentiate between rewards of different magnitude.
- RMSProp minibatches 32.  
  线性衰减 ε-greedy with ε annealed **linearly** from 1 to 0.1 over the first million frames, and fixed at 0.1 thereafter  
- frame-skipping,  agent sees and selects actions on every $k^{th}$ frame instead of every frame, and its last action is repeated on skipped frames. 因为env跑一个action比agent选一个action快的多, 所以相当于多训练了k倍而不增加训练时间. 主要的问题还是状态太类似了, 在图片上. 

##### Training and Stability

RL评估很麻烦.  more stable, metric is the policy’s estimated action-value function Q. Q函数一直都是稳增的. 说明, 虽然没有收敛保证 , 也能稳定学习. 

![image-20200406032546859](/img/2020-04-01-DQN.assets/image-20200406032546859.png)

##### Visualizing the Value Function

![image-20200406033144689](/img/2020-04-01-DQN.assets/image-20200406033144689.png)

##### Main Evaluation

![image-20200406034526266](/img/2020-04-01-DQN.assets/image-20200406034526266.png)

 

## Reference

Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Graves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and Riedmiller, Martin. Playing atari with deep reinforcement learning. In *NIPS Deep Learning Workshop*. 2013.



Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K., Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis, Demis. Human-level control through deep reinforcement learning. *Nature*, 518(7540):529–533, 02 2015. URL http://dx.doi.org/10.1038/nature14236.

























