---
layout:     post
title:      Grila, Massively Parallel Methods for Deep Reinforcement Learning 
subtitle:   DQN from Deepmind
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - DeepMind
    - Reinforcement Learning
    - DQN

---

# Grila



## Massively Parallel Methods for Deep Reinforcement Learning



### Abstract

提出了第一个用于深度强化学习的大规模分布式架构。这个架构使用四个主要组件：

- parallel actors,  generate new behaviour
- parallel learners , trained from stored experience;
- distributed neural network,  represent the value function or behaviour policy
- distributed experience replay memory

分布式DQN 性能超过之前DQN, 并且训练时间也缩短了一个数量级. 



### Introduction

DQN之前是单机架构，致训练时间较长。例如，在GPU上训练DQN算法需要12-14天的时间玩一个Atari游戏. 

deep learning的主要优势之一是计算可以很容易地并行化(parallelized)。为了利用这种可伸缩性(scalability)，深度学习算法广泛使用了GPU等硬件。  
然而，最近的方法主要集中在大规模分布式架构(massively distributed architectures)上，这些架构可以并行地从更多的数据中学习，因此表现优于单机.   
DistBelief 框架 , **asynchronous stochastic gradient descent (ASGD)**

RL的独特属性, agent与env交互, 会影响 training data distribution.   
为了产生更多的数据，我们部署了多个并行运行的agent，与相同env的多个实例进行交互。  
每个**actor**可以存储自己过去的experience，有效地提供了一个分布式*experience replay memory*，与单机实现相比，它的容量大大增加。  另外，这些experience可以聚合到一个分布式数据库中.  
Exploration. 分布式 actors 还可以更有效地探索状态空间，因为每个agent的策略都略微不同.

distributed learners: read exp from replay memory, update value func follow RL.   
像DistBelief一样, applies ASGD updates to the parameters of the Q-network, 使得参数可以分布式

*Gorila* (**General Reinforcement Learning Architecture**), 一个分布式版本的 DQN.



### Related Work

之前的几种并行或分布式RL.   

其中相当一部分工作集中在分布式multi-agent 系统上 (Weiss, 1995; Lauer & Riedmiller, 2000).   
这种方法中,许多agent在一个共享环境中run，为实现一个共同的目标而合作。显然这种很难.  
计算也可以说是分布式的, 因为非中心化的control. 但这些算法关注的是有效的团队合作和突发的群体行为。

另一范例是并发强化学习 concurrent reinforcement learning(Silver et al., 2013). 一个agent可以与分布式环境并行交互，例如，在互联网上与多个用户 优化交互行为。

我们的目标与这些分布式和并发的RL有很大的不同：只想利用并行计算来更有效地解决single-agent问题。

MapReduce框架已被应用到标准的MDP求解方法中，如策略评估、策略迭代和值迭代，将大矩阵乘法所涉及的计算弄成分布式. 然而，这项工作仅仅局限于linear function approximation的batch方法.

最接近我们工作的是对经典的 Sarsa 算法在多个机器上的并行化。每个机器都有自己的agent和env实例(Grounds & Kudenko, 2008)，运行一个强化学习算法(线性Sarsa)。linear function approximator的参数变化通过点对点机制定期传递，特别关注那些变化最大的参数。相比之下，我们的架构允许客户机与服务器之间的通信，并在acting、learning和parameter updates之间进行分离；更进一步，我们通过分布式深度学习框架进行，利用更加 richer function approximators。 





### Background

#### DistBelief

DistBelief (Dean et al., 2012) 是一个分布式系统，通过两种并行机制，在海量数据上高效地训练大型神经网络。  
Model parallelism，不同的机器负责存储和训练模型的不同部分，用于高效地训练巨型模型, 远大于单机或GPU上的能支撑的模型。  
Data parallelism，即每个模型的多个副本在不同的数据子集上并行训练，这使得在海量数据集上的训练效率比单一进程更高。

下面简单讨论一下DistBelief架构的两个主要组成部分。

- central parameter server,  保存着模型的主copy。参数服务器的工作是将从replicas传入的梯度应用到模型上，并在请求时，将其最新的模型copy发送给replicas。参数服务器可以在许多机器上进行分片，不同的分片独立于其他分片应用梯度。
- model replicas : 每个replica都保留了一个被训练的模型的copy。如果模型太大，无法在一台机器上安装，那么这个copy可以在多台机器上分片。replica的工作是计算出一个mini-batch的梯度，将其发送到参数服务器，并定期向参数服务器查询模型的最新版本。replica在发送梯度和请求更新参数时是独立的，因此在任何给定的时间内可能不会同步到相同的参数。



####  Reinforcement Learning

#### Deep Q-Networks

![image-20200411055539046](/img/2020-04-01-DQN.assets/image-20200411055539046.png)

DQN Loss

$$
L_{i}\left(\theta_{i}\right)=\mathbb{E}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]
$$

gradient $g_{i}$ of the loss with respect to $\theta$

$$
g_{i}=\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{i}} Q(s, a ; \theta)
$$



### Distributed Architecture

![image-20200411060155880](/img/2020-04-01-DQN.assets/image-20200411060155880.png)

- Actors : 任何强化学习agent最终都必须选择在其环境中应用的action $a_t$。该process称为acting.   
  - Gorila 包含 $N_\text{act}$ 个不同的actor processes.    
  - each actor may visit different parts of the state space. The quantity of experience that is generated by the actors after $T$ time-steps is approximately $T N_{a c t}$ .   
  - Each actor contains a replica of the Q-network
  - parameters are synchronized periodically from the parameter server.

- Experience replay memory , 两种形式.   
  - *local* replay memory  每个bufferM大小, 总共$M N_{a c t}$     
  - *global* replay memory,   distributed database

- Learners  

  - Gorila contains $N_\text{learn}$ learner processes.
  - Each learner contains a replica of the Q-network
  - compute desired changes to the parameters of the Q-network. 
  - 对每个 learner update k, a minibatch of   (s, a, r, s′ ) is sampled from local or global replay memory $D$,  对minibatch使用DQN , 生成梯度向量$g_i$.  
  - 将梯度向量$g_i$ 传到 parameter server. 
  - 定期从parameter server更新 Q-network的参数.

- Parameter server

  - central parameter server to maintain a distributed representation of the Q-network $Q(s,a;\theta^+)$

  - parameter vector $\theta^+$ is split disjointly across $N_\text{param}$ different machines. 

  - Each machine  applying gradient updates to a subset of the parameters.

  - parameter server receives gradients from the learners,  applies these gradients to modify the parameter vector $\theta^+$, using an asynchronous stochastic gradient descent algorithm. **ASGD**

    

Gorila架构在RL agent 如何并行化方面相当灵活。可以通过并行acting将大量数据生成到global replay database中，然后由单个serial learner 处理这些数据。相反，可以让单个actor将数据生成数据到local replay memory中，然后让多个learners并行处理这些数据，尽可能高效地从这些经验中学习。然而，为了避免任何单个组件成为瓶颈，一般来说，Gorila架构允许任意数量的actors、learners和parameter servers同时生成数据、从这些数据中学习，并以可扩展和分布式的方式更新模型。

在接下来的实验中考虑的Gorila最简单的实现方式, 是bundled mode，其中actors、replay memory和learners之间有一对一的对应关系（$N_\text{act} = N_\text{learn}$）。每个bundle都有一个actor，一个local replay memory 存那个actor生成的exp，一个learner根据local replay memory中的exp样本来更新参数。bundles之间唯一的通信是通过参数来进行的：learners将自己的梯度传递给parameter server；而actors和learners中的Qnetwork会定期与parameter server同步。



#### Gorila DQN

一个实现DQN算法的Gorila架构的具体实例。  

![image-20200411062043147](/img/2020-04-01-DQN.assets/image-20200411062043147.png)

#### Stability

- 虽然DQN是为了确保稳定性而设计的，但使用集群进行训练会带来额外的挑战。  
  Gorila DQN的实现使用了额外的措施来保障稳定性，应对 比如节点消失、网络不稳定和 单个机器卡住的情况.  
- 一个safeguard是设定一个参数，规定local $θ$（梯度$g_i$使用$θ$计算）和parameter server中的$θ^+$之间的最大延迟。所有超过阈值的梯度都会被参数服务器丢弃。  

- 每个actor/learner 一直在计算一个 DQN loss绝对值的running 均值和标准差，并丢弃损失绝对值高于平均值加上几个标准差的梯度。

- 使用了AdaGrad更新规则.



### Experiments

#### Experimental Set Up

用了200多个进程.   总buffer是 1个亿

Gorila DQN used: $N_{\text {param}}=31$ and $N_{\text {learn}}=N_{\text {act}}=100 .$     
use the bundled mode   
Replay memory size $D=1$ million frames   
used $\epsilon$ -greedy  behaviour policy ,  $\epsilon$ annealed from 1 to 0.1 over the first one million global updates.   
Each learner syncs the parameters $\theta^{-}$ of its target network after every $60 \mathrm{K}$ parameter updates performed in the parameter server.



#### Evaluation

- *null op starts*
- *human starts*  更考验泛化

通过对Breakout、Pong和Seaquest等游戏进行非正式搜索来选择超参数值, 然后对所有的游戏进行固定。  
Gorila DQN使用相同的固定超参数设置和随机网络初始化，对每个游戏进行了5次训练。  
跟DQN一样, 在训练期间定期评估每个模型，并保留性能最好的网络参数进行最终评估。  
我们将这些最终评估结果在5次运行中取平均值，并将平均评估结果与DQN和人类专家得分进行比较。





### Results

首先比较了训练了长达6天的Gorila DQN代理和训练了12-14天的单GPU DQN代理.



下图  human starts 实验结果. 49场比赛中，使用人类启动时，Gorila DQN在49场比赛中的41场比赛中，Gorila DQN的表现优于单GPU DQN，其训练时间约为单GPU DQN的一半。在22场比赛中，Gorila DQN的得分是单GPU DQN的两倍，在11场比赛中，Gorila DQN的得分是单GPU DQN的5倍.   
null op starts 中Gorila DQN的表现也优于单GPU DQN

结果表明，并行训练在更少的训练时间内显著提高了性能。此外，与null op相比，human starts的结果更好，这表明Gorila DQN比单一GPU DQN更善于对潜在的未见过状态进行泛化。 

![image-20200411062311234](/img/2020-04-01-DQN.assets/image-20200411062311234.png)

下图进一步显示了  Gorila DQN较DQN的提升.   
Gorila DQN在25场比赛中的表现类似于或优于人类职业选手（人类得分的75%或以上），尽管从人类比赛中抽样的状态开始。

一个可能的原因是，Gorila DQN在使用100个并行agent后，看到的状态数量显著增加。



![image-20200411235850986](/img/2020-04-01-DQN.assets/image-20200411235850986.png)

接下来看一下Gorila DQN在训练过程中的性能是如何提高的。图5显示了Gorila DQN如何快速达到单GPU DQN的性能，以及快速达到Gorila DQN在人类启动下的最佳成绩。Gorila DQN在6小时内超过了19场比赛的最佳单GPU DQN成绩，12小时内超过了23场，24小时内超过了30场，36小时内超过了38场（红色曲线）。这在达到单进程DQN得分所需的**训练时间上大致减少了一个数量级**。在一些游戏中，Gorila DQN在两天之内就达到了最好的分数，但在大多数游戏中，随着训练时间的延长，其性能不断提高（蓝色曲线）。

![image-20200411062223233](/img/2020-04-01-DQN.assets/image-20200411062223233.png)

### Conclusion

- 本文中，我们介绍了第一个用于深度强化学习的大规模分布式架构。  
- Gorila架构 acts and learns in parallel,  using a distributed replay memory and distributed neural network.
- 将Gorila应用于最先进的DQN算法的 asynchronous异步版本。
- 之前不知道DQN的良好性能是否会随着额外的计算量的增加而继续扩展。
- 利用大规模并行计算，Gorila DQN在49款游戏中的41款游戏中的表现明显优于单GPU DQN, 在该领域取得了迄今为止最好的结果。
- Gorila朝着实现深度学习在RL中的承诺又迈进了一步：一个可扩展的架构，在增加计算量和内存的情况下，性能越来越好。






## Reference

Massively Parallel Methods for Deep Reinforcement Learning  https://arxiv.org/abs/1507.04296







