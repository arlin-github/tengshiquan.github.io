---
layout:     post
title:      UCL Course on RL,  Exploration and Exploitation
subtitle:   David Silver 的课程笔记8
date:       2019-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---



# Exploration and Exploitation

核心: UCB 



1. Introduction
2. Multi-Armed Bandits
3. Contextual Bandits
4. MDPs





#### Exploration vs. Exploitation Dilemma

- **Online** decision-making involves a fundamental choice: 
  - **Exploitation** Make the best decision given current information 

  - **Exploration** Gather more information

- The best long-term strategy may involve short-term sacrifices 
- Gather enough information to make the best overall decisions 



#### Principles

- Naive Exploration 
  - Add **noise** to greedy policy (e.g. **ε-greedy**)   加噪声

- Optimistic Initialisation 
  - Assume the best until proven otherwise 
- Optimism in the Face of Uncertainty 
  - Prefer actions with uncertain values 
- Probability Matching 
  - Select actions according to probability they are best 
- **Information State Search** 
  - Lookahead search incorporating value of information 



### Multi-Armed Bandits

#### The Multi-Armed Bandit

- MAB 问题描述

- multi-armed bandit is a tuple $⟨\mathcal A, \mathcal R⟩$
- $\mathcal A$ is a known set of **m actions** (or “arms”) 
- $\mathcal R^a(r) =\mathbb P[r \vert a]$ is an unknown probability distribution over **rewards** 
- At each step t the agent selects an action $a_t ∈ \mathcal A$ 
- The environment generates a reward $r_t ∼\mathcal  R^{a_t}$ 
- The goal is to maximise cumulative reward $\sum_{\tau=1}^t r_{\tau}$



#### Regret

- 重要概念,  遗憾值

- The **action-value** is the mean reward for action a,  $Q(a) = \mathbb E[r \vert a]$ 
- The optimal value $V^∗$ is  $V^∗ = Q(a^∗) = \max_{a \in \mathcal A} Q(a)$
- The **regret** is the **opportunity loss** for **one step** $l_t =\mathbb E[V^∗−Q(a_t)] $
- The **total regret** is the total opportunity loss  $L_t=\mathbb E\Big[\sum_{\tau=1}^t V^∗ − Q(a_\tau) \Big] $ 
- **Maximise cumulative reward ≡ minimise total regret**   最大化收益= 最小化总regret



#### Counting Regret

- The **count** $N_t(a)$ is expected number of selections for action $a$
- The **gap** $∆_a$ is the **difference** in value between action $a$ and optimal action $a^∗$, $∆_a = V ^∗ − Q(a)$ 
- 概念,  **gap** $∆_a$ 是 reget 的真值
- **Regret** is a **function** of **gaps** and the **counts**

$$
L_t=\mathbb E\Big[\sum_{τ=1}^t V^∗−Q(a_τ) \Big] 
\\ = \sum_{a\in \mathcal A} \mathbb E[N_t(a)](V^* - Q(a))
\\ = \sum_{a\in \mathcal A} \mathbb E[N_t(a)]\Delta_a
$$

- A good algorithm ensures small counts for large gaps
- Problem: gaps are not known!  **难点**,  对MAB, 无法知道最优的期望回报是多少, 所以也很难算regret



##### Linear or Sublinear Regret

<img src="/img/2019-03-02-Silver.assets/image-20200702003350178.png" alt="image-20200702003350178" style="zoom:50%;" />

- If an algorithm **forever** explores it will have **linear total regret**
- If an algorithm **never** explores it will have **linear total regret**
- Is it possible to achieve sublinear total regret?

- 上图,  就算是对数关系, 总的遗憾值也是随着时间而增加的, 只是增加的很缓慢, 除非是找到了最优.



#### Greedy Algorithm

- 纯贪婪, 直接就找一个次优就不动了

- We consider algorithms that estimate $\hat Q_t(a) ≈ Q(a)$
- Estimate the value of each action by Monte-Carlo evaluation

$$
\hat Q_t(a) = \frac{1}{N_t(a)} \sum_{t=1}^Tr_t \mathbf 1(a_t=a)
$$

- The **greedy** algorithm selects action with **highest** value

$$
a^* = \mathop{\text{argmax}}_{a \in \mathcal A} \hat Q_t(a)
$$

- Greedy can lock onto a **suboptimal** action forever    
- ⇒ Greedy has linear total regret



#### ε-Greedy Algorithm

- 一直不变的探索, 无法有效的利用已经积累的知识

- The **ε-greedy** algorithm continues to **explore forever**
  - With probability 1 − ε select $a = \mathop{\text{argmax}}_{a∈\mathcal A} \hat Q(a) $ 

  - With probability ε select a **random** action 

- Constant $ε$ **ensures minimum regret** ,  regret 有个下限

$$
I_{t} \geq \frac{\epsilon}{\mathcal{A}} \sum_{a \in \mathcal{A}} \Delta_{a}
$$

- ⇒ ε-greedy has **linear total regret**



##### ~~Optimistic Initialisation~~

- 乐观初始化, 上面初始化的时候, 每个Q都是0, 现在给每个a的Q一个比较大的数值
- 仍然是线性total regret ,  但效果比较好, 因为前期的探索性不错; 不像之前, 找到一个正的就立刻greedy了
- Simple and practical idea: initialise Q(a) to high value
- Update action value by incremental Monte-Carlo evaluation
- Starting with $N(a) > 0$

$$
\hat Q_t(a_t) = \hat Q_{t-1} + \frac{1}{N_t(a_t)}(r_t - \hat Q_{t-1})
$$

- Encourages systematic exploration early on
- But can still **lock** onto suboptimal action
- ⇒ greedy + optimistic initialisation has linear total regret
- ⇒ ε-greedy + optimistic initialisation has linear total regret



#### Decaying $ε_t-Greedy$ Algorithm

- 首先, 该算法是 现实中不可行的, 需要知道 regret的 真值  gap, 即V*

- 显然 $ε$ 是应该越来越小的, 但不同问题, $ε$ 不能只根据步数来减少, 可以使用 gap 来安排 探索的程度.
- $ε$ 应该是 离真值越近, 即reget 越接近于0的时候 , 探索越小; 然后随着时间推移, 探索越小

- 该策略$ε$ 按照步数以及  这个是非线性的total regret, 但是需要建模, 需要知道gap

- Pick a **decay** schedule for $ε_1, ε_2, ...$
- Consider the following schedule ,  考虑一种方案,  误差越小 , 探索越多

$$
c>0 
\\ d= \min_{a \vert \Delta_a>0}\Delta_i
\\ \epsilon_t = \min \Big\{1, \frac{c \vert \mathcal A \vert }{d^2t} \Big\}
$$

- **Decaying** $ε_t-greedy$ has **logarithmic asymptotic total regret**! 
- Unfortunately, schedule requires advance **knowledge of gaps** 
- Goal: find an algorithm with sublinear regret for any multi-armed bandit (without knowledge of $\mathcal R$) 



#### Lower Bound

- 下限是指, 最优算法的total regret的下限; 即最优的算法, 其total regret 也不可能小于上图的对数形式. 

- 算法的performance 究竟取决于什么呢. performance的上限就是 total regret的下限. 

- The **performance** of any algorithm is determined by similarity between optimal arm and other arms
- Hard problems have **similar**-looking arms with different **means** , 问题的难度取决于什么, 取决于怎么找到最好的arm,  如果好的与坏的的分布非常重叠, 有很多噪声, 就很难.
- This is described formally by the gap $∆_a$ and the similarity in distributions $KL(R^a \vert  \vert R^{a^∗})$  



##### Theorem (Lai and Robbins)

Asymptotic total regret is **at least** **logarithmic** in number of steps ;  total regret **至少** 在步数上是对数的

$$
\lim_{t \to \infty} L_t \ge \log t \sum_{a \vert \Delta_a>0} \frac{\Delta_a}{KL(R^a \vert  \vert R^{a^∗})}
$$

直观解释就是,  好的arm与坏的arm 离得越远, 并且分布又很像, 则这个问题就越难,  算法的total regret的下限就越高. 





#### Optimism in the Face of Uncertainty

对不确定的机台, 优先探索. 

![image-20200704190648453](/img/2019-03-02-Silver.assets/image-20200704190648453.png)

- Which action should we pick?  如何选择action
- The more uncertain we are about an action-value   对action value越不确定, 越需要探索
- The more important it is to explore that action 
- It could turn out to be the best action   因为那个可能是潜在的最优




![image-20200704191608604](/img/2019-03-02-Silver.assets/image-20200704191608604.png)

- After picking blue action  选蓝色以后
- We are less uncertain about the value  慢慢确定以后, 就换别的
- And more likely to pick another action
- Until we home in on best action   直到找到最优的



### Upper Confidence Bounds

- UCB的算法框架

- 如何衡量对一个机台的确定程度呢.   **UCB ,**置信区间上界

- Estimate an **upper confidence** $\hat U_t(a)$ for each action value , 估计一个action价值在一定可信度上的价值上限 
- Such that $Q(a) ≤ \hat Q_t(a) + \hat U_t(a)$ with high probability  ;  则该action的Q有较高的可能性不高于一个值

- This depends on the number of times N(a) has been selected
  - Small $N_t (a)$ ⇒ large $\hat U_t (a)$ (estimated value is uncertain)   选择的次数越少, 则越不确定, 
  - Large $N_t (a)$ ⇒ small $\hat U_t (a)$ (estimated value is accurate)

- Select action maximising **Upper Confidence Bound (UCB)**   按照最大的UCB来取

$$
a_t =\mathop{\text{argmax}}_{a\in\mathcal A}\hat Q_t(a)+\hat U_t(a)
$$

如果机台的reward分布是已知的，则UCB就可以直接求解。例如对于高斯分布95%的置信区间上界是均值与两倍标准差的和。其实这个时候, 直接取均值最大的就行, 无需UCB算法. 

对分布未知的情况, UCB如何得到呢？



##### Hoeffding’s Inequality

- Let $X_{1}, \ldots, X_{t}$ be i.i.d. random variables in $[0,1],$ and let $\bar{X}_{t}=\frac{1}{\tau} \sum_{\tau=1}^{t} X_{\tau}$ be the sample mean. Then 
  
  $$
  \mathbb{P}\left[\mathbb{E}[X]>\bar{X}_{t}+u\right] \leq e^{-2 t u^{2}}
  $$

- We will apply Hoeffding’s Inequality to rewards of the bandit

- conditioned on selecting action a   

- 随着选择次数的变多, UCB 估计的越来越准

$$
\mathbb P \Big[ Q(a)> \hat Q_t(a)+U_t(a) \Big ]≤e ^{−2N_t(a)U_t(a)^2}
$$



##### Calculating Upper Confidence Bounds

- Pick a probability p that true value exceeds UCB
- Now solve for $U_t(a)$

$$
e ^{−2N_t(a)U_t(a)^2} = p
\\ U_t(a) = \sqrt{ \frac{-\log p}{2N_t(a)} }
$$

- Reduce p as we observe more rewards
- Ensures we select optimal action as t → ∞



##### UCB1

$$
a_t = \mathop{\text{argmax}}_{a\in\mathcal A} Q_t(a) +  \sqrt{ \frac{-\log p}{2N_t(a)} }
$$

- The **UCB** algorithm achieves **logarithmic** asymptotic **total regret**

$$
\lim_{t \to \infty} L_t \ge 8 \log t \sum_{a \vert \Delta_a>0}  {\Delta_a}
$$



#### Example: UCB vs. ε-Greedy On 10-armed Bandit

![image-20200704194347372](/img/2019-03-02-Silver.assets/image-20200704194347372.png)

事实表明，Ɛ-greedy算法如果参数调整得当，可以表现的很好，反之则可能是灾难。UCB在没有掌握任何信息的前提下也能做得很好。





### Bayesian Bandits

- So far we have made no assumptions about the reward distribution $\mathcal R$  之前都没有利用rewar的分布
  - Except bounds on rewards
- **Bayesian bandits** exploit prior knowledge of rewards, $p [\mathcal R]$ ; 贝叶斯方法, 利用历史信息构建reward分布
- They compute **posterior distribution** of rewards $p [\mathcal R  \vert  h_t ]$   后验概率
  - where $h_t =a_1,r_1,...,a_{t−1},r_{t−1}$ is the history 
- Use **posterior** to **guide** exploration    两个方法
  - Upper confidence bounds (**Bayesian UCB**) 
  - **Probability matching** (Thompson sampling) 

- Better performance if prior knowledge is accurate   性能更好



#### Bayesian UCB Example: Independent Gaussians

![image-20200704195322014](/img/2019-03-02-Silver.assets/image-20200704195322014.png)



- Assume reward distribution is Gaussian, $ \mathcal R_a(r) = \mathcal N(r_t;μ_a,σ_a^2)$ ;  假设是高斯分布
- Compute Gaussian posterior over $μ_a$ and $σ_a^2$ (by Bayes law)  通过贝叶斯来计算后验概率的参数.

$$
p[􏰀μ_a,σ_a^2  \vert h_t]􏰁∝p[􏰀μ_a,σ_a^2]􏰁 􏰊\prod_{t \vert a_t=a} \mathcal N (r_t;μ_a,σ_a^2)
$$

- Pick action that maximises standard deviation of Q(a)  ,  取下式中最大的

$$
a_t = \mathop{\text{argmax}}_{a\in\mathcal A}\mu_a +  c\sigma_a /\sqrt{  N(a)}
$$

#### Probability Matching

- **Probability matching** selects action a according to probability that a is the optimal action 选概率上最可能是最优的那个action , 所以要评估所有action的最优概率

$$
\pi(a \vert h_t) = \mathbb P[Q(a)>Q(a'), \forall a' \neq a \vert h_t]
$$

- Probability matching is optimistic in the face of uncertainty 
  - Uncertain actions have higher probability of being max 

- Can be difficult to compute analytically from posterior 



##### Thompson Sampling

- **Thompson sampling** implements probability matching

$$
\pi(a \vert h_t) = \mathbb P[Q(a)>Q(a'), \forall a' \neq a \vert h_t]
\\ = \mathbb E_{\mathcal R \vert h_t} \Big[\mathbf 1(a = \mathop{\text{argmax}}_{a\in\mathcal A} Q_t(a)) \Big]
$$

- Use Bayes law to compute posterior distribution $p [\mathcal R  \vert  h_t ] $  
- **Sample** a reward distribution $\mathcal R$ from posterior  
- Compute action-value function $Q(a) = \mathbb E [\mathcal R_a]$  
- Select action maximising value on sample,$a_t =\mathop{\text{argmax}}_{a\in\mathcal A} Q_t(a)$ 
- Thompson sampling achieves Lai and Robbins lower bound! 



## Information State Search

#### Value of Information  信息的价值

- Exploration is useful because it gains information  探索带来信息
- Can we quantify the value of information?   量化该信息
  - How much reward a decision-maker would be prepared to pay in order to have that information, prior to making a decision 
  - **Long-term** reward after getting information - **immediate** reward 

-  **Information gain** is **higher** in **uncertain** situations 
-  Therefore it makes sense to **explore** **uncertain** situations **more**
-  If we know value of information, we can **trade-off** exploration and exploitation **optimally**



对于一个2臂赌博机。假如个体当前对行为a1的价值有一个较为准确的估计，比如是100镑，这意味着执行行为a1可以得到的即时奖励的期望，个体虽然对于行为a2的价值也有一个估计，假如说是70镑，但这个数字非常不准确，因为个体仅执行了非常少次的行为a2。那么获取“较为准确的行为a2的价值”这条信息的价值有多少呢？这取决于很多因素，其中之一就是个体有没有足够多的行为次数来获取累计奖励，假如个体只有非常有限的行为次数，那么个体可能会倾向于**保守**的选择a1而不去通过探索行为a2而得到较为准确的行为a2的价值。因为探索本身会带来一定几率的后悔。相反如果个体有数千次的行为次数，那么得到一个更准确的行为a2的价值就显得非常必要了，因为即使通过一定次数的探索a2，后悔值也是可控的。而一旦得到的行为a2的价值超过a1，则将影响后续数千次的行为的选择。





### Information State Space

- 建模为MDP , 将信息作为MDP的状态构建对其价值的估计
- 继续使用2臂赌博机的例子来解释信息状态的内容。在这个例子中，一个信息状态对应于分别采取了行为a1和a2的次数，例如S0<5,3>可以表示一个信息状态，它意味着个体在这个状态时已经对行为a1执行了5次，a2执行了3次。随后个体又执行了一个行为a1，那么状态转移至S1<6,3>。事实上这样的信息状态构建得到的MDP是一个规模非常庞大的MDP，解决它需要使用之前将结果的函数近似。



- We have viewed bandits as one-step decision-making problems
- Can also view as sequential decision-making problems
- At each step there is an information state  $\tilde s$
  - $\tilde s$ is a statistic of the history, $\tilde s  = f (h_t)$ 
  - summarising all information accumulated so far
- Each action a causes a transition to a new information state $ \tilde s′$ (by adding information), with probability $\tilde{ \mathcal P}^a_{\tilde s,\tilde s'}$
- This defines MDP $\mathcal M$ in augmented information state space

$$
\tilde {\mathcal M} = ⟨ \tilde {\mathcal S} , \mathcal A , \tilde {\mathcal P} , \mathcal R , γ ⟩
$$

#### Example: Bernoulli Bandits

再举一个例子来更加清楚的解释信息状态是如何转换以及转换概率是如何计算的

- 考虑一个即时奖励服从伯努利分布的赌博机, 0-1分布

- Consider a Bernoulli bandit, such that $ \mathcal R^a =\mathcal B(μ_a)$ 
- e.g. Win or lose a game with probability $μ_a$ 
- Want to find which arm has the highest $μ_a$  
- The information state is $  \tilde s = ⟨α, β⟩$ 
  - $α_a$ counts the pulls of arm a where reward was 0 
  - $β_a$ counts the pulls of arm a where reward was 1 





#### Solving Information State Space Bandits

- We now have an infinite MDP over information states   MDP, 可以用RL来解决
- This **MDP** can be solved by **reinforcement learning** 
- Model-free reinforcement learning 
  - e.g. Q-learning (Duff, 1994)
- Bayesian model-based reinforcement learning 
  - e.g. Gittins indices (Gittins, 1979)
  - This approach is known as Bayes-adaptive RL
  - Finds Bayes-optimal exploration/exploitation trade-off with respect to prior distribution 



##### Bayes-Adaptive Bernoulli Bandits

<img src="/img/2019-03-02-Silver.assets/image-20200704201544994.png" alt="image-20200704201544994" style="zoom:50%;" />

- Start with $Beta(α_a, β_a)$ prior over reward function $\mathcal R_a$ 
- Each time a is selected, update posterior for $\mathcal R_a$ 
  - $Beta(α_a + 1, β_a)$ if r = 0 
  - $Beta(α_a, β_a + 1)$ if r = 1 
- This defines transition function $\tilde P $ for yh the Bayes-adaptive MDP 
- Information state ⟨α, β⟩ corresponds to reward model Beta(α,β) 
- Each state transition corresponds to a Bayesian model update 



![image-20200704201649158](/img/2019-03-02-Silver.assets/image-20200704201649158.png)



##### Gittins Indices for Bernoulli Bandits 

- **Bayes-adaptive MDP** can be solved by **dynamic programming** 
- The solution is known as the **Gittins index**
- Exact solution to Bayes-adaptive MDP is typically intractable 
  - Information state space is too large
- Recent idea: apply simulation-based search (Guez et al. 2012) 
  - Forward search in information state space
  - Using simulations from current information state 



## Contextual Bandits

如果把状态考虑到探索方法中，相关的算法要略作修改。举了一个在线广告展示的例子。目标在于如何向不同的用户展示一组广告，吸引用户点击。这里的状态代表这不同用户的喜好信息。行为指的是用户点击其中某一条广告。

- A contextual bandit is a tuple $⟨\mathcal A, \color{red}{\mathcal S} , \mathcal R⟩$ 
- $\mathcal A$ is a known set of actions (or “arms”) 
- $\mathcal S = \mathbb P[s]$ is an unknown distribution over states (or “contexts”) 
- $\mathcal R^a_s(r) = \mathbb P[r \vert s,a]$ is an unknown probability distribution over rewards 
- At each step t 
  - Environment generates state $ s_t ∼ S$ 
  - Agent selects action $a_t ∈ \mathcal  A$  
  - Environment generates reward $r_T ∼ \mathcal R^{a_t}_{s_t}$  
- Goal is to maximise cumulative reward $\sum_{\tau=1}^t r_\tau$



下面视频都没怎么讲

#### Linear UCB

##### Linear Regression

- Action-value function is expected reward for state s and action a

$$
Q(s,a) = \mathbb E[r \vert s,a]
$$

- Estimate value function with a linear function approximator

$$
Q_θ(s, a) = \phi(s, a)^⊤θ ≈ Q(s, a)
$$

- Estimate parameters by least squares regression

$$
\begin{aligned}
A_{t} &=\sum_{\tau=1}^{t} \phi\left(s_{\tau}, a_{\tau}\right) \phi\left(s_{\tau}, a_{\tau}\right)^{\top} \\
b_{t} &=\sum_{\tau=1}^{t} \phi\left(s_{\tau}, a_{\tau}\right) r_{\tau} \\
\theta_{t} &=A_{t}^{-1} b_{t}
\end{aligned}
$$

##### Linear Upper Confidence Bounds

- Least squares regression estimates the mean action-value $Q_θ(s, a)$ 

- But it can also estimate the variance of the action-value $σ_θ^2 ( s , a )$ 

- i.e. the uncertainty due to parameter estimation error
- Add on a bonus for uncertainty, $U_θ(s,a) = cσ$ 
- i.e. define UCB to be c standard deviations above the mean 



##### Geometric Interpretation

- Define confidence ellipsoid $\varepsilon_t$ around parameters $θ_t$
- Such that $\varepsilon_t$ includes true parameters $θ^∗$ with high probability
- Use this ellipsoid to estimate the uncertainty of action values
- Pick parameters within ellipsoid that maximise action value

$$
\mathop{\text{argmax}}_{θ∈\varepsilon} Q_θ(s,a)
$$



##### Calculating Linear Upper Confidence Bounds

- For least squares regression, parameter covariance is $A^{−1}$  
- Action-value is linear in features, $Q_θ(s, a) = \phi(s, a)^⊤θ$  
- So action-value variance is quadratic, $σ_θ^2(s, a) = \phi(s, a)^⊤A^{−1}\phi(s, a)$  
- Upper confidence bound is $Q_θ(s, a) + c\sqrt{\phi(s, a)^⊤A^{−1}\phi(s, a)}$ 
- Select action maximising upper confidence bound 

$$
a_t = \mathop{\text{argmax}}_{a∈\mathcal A} Q_θ(s, a) + c\sqrt{\phi(s, a)^⊤A^{−1}\phi(s, a)}
$$


### MDPs

##### Exploration/Exploitation Principles to MDPs

The same principles for exploration/exploitation apply to MDPs 

- Naive Exploration 

- Optimistic Initialisation

- Optimism in the Face of Uncertainty 

- Probability Matching

- Information State Search 

  

#### Optimistic Initialisation

##### Optimistic Initialisation: Model-Free RL

- Initialise action-value function Q(s,a) to rmax $\frac{r_{max}}{1−γ}$  
- Run favourite model-free RL algorithm 
  - Monte-Carlo control 
  - Sarsa 
  - Q-learning ... 

- Encourages systematic exploration of states and actions 



##### Optimistic Initialisation: Model-Based RL

- Construct an **optimistic** model of the MDP 
- Initialise transitions to **go to heaven** 
  - (i.e. transition to terminal state with rmax reward) 
- Solve optimistic MDP by favourite planning algorithm 
  - policy iteration 
  - value iteration 
  - tree search
  - ... 
- Encourages systematic exploration of states and actions 
- e.g. RMax algorithm (Brafman and Tennenholtz) 



#### Optimism in the Face of Uncertainty

##### Upper Confidence Bounds: Model-Free RL

- Maximise UCB on action-value function $Q^π(s,a)$ 	

$$
a_t = \mathop{\text{argmax}}_{a∈\mathcal A}(s_t,a)+U(s_t,a)
$$

  - Estimate uncertainty in policy evaluation (easy) 
  - Ignores uncertainty from policy improvement

- Maximise UCB on optimal action-value function $Q^∗(s,a)$ 

$$
a_t = \mathop{\text{argmax}}_{a∈\mathcal A}(s_t,a)+U_1(s_t,a)+U_2(s_t,a)
$$

  - Estimate uncertainty in policy evaluation (easy) 
  - plus uncertainty from policy improvement (hard) 



##### Bayesian Model-Based RL

- Maintain posterior distribution over MDP models 
- Estimate both transitions and rewards, $p[\mathcal P,\mathcal R  \vert  h_t]$ 
  - whereht =s1,a1,r2,...,st isthehistory 
- Use posterior to guide exploration 
  - Upper confidence bounds (Bayesian UCB) 
  - Probability matching (Thompson sampling) 



#### Probability Matching

##### Thompson Sampling: Model-Based RL

- Thompson sampling implements probability matching

$$
\begin{aligned}
\pi\left(s, a \mid h_{t}\right) &=\mathbb{P}\left[Q^{*}(s, a)>Q^{*}\left(s, a^{\prime}\right), \forall a^{\prime} \neq a \mid h_{t}\right] \\
&=\mathbb{E}_{\mathcal{P}, \mathcal{R} \mid h_{t}}\left[1\left(a=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q^{*}(s, a)\right)\right]
\end{aligned}
$$

- Use Bayes law to compute posterior distribution $p[\mathcal P,\mathcal R \vert h_t] $ 
- Sample an MDP P, R from posterior 
- Solve MDP using favourite planning algorithm to get $Q^∗(s,a)$  
- Select optimal action for sample MDP, $a_t = \mathop{\text{argmax}}_{a∈\mathcal A}Q^*(s,a)$



#### Information State Search

##### Information State Search in MDPs

- MDPs can be augmented to include information state 
- Now the augmented state is $⟨s, \tilde s⟩$  
  - where s is original state within MDP
  - and $\tilde s$ is a statistic of the history (accumulated information) 

- Each action a causes a transition
  - to a new state s′ with probability $\mathcal P^a_{s,s'}$  
  - to a new information state  $\tilde s′$  
- Defines MDP $\tilde {\mathcal M}$  in augmented information state space 

$$
\tilde {\mathcal M} = ⟨ \tilde {\mathcal S} , \mathcal A , \tilde {\mathcal P} , \mathcal R , γ ⟩
$$

##### Bayes Adaptive MDPs

- Posterior distribution over MDP model is an information state $ \tilde s_t =\mathbb P[\mathcal P,\mathcal R \vert h_t]$  
- Augmented MDP over $⟨s, \tilde s⟩$ is called Bayes-adaptive MDP 
- Solve this MDP to find optimal exploration/exploitation  trade-off (with respect to prior)
- However, Bayes-adaptive MDP is typically enormous 
- Simulation-based search has proven effective (Guez et al.) 



### Conclusion

- Have covered several principles for exploration/exploitation 
  - Naive methods such as ε-greedy 
  - Optimistic initialisation 
  - Upper confidence bounds 
  - Probability matching 
  - Information state search 

- Each principle was developed in bandit setting 
- But same principles also apply to MDP setting 








