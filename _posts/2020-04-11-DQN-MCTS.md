---
layout:     post
title:      Offline MCTS planning
subtitle:   Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning
date:       2020-04-09 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - MCTS
    - Reinforcement Learning
    - DQN

---

 

# Offline MCTS planning

思路 :  MCTS(UCT) agents gen training data => CNN学习策略,  real-time play.  

训练很慢,  只是证明了 时间足够, UCT 表现的会比 DQN好.   
然后这里的 UCT 没有使用value网络引导,  纯的.  同时要解决一个 domain shift问题 , 学的一般的CNN 会去选择action, 然后新轨迹又送去 UCT 计算结果.



CS285 课程提过本论文,  核心是 **imitation learning from MCTS**  
只用UCT, 则完全是一个**统计方法**,  是概率上的平均 , 去近似穷举; 慢  
UCT 可以被描述为 MCTS 的一个特例：**UCT = MCTS + UCB**  
UCT 本身没有啥泛化能力, 完全靠硬算, 虽然也有EE的能力.

MCTS 是 plan算法, 没有显式的policy, 是 open-loop



DAgger , 然后把3 human打标改为 MCTS

1. $$\operatorname{train} \pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ from human data $$\mathcal{D}=\left\{\mathbf{o}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{o}_{N}, \mathbf{a}_{N}\right\}$$
2. run $$\pi_{\theta}\left(\mathbf{u}_{t} \vert  \mathbf{o}_{t}\right)$$ to get dataset $$\mathcal{D}_{\pi}=\left\{\mathbf{o}_{1}, \ldots, \mathbf{o}_{M}\right\}$$
3. Choose actions for states in $\mathcal{D}_{\pi}$ using MCTS  ,   注意这里, MCTS没有利用NN,完全是利用统计
4. Aggregate: $\mathcal{D} \leftarrow \mathcal{D} \cup \mathcal{D}_{\pi}$



## Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning

2014



### Abstract

- DQN = model-free RL + DNN
- **Planning-based** approaches achieve far higher scores than the best **model-free** approaches. 
- exploit information not available to human players 不知道model
- slow for real-time play 

**central idea**  use the slow **planning-based** agents to provide training data for a deep-learning architecture capable of real-time play. 



### Introduction

RL problems:  **closed-loop action (or policy) selection** +  **high-dimensional perception** (SL problems)

RL 在 policy selection 有进展 ,   DL 在 perception 问题上有进展.  RL 和 DL 目标都是 通用性 generality, 去除 domain-specific engineering.   
所以 RL+ DL = rich perception and policy-selection = general methods

本文方法,  RL+ DL ,  use slow, off-line Monte Carlo tree search planning methods to generate training data for a deep-learned classifier capable of state-of-the-art real-time play.

**MCTS => data => DNN => play**

**Arcade Learning Environment (ALE)**

### background

**RL** , 以及 更广义的 **decision-theoretic 决策理论 planning 规划** 有一整套方法来解决 **selecting/learning**  policies, 包含: **value function approximation**, **policy search**, and **Monte-Carlo Tree Search (MCTS)** . 

**perception问题**一直没有解决, 两个组成部分: 

1. the sensors at any time step do not capture all the information in the history of observations, leading to **partial observability** .  **PO**  部分观测
2. **high-dimensional observations**   高维



解决perception问题的方法

- **model** available时，一个方法可以避开perception,  避免建立一个explicit policy，通过**repeated incremental planning** via **MCTS** methods such as **UCT** . 
- model 不可用, 或 explicit representation of the policy is required ,  常用方法: use **expert**-developed task-specific **features** of a short history of observations in combination with **function approximation** methods and some trial-and-error on the part of the application developer. (on small enough problems this can be augmented with some automated feature selection methods)  
  **给出 policy : 人工特征 + FA ; 小规模问题, 直接 DL**

消除 engineered features 的依赖性，motivate  RL + DL . 

DL : powerful technique for **learning feature representations** from data.   
features are learned in a **compositional hierarchy**. 组成层次结构    
low-level features are learned to **encode low-level statistical dependencies** (e.g., “edges” in images)  
higher-level features **encode higher-order dependencies of the lower-level features** (e.g., “object parts”)    
特别是，对于具有很强的空间或时间依赖性的数据，卷积神经网络已经被证明可以学习不变的高阶特征.  



### Existing Work on Atari Games and a Performance Gap

atari游戏 对人类来说,是  **Partially-Observable Markov Decision Processes (POMDPs)**.  
**Stochasticity** is in these games limited to the choice of the initial state  (include a random seed). So even though the state transitions are deterministic, the transitions from history of observations and actions to next observation can be stochastic (because of the stochastic initial hidden state). 



##### Model-Free RL Agents for Atari Games

- 下面讨论, 不访问游戏中的状态state, 从而将游戏作为POMDP求解.   即用O代替s
- 原则上，人们可以学习一个状态表示 state representation，并利用 frame-observation 和 action trajectories 推断出相关的MDP模型，但这些游戏非常复杂，不可行.
- **partial observability** is dealt with by **hand-engineering features** of short histories of frames observed so far and **model-free RL** methods are used to learn good **policies as a function of those feature representations**.  解决PO:  人工特征+ FA +model-free 方法
- 例如, SARSA with several different hand-engineered features sets
- The contingency awareness approach improve  SARSA  ,  人工特征 + screen  representation, 人工特征与学习的特征结合起来.
- sketch-based approach
- HyperNEAT-GGP
- DQN



##### Planning Agents for Atari Games based on UCT

UCT算法.  这里将(s,d) 视为 state-depth pair

- 这些方法, 从模拟器中直接获取游戏的状态，因此面对的是 deterministic MDP（初始状态的随机选择除外）

- incrementally plan the action to take in the current state using **UCT** 

- UCT has three parameters : **number of trajectories N, maximum-depth, exploration parameter**

- larger the trajectory & depth, UCT  slower but better.

- UCT uses **emulator as model** to simulate trajectories as follows:  

  - generating the $k^{th}$ trajectory,  current node  at depth $d$ ,  current state $s$  ,  这里d可以看成t
  - computes a score for each possible action $a$ in state-depth pair $(s, d)$ as the sum of **two terms**
    - **exploitation term** ,  MonteCarlo average of the discounted **sum of rewards** obtained from experiences with state-depth pair $(s, d)$ in the previous $k-1$ trajectories
    - **exploration term** , $\sqrt{\log (n(s, d)) / n(s, a, d)}$  , where $n(s, a, d)$ and $n(s, d)$ are the number of experiences of action $a$ with state-depth pair $(s, d)$ and with state-depth pair $(s, d)$ respectively in the previous $k-1$ trajectories.
  - UCT selects action to simulate to extend the trajectory **greedily with score** 
  - Once the input-parameter number of trajectories are generated each to maximum depth, UCT returns the exploitation term for each action at the root node (which is the current state it is planning an action for) as its estimate of the utility of taking that action in the current state of the game.   遍历完成, 算score

- UCT has the **nice theoretical property** that the **number of simulation steps** (number of trajectories × maximum-depth) needed to ensure any **bound on the loss** of following the UCT-based policy is **independent of the size of the state space** .  很好的理论属性, 所需的仿真步数独立于状态空间的大小

- this result expresses the fact that the use of UCT avoids the perception problem, but at the cost of requiring substantial computation for every time step of action selection because it never builds an explicit policy.  **使用UCT避免了感知问题，但代价是行动选择的每一个时间步骤都需要大量的计算，因为它从来没有建立一个显式策略。** 因为是统计, 所以就不用学特征

  

##### Performance Gap & our Opportunity

本文的机会来自于以下观察,      
model-free RL agents for Atari games are fast (indeed faster than real-time, e.g., the CNN-based approach takes $10^{−4}$ seconds to select an action)  而 UCT-based planning agents **慢**几个数量级 (much slower than real-time, e.g., take seconds to select an action).  
另一方面, UCT-based planning agents 的**performance**要好得多.

所以, 提前用UCT算好policy, 生成data用CNN学习.   **缺陷: 明显没有利用value net 来引导MCTS.**

goal: DL advantage of not needing hand crafted features   
online real-time play ability of model-free RL agents by exploiting data generated by UCT-planning agents.



### Methods for Combining UCT-based RL with DL

#### Baseline UCT agent that provides training data

- 该agent**不需要训练**
- 需要指定它的**两个参数**，即**轨迹的数量和最大深度**
- 下面提出的新agent都将使用这种UCT-agent的数据来训练CNN-based 策略，因此我们提出的新agent的性能会比UCT-agent的性能差也是合理的。
- 实验中，我们将这两个参数设置得足够大，以确保它们优于已公布的DQN分数，但又不至于大到无法忍受
- 300作为最大深度，10000作为所有游戏的轨迹数
- 从第5节的结果中可以看出，这使得UCTagent在所有游戏中的表现都明显优于DQN
- UCT agent 不能 real-time play, 用UCTagent玩一个游戏仅仅800次（这样做是为了收集下面的训练数据），在最近的多核计算机上每个游戏都需要几天的时间。



#### Our three methods and their corresponding agents

显然, NN的agent的都有一定的泛化能力

##### Method 1: UCTtoRegression (for UCT to CNN via Regression) 

- key idea : use the action values computed by the UCT-agent to train a regression-based CNN. 
- for each game.  playing the game 800 times from start to finish using the UCT agent above. 
- Build a dataset (table) from these runs as follows. Map the **last four frames** of each state along each trajectory into the **action-values of all the actions** as computed by UCT. 

##### Method 2: UCTtoClassification (for UCT to CNN via Classification)

- key idea : use the action choice computed by the UCT-agent (selected greedily from action-values) to train a classifier- based CNN. 
- Collect 800 UCT-agent runs as above. 
- These runs yield a table in which the rows correspond to the **last four frames** at each state along each trajectory and the **single column is the choice of action** that is best according to the UCT-agent at that state of the trajectory. 
- This training data is used to train the CNN via **multinomial classification**. 



##### domain shift

上述两个方法的一个潜在问题是，训练数据的输入分布是由UCT-agent产生的，而在测试过程中，UCTtoRegression和UCTtoClassification agent的表现与UCT-agent不同，可能分布有很大的差异。  
因此，以某种方式使输入的分布偏向于这些agent可能遇到的输入，这可能是可取的；这一观察结果激励了我们的下一个方法。



##### Method 3: UCTtoClassification-Interleaved (for UCT to CNN via Classification-Interleaved)

- key idea : focus UCT planning on that part of the state space experienced by the (**partially trained**) CNN player.
- 该方法通过将训练和数据收集交织在一起完成
- Collect 200 UCT-agent runs
- data from these runs is used to train the CNN via multinomial classification
- The trained CNN is then used to decide action choices in collecting a further 200 runs (though 5% of the time a random action is chosen to ensure some exploration).  CNN来action
- At each state of the game along each trajectory, UCT is asked to compute its choice of action and the original data set is augmented with the last four frames for each state as the rows and the column as UCT’s action choice.  新轨迹的所有状态,都由UCT来计算其Q值,  这里的UCT还是纯的MCTS.
- This 400 trajectory dataset’s input distribution is now potentially different from that of the UCT-agent.  新的input的分布与UCT-agent的不同
- This dataset is used to train the CNN again via multinomial classification.
- This interleaved procedure is repeated until there are a total of 800 runs worth of data in the dataset for the final round of training of the CNN. 这个交错过程被重复进行，直到数据集中总共有800个运行的数据，用于CNN的最后一轮训练。  **DAgger**



为了将我们的经验评估重点放在我们三个新agents的non-DL部分的贡献上，我们复用了与DQN工作中的CNN架构  
In order to focus our empirical evaluation on the contribution of the non-DL part of our three new agents, we reused exactly the same convolutional neural network architecture as used in the DQN work .





### Details of Data Preprocessing and CNN Architecture

##### Preprocessing (identical to DQN to the best of our understanding).

##### CNN Architecture

![image-20200416112723906](/img/2020-04-01-DQN.assets/image-20200416112723906.png)

multi-regression-based agent (UCTtoRegression), the output layer is a fully connected **linear** layer with a single output for each valid action.  
classification-based agents (UCTtoClassification, UCTtoClassification-Interleaved), a **softmax** (instead of linear) function is applied to the final output layer.



### Experimental Results

$$
\begin{array}{llllllll}
\hline \text { Agent } & \text {B.Rider} & \text {Breakout} & \text {Enduro} & \text {Pong} & Q^{* \text {bert}} & \text {Seaquest} & \text {S.Invaders} \\
\hline \text { DQN } & 4092 & 168 & 470 & 20 & 1952 & 1705 & 581 \\
\text {-best} & 5184 & 225 & 661 & 21 & 4500 & 1740 & 1075 \\
\hline \text { UCC } & 5342(20) & 175(5.63) & 558(14) & 19(0.3) & 11574(44) & 2273(23) & 672(5.3) \\
\text {-best} & 10514 & 351 & 942 & 21 & 29725 & 5100 & 1200 \\
\text {-greedy} & 5676 & 269 & 692 & 21 & 19890 & 2760 & 680 \\
\hline \text { UCC-I } & 5388(4.6) & 215(6.69) & 601(11) & 19(0.14) & 13189(35.3) & 2701(6.09) & 670(4.24) \\
\text {-best} & 10732 & 413 & 1026 & 21 & 29900 & 6100 & 910 \\
\text {-greedy} & 5702 & 380 & 741 & 21 & 20025 & 2995 & 692 \\
\hline \text { UCR } & 2405(12) & 143(6.7) & 566(10.2) & 19(0.3) & 12755(40.7) & 1024(13.8) & 441(8.1) \\
\hline
\end{array}
$$

DQN-best行是DQN在每一个游戏中的所有尝试中的最佳性能

used 5% exploration in our agents to match what the DQN agent does, but it is not clear why one should consider random action selection during testing. 去掉探索. greedy, 在test的时候,性能更好.

**分类的比回归的性能好**

UCT-baseline的性能 , the performance of our non-realtime UCT agent (again, with 5% exploration)  
对于UCT-agent的性能, 可以保证找到找过DQN的. 
$$
\begin{array}{llllllll}
\hline \text { Agent } & \text {B.Rider} & \text {Breakout} & \text {Enduro} & \text {Pong} & Q^{*} \text {bert} & \text {Seaquest} & \text {S.Invaders} \\
\hline \text { UCT } & 7233 & 406 & 788 & 21 & 18850 & 3257 & 2354 \\
\hline
\end{array}
$$


随着训练数据量的增加，交织方法在绝对值和百分比上都有了更大的提高。



#### Learned Features from Convolutional Layers

apply the “**optimal stimuli 最优刺激**” method to visualize the features CNN. 

在图2中， CNN的 four first-layer filters. 具体来说，每个滤波器覆盖了4帧8*8像素，可以看成是一个时空模板 spatio-temporal template，可以捕捉到特定的模式及其时空的变化。我们还展示了一个例子的屏幕截图和在图像的灰度版本中（CNN模型的实际输入），可视化地显示了filters被激活的位置。可视化表明，第一层filters捕捉到了 "object-part” patterns及其temporal movements. 

![image-20200416113238788](/img/2020-04-01-DQN.assets/image-20200416113238788.png)



图3通过最优刺激法直观地展示了四个第二层特征，每一行对应一个滤波器。我们可以看到，第二层特征捕捉到了更大的空间模式（往往覆盖了单个物体的大小之外），同时编码了物体之间的相互作用，比如两个敌人一起移动，潜艇沿着一个方向移动等。总的来说，这些定性结果表明，CNN学习到的相关模式对游戏中有用。

<img src="/img/2020-04-01-DQN.assets/image-20200416113325262.png" alt="image-20200416113325262" style="zoom:50%;" />





#### Visualization of Learned Policy

![image-20200416114927563](/img/2020-04-01-DQN.assets/image-20200416114927563.png)

图4显示了UCT-toClassification学习到的消灭附近敌人的策略。表明了UCTtoClassification agent的处理延迟奖励的能力，因为当它在最终摧毁一个敌人时，它学会了在获得任何奖励之前采取一连串没有奖励的动作，然后再获得奖励。

图4还显示了UCTtoClassification agent的策略中的一个缺陷，即它不会有目的地采取拯救一个潜水员的行动（拯救一个潜水员可以获得大量奖励）。例如，在t=69的时候，即使在潜艇下面和右边有两个潜水员，策略也没有将潜艇向下移动。这种现象经常被观察到。造成这种缺陷的原因是，抓到6个潜水员并将其带到水面上需要大量的时间步数（将较少的潜水员带到水面上并不能获得奖励）；这**需要比UCT的规划深度planning depth更长的时间**。因此，是UCT没有刻意去拯救潜水员，因此通过UCT收集到的训练数据反映了这一缺陷，而这一缺陷在UCTtoClassification（和UCTtoClassification-Interleaved）agent的发挥中也是如此。



### Conclusion

- UCT-based planning agents至少在两个方面对雅达利游戏是不现实的。首先，为了玩游戏，它们需要访问游戏的状态，而这是人类玩家无法获得的，其次，它们的速度比实时游戏慢了几个数量级。另一方面，通过放慢游戏的速度，让UCT玩的足够慢，会让它们在试玩过的游戏中获得最高分。

- 事实上，通过允许UCT在move之间有越来越多的时间（从而允许更多的轨迹数量和更大的最大深度）, 可以推测，可以把分数提高越来越多。  显然的

- 发现了UCT-based planning agents的性能与最佳realtime玩家DQN的性能之间存在差距，并开发了新的agent来部分填补这一差距。

- 最后，我们假设产生训练数据的UCT agent与我们的learned agents 的输入分布之间的差异会降低性能。我们为解决这个问题而开发的UCTtoClassification-Interleaved确实比UCTtoClassification表现得更好，间接地证实了我们的假设，解决了根本问题。





## Reference

Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning  http://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning


















