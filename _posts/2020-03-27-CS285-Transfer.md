---
layout:     post
title:      CS 285. Transfer and Multi-Task Learning
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

 

##  Transfer and Multi-Task Learning

引入先验知识, 已解决问题,  transfer. 

### Lecture

1. **Transfer** from prior tasks to learn new tasks more quickly
2. Forward transfer: train on source task in such a way as to do better on target task
3. Randomization of source tasks
4. **Multi-task transfer**
5. Contextual policies  上下文策略, 根据上下文决定目标以及策略
6. Modular policies  模块化的策略, 有共性, 可以复用

##### Goals:

- Understand (at a high level) the landscape of research work on transfer learning





#### What’s the problem?

##### Montezuma’s revenge

<img src="/img/CS285.assets/image-20200327110446879.png" alt="image-20200327110446879" style="zoom: 33%;" />

- Getting key = reward
- Opening door = reward
- Getting killed by skull = bad



- We know what to do because we **understand** what these sprites mean!
- Key: we know it opens doors!
- Ladders: we know we can climb them!
- Skull: we don’t know what it does, but we know it can’t be good!
- **Prior understanding of problem structure can help us solve complex tasks quickly!**



#### Can RL use the same prior knowledge as us?

- If we’ve solved prior tasks, we might acquire useful knowledge for solving a new task
- How is the knowledge **stored**?
  - **Q-function**: tells us which actions or states are good
  - Policy: tells us which actions are potentially useful  
    - some actions are never useful!
  - Models: what are the laws of physics that govern the world?
  - Features/hidden states: provide us with a good representation 
    - Don’t underestimate this!  
    - 这个还是比较重要的, 就是说在其他游戏里面如果学到了, 骷髅是不好的,钥匙是好的, 那么换一个游戏, 主要玩法不一样, 但骷髅可能仍然是不好的, 所以这些已经学到的feature还是比较有用, 有普适性的.
    - 想法, 人类可以在一个新游戏中, 发现设定是完全不一样的,比如某游戏中骷髅是好的,并且迅速适应, 知道这是个特例啥的. 但数学上就很难表示.如果用一个数值, 或者高斯分布或者神经网络来表示一个东西好不好, 那在其他特征都变化的情况下, 却是慢慢拟合过去的. 神经网络说不定可以改变网络靠近输出层的weight.   是不是弄成很多feature的小网络, 然后后面再用神经网络拼起来,能方便弄成异或啥的这种结构.



#### Aside: the representation bottleneck

<img src="/img/CS285.assets/image-20200327111005924.png" alt="image-20200327111005924" style="zoom:33%;" />

这里的实验就是去掉了最后一层, 但前面的cnn层啥的参数没变, 然后再补上随机初始化的最后一层,再重新训练, 会发现恢复很快. 

- To decouple reinforcement learning from representation learning, we decapitate an agent by destroying its policy and value outputs and then re-train end-to-end.
- The representation remains and the policy is swiftly recovered. 
- The gap between initial optimization and recovery shows a representation learning bottleneck.

slide adapted from E. Schelhamer, “Loss is its own reward”



### Transfer learning terminology

**transfer learning:** using experience from **one set of tasks** for faster learning and better performance on a **new task**

**in RL,** **task** **=**  **MDP**  

**source domain** ==> **target domain**

- **“shot”:** number of attempts in the target domain  
- **0-shot:** just run a policy trained in the source domain  必须两个足够相似.
- **1-shot:** try the task once
- **few shot:** try the task a few times



#### How can we frame transfer learning problems?

**No single solution! Survey of various recent research papers**

##### “Forward” transfer: train on one task, transfer to a new task

1.   Just try it and hope for the best
2.   **Finetune** on the new task
3.   Randomize source domain ,  在source上选择

##### Multi-task transfer: train on many tasks, transfer to a new task

1.   Generate highly randomized source domains,  而且可以在多个task上并发学习
2.   Model-based reinforcement learning
3.   **Model distillation**
4.   Contextual policies
5.   Modular policy networks

##### Multi-task meta-learning: learn to learn from many tasks,  transfer learning strategy

1.   RNN-based **meta-learning**
2.   Gradient-based meta-learning



#### How can we frame transfer learning problems?

Try it and hope for the best  完全看运气

**Policies trained for one set of circumstances might just work in a new domain, but no promises or guarantees**  

<img src="/img/CS285.assets/image-20200327145427961.png" alt="image-20200327145427961" style="zoom:33%;" />



#### Finetuning

The most popular transfer learning method in (supervised) deep learning!

<img src="/img/CS285.assets/image-20200327150109840.png" alt="image-20200327150109840" style="zoom:33%;" />

source task  要非常 broad , diverse  ,  是后面的base



##### Challenges with finetuning in RL

- RL tasks are generally much less diverse
  - **Features** are less general , 不像计算机视觉的特征有很大的普适性
  - **Policies** & value functions become overly specialized 策略也没普适性
- Optimal policies in fully observed MDPs are **deterministic**  对PG, 增加回报期望就是减少随机性, 降低 action distribution 的entropy , 会变的更加deterministic.
  - **Loss of exploration** at convergence
  - **Low-entropy** policies adapt very slowly to new settings

所以希望, source domain 策略的熵是尽可能大的. 



#### Finetuning with maximum-entropy policies

How can we increase diversity and entropy?

$$
\pi(\mathbf{a} | \mathbf{s})=\exp \left(Q_{\phi}(\mathbf{s}, \mathbf{a})-V(\mathbf{s})\right) \text { optimizes } \sum_{t} E_{\pi\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)}\left[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]+E_{\pi\left(\mathbf{s}_{t}\right)}\left[\mathcal{H}\left(\pi\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)\right]
$$

Act **as randomly as possible** while collecting high rewards!  核心逻辑



##### Example: pre-training for robustness

<img src="/img/CS285.assets/image-20200327162725743.png" alt="image-20200327162725743" style="zoom:33%;" />

Learning to solve a task **in all possible ways** provides for more robust transfer!



##### Example: pre-training for diversity

Haarnoja*, Tang*, et al. “Reinforcement Learning with Deep Energy-Based Policies”

<img src="/img/CS285.assets/image-20200327181725527.png" alt="image-20200327181725527" style="zoom:33%;" />

soft Q 比 DDPG要更有探索性. 



#### Finetuning in RL: suggested readings

- Finetuning via MaxEnt RL: Haarnoja*, Tang*, et al. (2017). **Reinforcement Learning with Deep Energy-Based Policies.**
- Finetuning from transferred visual features (via VAE): Higgins et al. **DARLA: improving zero-shot transfer in reinforcement learning.** 2017.
- Pretraining with hierarchical RL methods:
  - Andreas et al. **Modular multitask reinforcement learning with policy sketches.** 2017. 
  - Florensa et al. **Stochastic neural networks for hierarchical reinforcement learning.** 2017.





#### What if we can manipulate the source domain?

- So far: source domain (e.g., empty room) and target domain (e.g., corridor) are **fixed**

- What if we can **design** the source domain, and we have a **difficult** target domain?
  - Often the case for **simulation to real world transfer**  仿真到部署也算
- Same idea: the more **diversity** we see at training time, the better we will transfer!



##### EPOpt: randomizing physical parameters

Rajeswaran et al., “EPOpt: Learning robust neural network policies...”

<img src="/img/CS285.assets/image-20200327163232476.png" alt="image-20200327163232476" style="zoom:33%;" />

train一个腿跳, 只训练粗腿 , 无法在test上work, 练了各种腿型以后, 就可以work. 



##### Preparing for the unknown: explicit system ID

Yu et al., “Preparing for the Unknown: Learning a Universal Policy with Online System Identification”

<img src="/img/CS285.assets/image-20200327170928787.png" alt="image-20200327170928787" style="zoom:33%;" />



##### Another example

Xue Bin Peng et al., “Sim-to-Real Transfer of Robotic Control with Dynamics Randomization”

<img src="/img/CS285.assets/image-20200327171002162.png" alt="image-20200327171002162" style="zoom:33%;" />



##### CAD2RL: randomization for real-world control

Sadeghi et al., “CAD2RL: Real Single-Image Flight without a Single Real Image”

<img src="/img/CS285.assets/image-20200327191937009.png" alt="image-20200327191937009" style="zoom:33%;" />



##### Randomization for manipulation

Tobin, Fong, Ray, Schneider, Zaremba, Abbeel

<img src="/img/CS285.assets/image-20200327192000116.png" alt="image-20200327192000116" style="zoom:33%;" />

James, Davison, Johns



<img src="/img/CS285.assets/image-20200327192021885.png" alt="image-20200327192021885" style="zoom:33%;" />



#### What if we can peek at the target domain?

- So far: pure 0-shot transfer: learn in source domain so that we can succeed in **unknown** target domain
- Not possible in general: if we know nothing about the target domain, the best we can do is be as robust as possible
- What if we saw a few images of the target domain?



##### Better transfer through domain adaptation

Tzeng*, Devin*, et al., “Adapting Visuomotor Representations with Weak Pairwise Constraints”



<img src="/img/CS285.assets/image-20200327192233641.png" alt="image-20200327192233641" style="zoom:33%;" />

<img src="/img/CS285.assets/image-20200327192301455.png" alt="image-20200327192301455" style="zoom:50%;" />



##### Domain adaptation at the pixel level

Bousmalis et al., “Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping”

can we *learn* to turn synthetic images into *realistic* ones?



<img src="/img/CS285.assets/image-20200327192418208.png" alt="image-20200327192418208" style="zoom:50%;" />



### Forward transfer summary

- Pretraining and finetuning
  - Standard finetuning with RL is hard
  - **Maximum entropy** formulation can help

- How can we modify the source domain for transfer?
  - **Randomization** can help a lot: the more **diverse** the better!
- How can we use modest amounts of target domain data?
  - **Domain adaptation**: make the network unable to distinguish observations from the two domains
  - ...or modify the source domain observations to look like target domain
  - Only provides **invariance** – assumes all differences are functionally irrelevant; this is not always enough!



#### Source domain randomization and domain adaptation suggested readings

- Rajeswaran, et al. (2017). **EPOpt: Learning Robust Neural Network Policies Using Model Ensembles.**

- Yu et al. (2017). **Preparing for the Unknown: Learning a Universal Policy with Online System Identification.**

- Sadeghi & Levine. (2017). **CAD2RL: Real Single Image Flight without a Single Real Image.**

- Tobin et al. (2017). **Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World.**

- James et al. (2017). **Transferring End-to-End Visuomotor Control from Simulation to Real World for a Multi-Stage Task.**

- Tzeng*, Devin*, et al. (2016). **Adapting Deep Visuomotor Representations with Weak Pairwise Constraints.** 
- Bousmalis et al. (2017). **Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping.**





### Multiple source domains

- So far: more diversity = better transfer
- Need to design this diversity
  - E.g., simulation to real world transfer: randomize the simulation
- What if we transfer from multiple *different* tasks?
  - In a sense, closer to what people do: build on a lifetime of experience
  - Substantially harder: past tasks don’t directly tell us how to solve the task in the target domain!



#### Model-based reinforcement learning

- If the past tasks are all different, what do they have in common?
- Idea 1: the laws of physics
  - Same robot doing different chores
  - Same car driving to different destinations
  - Trying to accomplish different things in the same open-ended video game
- Simple version: train model on past tasks, and then use it to solve new tasks
- More complex version: adapt or finetune the model to new task
  - Easier than finetuning the policy if task is very different but physics are mostly the same



##### Example: 1-shot learning with model priors

Fu et al., “One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation...”

<img src="/img/CS285.assets/image-20200327193612482.png" alt="image-20200327193612482" style="zoom:50%;" />



#### Can we solve multiple tasks at once?

- Sometimes learning a model is very hard
- Can we learn a multi-task policy that can *simultaneously* perform many tasks?

- This policy might then be easier to finetune to new tasks
- Idea 1: construct a **joint MDP**

<img src="/img/CS285.assets/image-20200327193711693.png" alt="image-20200327193711693" style="zoom:50%;" />

- Idea 2: train in each MDP separately, and then **combine the policies**



#### Actor-mimic and policy distillation

<img src="/img/CS285.assets/image-20200327193739798.png" alt="image-20200327193739798" style="zoom: 33%;" />

#### Distillation for Multi-Task Transfer

<img src="/img/CS285.assets/image-20200327223210687.png" alt="image-20200327223210687" style="zoom:50%;" />

$$
\mathcal{L}=\sum_{\mathbf{a}} \pi_{E_{i}}(\mathbf{a} | \mathbf{s}) \log \pi_{A M N}(\mathbf{a} | \mathbf{s})
$$

(just supervised learning/distillation)

analogous to guided policy search, but for transfer learning



##### Distillation Transfer Results

Parisotto et al. “Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning”

<img src="/img/CS285.assets/image-20200327225225262.png" alt="image-20200327225225262" style="zoom:50%;" />

橘色是Qlearning ,  蓝色是 AMN 算法,拥有其他游戏的知识. 

可以看到一些游戏, 有些提高. 一些效果更差.



#### How does the model know what to do?

- So far: what to do is apparent from the input (e.g., which game is being played)
- What if the policy can do *multiple* things in the *same* environment?

<img src="/img/CS285.assets/image-20200327225515546.png" alt="image-20200327225515546" style="zoom:33%;" />

比如要让机器人知道或者学习到什么时候该做什么任务. 

meta-learning就是去学习这个



#### Contextual policies

- standard policy: $\pi_\theta(\mathbf a \vert \mathbf s)$
- Contextual policy: $\pi_\theta(\mathbf a \vert \mathbf s, \omega)$

formally, simply defines augmented state space: $\quad \tilde{\mathbf{s}}=\left[\begin{array}{c}\mathbf{s} \\ \omega\end{array}\right] \quad \tilde{\mathcal{S}}=\mathcal{S} \times \Omega$



#### Architectures for multi-task transfer

- So far: single neural network for all tasks (in the end)
- What if tasks have **some shared parts and some distinct parts**?
  - Example: two cars, one with camera and one with LIDAR, driving in two different cities
  - Example: ten different robots trying to do ten different tasks
- Can we design architectures with *reusable components*?



#### Modular networks in deep learning

Andreas et al. “Neural Module Networks.” 2015

监督学习

<img src="/img/CS285.assets/image-20200327230458389.png" alt="image-20200327230458389" style="zoom: 33%;" />



<img src="/img/CS285.assets/image-20200327230527512.png" alt="image-20200327230527512" style="zoom:33%;" />

#### Modular networks in RL

Devin*, Gupta*, et al. “Learning Modular Neural Network Policies...”

训练一些组合, 然后在未见过的组合上看看是不是work, 加上finetuning还不错

<img src="/img/CS285.assets/image-20200327230805434.png" alt="image-20200327230805434" style="zoom:33%;" />



<img src="/img/CS285.assets/image-20200327230831424.png" alt="image-20200327230831424" style="zoom:33%;" />



### Multi-task learning summary

- **More tasks = more diversity = better transfer**
- Often easier to obtain multiple different but relevant prior tasks
- Model-based RL: **transfer the physics, not the behavior**
- **Distillation**: **combine multiple policies into one**, for concurrent multi- task learning (accelerate all tasks through sharing)
- **Contextual policies: policies that are told *what* to do**
- Architectures for multi-task learning: **modular networks**



### Suggested readings

- Fu etal. (2016). **One-Shot Learning of Manipulation Skills with Online Dynamics Adaptation and Neural Network Priors.**

- Rusu et al. (2016). **Policy Distillation.** 
- Parisotto et al. (2016). **Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning.**
- Devin*, Gupta*, et al. (2017). **Learning Modular Neural Network Policies for Multi-Task and Multi-Robot Transfer.**






