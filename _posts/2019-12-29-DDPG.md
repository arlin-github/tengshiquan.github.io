---
layout:     post
title:      CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING
subtitle:   Note on "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"
date:       2019-12-29 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-cartpole.jpg"
catalog: true
tags:
    - Reinforcement Learning
    - AI
    - PG
    - DDPG
---



# Note on "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"

论文笔记

[^_^_]: 想法:   discounter $\gamma$  是否可以弄成可以学习的, 或者是一种什么样的结构. 这个参数作为超参感觉没发挥多大的作用, 在小规模问题上, 对于时间因素的影响, 只用这一个参数可能也过于简化了. 

 

### Abstract

作者将Deep Q-Learing 扩展到 continuous action space,   提出一个 actor-critic model-free 的DPG算法. 

DDPG 使用同一个网络,相同的参数, 解决了超过20个问题. 

最后介绍了, 很多任务可以通过 end-to-end来学习, 即直接输入原始像素. 



### INTRODUCTION

AI要解决的主要目标, 输入是 unprocessed, high-dimensional, sensory input的复杂任务.   
deep learning for sensory processing (Krizhevsky et al., 2012) + RL = DQN.  DQN用DNN来estimate action-value. 

然而, DQN 不适用于高纬离散空间.   maximizes the action-value function, which in the continuous valued case requires an iterative optimization process at every step.

要把DQN用到连续空间, 一个简单做法是, 将动作空间离散化, 不过这样做有很多限制, 特别是带来了"curse of dimensionality", 自由度越高, 带来的维度成指数级增长. 如此高的动作空间很难高效地探索.  并且, 离散化, 会不必要地丢弃 动作空间的结构信息, 说不定这些信息对解决问题是至关重要的. 

在这里, 作者提出一个 model-free, off-policy, actor-critic算法, 在高纬度连续动作空间, 使用NN近似函数来学习策略. 以DPG算法为基础. 然后, 下面会提到, 原始的 带NN近似函数的 actor-critic 方法 在处理有挑战性的问题时, 是不稳定的. 

在DQN之前, 广泛认为, 大型非线性函数拟合是不稳定, 训练很困难的. DQN 采用了两个策略来解决不稳定的问题: 

1. replay buffer , 用来尽量减少 samples 之间的相关性. 
2. 使用 一个target Q network , 在TD buckup 的时候, 给出恒定的 targets. 

除了上面2条, 在这篇论文里面, 还采用 batch normalization 技术. 

DDPG的一个关键特征是其简洁性, 只需一个 actor-critic 结构 和 一个学习算法. 



### Background

下面,假定 env都是 fully-boserved. 即$s_{t}=x_{t}$ , env 可能是 stochastic的. 

Return: sum of discounted future reward: $R_{t}=\sum_{i=t}^{T} \gamma^{(i-t)} r\left(s_{i}, a_{i}\right)$

目标函数,   $J=\mathbb E_{r_{i}, s_{i} \sim E, a_{i} \sim \pi} [R_{1} ]$

discounted state visitation distribution: $\rho^{\pi}$

action-value: 	$Q^{\pi}\left(s_{t}, a_{t}\right)=\mathbb E_{r_{i \geq t}, s_{i>t} \sim E, a_{i>t} \sim \pi}\left[R_{t} \vert s_{t}, a_{t}\right]$

Bellman equation: 	$Q^{\pi}\left(s_{t}, a_{t}\right)=\mathbb E_{r_{t}, s_{t+1} \sim E}\left[r\left(s_{t}, a_{t}\right)+\gamma \mathbb E_{a_{t+1} \sim \pi}\left[Q^{\pi}\left(s_{t+1}, a_{t+1}\right)\right]\right]$

如果target policy是 deterministic, 即 $\mu: \mathcal{S} \leftarrow \mathcal{A}$, 可以去掉上式里第二个期望: 

$$
Q^{\mu}\left(s_{t}, a_{t}\right)=\mathbb{E}_{r_{t}, s_{t+1} \sim E}\left[r\left(s_{t}, a_{t}\right)+\gamma Q^{\mu}\left(s_{t+1}, \mu\left(s_{t+1}\right)\right)\right] \tag{3}
$$

该期望只取决于env. 这意味着通过 off-policy来学习$Q^\mu$ 是可能的.

Q-learning 是很常用的 off-policy 算法, 通过一个 greedy 策略: $\mu(s)=\arg \max _{a} Q(s, a)$ 

考虑以$\theta^Q$为参数的近似函数, 最小化下面的Loss:

$$
L\left(\theta^{Q}\right)=\mathbb{E}_{s_{t} \sim \rho^{\beta}, a_{t} \sim \beta, r_{t} \sim E}\left[\left(Q\left(s_{t}, a_{t} \vert \theta^{Q}\right)-y_{t}\right)^{2}\right] \tag{4}
$$

where $y_{t}=r\left(s_{t}, a_{t}\right)+\gamma Q\left(s_{t+1}, \mu\left(s_{t+1}\right) \vert \theta^{Q}\right)$ , $y_t$取决于$\theta^Q$,  通常被忽略.



过去 使用大型非线性近似函数来学习 value或者action value 通常被避免, 因为理论上无法保证性能, 实践中, 学习通常是不稳定的.  直到最近DQN. 采用了 *replay buffer* 以及 *target network* 来计算 $y_t$ . 



### ALGORITHM

直接应用Q-learning到continuous action spaces是不可行的,因为在continuous空间, greedy policy需要在每个timestep计算$a_t$的最大值, 当使用大型无约束近似函数以及在非平凡动作空间(with large, unconstrained function approximators and nontrivial action spaces)是不可行的. 

这里使用以DPG为基础的actor-critic方法. 

DPG使用 $\mu\left(s \vert \theta^{\mu}\right)$ 当作actor函数, 直接把当前state确定地映射为一个特定action.  critic函数像Q-learning一样使用Bellman公式来学习$Q(s,a)$.  actor参数通过下面方式更新, 对开始状态分布J的return求期望, 并应用链式法则对actor参数求导, The actor is updated by following the applying the chain rule to the expected return from the start distribution $J$ with respect to the actor parameters:

$$
\begin{aligned}
\nabla_{\theta^{\mu}} J & \approx \mathbb{E}_{s_{t} \sim \rho^{\beta}}\left[ \nabla_{\theta^{\mu}} Q (s, a | \theta^{Q} )    |_{ s=s_{t}, a=\mu (s_{t} | \theta^{\mu} )  }  \right] \\
&=\mathbb{E}_{s_{t} \sim \rho^{\beta}}\left[    
 \nabla_{a} Q (s, a | \theta^{Q} ) |_{s=s_{t}, a=\mu (s_{t} )} 
  \nabla_{\theta_{\mu}} \mu (s | \theta^{\mu} ) |_{s = s_t}
\right]
\end{aligned}
$$

像Q-learning 一样, 使用非线性近似函数意味着收敛性无法保证. 但这种类型的近似函数在巨大状态空间问题上是必须的. NFQCA使用与DPG一样的更新方式, 不过使用了neural network作为近似函数, 并使用 batch learning保证稳定性, 很难搞定大型网络.  minibatch版本的NFQCA在每次更新的时候, 不会重置策略（这是扩展到大型网络所必需的），与原始DPG等效。 

这里, 提出一个DPG的修改版: DDPG, 受到DQN的启发,  DQN使用NN近似函数在大型state和action空间里 online 学习.

一个问题, NN optimization算法假定 samples都是独立同分布的 iid. 显然通过顺序与env交互得到的sample不符合. 另外, 为了充分利用硬件优化, 必须通过mini-batch学习, 而不是online.  

像DQN一样, 使用 replay buffer 来解决这个问题. replay buffer is a finite sized cache $\mathcal R$, FIFO queue, store $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$.   each timestep, sample a minibatch uniformly from cache. 因为 DDPG off-policy, buffer 可以很大, 可以抽出不相干的samples(uncorrelated transitions),有利于学习. 

Q-learning unstable. 因为更新中的 network Q 同时被用于计算target value, Q update易于 divergence. 像DQN里的target network一样, modified for actor-critic, 使用 "soft" target updates, 而不是直接复制 weights. AC网络都复制一份. create a copy of the actor and critic networks, $Q^{\prime}\left(s, a \vert \theta^{Q}\right)$ and $\mu^{\prime}\left(s \vert \theta^{\mu^{\prime}}\right)$ respectively, used for calculating the targes.  The weights updated by slowly track the learned networks: $\theta^{\prime} \leftarrow \tau \theta+(1=$ $\tau) \theta^{\prime}$ with $\tau \ll 1 .$ 限制学得慢, 保稳定.  这样learning action-value 更像监督学习, 有很多解决方案. 

found that having both a target $\mu^{\prime}$ and $Q^{\prime}$ was required to have stable targets $y_{i}$ in order to consistently train the critic without divergence. 两个网络要想不发散, 需要有一致的y.  This may slow learning, since the target network delays the propagation of value estimations. However, in practice we found this was greatly outweighed by the stability of learning. 可能减慢学习,但稳定性的提升克服了这点. 

[^_^]:没有学习中,那种突然醍醐灌顶, 顿悟

当从 low dimensional feature vector observations中学习时, 不同维度有不同的物理单位, 也有不同的取值范围, 导致无法高效学习和调参. 一个办法是手动归一化.  作者使用**batch normalization**, normalizes each dimension across the samples in a minibatch to have <u>unit mean and variance</u>.  maintains a running average of the mean and variance to use for normalization during testing. In deep networks, it is used to <u>minimize covariance shift</u> during training, by ensuring that each layer receives <u>whitened input</u>. 对state input 和两个网络的所有层都使用了batch normalization, 这样对不同任务都可以高效学习而无需手工去做normalization.

A major challenge of learning in continuous action spaces is **exploration**.探索性是个问题. off-policy可以隔离探索与学习算法.  constructed an exploration policy $\mu^{\prime}$ by adding noise sampled from a noise process $\mathcal{N}$ to our actor policy . 给确定性策略增加噪声从而变成探索性的策略.

$$
\mu^{\prime}\left(s_{t}\right)=\mu\left(s_{t} | \theta_{t}^{\mu}\right)+\mathcal{N}
$$

选择适合env的noise process $\mathcal{N}$.  为了高效探索惯性物理问题, 这里使用了Ornstein-Uhlenbeck process.



#### RESULTS

DDPG algorithm

> Randomly initialize critic network $Q\left(s, a \vert \theta^{Q}\right)$ and actor $\mu\left(s \vert \theta^{\mu}\right)$ with weights $\theta^{Q}$ and $\theta^{\mu}$ 
>
> Initialize target network $Q^{\prime}$ and $\mu^{\prime}$ with weights $\theta^{Q^{\prime}} \leftarrow \theta^{Q}, \theta^{\mu^{\prime}} \leftarrow \theta^{\mu}$
>
> Initialize replay buffer $R$ 
>
> **for** episode $=1, M$ **do**
>
> ​	    Initialize a random process $\mathcal{N}$ for action exploration 
>
> ​	    Receive initial observation state $s_{1}$ 
>
> ​			**for** $\mathrm{t}=1, \mathrm{T}$ **do**
>
> ​					Select action $a_{t}=\mu\left(s_{t} \vert \theta^{\mu}\right)+\mathcal{N}_{t}$ according to the current policy and exploration noise 
>
> ​					Execute action $a_{t}$ and observe reward $r_{t}$ and observe new state $s_{t+1}$ 
>
> ​					Store transition $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$ in $R$
>
> ​					Sample a random minibatch of $N$ transitions $\left(s_{i}, a_{i}, r_{i}, s_{i+1}\right)$ from $R$
>
> ​					Set $y_{i}=r_{i}+\gamma Q^{\prime}\left(s_{i+1}, \mu^{\prime}\left(s_{i+1} \vert \theta^{\mu^{\prime}}\right) \vert \theta^{Q^{\prime}}\right)$
>
> ​					Update critic by minimizing the loss: $L=\frac{1}{N} \sum_{i}\left(y_{i}-Q\left(s_{i}, a_{i} \vert \theta^{Q}\right)\right)^{2}$
>
> ​					Update the actor policy using the sampled policy gradient:
> 
> $$
>  \nabla_{\theta^{\mu}} J \approx \frac{1}{N} \sum_{i} \nabla_{a} Q\left(s, a | \theta^{Q}\right) |_{s=s_{i}, a=\mu\left(s_{i}\right)} \nabla_{\theta^{\mu}} \mu\left(s | \theta^{\mu}\right) |_{s_{i}}
> $$
> 
> ​					Update the target networks:
> 
> $$
> \begin{array}{l}
> {\theta^{Q^{\prime}} \leftarrow \tau \theta^{Q}+(1-\tau) \theta^{Q^{\prime}}} \\
> {\theta^{\mu^{\prime}} \leftarrow \tau \theta^{\mu}+(1-\tau) \theta^{\mu^{\prime}}}
> \end{array}
> $$
> 
> ​			**end for**
>
> **end for**



所有任务都使用了低纬度的state信息以及高纬度的env画面两种方式.  使用了一些DQN论文里的一些数据处理方式, 为了是高纬数据重现,使用了action repeats. 还有一些降维处理.

图标展示了各种组合的表现, 其中 target network 非常关键. 有些简单任务, 从图像输入跟state描述信息输入 学习速度一样快. 可能是action repeats 让问题变的简单. CNN也可能发挥了作用, provide an easily separable representation of state space, which is straightforward for the higher layers to learn on quickly.

![image-20200102170319142](/img/2019-12-29-DDPG.assets/image-20200102170319142.png)

Q-learning倾向于高估value. DDPG要更准.   DDPG效果也比DQN要效率高. 

DDPG缺点, 需要大量episode来找到solution.



#### EXPERIMENT DETAILS

Adam , learning rate of $10^{-4}$ and $10^{-3}$ for the actor and critic . 

For $Q$  : $L_{2}$ weight decay of $10^{-2}$ ,  discount factor  $\gamma=0.99 .$ 

For soft target updates $\tau=0.001 .$ 

NN: rectified non-linearity for all hidden layers. 
final output layer of the actor:  tanh, to bound the actions. 

low-dimensional networks : 2 hidden layers with 400 and 300 units respectively $(\approx 130,000$ parameters). Actions were not included until the 2 nd hidden layer of $Q .$ 

High dimensional:   3 convolutional layers (no pooling) with 32 filters at each layer.  two fully connected layers with 200 units $(\approx 430,000$ parameters). 

final layer weights and biases of both the actor and critic were initialized from a uniform distribution $\left[-3 \times 10^{-3}, 3 \times 10^{-3}\right]$ and $\left[3 \times 10^{-4}, 3 \times 10^{-4}\right]$ for the low dimensional and pixel cases respectively. This was to ensure the initial outputs for the policy and value estimates were near zero. 为了一开始估值接近0. other layers were initialized from uniform distributions $\left[-\frac{1}{\sqrt{f}}, \frac{1}{\sqrt{f}}\right]$ where $f$ is the fan-in of the layer. fan-in是隐层的输入节点数目. The actions were not included until the fully-connected layers. 

minibatch sizes  64 for the low dimensional problems ,   16 on pixels.

replay buffer size : $10^{6} .$

Ornstein-Uhlenbeck process  with $\theta=0.15$ and $\sigma=0.2 .$ 




## References

[Lillicrap, Timothy P., et al. "Continuous control with deep reinforcement learning." arXiv preprint arXiv:1509.02971 (2015).](https://arxiv.org/pdf/1509.02971)


