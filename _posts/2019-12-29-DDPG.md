---
layout:     post
title:      CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING
subtitle:   Note on "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"
date:       2019-12-29 12:00:00
author:     "tengshiquan"
header-img: ""
catalog: true
tags:
    - ai
    - pg
    - ddpg
---



# Note on "CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING"

论文笔记

[^_^_]: 想法:   discounter $\gamma$  是否可以弄成可以学习的, 或者是一种什么样的结构. 这个参数作为超参挺难设置, 对于时间因素的影响, 只用这一个参数也过于简化了. 

 

### Abstract

将Deep Q-Learing 扩展到 continuous action space.  使用的是一个 actor-critic model-free 的DPG算法. 

DDPG 使用同一个网络,相同的参数, 解决了超过20个问题. 

最后介绍了, 很多任务可以通过 end-to-end来学习, 即直接输入原始像素. 



### INTRODUCTION

AI要解决的主要目标, 输入是 unpro- cessed, high-dimensional, sensory 的复杂问题.   
deep learning for sensory processing + RL = DQN.  
DQN用DNN来estimate action-value. 

然而, DQN 不适用于 高纬离散空间. 

一个简单做法是, 将动作空间离散化, 不过这样做有很多限制, 特别是带来了"curse of dimensionality", 自由度越高, 带来的维度成指数级增长. 如此高的动作空间很难高效地探索.  并且, 离散化, 会不必要地丢弃 动作空间的结构信息, 说不定这些信息对解决问题是至关重要的. 

原始的 带NN近似函数的 actor-critic DPG算法 在处理有挑战性的问题时, 是不稳定的. 

这里, actor-critic DPG + DQN.  

在DQN之前, 广泛认为, 大型非线性函数拟合是不稳定, 训练很困难的. DQN 采用了两个策略来解决不稳定的问题: 

1. replay buffer , 用来尽量减少 samples 之间的相关性. 
2. 使用 一个target Q network , 在TD buckup 的时候, 给出恒定的 targets. 

除了上面2条, 在这篇论文里面, 还采用 batch normalization 技术. 

DDPG的一个关键特征是其简洁性, 只需一个 actor-critic 结构 和 一个学习算法. 



### Background



### ALGORITHM

































## References

[Lillicrap, Timothy P., et al. "Continuous control with deep reinforcement learning." arXiv preprint arXiv:1509.02971 (2015).](https%3A//arxiv.org/pdf/1509.02971)

