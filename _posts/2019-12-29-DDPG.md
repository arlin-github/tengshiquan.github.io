---
layout:     post
title:      DDPG, Continuous control with deep reinforcement learning 
subtitle:   continuous Q-learning with actor network for approximate maximization
date:       2019-12-29 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-cartpole.jpg"
catalog: true
tags:
    - Reinforcement Learning
    - AI
    - Deepmind
    - PG
    - DDPG
---



# DDPG,  Continuous control with deep reinforcement learning

论文笔记.  Deepmind , Timothy P. Lillicrap

[^_^_]: 想法:   discounter $\gamma$  是否可以弄成可以学习的, 或者是一种什么样的结构. 这个参数作为超参感觉没发挥多大的作用, 在小规模问题上, 对于时间因素的影响, 只用这一个参数可能也过于简化了. 



ppt from [Shan-Hung Wu](https://www.youtube.com/channel/UCBtDVX6tpl1SCPsT5SQEnYQ)

这个表达比较简洁: 

<img src="/img/2019-12-29-DDPG.assets/image-20200402160659485.png" alt="image-20200402160659485" style="zoom:50%;" />

- Based on DQN  , **DDPG 是DQN的衍生**.  也可以看成 AC架构, actor的policy是DPG.
  - Q-learning is off-policy and works with changing exploration strategies 
- Deterministic policy: $$g_{\pi^{*}}(\mathbf s ; \Phi)=\mathbf a \in \mathbb{R}$$ 
- Goal: to find $\Phi$ maximizing $$\mathrm{E}_{\mathbf{s}}\left[f_{Q^{*}}(\mathbf{s}, \mathbf{a} ; \mathbf{\Theta})\right]$$, where $$\mathbf{a}=g_{\pi^{*}}(\mathbf{s} ; \Phi)$$
- SGD update rule:

$$
\begin{aligned}
\Phi & \leftarrow \Phi+\eta \frac{\partial \mathrm{E}_{\mathrm{s}}\left[f_{Q^*}(\mathbf{s}, \mathbf{a} ; \Theta)\right]}{\partial \Phi} \\
&=\Phi+\eta \mathrm{E}_{\mathbf{s}}\left[\frac{\partial f_{Q^*}}{\partial \mathbf a}(\mathbf{s}, \mathbf{a} ; \Theta) \cdot \frac{\partial g_{\pi^{*}}}{\partial \Phi}(\mathbf{s} ; \Phi)\right]
\end{aligned}
$$

核心思想就是, 用一个网络去拟合 $\arg\max_a$ ,  这个是deterministic的,  然后输入到 DQN. 

因为DPG, 要保证探索性, 加随机过程.

 

### Abstract

作者将Deep Q-Learing 扩展到 continuous action space,   提出一个 actor-critic model-free 的DPG算法.   
DDPG 使用同一个网络,相同的参数, 解决了超过20个问题.   
最后介绍了, 很多任务可以通过 end-to-end来学习, 即直接输入原始像素. 



### INTRODUCTION

- AI要解决的主要目标, 输入是 unprocessed, high-dimensional, sensory input的复杂任务.   
  deep learning for sensory processing (Krizhevsky et al., 2012) + RL = DQN.  DQN用DNN来estimate action-value. 
- 然而, DQN只能处理离散的、低维的动作空间,  不适用于高纬连续动作空间.   DQN cannot be straight- forwardly applied to continuous domains since it relies on a finding the action that maximizes the action-value function, which in the continuous valued case requires an iterative optimization process at every step.     $\arg\max_a$ 无法处理连续的情况. 
- 要把DQN用到连续空间, 一个简单做法是, 将动作空间离散化, 不过这样做有很多限制, 特别是带来了"**curse of dimensionality**", 自由度越高, 带来的维度成指数级增长. 如此高的动作空间很难高效地探索.  并且, 离散化, 会不必要地丢弃 动作空间的结构信息, 说不定这些信息对解决问题是至关重要的. 

在这里, 作者提出一个 **model-free, off-policy, actor-critic**算法, 在高纬度连续动作空间, 使用NN近似函数来学习策略. 以DPG算法为基础.   
然后, 下面会提到, 原始的 带NN近似函数的 actor-critic 方法 在处理有挑战性的问题时, 是不稳定的. 

在DQN之前, 广泛认为, 大型非线性函数拟合是不稳定, 训练很困难的. DQN 采用了两个策略来解决不稳定的问题: 

1. replay buffer , 用来尽量减少 samples 之间的相关性. 
2. 使用 一个target Q network , 在TD buckup 的时候, 给出恒定的 targets. 

除了上面2条, 在这篇论文里面, 还采用 **batch normalization** 技术. 

Deep DPG (DDPG) 的一个关键特征是其简洁性, 只需一个 actor-critic 结构 和 一个学习算法. 



### Background

下面,假定 env都是 fully-boserved. 即$s_{t}=x_{t}$ , env 可能是 stochastic的. 

- Return: sum of discounted future reward: $R_{t}=\sum_{i=t}^{T} \gamma^{(i-t)} r\left(s_{i}, a_{i}\right)$

- 目标函数,   $J=\mathbb E_{r_{i}, s_{i} \sim E, a_{i} \sim \pi} [R_{1} ]$

- discounted state visitation distribution: $\rho^{\pi}$ 
- action-value: 	$Q^{\pi}\left(s_{t}, a_{t}\right)=\mathbb E_{r_{i \geq t}, s_{i>t} \sim E, a_{i>t} \sim \pi}\left[R_{t} \vert s_{t}, a_{t}\right]$

- Bellman equation: 	$Q^{\pi}\left(s_{t}, a_{t}\right)=\mathbb E_{r_{t}, s_{t+1} \sim E}\left[r\left(s_{t}, a_{t}\right)+\gamma \mathbb E_{a_{t+1} \sim \pi}\left[Q^{\pi}\left(s_{t+1}, a_{t+1}\right)\right]\right]$

- 如果target policy是 deterministic, 即 $\mu: \mathcal{S} \leftarrow \mathcal{A}$, 可以去掉上式里第二个期望: 

$$
Q^{\mu}\left(s_{t}, a_{t}\right)=\mathbb{E}_{r_{t}, s_{t+1} \sim E}\left[r\left(s_{t}, a_{t}\right)+\gamma Q^{\mu}\left(s_{t+1}, \mu\left(s_{t+1}\right)\right)\right] \tag{3}
$$

该期望只取决于env. 这意味着通过 off-policy来学习$Q^\mu$ 是可能的. using transitions which are generated from a different stochastic behavior policy β.

Q-learning 是很常用的 off-policy 算法, 通过一个 greedy 策略: $\mu(s)=\arg \max _{a} Q(s, a)$   
设计以$\theta^Q$为参数的近似函数Qfunction, 最小化下面的Loss:
$$
L\left(\theta^{Q}\right)=\mathbb{E}_{s_{t} \sim \rho^{\beta}, a_{t} \sim \beta, r_{t} \sim E}\left[\left(Q\left(s_{t}, a_{t} \vert \theta^{Q}\right)-y_{t}\right)^{2}\right] \tag{4}
$$
where $$y_{t}=r\left(s_{t}, a_{t}\right)+\gamma Q\left(s_{t+1}, \mu\left(s_{t+1}\right) \vert \theta^{Q}\right)$$ , $y_t$取决于$\theta^Q$,  通常被忽略.

过去 使用大型非线性近似函数来学习 value或者action value 通常被避免, 因为理论上无法保证性能, 实践中, 学习通常是不稳定的.  直到最近DQN. 采用了 *replay buffer* 以及 *target network* 来计算 $y_t$ . 



### ALGORITHM

直接应用Q-learning到continuous action spaces是不可行的:  
in continuous spaces finding the greedy policy requires an optimization of $a_t$ at every timestep , this opti- mization is too slow to be practical with large, unconstrained function approximators and nontrivial action spaces.  在连续空间中寻找贪婪策略需要在每一个时间点上对at进行优化,  这种优化对于大型、无约束的近似函数和复杂的动作空间来说，这种优化方法太过缓慢，不符合实际。 

所以,这里使用以DPG为基础的actor-critic方法.    

- actor : DPG ,  $\mu\left(s \vert \theta^{\mu}\right)$ 
- critic :  $r\left(s_{t}, a_{t}\right)+\gamma Q\left(s_{t+1}, \mu\left(s_{t+1}\right) \vert \theta^{Q}\right)$ 
- 然后两个网络串联.

DPG使用 $\mu\left(s \vert \theta^{\mu}\right)$ 当作actor函数, 直接把当前state确定地映射为一个特定action.  critic函数像Q-learning一样使用Bellman公式来学习$Q(s,a)$.   actor参数通过对下面目标函数链式求导来更新 .  The actor is updated by following the applying the chain rule to the expected return from the start distribution $J$ with respect to the actor parameters:
$$
\begin{aligned}
\nabla_{\theta^{\mu}} J & \approx \mathbb{E}_{s_{t} \sim \rho^{\beta}}\left[ \nabla_{\theta^{\mu}} Q (s, a | \theta^{Q} )    |_{ s=s_{t}, a=\mu (s_{t} | \theta^{\mu} )  }  \right] \\
&=\mathbb{E}_{s_{t} \sim \rho^{\beta}}\left[    
 \nabla_{a} Q (s, a | \theta^{Q} ) |_{s=s_{t}, a=\mu (s_{t} )} 
  \nabla_{\theta_{\mu}} \mu (s | \theta^{\mu} ) |_{s = s_t}
\right]
\end{aligned}   \tag{6}
$$

Silver et al. (2014) proved that this is the *policy gradient*, the gradient of the policy’s performance. 

像Q-learning 一样, 使用非线性近似函数意味着收敛性无法保证. 但非线性近似函数在大型状态空间上是必须的.  

NFQCA使用与DPG一样的更新方式, 不过使用了neural network作为近似函数, 并使用 batch learning保证稳定性, 很难搞定大型网络.   A minibatch version of NFQCA which does not reset the policy at each update, as would be required to scale to large networks, is equivalent to the original DPG.  

这里, 提出一个DPG的修改版: DDPG, 受到DQN的启发,  DQN使用NN近似函数在大型state和action空间里online 学习.  

一个问题, NN for RL optimization算法假定 samples都是独立同分布的 iid. 显然通过顺序与env交互得到的sample不符合. 另外, 为了充分利用硬件优化, 必须通过mini-batch学习, 而不是online.  
像DQN一样, 使用 **replay buffer** 来解决这个问题. replay buffer is a finite sized cache $\mathcal R$, FIFO queue, store $\left(s_{t}, a_{t}, r_{t}, s_{t+1}\right)$.   each timestep, sample a minibatch uniformly from cache. 因为 DDPG off-policy, buffer 可以很大, 可以抽出不相干的samples(uncorrelated transitions),有利于学习. 

Q-learning unstable. 因为更新中的 network Q 同时被用于计算target value, Q update易于 divergence. 像DQN里的**target network**一样, modified for actor-critic, 使用 **"soft" target updates**, 而不是直接复制 weights. AC网络都复制一份. create a copy of the actor and critic networks, $Q^{\prime}\left(s, a \vert \theta^{Q}\right)$ and $\mu^{\prime}\left(s \vert \theta^{\mu^{\prime}}\right)$ respectively, used for calculating the targes.  The weights updated by slowly track the learned networks: $\theta^{\prime} \leftarrow \tau \theta+(1 - \tau) \theta^{\prime}$ with $\tau \ll 1 .$ 限制target values, 使其变化得很慢, 提升学习稳定性.  这样learning action-value 更像监督学习, 有很多解决方案.   
found that having both a target $\mu^{\prime}$ and $Q^{\prime}$ was required to have stable targets $y_{i}$ in order to consistently train the critic without divergence. actor critic两个网络要想不发散, 需要有稳定的训练target的y.  

当从 low dimensional feature vector observations中学习时, 不同维度有不同的物理单位, 也有不同的取值范围, 导致无法高效学习和调参. 一个办法是手动归一化.  作者使用**batch normalization**, normalizes each dimension across the samples in a minibatch to have <u>unit mean and variance</u>.  maintains a running average of the mean and variance to use for normalization during testing. In deep networks, it is used to <u>minimize covariance shift</u> during training, by ensuring that each layer receives <u>whitened input</u>. 对state input 和两个网络的所有层都使用了batch normalization, 这样对不同任务都可以高效学习而无需手工去做normalization.

A major challenge of learning in continuous action spaces is **exploration**.连续action空间探索性是个问题. off-policy的优势探索独立 treat exploration independently from the learning algorithm.  constructed an exploration policy $\mu^{\prime}$ by adding noise sampled from a noise process $\mathcal{N}$ to our actor policy . 给确定性策略增加噪声从而变成探索性的策略.

$$
\mu^{\prime}\left(s_{t}\right)=\mu\left(s_{t} | \theta_{t}^{\mu}\right)+\mathcal{N}
$$

选择适合env的noise process $\mathcal{N}$.  附录里,为了高效探索惯性物理问题, 使用了**Ornstein-Uhlenbeck process**一种随机过程. 



#### RESULTS

DDPG algorithm

![image-20200403011415267](/img/2019-12-29-DDPG.assets/image-20200403011415267.png)



![20171108090350229](/img/2019-12-29-DDPG.assets/20171108090350229.jpeg)





所有任务都使用了低维度的state信息以及高维度的视频画面两种方式.  使用了一些DQN论文里的一些数据处理方式, 为了是高维度数据重现,使用了action repeats. 还有一些降维处理.

图标展示了各种组合的表现, 其中 target network 非常关键. 有些简单任务, 从图像输入跟state描述信息输入 学习速度一样快. 可能是action repeats 让问题变的简单. CNN也可能发挥了作用, provide an easily separable representation of state space, which is straightforward for the higher layers to learn on quickly.

![image-20200102170319142](/img/2019-12-29-DDPG.assets/image-20200102170319142.png)



#### CONCLUSION

跟RL其他算法一样, 非线性近似函数不保证收敛; 但DDPG在许多task上很稳定.

**Q-learning倾向于高估value. DDPG要更准.   **  
DDPG效果也比DQN要效率高, 步数少. 

**DDPG缺点, 仍然需要大量episode来找到solution.**



#### EXPERIMENT DETAILS

Adam , learning rate of $10^{-4}$ and $10^{-3}$ for the actor and critic . 

For $Q$  : $L_{2}$ weight decay of $10^{-2}$ ,  discount factor  $\gamma=0.99 .$ 

For soft target updates $\tau=0.001 .$ 

NN: rectified non-linearity for all hidden layers. 
final output layer of the actor:  tanh, to bound the actions. 

low-dimensional networks : 2 hidden layers with 400 and 300 units respectively $(\approx 130,000$ parameters). Actions were not included until the 2 nd hidden layer of $Q .$ 

High dimensional:   3 convolutional layers (no pooling) with 32 filters at each layer.  two fully connected layers with 200 units $(\approx 430,000$ parameters). 

final layer weights and biases of both the actor and critic were initialized from a uniform distribution $\left[-3 \times 10^{-3}, 3 \times 10^{-3}\right]$ and $\left[3 \times 10^{-4}, 3 \times 10^{-4}\right]$ for the low dimensional and pixel cases respectively. This was to ensure the initial outputs for the policy and value estimates were near zero. 为了一开始估值接近0. other layers were initialized from uniform distributions $\left[-\frac{1}{\sqrt{f}}, \frac{1}{\sqrt{f}}\right]$ where $f$ is the fan-in of the layer. fan-in是隐层的输入节点数目. The actions were not included until the fully-connected layers. 

minibatch sizes  64 for the low dimensional problems ,   16 on pixels.

replay buffer size : $10^{6} .$

Ornstein-Uhlenbeck process  with $\theta=0.15$ and $\sigma=0.2 .$ 




## References

[Lillicrap, Timothy P., et al. "Continuous control with deep reinforcement learning." arXiv preprint arXiv:1509.02971 (2015).](https://arxiv.org/pdf/1509.02971)

https://blog.csdn.net/kenneth_yu/article/details/78478356 图很好