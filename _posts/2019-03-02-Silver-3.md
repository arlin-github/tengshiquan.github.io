---
layout:     post
title:      UCL Course on RL,  Model-Free Prediction
subtitle:   David Silver 的课程笔记3
date:       2019-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---




# Model-Free Prediction

model-free，通过采样来获得env的知识



- Last lecture:
  - **Planning** by **dynamic programming**
  - Solve a known MDP 
- This lecture:
  - Model-free prediction
  - **Estimate** the value function of an unknown MDP 
- Next lecture:
  - Model-free control
  - **Optimise** the value function of an unknown MDP





### Monte-Carlo Learning

- MC methods learn directly from **episodes of experience**
- MC is **model-free**: no knowledge of MDP transitions / rewards  
  因为不知道转移概率，所以无法用bellman方程
- MC learns from **complete episode**s: no **bootstrapping**  
  完整的episode不要求起始状态是某一个特定的状态，但要求agent最终进入某一个终止状态
- MC uses the **simplest** possible idea: **value = mean return** 
- 缺点: can only apply MC to **episodic** MDPs  ,  只适用于**episodic** MDP,  否则没法算G 

- 让agent与environment交互，得到一些经历（样本），本质上相当于从概率分布 ${\cal P}_{ss^{'}}^{a} 、 {\cal R}_s^a$ 中进行采样。然后通过这些经历来进行策略评估与策略迭代。从而最终得到最优策略。
- 通过采样来了解环境，来了解各种R，P
- MDP是通过5元组： $$\left\{ {\cal S, \cal A, \cal P, \cal R,\gamma} \right\}$$ 来做决策的，在现实世界中，可能无法同时知道这个5元组。就无法使用Bellman方程来求解 V 和 Q 值。 





### Monte-Carlo Policy Evaluation

- Goal: learn $v_π$ from episodes of experience under policy $π$

$$
S_1,A_1,R_2,...,S_k ∼π
$$

- Monte-Carlo policy evaluation uses ***empirical mean*** return instead of **expected** return



#### First-Visit Monte-Carlo Policy Evaluation

- The **first** time-step t that state s is visited in an episode
- Increment counter $N(s) ← N(s) + 1$
- Increment total return $S(s) ← S(s) + G_t$
- Value is estimated by **mean return** $V(s) = S(s)/N(s)$
- By **law of large numbers**, $V(s) → v_π(s)$ as $N(s) → ∞$    大数定理

如果在一个episode里，一个状态s出现多次，因为算法是倒序来累加出来G的，一般都考虑first-visit, 比较稳.



#### Every-Visit Monte-Carlo Policy Evaluation

Every time-step t that state s is visited in an episode,  其他同上



#### Blackjack Example

Policy: stick if sum of cards ≥ 20, otherwise twist



### Incremental Monte-Carlo

#### Incremental Mean

$$
\mu_k = \frac{1}{k} \sum_{j=1}^k x_j = \mu_{k-1} + \frac{1}{k} (x_k - \mu_{k-1})
$$

#### Incremental Monte-Carlo Updates

$$
N(s) ← N(s) + 1
\\ V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)} (G_t - V(S_t))
$$

- In **non-stationary** problems, it can be useful to track a running mean, i.e. forget old episodes. 

$$
V(S_t) \leftarrow V(S_t) + \alpha (G_t - V(S_t))
$$



> 对于 $\alpha$ 的思考， 如果只是evaluate当前的策略， 那肯定 是按照 1/N 的定义是最准的， $\alpha$ 需要慢慢衰减
>
> 如果$\alpha$ 用固定值，刚开始的时候，对目标的改动偏小，采样的比重比较小，造成迭代慢； 到了比较稳定的后期， 新采样的比重过大，容易被新采样的值造成来回的波动，难以收敛；如果新采样值的方差比较小的情况小，也还是能正常收敛的；
>
> 对于 policy 会变化的情况，$\alpha$  取固定值也能说得通， 因为前面采样是过时的策略采样的， 比重就应该小；后面策略慢慢固定的话，值也固定的话，也能收敛



- 静态问题和非静态问题，静态就是MDP是不变的，比如转移矩阵，比如奖励函数，而非静态会随着时间的推移，MDP中的某些参数将会发生改变。
- 这里将MC方法变为增量式，可以使得算法不用计数值N(s),  换为类似于学习率的参数，该参数的选择关乎算法的收敛性。
- MC只用于episodic MDP，因为 $G_t$ 是从 t 时刻开始，对整个episode的奖励进行累加.  显然，这种方法计算量会比较大，所以，引出了时间差分方法。
- **MC有个很大的劣势， 要走到底才能知道一个结果，然后层层反推， 产生的数据量少又慢**



### Temporal-Difference Learning

- TD methods learn directly from **episodes of experience**
- TD is **model-free**: no knowledge of MDP transitions / rewards
- TD learns from **incomplete** episodes, by **bootstrapping**
- TD updates a **guess** towards a guess



**MC TD 主要差异在 Prediction 方面** 

- **On-line** : step-by-step   ,   agent一边执行策略边评估或者学习

- **Off-line** : episode-by-episode  ,   agent先按照某个策略跑完一个episode, 再update啥的



##### MC and TD

- Goal: learn $v_π$  **online** from experience under policy $π$

- **Incremental every-visit Monte-Carlo** : 

  - Update value $V(S_t)$ toward **actual return** $G_t$ 
  - $V(S_t)←V(S_t)+α({\color{red}{ G_t}} −V(S_t))$ 

- Simplest **temporal-difference** learning algorithm:  TD(0)  

  - toward **estimated return**
  - $V(S_t)←V(S_t)+α({\color{red} {R_{t+1} + \gamma V(S_{t+1})}} −V(S_t))$
  - 用后面的状态值 backup 迭代
  - **TD target** : $R_{t+1} + \gamma V(S_{t+1})$
  - **TD error** : $\delta_t = R_{t+1} + \gamma V(S_{t+1}) −V(S_t)$



##### Driving Home Example

<img src="/img/2019-03-02-Silver.assets/image-20190216223334838.png" style="zoom: 60%;" />

下班，预估花30分钟到家，找到车花了5分钟，发现下雨，这个时候根据以往经验，觉得还要35分钟，5+35-30 为下班这个评估值的修正；一个问题就是，下班以后，进入到的状态可能是随机的，下雨不下雨， 这个要足够多的采样才能算出均值

- reward 是消耗的时间 the return for each state is the actual time to go from that state. 
- The value of each state is the expected time to go

![image-20190216223634267](/img/2019-03-02-Silver.assets/image-20190216223634267.png)

- 使用MC算法和TD算法得到的对于各状态返家还需耗时的更新结果和更新时机都是不一样的。
- 用了MC，只有到家了，有了最终结果，才能更新迭代路上的预估； 其实 不用走到底，只要有以前的经验，就可以预估，预估都是一样的； 差别在于 预估迭代更新的时刻以及更新的多少
- MC的每个状态修正的值都很大，因为都是根据最终结果来的；而TD则要小一些，主要是修正该状态的预测错误，**不会因为某个状态的特别异常造成更新的值波动太大， 所以方差较小**！！
- **TD  比起 MC ，更倾向于让对当前这个状态之后的R 预测准**， 而不是整个episode的G





### Advantages and Disadvantages of MC vs. TD

- TD can learn ***before*** knowing the **final outcome** 不用等到最后
  - TD can learn **online** after every step    TD 可以online
  - MC must wait until end of episode before return is known 
- TD can learn **without** the final outcome    不需要最后的G_t，只要前中期有R就可以
  - TD can learn from **incomplete** sequences 因为只要一段里面有R，就可以更新
  - MC can only learn from **complete** sequences
  - TD works in **continuing** (non-terminating) environments   TD可以处理 **持续问题**
  - MC only works for **episodic** (terminating) environments 



#### Bias/Variance Trade-Off

- Return $G_t = R_{t+1} + γR_{t+2} + ... + γ^{T−1}R_T$ is **unbiased** estimate of $v_π(S_t)$ 
- **True TD target** $R_{t+1} + γv_π(S_{t+1})$ is **unbiased** estimate of $v_π(S_t)$ 
- **TD target** $R_{t+1} +γV(S_{t+1})$ is **biased** estimate of $v_π(S_t)$   有偏估计
- TD target is much lower variance than the return:   比MC方差要低 
  - **Return** depends on **many** random actions, transitions, rewards 
  - **TD target** depends on **one** random action, transition, reward 



##### MC vs  TD

- MC has **high variance, zero bias**  高方差，无偏估计
  - Good convergence properties  收敛性好
  - (even with function approximation)  即使采用函数逼近,  NN近似也收敛
  - Not very sensitive to initial value  对初始值不敏感
  - Very simple to understand and use 简单
  - 随着样本数量的增加，方差逐渐减小，趋近于0

- TD has low variance, some bias  低方差， 有偏估计
  - Usually **more efficient** than MC   高效
  - TD(0) **converges** to $v_π(s)$ 
  - (but not always with function approximation)  用NN拟合不一定收敛
  - More sensitive to initial value **对初始值更敏感（用到了贝尔曼方程）**



#### Random Walk Example

![image-20190217114449262](/img/2019-03-02-Silver.assets/image-20190217114449262.png)

true value 是解析解， 一条斜线;  

![image-20190217114947066](/img/2019-03-02-Silver.assets/image-20190217114947066.png)

当step-size  $\alpha$不是非常小的情况下，TD有可能得不到最终的实际价值，将会在某一区间震荡



#### Batch MC and TD

- MC and TD **converge**: $V(s) \rightarrow v_{\pi}(s)$ as experience $\rightarrow \infty$
- what about **batch solution** for **finite experience**?   
  利用有限的经验来对值函数进行估计的batch方案

$$
s_1^1, a_1^1,r_2^1,\dots,S_{T_1}^1 
\\ \vdots
\\ s_1^K, a_1^K,r_2^K,\dots,S_{T_K}^K
$$

- e.g. Repeatedly sample episode k ∈ [1, K ] ,Apply MC or TD(0) to episode k  
  实际操作:重复从这k个episode中进行采样，迭代得到符合有限sample的收敛值

按照这种方式，下面的AB Example , 两种算法有两个答案

![image-20190217020236552](/img/2019-03-02-Silver.assets/image-20190217020236552.png)

- MC , V (A) = 0    与实际情况方差最小 
- TD , V(A) = 0.75



#### Certainty Equivalence  确定性等价估计

- MC **converges** to solution with **minimum mse** 
  - Best fit to the observed returns

$$
\sum_{k=1}^K \sum_{t=1}^{T_k} (G_t^k - V(S_t^k))^2
$$


- TD(0) **converges** to solution of **max likelihood** Markov model
  	- Solution to the MDP $⟨S, A, \hat P,\hat R, γ⟩$ that best fits the data
  	- 对上例，A状态只会跳转到B状态，等价于**内在动态过程是确定性的估计** , 相当于学到 **dynamics**
  	- 根据已有经验估计状态间的转移概率，同时估计某一个状态的即时奖励，多了一个计算转移概率的情况

$$
\mathcal {\hat P}_{s,s'}^a = \frac{1}{N(s,a)} \sum_{k=1}^K \sum_{t=1}^{T_k} \mathbf 1(s_t^k,a_t^k,s_{t+1}^k = s,a,s')
\\\mathcal {\hat R}_{s}^a = \frac{1}{N(s,a)} \sum_{k=1}^K \sum_{t=1}^{T_k} \mathbf 1(s_t^k,a_t^k  = s,a )r_t^k
$$



##### MC vs. TD  (3)

- TD exploits Markov property
  - Usually more efficient in Markov environments 

- MC does not exploit Markov property  **MC没有利用马尔科夫性**
  - Usually more effective in non-Markov environments   在非马尔科夫环境中比较好用



### Unified View

#### Bootstrapping and Sampling

- **Bootstrapping**: update involves an estimate 

  - MC does not bootstrap
  - DP, TD bootstraps  从后往前精确求解

- **Sampling**: update samples an expectation 

  - MC, TD samples

  - DP does not sample 

    



<img src="/img/2019-03-02-Silver.assets/image-20190202161212996.png" style="zoom: 60%;" />
<img src="/img/2019-03-02-Silver.assets/image-20190202161318727.png" style="zoom: 60%;" />

<img src="/img/2019-03-02-Silver.assets/image-20190202161352677.png" style="zoom: 60%;" />

上面两个都是sample算法； DP是考虑$\pi$ 下所有 a, r, s' 然后算出来精确解

 <img src="/img/2019-03-02-Silver.assets/image-20190202160955780.png" style="zoom: 70%;" />

> 当使用单个采样，同时不经历完整的状态序列更新价值的算法是TD学习；
> 当使用单个采样，但依赖完整状态序列的算法是MC学习；
> 当考虑全宽度采样，但对每一个采样经历只考虑后续一个状态时的算法是DP学习；
> 如果既考虑所有状态转移的可能性，同时又依赖完整状态序列的，那么这种算法是穷举（exhausive search）法。

需要说明的是：DP利用的是MDP问题的model，也就是状态转移概率，虽然它并不实际利用采样，但它利用了整个模型的规律，因此也被认为是全宽度（full width） 采样的。





### TD(λ) 

 n-step 用的较少, 整理的有点乱

#### n-Step TD

- 用介于 TD(0) 与 MC 之间的 target 来prediction  **n-step  n=1 TD;  n=∞  MC **

- **n-Step Prediction**  : TD target look n steps into the future

- **n-Step Return** :   $$G_t^{(n)} =R_{t+1} +γR_{t+2} +...+γ^{n−1}R_{t+n} +γ^nV(S_{t+n})$$

- **n-step TD** :  $V(S_t) \leftarrow V(S_t) + \alpha (G_t^{(n)} - V(S_t))$



##### *Learning with n-step Backups*

Backup computes an increment

$$
\Delta_t(S_t) \doteq \alpha \left[G_t^{(n)} - V_t(S_t) \right]
\\  \Delta_t(s) = 0  ,\forall s \neq S_t
$$

- Online updating : $V_{t+1}(s) = V_t(s) + \Delta_t(s),  \forall s \in \cal S$

- Off-line updating : $V(s) = V(s) + \sum_{t=0}^{T-1} \Delta_t(s),  \forall s \in \cal S$

- 这里的 $V_{t+1}$ 是 $V_t$ 的迭代值， 与 $S_{t+1}$ 无关

- Off-line update的意义？？ 可以批处理 并行 ？？ 这里off 跟on有个很大的差别，就是online的V有多次的incremental 迭代； off-line是把很多误差加起来，一次incremental 迭代；



##### Large Random Walk Example

![image-20190218230909485](/img/2019-03-02-Silver.assets/image-20190218230909485.png)

- On-line is better than off-line 

- An *intermediate n* is best 

- offline是指更新状态的v值是在episode by episode

结果如图表明，离线和在线之间曲线的形态差别不明显；从步数上来看，步数越长，越接近MC算法，均方差越大。对于这个大规模随机行走示例，在线计算比较好的步数是3-5步，离线计算比较好的是6-8步。 超参, 要调



##### Averaging n-Step Returns

e.g. average the 2-step and 4-step returns  $\frac{1}{2}G^{(2)}+\frac{1}{2}G^{(4)}$



#### Forward-view TD(λ)

##### λ-return

$\lambda=0$ 退化成 $\mathrm{TD}\left(0 \right)$ ， $\lambda=1$  退化成 $\mathrm{MC}$
$$
G_t^\lambda = (1 - \lambda) \sum_{n=1}^\infty \lambda^{n-1} G_t^{(n)}
$$


<img src="/img/2019-03-02-Silver.assets/image-20190202195246475.png" style="zoom: 40%;" />

![image-20190218231850456](/img/2019-03-02-Silver.assets/image-20190218231850456.png)

$$
G_t^{\lambda} \doteq (1- \lambda) \sum_{n=1}^{T-t-1} \lambda^{n-1} G_{t:t+n} + \lambda^{T-t-1} G_t
$$


##### Forward-view TD(λ)

向未来看

![image-20190218194537215](/img/2019-03-02-Silver.assets/image-20190218194537215.png)

- Update value function towards the λ-return
- Forward-view looks into the future to compute $G_t^λ$
- Like MC, can only be computed from complete episodes   **offline**



##### Forward-View TD(λ) on Large Random Walk

![image-20190218231259218](/img/2019-03-02-Silver.assets/image-20190218231259218.png)

- On-line >> Off-line
- Intermediate values of λ best
- λ-return better than *n*-step return



#### Backward View TD(λ)

- 下面介绍转化为 online 的方法，向过去看
- Forward view provides theory
- **Backward view provides mechanism**
- Update **online**, every step, from incomplete sequences



### Eligibility Traces

- **Frequency heuristic**: assign credit to most frequent states 出现频率最高的
- **Recency heuristic**: assign credit to most recent states  最近的，远的衰减的厉害
- Eligibility traces combine both heuristics

$$
\begin{aligned} E_0(s) & = 0
\\ E_t(s) & = γλ E_{t−1}(s) + \mathbf 1(S_t = s)
\end{aligned}
$$

- 其中 $\mathbf 1(S_t =s)$ 是一个条件判断表达式。

- Accumulating race



考虑一个最简单情况 $E_0(s) = 0 , E_1(s)=1$ ,后面该s未出现，则 $E_t(s) = (\gamma\lambda)^{t-1}$ 指数衰减

<img src="/img/2019-03-02-Silver.assets/image-20190202161639200.png" style="zoom: 70%;" />

典型的函数曲线: 对序列中，每访问到某个状态，则该状态的ET值增加，然后随时间指数衰减； ET值可以看成 贡献度 credit 值



> 资格迹的提出是基于一个**信度分配（Credit Assignment）**问题的，打个比方，最后我们去跟别人下围棋，最后输了，那到底该中间我们下的哪一步负责？或者说，每一步对于最后我们输掉比赛这个结果，分别承担多少责任？这就是一个信度分配问题。
>
> 可以将资格迹理解为一个权重，状态 s 被访问的时间离现在越久远，其对于值函数的影响就越小，状态 s 被访问的次数越少，其对于值函数的影响也越小。



##### Backward View TD(λ)

迭代增量的核心还是 TDerror，但是不需要G了，只需要下一步的R，另外需要记录更新过去每个s的E值

- Keep an eligibility trace for every state s
- Update value V(s) for every state s
- In proportion to TD-error $δ_t$ and eligibility trace $E_t(s)$

$$
δ_t =R_{t+1}+γV(S_{t+1})−V(S_t) 
\\ V (s ) ← V (s ) + αδ_t E_t (s )
$$

##### On-line tabular TD(λ) 

<img src="/img/2019-03-02-Silver.assets/image-20190218233210905.png" style="zoom: 80%;" />

**把当前状态的TDerror 按照之前各个状态的贡献度，给分配到这些个状态上面去,for all s**

<img src="/img/2019-03-02-Silver.assets/image-20190218194406826.png" style="zoom: 60%;" />



#### Relationship Between Forward and Backward TD

##### TD(λ=0)  ==  TD(0)

λ = 0,  only current state is updated

$$
E_t(s) = \mathbf 1(S_t = s)					
\\ V (s ) ← V (s ) + αδ_t E_t (s)
$$

exactly equivalent to TD(0) update

$$
 V (S_t ) ← V (S_t ) + αδ_t 
$$

##### TD(λ=1)  ==  MC

If you set λ to 1, you get MC but in a better way 

- Can apply TD(1) to continuing tasks
- Works incrementally and on-line

- When λ = 1, credit is deferred until end of episode    credit推迟到episode结束 啥意思
- Consider episodic environments with offline updates

- 定理: The sum of **offline** updates is identical for forward-view and backward-view TD(λ)

对于某个s，一个episode的迭代增量总和为, 下面证明:
$$
\sum_{t=1}^T \alpha\delta_tE_t(s) = \sum_{t=1}^T\alpha \Big(G_t^\lambda - V(S_t) \Big) \mathbf 1(S_t =s)
$$



#### Forward and Backward Equivalence

##### TD(1) = MC

- 先考虑特例， s只访问一次的情况
- Consider an episode where s is visited once at time-step k
- TD(1) eligibility trace discounts time since visit,

$$
\begin{align*} E_t\left( s \right) &= \gamma E_{t-1}\left( s \right)+\textbf{1}\left( S_t=s \right)\\ &= \left\{\begin{array}{lr} 0 &  \textsf{if}\ t<k\\  \gamma^{t-k} & \textsf{if}\ t\geq k \\ \end{array}  \right. \end{align*}
$$

- TD(1) updates accumulate error **online**

$$
\sum \limits_{t=1}^{T-1}\alpha\delta_tE_t\left( s \right)  =\alpha   \sum \limits_{t=k}^{T-1}\gamma^{t-k} \delta_t = \alpha(G_k - V(S_k))
$$

- By end of episode it accumulates total error

$$
\delta_k+\gamma\delta_{k+1}+\gamma^2\delta_{k+2}+...+\gamma^{T-1-k}\delta_{T-1}
$$

- When λ = 1, sum of TD errors telescopes into MC error

$$
\begin{align*} &\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+...+\gamma^{T-1-t}\delta_{T-1}\\  &=R_{t+1}+\cancel{\gamma V\left( S_{t+1} \right)}-V\left( S_t \right)\\ & +\gamma R_{t+2}+\cancel{\gamma^2V\left( S_{t+2} \right)}-\cancel{\gamma V\left( S_{t+1} \right)}\\  &+\gamma^2 R_{t+3}+\cancel{\gamma^3V\left( S_{t+3} \right)}-\cancel{\gamma^2 V\left( S_{t+2} \right)}\\  &\vdots\\  &+\gamma^{T-2-t} R_{T-1}+\cancel{\gamma^{T-1-t}V\left( S_{T-1} \right)}-\cancel{\gamma^{T-2-t} V\left( S_{T-2} \right)}\\  &+\gamma^{T-1-t} R_T+\gamma^{T-t}V\left( S_{T} \right)-\cancel{\gamma^{T-1-t} V\left( S_{T-1} \right)}\\  &=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{T-2-t}R_{T-1}+\gamma^{T-1-t} R_T+\gamma^{T-t}V\left( S_T \right)-V\left( S_t \right)\\  &=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{T-2-t}R_{T-1}+\gamma^{T-1-t} R_T-V\left( S_t \right)\\ &=G_t-V\left( S_t \right)\end{align*}
$$

- 对于多次访问的情况比如n，MC everyvisit 就是把一个episode中n个G加起来再求平均; 所以TD(1)如果也是offline更新，并且遇到多次s，则系数就是多次的叠加而已，就与MC完全一样；online其他时候优于MC
- TD(1) is **roughly equivalent** to **every-visit** Monte-Carlo TD(1) ,  大约等价于每次访问的 MC 算法
- Error is accumulated online, step-by-step
- If value function is only updated **offline** at end of episode
- Then total update is exactly the **same** as MC


区别是： TD(1)  为在线误差累计，每步更新; 如果 TD(1)  也等到片段结束后离线更新，那么TD(1) 就是MC

> 为什么是基本等价于， 是因为 一个episode里面， 如果一个s被更新了一次，则第二次被利用的时候，v(s)已经被更新了， 就会与offline的情况有区别；  应该说 online的效率更高
>
> 一个特例: 如果在一个episode中,每个状态都只出现一次， 是不是online与offline等价呢？是的, 因为都是利用后继的tderror



##### Telescoping in TD(λ)

**对TD(λ)化简后得到前向视角和后向视角下的误差等价**

For general λ, TD errors also telescope to λ-error, $G_t^λ − V (S_t )$ :   $G_t^λ$ 是各种stepReturn的和

$$
\begin{align*} G{_t^\lambda}-V\left( S_t \right) =-V\left( S_t \right) &+\left( \gamma \lambda \right)^0\left( R_{t+1}+\gamma V\left( S_{t+1} \right)-\gamma\lambda V\left( S_{t+1} \right) \right)\\ &+\left( \gamma \lambda \right)^1\left( R_{t+2}+\gamma V\left( S_{t+2} \right)-\gamma\lambda V\left( S_{t+2} \right) \right)\\ &+\left( \gamma \lambda \right)^2\left( R_{t+3}+\gamma V\left( S_{t+3} \right)-\gamma\lambda V\left( S_{t+3} \right) \right)\\  &+\left( \gamma\lambda \right)^3\left( R_{t+4}+\gamma V\left( S_{t+4} \right)-\gamma\lambda V\left( S_{t+4} \right)  \right)\\ &+...\\=\ \ \ \ \ \ \ \ \ \ \ \  &\left( \gamma\lambda \right)^0\left( R_{t+1}+\gamma V\left( S_{t+1} \right)-V\left( S_t \right) \right)\\ &+\left( \gamma\lambda \right)^1\left( R_{t+2}+\gamma V\left( S_{t+2} \right)-V\left( S_{t+1} \right) \right)\\ &+\left( \gamma\lambda \right)^2\left( R_{t+3}+\gamma V\left( S_{t+3} \right)-V\left( S_{t+2} \right) \right)\\ &+...\\=\ \ \ \ \ \ \ \ \ \ \ \  &\delta_t+\gamma\lambda\delta_{t+1}+\left( \gamma\lambda \right)^2\delta_{t+2}+... \end{align*}
$$

##### Forwards and Backwards TD(λ）

- 这里讨论的就是 在episode里面只访问一次的情况， Back TD(λ) 的online增量更新就等于offline更新，forward == backward ; 
- 当s会被访问多次的时候， 也是成立的, Backward 主要是依靠eligibility的值的累加，来实现

- Consider an episode where s is visited once at time-step k,
- TD(λ) eligibility trace discounts time since visit,

$$
\begin{align*} E_t\left( s \right) &= \gamma\lambda E_{t-1}\left( s \right)+1\left( S_t=s \right)\\ &= \left\{\begin{array}{lr} 0 &  \textsf{if}\ t<k\\  \left( {\gamma\lambda} \right)^{t-k} & \textsf{if}\ t\geq k \\ \end{array}  \right. \end{align*}
$$

- Backward TD(λ) updates accumulate error **online**

$$
\sum \limits_{t=1}^{T-1}\alpha\delta_tE_t\left( s \right)  =\alpha   \sum \limits_{t=k}^{T-1}(\gamma \lambda)^{t-k} \delta_t = \alpha \Big(G_k^\lambda - V(S_k)\Big)
$$

- By end of episode it accumulates total error for λ-return   
  当整个片段完成时，后向视角方法对于值函数 $V(s)$ 的增量等于 $\lambda-\mathrm{return}$；
- For multiple visits to s, Et(s) accumulates many errors  
  如果状态 s 被访问了多次，那么资格迹就会累积，从而相当于累积了更多的 $V(s)$ 的增量。



##### Equivalence of  Forward and Backward TD

- **Offline** updates

  - 这里说的就是 就算是TD 也可以offline ，就是在 end of episode 一起更新掉
- Updates are accumulated within episode   episode结束后update
  
  - but applied in batch at the end of episode    离线更新下，前向视角和后向视角等价
  
- **Online** updates

  **关键是，有些s在episode里面出现多次，如果是online，则在遍历这个episode的时候，几次用到的v(s)会发生变化；在episode里面，是不是要用bellman公式来递推其他的值，这个是最影响理论分析的！！**
  
  - TD(λ) updates are applied online at each step within episode
  - Forward and backward-view TD(λ) are slightly different
  - NEW: **Exact online TD(λ)** achieves perfect equivalence
  - By using a slightly different form of eligibility trace



<img src="/img/2019-03-02-Silver.assets/image-20200701024503415.png" alt="image-20200701024503415" style="zoom:50%;" />