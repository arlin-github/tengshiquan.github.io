---
layout:     post
title:      CS 285. Deep RL with Q-Functions
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---



## Deep RL with Q-Functions

进入到深度学习, 开始面对 数据怎么预处理, 什么模型合适,  怎么train 模型等问题. 



online Q-learning  目前的问题

1. Sample 数据是 **correlated** . 相关的;  而SGD算法这些,都假定batch数据是不相关的.
2. **no gradient through target value**.   



改写 online Q-learning 为两步:

1. take some action $\mathbf a_i$ and observe $$\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)$$
2. $$\phi \leftarrow \phi-\alpha \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)\left(Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\left[r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)\right]\right)$$ . 



y数据方面的问题: 

1. sequential states are strongly correlated 
2. target value is always changing

<img src="/img/CS285.assets/image-20200320145748201.png" alt="image-20200320145748201" style="zoom: 33%;" />

一个例子, 如图, 比如trajectory 类似正弦波, 每次梯度(方框内)都获得时序上相关的一些点, 然后网络很好的拟合了这些点, 但拟合的越好, 就会忘记其他快情况的点, 实际上是 locally overfit 局部的过拟合这些点,  因为这批sample都是局部, 没有体现全局. 

一个简单的解决方案就是 :   使用 parallel worker  ,  分为 

- synchronized parallel Q-learning
- asynchronous parallel Q-learning



##### replay buffers

这是另外一个解决方案.   适用于 off-policy 的情况.   所以actor-critic方法不适用. 

<img src="/img/CS285.assets/image-20200320171532964.png" alt="image-20200320171532964" style="zoom: 33%;" />

**Q-learning with replay buffer**:

1. collect dataset  $$\{ \left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right) \}$$ using some policy , add it to $\mathcal B$. 
2. loop K     K=1 is common, larger K more  efficient
   1. Sample a batch  $$ \left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)$$  from $\mathcal B$ .      +**samples are no longer correlated**
   2. $$\phi \leftarrow \phi-\alpha  \sum_i \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)\left(Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\left[r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)\right]\right)$$ .    + **multiple samples in the batch (low-variance gradient)**   类似minibatch-SGD

 

下面解决第二个问题,  no gradient through target value 梯度问题. 

#### Q-Learning and Regression

看下Q-learning与 回归问题的联系

- Q-learning with replay buffer :  $$\phi \leftarrow \phi-\alpha  \sum_i \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)\left(Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\left[r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}^{\prime}} Q_{\phi}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)\right]\right)$$ . 

**one gradient step, moving target**,  该公式看起来就想是某个目标 r+maxQ的MSE的梯度. 下面的公式可以看的更明显.  只不过要学习的目标label依赖于当前model. 

- fitted Q-iteration:  $$\phi \leftarrow \arg \min _{\phi} \frac{1}{2} \sum_{i}\left\|Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{y}_{i}\right\|^{2}$$. 

**perfectly well-defined, stable regression**,  这个公式有明确的y, 所以只要不停执行 gradient step 就可以到收敛.  一般认为不需要每次都训练到收敛, 因为y在 model更新后, 值会发生变化, 而且那样效率比较低.  所以之前的收敛也可以理解为过拟合.  作者说在sample数据规模比较小的情况下, 一般不拟合到收敛.  不过这种方式的一个好处是 stable.  所以思路就是想要目标尽可能稳定, 但是model不一定要收敛到这个值.



#### Q-Learning with target networks

所以, 主要思路就是 要利用上buffer , 并且 要让 回归目标尽可能的 稳定. 

Q-learning with **replay buffer** and **target network**:
- save target network parameters: $\phi^{\prime} \leftarrow \phi$
- Loop  N : collect dataset $$\left\{\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)\right\}$$ using some policy, add it to $\mathcal{B}$  以下 **supervised regression**
  - Loop K  
    - Sample a batch  $$ \left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)$$  from $\mathcal B$ .  
    - 
      $$\phi \leftarrow \phi-\alpha \sum_{i} \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)\left(Q_{\phi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\left[r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \max _{\mathbf{a}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}_{i}^{\prime}, \mathbf{a}_{i}^{\prime}\right)\right]\right)$$.  

**targets don’t change in inner loop!**  



#### “Classic” deep Q-learning algorithm (DQN)

将上面的K=1的特例写成下面的形式 , 就是 经典的DQN :  

“Classic” deep Q-learning algorithm (DQN) ,  N仍然保留了
$$
\begin{aligned}
& \  \text{  1. take some action }\mathbf{a}_{i} \text{and observe }\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right), \text{add it to }\mathcal{B} \\
& \left.  \begin{array}{l}\text { 2. sample mini-batch }\left\{\mathrm{s}_{j}, \mathrm{a}_{j}, \mathrm{s}_{j}^{\prime}, r_{j}\right\} \text { from } \mathcal{B} \text { uniformly } \\ \text { 3. compute } y_{j}=r_{j}+\gamma \max _{\mathbf{a}_{j}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \mathbf{a}_{j}^{\prime}\right) \text { using target network } Q_{\phi^{\prime}} \\ \text { 4. } \phi \leftarrow \phi-\alpha \sum_{j} \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{j}, \mathbf{a}_{j}\right)\left(Q_{\phi}\left(\mathbf{s}_{j}, \mathbf{a}_{j}\right)-y_{j}\right)\end{array}  \right \} \quad K=1
\\
& \  \text{  5. update }\phi^{\prime}:\text{  copy }\phi \text{ every N steps , goto 1} 
\end{aligned}
$$


#### Alternative target network

下面说了一个情况, 就是上面的N=4的时候, 每次$y_i$都是从 $\phi$ 计算, 所以会有些延迟. 各个阶段的延迟不一样, 对后面时刻, 显然还用的是很久以前老的$\phi$, 延迟越来越大. 造成了一些不平衡不稳定的step.

考虑一个情况, y = r+Q' , 如果Q'是垃圾, 则只能从reward里面学,  如果更新了一次Q(即更新$\phi'$), 那Q则有一步是好的, 然后剩下的是垃圾, ..这样一直下去. 想象r=0直到最后一步goal, r>0 , 那学到的有点类似最短路径, r在每次flip$\phi$(即更新$\phi'$)的时候, 会反向传播一步, 离起点更近一步. 所以这个点就是有点bursty. 突变.

<img src="/img/CS285.assets/image-20200320195225766.png" alt="image-20200320195225766" style="zoom: 33%;" />

如果希望信息的传播更加平滑, 则可以利用一个更加连续的方式更新$\phi$, 不是在这些离散离得远的点.  可以每个时间点都更新, 但抑制其变化, 比较慢

**Popular alternative (similar to Polyak averaging):**   damped update

将上面算法的 5 改为 ,   $\phi'\leftarrow\tau\phi'+(1-\tau)\phi$ ,  $\tau=0.999$ works well , 然后令N=1

这个参数也是与具体问题相关, 是一个trick,  本质是在传播reward 信息的速度与 稳定性之间的一个权衡.



这里提了一个问题, 就是该怎么从buffer里面选sample , 是均匀随机还是啥.  一般都是用个fifo的大的队列来做buffer,然后随机均匀sample,  而且这个队列最大, 效果越好.  当然也可以带优先级.  **Prioritized experience replay**.



#### more general view

<img src="/img/CS285.assets/image-20200321003157163.png" alt="image-20200321003157163" style="zoom: 33%;" />



- Online Q-learning (last lecture): evict immediately, process 1, process 2, and process 3 all run at the same speed
- DQN: process 1 and process 3 run at the same speed, process 2 is slow
- Fitted Q-iteration: process 3 in the inner loop of process 2, which is in the inner loop of process 1





### Are the Q-values accurate?

**As predicted Q increases, so does the return**

当估计的Q值上升的时候，总体来说收益也呈一个上升的趋势；但是在训练片段中收益波动相当大，Q值虽然不能很好去拟合收益，但是波动相对小，相对光滑

<img src="/img/CS285.assets/image-20200321004455501.png" alt="image-20200321004455501" style="zoom:33%;" />

可以看出 , 随着训练次数的增多, total reward 还是会有巨大的波动 , 但平均Q是一直上升的.

<img src="/img/CS285.assets/image-20200321005940152.png" alt="image-20200321005940152" style="zoom:33%;" />

上图是说, DQN总是比实际回报(直线) 高估. 



#### Overestimation in Q-learning

- target value:  $$y_{j}=r_{j}+\gamma \max _{\mathbf{a}_{j}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \mathbf{a}_{j}^{\prime}\right)$$

  - this last term is the problem, 即取max的那部分有问题

- imagine we have two random variables: $X_{1}$ and $X_{2}$  

  - $E\left[\max \left(X_{1}, X_{2}\right)\right] \geq \max \left(E\left[X_{1}\right], E\left[X_{2}\right]\right)$.  前面是max的期望, 总体上是拔高的, 比右边, 想象两个分布就能得到这个结论, 所以就会放大正向的noise, 低估负的noise.
  - $Q_{\phi^{\prime}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)$ is not perfect - it looks "noisy"  时高时低
  - hence $$\max _{\mathbf{a}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)$$ **overestimates** the next value!
  - note that $$\max _{\mathbf{a}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)=Q_{\phi^{\prime}}\left(\mathbf{s}^{\prime}, \arg \max _{\mathbf{a}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}^{\prime}, \mathbf{a}^{\prime}\right)\right)$$  
                                    value also comes from $Q_{\phi^{\prime}}$  ;       action selected according to $Q_{\phi^{\prime}}$

  - **过高估计的行动 - 过高估计的值的传导** 

#### Double Q-learning

- 有两个Qfunction ,  解耦 errors in the value  from errors  in the action.   
- 这两个函数对应两个不同的策略, 可能会在同一个点上有不同的误差,  所以 maxQ 的误差可能不会一直的放大. 
- 双重Q学习的想法就是，不要使用同一个网络来确定行动和估计Q函数值。它使用两个网络，更新时候采用以下交错手段：

$$
Q_{\phi_A}(\mathbf{s},\mathbf{a})\leftarrow r+\gamma Q_{\phi_B}(\mathbf{s}',\arg\max_{\mathbf{a}'}Q_{\phi_A}(\mathbf{s}',\mathbf{a}')) \\
Q_{\phi_B}(\mathbf{s},\mathbf{a})\leftarrow r+\gamma Q_{\phi_A}(\mathbf{s}',\arg\max_{\mathbf{a}'}Q_{\phi_B}(\mathbf{s}',\mathbf{a}'))
$$

- 并不能完全解决 overestimate 问题, 但能缓解 . 
- 感觉能弄成N个

##### Double Q-learning in practice

just use current  and target network!

- standard Q-learning: $$y=r+\gamma Q_{\phi'}(\mathbf{s}',\arg\max_{\mathbf{a}'}Q_{\phi'}(\mathbf{s}',\mathbf{a}'))$$

- double Q-learning:  $$y=r+\gamma Q_{\phi'}(\mathbf{s}',\arg\max_{\mathbf{a}'}Q_\phi(\mathbf{s}',\mathbf{a}'))$$    argmaxQ的下标跟之前不一样

just use current network (not target network) to evaluate action still use target network to evaluate value!



#### Multi-step returns

- Q-learning  target : $$y_{j,t}=r_{j,t}+\gamma\max_{\mathbf{a}_{j,t+1}}Q_{\phi'}(\mathbf{s}_{j,t+1},\mathbf{a}_{j,t+1})$$ 
  - r only values that matter if Q  is bad!  前期希望多学到r
  - Q  are important if Q is good   Q准了以后,希望多学习这部分

- construct multi-step targets, like in actor-critic ,  **N-step return estimator**
  $$
  y_{j,t}=\sum_{t'=t}^{t+N-1}r_{j,t'}+\gamma^N\max_{\mathbf{a}_{j,t+N}}Q_{\phi'}(\mathbf{s}_{j,t+N},\mathbf{a}_{j,t+N})
  $$

  - less biased target values when Q-values are inaccurate  
  - typically faster learning, especially early on
  - only actually correct when learning on-policy

- 为什么只适用于 on-policy  , 因为  we need transitions $$\mathbf{s}_{j, t^{\prime}}, \mathbf{a}_{j, t^{\prime}}, \mathbf{s}_{j, t^{\prime}+1}$$  to come from $\pi$ for $$t^{\prime}-t<N-1$$   (not an issue when $N=1$ ) . 我们需要的sample都是来自同一策略的,才能评估当前的$Q_\pi$, 如果sample 来自各个不同的策略, 就不能加起来 .

- 如何修正:

  - ignore the problem : often works very well
  - cut the trace – dynamically choose N to get only on-policy data : works well when data mostly on-policy, and action space is small   就是在buffer里面,只找自己的policy , 发现不是自己的policy就停止; 其实打个标即可, 搞个多学习agent , 还可跟上面的交叉选action结合起来. 

  - importance sampling

- For more details, see: “Safe and efficient off-policy reinforcement learning.” Munos et al. ‘16



### Q-learning with continuous actions

连续动作空间, 怎么取max  

#### Option 1: **optimization**

- **gradient based optimization** (e.g., SGD) a bit slow in the inner loop 因为要在每个gradient step都要执行一次SGD
- action space typically low-dimensional – what about **stochastic optimization**? 下面介绍实现



##### Q-learning with stochastic optimization

Simple solution : 离散随机踩点

 $\max_\mathbf{a}Q(\mathbf{s},\mathbf{a})\approx\max\{Q(\mathbf{s},\mathbf{a}_1),\ldots,Q(\mathbf{s},\mathbf{a}_N)\}$  , $(\mathbf a_1, \dots,\mathbf a_N)$ , 	sampled from some distribution (e.g. uniform)

- **+ dead simple** 实现超简单
- **+ efficiently parallelizable**   可并行
- **- not very accurate**  能给一个不错的action

因为过拟合也是个问题, 所以不需要有太准确的max, 一些问题下也是ok的



More accurate solution:

- **cross-entropy method (CEM)**  **works OK, for up to about 40 dimensions**
  - simple iterative stochastic optimization
- CMA-ES Covariance Matrix Adaptation Evolutionary Strategies
  - substantially less simple iterative stochastic optimization



#### Option 2 : Easily maximizable Q-functions

了解即可. use function class that is easy to optimize  选取一个比较容易优化的函数簇来拟合我们的Q函数。

能找到准确优化的梯度, 但表达泛化能力等其他方面比较差劲.

一个方法 **NAF**: **N**ormalized **A**dvantage **F**unctions 

Q函数是关于 action 的二次函数
$$
Q_\phi(\mathbf{s},\mathbf{a})=-\frac{1}{2}(\mathbf{a}-\mu_\phi(\mathbf{s}))^\top P_\phi(\mathbf{s})(\mathbf{a}-\mu_\phi(\mathbf{s}))+V_\phi(\mathbf{s})
$$
然后利用二次函数的极值点是线性的, 方便求解

训练一个神经网络或者其他结构，输入状态，输出$(\mu, P, V)$，其中$P$是矩阵（可以用如低秩形式表示）

<img src="/img/CS285.assets/image-20200321040452256.png" alt="image-20200321040452256" style="zoom:33%;" />

$$\mu_\phi(\mathbf{s})=\arg\max_\mathbf{a}Q_\phi(\mathbf{s},\mathbf{a})$$   $$V_\phi(\mathbf{s})=\max_\mathbf{a}Q_\phi(\mathbf{s},\mathbf{a})$$



- **+ no change to algorithm**
- **\+ just as efficient as Q-learning**
-  **- loses representational power**



#### Option 3: learn an approximate maximizer

是 option1 和 option2 的折中

##### DDPG (Lillicrap et al., ICLR 2016)

**“deterministic” actor-critic (really approximate Q-learning)**



- $$\max _{\mathbf{a}} Q_{\phi}(\mathbf{s}, \mathbf{a})=Q_{\phi}\left(\mathbf{s}, \arg \max _{\mathbf{a}} Q_{\phi}(\mathbf{s}, \mathbf{a})\right)$$.  需要evaluate argmax
- idea: train another network $\mu_{\theta}(\mathbf{s})$ such that $$\mu_{\theta}(\mathbf{s}) \approx \arg \max _{\mathbf{a}} Q_{\phi}(\mathbf{s}, \mathbf{a})$$  , 用网络来拟合max
- how?  just solve $$\theta \leftarrow \arg \max _{\theta} Q_{\phi}\left(\mathbf{s}, \mu_{\theta}(\mathbf{s})\right)$$  是个最优化问题,找到$\theta$最大化Q,  $$\frac{d Q_{\phi}}{d \theta}=\frac{d \mathbf{a}}{d \theta} \frac{d Q_{\phi}}{d \mathbf{a}}$$
- new target $$y_{j}=r_{j}+\gamma Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \mu_{\theta}\left(\mathbf{s}_{j}^{\prime}\right)\right) \approx r_{j}+\gamma Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \arg \max _{\mathbf{a}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \mathbf{a}_{j}^{\prime}\right)\right)$$ 



**DDPG**:

1. take some action $$\mathbf{a}_{i}$$ and observe $$\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right)$$, add it to $$\mathcal{B} $$
2. sample mini-batch $$\left\{\mathbf{s}_{j}, \mathbf{a}_{j}, \mathbf{s}_{j}^{\prime}, r_{j}\right\}$$ from $$\mathcal{B}$$ uniformly
3. compute $$y_{j}=r_{j}+\gamma \max _{\mathbf{a}_{j}^{\prime}} Q_{\phi^{\prime}}\left(\mathbf{s}_{j}^{\prime}, \mu_{\theta^{\prime}}\left(\mathbf{s}_{j}^{\prime}\right)\right)$$ using target nets $$Q_{\phi^{\prime}}$$ and $$\mu_{\theta^{\prime}}$$
4. $$\phi \leftarrow \phi-\alpha \sum_{j} \frac{d Q_{\phi}}{d \phi}\left(\mathbf{s}_{j}, \mathbf{a}_{j}\right)\left(Q_{\phi}\left(\mathbf{s}_{j}, \mathbf{a}_{j}\right)-y_{j}\right)$$.
5. $$\theta \leftarrow \theta+\beta \sum_{j} \frac{d \mu}{d \theta}\left(\mathbf{s}_{j}\right) \frac{d Q_{\phi}}{d \mathbf{a}}\left(\mathbf{s}_{j}, \mathbf{a}\right)$$.
6. update $\phi^{\prime}$ and $\theta^{\prime}$ (e.g., Polyak averaging)



#### Advanced tips for Q-learning

<img src="/img/CS285.assets/image-20200321132634040.png" alt="image-20200321132634040" style="zoom:33%;" />

1. 上图显示了几个问题的几种不同Q学习的效果 (Schaul et al., 2015)。发现对于不同的问题，Q学习在有些问题上很可靠，在有些问题上波动很大，需要花很多力气来让Q学习稳定下来。因此发现几个能让Q学习比较可靠的问题来试验程序，譬如Pong和Breakout。如果这些例子上表现不好，那就说明程序有问题。

2. replay buffer的大小越大，Q学习的稳定性越好。我们往往会用到上百万个回放样本，那么内存上怎么处理是决定性的。建议图像使用uint8 (1字节无符号整型) 存储，然后在存储$$(\mathbf{s},\mathbf{a},r,\mathbf{s}')$$的时候不要重复存储同样的数据。

3. 训练的时候要耐心。DQN的收敛速度很慢，对于Atari游戏经常需要1000-4000万帧，训练GPU也得几个小时到一天的时间，这样才能看出能显著地比随机策略要来的好。

4. 在使用 $\epsilon$ 贪心等策略的时候，一开始把探索率调高一些，然后逐渐下降。

5. **Bellman error 可能会非常大**，因此可以**对梯度进行裁剪**（clipping，也就是设一个上下限），或者使用Huber损失 $$L(x)=\left\{\begin{array}{ll}x^2/2,& \vert x\vert\leq\delta\\\delta\vert x\vert-\delta^2/2,&\text{otherwise}\end{array}\right.$$ 进行光滑。

   <img src="/img/CS285.assets/image-20200321045500472.png" alt="image-20200321045500472" style="zoom: 33%;" />

6. 在实践中，使用Double Q学习很有帮助，改程序也很简单，而且**几乎没有任何坏处**。

7. 使用 N-step returns 也很有帮助，但是可能会带来一些问题。

8. 除了探索率外，学习率 (Learning Rate, 也就是步长) 也很重要，可以在一开始的时候把步长调大一点，然后逐渐降低，也可以使用诸如ADAM的自适应步长方案。

9. 多用几个随机种子试一试，有时候表现差异会很大。



#### Review

- Q-learning in practice 
  - Replay buffers
  - Target networks
- Generalized fitted Q-iteration
- Double Q-learning
- Multi-step Q-learning
- Q-learning with continuous actions
  - Random sampling
  - Analytic optimization
  - Second “actor” network



### Examples

##### Fitted Q-iteration in a latent space

<img src="/img/CS285.assets/image-20200321050258365.png" alt="image-20200321050258365" style="zoom:50%;" />



- “Autonomous reinforcement learning from raw visual data,” Lange & Riedmiller ‘12
- Q-learning on top of latent space learned with autoencoder
- Uses fitted Q-iteration
- Extra random trees for function approximation (but neural net for embedding)



##### Q-learning with convolutional networks

- “Human-level control through deep reinforcement learning,” Mnih et al. ‘13
- Q-learning with convolutional networks
- Uses **replay buffer** and **target network**
- One-step backup
- One gradient step
- Can be improved a lot with double Q-learning (and other tricks)



##### Q-learning with continuous actions

- “Continuous control with deep reinforcement learning,” Lillicrap et al. ‘15
- Continuous actions with **maximizer network**
- Uses replay buffer and target network (with Polyak averaging)
- One-step backup
- One gradient step per simulator step



##### Q-learning on a real robot

<img src="/img/CS285.assets/image-20200321050547650.png" alt="image-20200321050547650" style="zoom: 25%;" />

- “Robotic manipulation with deep reinforcement learning and ...,” Gu*, Holly*, et al. ‘17
- Continuous actions with **NAF (quadratic in actions)**
- Uses replay buffer and target network
- One-step backup
- Four gradient steps per simulator step for efficiency
- Parallelized across multiple robots



##### Large-scale Q-learning with continuous actions (QT-Opt)

<img src="/img/CS285.assets/image-20200321050801898.png" alt="image-20200321050801898" style="zoom:50%;" />



### Q-learning suggested readings

- Classic papers
  - Watkins. (1989). Learning from delayed rewards: introduces Q-learning
  - Riedmiller. (2005). Neural fitted Q-iteration: batch-mode Q-learning with neural networks

- Deep reinforcement learning Q-learning papers
  - Lange, Riedmiller. (2010). Deep auto-encoder neural networks in reinforcement learning: early image-based Q-learning method using autoencoders to construct embeddings
  - Mnih et al. (2013). Human-level control through deep reinforcement learning: Q- learning with convolutional networks for playing Atari.
  - Van Hasselt, Guez, Silver. (2015). Deep reinforcement learning with double Q-learning: a very effective trick to improve performance of deep Q-learning.
  - Lillicrap et al. (2016). Continuous control with deep reinforcement learning: continuous Q-learning with actor network for approximate maximization.
  - Gu, Lillicrap, Stuskever, L. (2016). Continuous deep Q-learning with model-based acceleration: continuous Q-learning with action-quadratic value functions.
  - Wang, Schaul, Hessel, van Hasselt, Lanctot, de Freitas (2016). Dueling network architectures for deep reinforcement learning: separates value and advantage estimation in Q-function.













































































