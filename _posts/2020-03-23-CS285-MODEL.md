---
layout:     post
title:      CS 285. Model-Based Reinforcement Learning
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

 

## Model-Based Reinforcement Learning

本课 model 未知 , 要 learn model

0.5 监督   1.0 DAgger  1.5 MPC

Model-based 的问题 , 过拟合, 有误差

计算 uncertainty 

Pomdp 的拟合



### Lecture

1. Basics of model-based RL: learn a model, use model for control 
   - Why does naïve approach not work?
   - The effect of **distributional shift** in model-based RL

2. **Uncertainty** in model-based RL

3. Model-based RL with complex observations 
4. Next time: **policy learning** with model-based RL

Goals:

- Understand how to build model-based RL algorithms
- Understand the important considerations for model-based RL
- Understand the tradeoffs between different model class choices



#### Why learn the model?

- If we knew $$f\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\mathbf{s}_{t+1}$$,  we could use the tools from last week.   (or  $$p\left(\mathbf{s}_{t+1} \vert \mathbf{s}_{t}, \mathbf{a}_{t}\right)$$  in the stochastic case)   

- So let's learn $$f\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$   from data, and then plan through it!     学习model, 拟合转移函数



model-based reinforcement learning version 0.5:  算法思路
$$
\begin{array}{l} \text { 1. run base policy } \pi_{0}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \text { (e.g., random policy) to collect } \mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_{i}\right\} \\ \text { 2. learn dynamics model } f(\mathbf{s}, \mathbf{a}) \text { to minimize } \sum_{i}\left\|f\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{s}_{i}^{\prime}\right\|^{2} \\ \text { 3. plan through } f(\mathbf{s}, \mathbf{a}) \text { to choose actions }\end{array}
$$


Does it work? Yes!  一些情况ok  对env足够了解的情况下

- Essentially how system identification works in **classical robotics**

- Some care should be taken to design a good base policy
- Particularly effective if we can hand-engineer a dynamics representation using our knowledge of physics, and fit just a few parameters



Does it work? No!   大多数情况

<img src="/img/CS285.assets/image-20200324112349409.png" alt="image-20200324112349409" style="zoom:33%;" />

上图, 目标:  去最高点.  

1. run policy 收集data ,  policy  假设是随机的.  红色线就是乱走. 

 	2. learn dynamics model,  model会预测能爬多高,  绿色线,  结论就是往右越来越高
 	3. plan through the model ,choose actions.  黄线, 最后掉下去.. $p_{\pi_f} $  就是实际的转移概率

$$
p_{\pi_f} (\mathbf s_t) \neq  p_{\pi_0} (\mathbf s_t)
$$

**Distribution mismatch** problem becomes exacerbated as we use more expressive model classes.

1.0  先用 DAgger 解决.



model-based reinforcement learning version  1.0  改进一下

$$
\begin{array}{l}  \text { 1. run base policy } \pi_{0}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \text { (e.g., random policy) to collect } \mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_{i}\right\} \\ \text { 2. learn dynamics model } f(\mathbf{s}, \mathbf{a}) \text { to minimize } \sum_{i}\left\|f\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{s}_{i}^{\prime}\right\|^{2} \\ \text { 3. plan through } f(\mathbf{s}, \mathbf{a}) \text { to choose actions } \\ \text { 4. execute those actions and add the resulting data }\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_{j}\right\} \text { to } \mathcal{D} , goto  2\end{array}
$$


##### What if we make a mistake?

<img src="/img/CS285.assets/image-20200324114731218.png" alt="image-20200324114731218" style="zoom:33%;" />

上图, 目标是沿着线走.  假设model有一点小错误, 会往稍微右边偏, 即model反馈的错误信息是,往左开一点点就能保持在直线上.  那策略就会让方向盘稍微往左, 如果稍微过于偏左,则会有小的误差.  这些非常小的误差,一直在累积. 最终可能造成大的错误.  1.0 的版本, append resulting data ,然后希望得到一个更好的model,但拟合的model不可能是 perfect. 就算要得到一个还不错的model,都可能需要很久才能学到.



改进 

在model开始偏的时候, 就是更多的关注plan的执行,如果感觉轨迹偏了, 立刻应该生成一个新的plan, correct it, 有点像每个timestep 都replan 

**Model predictive control (*MPC*)**

model-based reinforcement learning version 1.5:
$$
\begin{array}{l}  \text { 1. run base policy } \pi_{0}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \text { (e.g., random policy) to collect } \mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_{i}\right\} \\ \text { 2. learn dynamics model } f(\mathbf{s}, \mathbf{a}) \text { to minimize } \sum_{i}\left\|f\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{s}_{i}^{\prime}\right\|^{2} \\ \text { 3. plan through } f(\mathbf{s}, \mathbf{a}) \text { to choose actions } \\ \text { 4. execute the first planned action, observe resulting state } \mathbf{s}^{\prime}(\mathrm{MPC}) \\ \text { 5. append }\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right) \text { to dataset } \mathcal{D}.  \text{ goto 3, or goto 2 every N steps}  \end{array}
$$

因为replan之后,面对的(s,a)都是最新的, 每步都replan,则每个(s,a)都是准确的, 弥补了model的不准确. 

- The more you **replan**, the less perfect each individual plan needs to be
- Can use shorter horizons
- Even random sampling can often work well here!



#### A performance gap in model-based RL

<img src="/img/CS285.assets/image-20200324152737166.png" alt="image-20200324152737166" style="zoom: 33%;" />

上图说明,  model-based 一开始学的不错, 但很快到瓶颈,  model-free 开始慢, 但后期一直提升.

 Nagabandi, Kahn, Fearing, L. ICRA 2018



<img src="/img/CS285.assets/image-20200324154035211.png" alt="image-20200324154035211" style="zoom:33%;" />

对modelbase方法, 前期data非常少,  需要不过拟合. 然后也需要很高的表达性, 能够拟合到正确的model.  这样的要求神经网络很难做到. 特别是对model-base方法.  model-base 方法, 是planner 尝试找到 model的预测是高回报的plan,  如果model有错误, 那这些出错的高回报path, planner都会选择. 大意就是质疑model本身的准确性和过拟合.



<img src="/img/CS285.assets/image-20200324162727156.png" alt="image-20200324162727156" style="zoom:33%;" />

上图, 蓝色是一个表达能力很强的model , 黑色线是真实的reward的分布, 线性加一个高斯噪声. 可以看到model对sample点吻合的很好并且过拟合了, 然后后面就会一直尝试选择那个过拟合的估值高点.  对于高纬情况, 会有很多这些错误的点, 会造成, 就算跑了很多on-policy来采样数据改正这些错误, 可能需要超级多的时间来收集data再改正完所有的错误. 



#### How can uncertainty estimation help?

用不确定性来评估当前model出错的情况.   

<img src="/img/CS285.assets/image-20200324165558199.png" alt="image-20200324165558199" style="zoom:33%;" />

如图, 一个高斯过程.  红色是真值.  显然远离众多采样点的区域, 蓝色区域就扩大了很多, 说明都有可能出现在这些地方, 非常不确定. 

<img src="/img/CS285.assets/image-20200324170000713.png" alt="image-20200324170000713" style="zoom:33%;" />

上图, 要去悬崖上的星号点. 这个比较难, 如果不熟悉dynamics,很容易掉下去.  红色小圈,表示如果有一个比较certain的model, 那么就能比较明确的表示出要怎么走比较安全.  红色大圈表示一个highly uncertain 的model, 那应该比较小心, 每步走的小一些.   显然不确定性高就是方差大. 

expected reward under high-variance prediction is **very** low, even though mean is the same!
$$
p_{\pi_f} (\mathbf s_t) \neq  p_{\pi_0} (\mathbf s_t)
$$
同时还有 **Distribution mismatch** 的问题,  我们的model都是tested on data from same  distribution  as it trained on, 这样看是没有的mismatch问题的.  但因为model有非常多的机会犯错, 所以每次update model 都会改变  $\pi_f$, 所以可能要花费很久才能 converge.



model-based reinforcement learning version 1.5:
$$
\begin{array}{l}  \text { 1. run base policy } \pi_{0}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right) \text { (e.g., random policy) to collect } \mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_{i}\right\} \\ \text { 2. learn dynamics model } f(\mathbf{s}, \mathbf{a}) \text { to minimize } \sum_{i}\left\|f\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{s}_{i}^{\prime}\right\|^{2} \\ \text { 3. plan through } f(\mathbf{s}, \mathbf{a}) \text { to choose actions } \\ \text { 4. execute the first planned action, observe resulting state } \mathbf{s}^{\prime}(\mathrm{MPC}) \\ \text { 5. append }\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right) \text { to dataset } \mathcal{D}.  \text{ goto 3, or goto 2 every N steps}  \end{array}
$$


因为有uncertainty的问题, 所以第3步改进一下, 取期望好的:  only take actions for which we think we’ll get high reward in expectation (w.r.t. uncertain dynamics) . 这样就阻止了 planner  "exploiting" the model . The model will then adapt and get better.  model会在好的区域得到更好的certainty.    其实还是利用期望来降低方差.



#### a few caveats

- Need to explore to get better 仍然需要一些探索
- Expected value is not the same as pessimistic value  保守
- Expected value is not the same as optimistic value ... 积极探索
- but expected value is often a good start





### How can we have uncertainty-aware models?

怎么计算. 

##### Idea 1: use output entropy

<img src="/img/CS285.assets/image-20200324182336558.png" alt="image-20200324182336558" style="zoom:33%;" />

<img src="/img/CS285.assets/image-20200324182609638.png" alt="image-20200324182609638" style="zoom: 50%;" />

神经网络, 输出softmax .    或者 连续情况,  multivariate normal distribution.

是不是可以说如果输出是 high entropy , 则是非常 uncertainty.  信息量=不确定度

不行, 这不够.  因为model 可能overfit. 



<img src="/img/CS285.assets/image-20200324182922622.png" alt="image-20200324182922622" style="zoom:33%;" />

上图, 蓝线的对数据的 average error 差不多接近0 . 所以这个model的output 的entropy 非常low.  variance for the model is low.  但这个model的uncertainty 在某些区域是非常高的. 

有两种uncertainty:

1. aleatoric or statistical uncertainty ,  是关于数据的. 图示, 数据本身是有很多noise, 非常uncertainty,  但model是个好的model 
  <img src="/img/CS285.assets/image-20200324183904421.png" alt="image-20200324183904421" style="zoom:50%;" />

2. *epistemic* or *model* uncertainty .

*the model is certain about the data, but we are not certain about the model*

所以不能用model 的输出.



##### Idea 2: estimate model uncertainty

usually, we estimate   $$\arg \max _{\theta} \log p(\theta \vert  \mathcal{D})=\arg \max _{\theta} \log p(\mathcal{D} \vert \theta)$$

can we instead estimate $$p(\theta \vert  \mathcal{D})$$ ?  <=== the entropy of this tells us the model uncertainty!

如果estimator 指出, 只有这个theta能解释这个数据集, 则theta的model应该有很高的certianty.

predict according to: $$\int p\left(\mathbf{s}_{t+1} \vert \mathbf{s}_{t}, \mathbf{a}_{t}, \theta\right) p(\theta \vert \mathcal{D}) d \theta$$ , 这个实际上计算很难, 数学上分析还行

<img src="/img/CS285.assets/image-20200324185913272.png" alt="image-20200324185913272" style="zoom: 33%;" />

这个图略过没讲



#### Quick overview of Bayesian neural networks

<img src="/img/CS285.assets/image-20200324190105318.png" alt="image-20200324190105318" style="zoom: 33%;" />

左边是一般神经网络, 连接都是参数. 右边是贝叶斯神经网络. 连接线上都是一个distribution.

common approximation:

 $$p(\theta | \mathcal{D})=\prod_{i} p\left(\theta_{i} | \mathcal{D}\right) \\ p\left(\theta_{i} | \mathcal{D}\right)=\mathcal{N}\left(\mu_{i}, \sigma_{i}\right) $$

$\mu_i$: expected weight  ,  $\sigma_i$ : **uncertainty** about the weight

Blundell et al., Weight Uncertainty in Neural Networks Gal et al., Concrete Dropout



#### Bootstrap ensembles  集成

Train multiple models and see if they agree!  训练多个网络

<img src="/img/CS285.assets/image-20200324192429267.png" alt="image-20200324192429267" style="zoom: 33%;" />

*Posterior* distribution 后验概率. 

Dirac delta function 狄拉克delta函数 :  

$$
\delta (x)={\begin{cases}+\infty ,&x=0\\0,&x\neq 0\end{cases}}  \quad \int _{-\infty }^{\infty }\delta (x)\,dx=1.
$$

formally: $\quad p(\theta \vert  \mathcal{D}) \approx \frac{1}{N} \sum_{i} \delta\left(\theta_{i}\right)$

$$
\int p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}, \theta\right) p(\theta | \mathcal{D}) d \theta \, \approx \frac{1}{N} \sum_{i} p\left(\mathbf{s}_{t+1} | \mathbf{s}_{t}, \mathbf{a}_{t}, \theta_{i}\right)
$$

对所有model的产出求平均. 



How to train?

- Main idea: need to generate “independent” datasets to get “independent” models
- one way: $\theta_{i}$ is trained on $\mathcal{D}_{i},$ sampled with replacement from $\mathcal{D}$



##### Bootstrap ensembles in deep learning

- This basically works

- Very crude approximation, because the number of models is usually small (< 10)

- Resampling with replacement is usually unnecessary, because SGD and random initialization usually makes the models sufficiently independent



##### How to plan with uncertainty

- Before: $$J\left(\mathbf{a}_{1}, \ldots, \mathbf{a}_{H}\right)=\sum_{t=1}^{H} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right),$$ where $$\mathbf{s}_{t+1}=f\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$
- Now: $$J\left(\mathbf{a}_{1}, \ldots, \mathbf{a}_{H}\right)=\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{H} r\left(\mathbf{s}_{t, i}, \mathbf{a}_{t}\right),$$ where $$\mathbf{s}_{t+1, i}=f_{i}\left(\mathbf{s}_{t, i}, \mathbf{a}_{t}\right)$$
  -  $$f_i$$ : distribution over deterministic models
- In general, for candidate action sequence $$\mathbf{a}_{1}, \ldots, \mathbf{a}_{H}:$$
  - Step 1: sample $$\theta \sim p(\theta  \vert  \mathcal{D})$$
  - Step 2: at each time step $$t,$$ sample $$\mathbf{s}_{t+1} \sim p\left(\mathbf{s}_{t+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}, \theta\right)$$
  - Step 3: calculate $$R=\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$
  - Step 4: repeat steps 1 to 3 and accumulate the average reward

**Other options:** moment matching, more complex posterior estimation with BNNs, etc.



##### Example: model-based RL with ensembles

<img src="/img/CS285.assets/image-20200324230755695.png" alt="image-20200324230755695" style="zoom:50%;" />



##### More recent example: PDDM

<img src="/img/CS285.assets/image-20200324230955074.png" alt="image-20200324230955074" style="zoom:50%;" />

**Deep Dynamics Models for Learning Dexterous Manipulation.** Nagabandi et al. 2019



#### Further readings

- Deisenroth et al. PILCO: A Model-Based and Data-Efficient Approach to Policy Search.

Recent papers:

- Nagabandi et al. Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning.
- Chua et al. Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models.
- Feinberg et al. Model-Based Value Expansion for Efficient Model-Free Reinforcement Learning.
- Buckman et al. Sample-Efficient Reinforcement Learning with Stochastic **Ensemble** Value Expansion.



### What about complex observations?

<img src="/img/CS285.assets/image-20200324231129893.png" alt="image-20200324231129893" style="zoom:33%;" />

$$f\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=\mathbf{s}_{t+1}$$

What is hard about this?

- High dimensionality  维度
- Redundancy   状态数目的巨大与冗余
- Partial observability ,  只能看到observation produced by underlying  state. 不知道真实state

separately learn $$p\left(\mathbf{o}_{t}, \mathbf{s}_{t}\right) $$ (high-dimensional but not dynamic) and $$p\left(\mathbf{s}_{t+1} \vert \mathbf{s}_{t}, \mathbf{a}_{t}\right)$$ (low-dimensional but dynamic). 



#### State space (latent space) models

<img src="/img/CS285.assets/image-20200324232406221.png" alt="image-20200324232406221" style="zoom:33%;" />

需要学习3个model:

- $$p\left(\mathbf{o}_{t} \vert \mathbf{s}_{t}\right)$$ 	observation model
- $$p\left(\mathbf{s}_{t+1} \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right) \quad$$ dynamics model
- $$p\left(r_{t} \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right) \quad$$ reward model



**How to train?** 都做 maximum likelihood training 

p 连乘, 几率最大, 然后取log . 

- standard (fully observed) model: $$\max _{\phi} \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \log p_{\phi}\left(\mathbf{s}_{t+1, i}  \vert  \mathbf{s}_{t, i}, \mathbf{a}_{t, i}\right)$$

- latent space model: $$\max _{\phi} \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} E\left[\log p_{\phi}\left(\mathbf{s}_{t+1, i}  \vert  \mathbf{s}_{t, i}, \mathbf{a}_{t, i}\right)+\log p_{\phi}\left(\mathbf{o}_{t, i}  \vert  \mathbf{s}_{t, i}\right)\right]$$

  ​								expectation w.r.t. $$\left(\mathbf{s}_{t}, \mathbf{s}_{t+1}\right) \sim p\left(\mathbf{s}_{t}, \mathbf{s}_{t+1}  \vert  \mathbf{o}_{1: T}, \mathbf{a}_{1: T}\right)$$ 
  ​			显然, 思路是  $log p(o \vert s) log p(s'\vert s,a) $ 





#### Model-based RL with latent space models

下面继续近似. 

- learn approximate posterior $$q_{\psi}\left(\mathbf{s}_{t}  \vert  \mathbf{o}_{1: t}, \mathbf{a}_{1: t}\right) \quad$$   可以称为  "encoder"
- many other choices for approximate posterior:  其他的近似方式
  1. $$q_{\psi}\left(\mathbf{s}_{t}, \mathbf{s}_{t+1}  \vert  \mathbf{o}_{1: T}, \mathbf{a}_{1: T}\right) $$      full smoothing posterior 

     most accurate   most complicated
  
  2. $$q_{\psi}\left(\mathbf{s}_{t}  \vert  \mathbf{o}_{t} \right) $$       single-step encoder
  
        simplest     least accurate



先考虑简单情况, 确定性

$$q_{\psi}\left(\mathbf{s}_{t}  \vert  \mathbf{o}_{t} \right) $$ 

- simple special case: $$q\left(\mathbf{s}_{t}  \vert  \mathbf{o}_{t}\right)$$ is deterministic

- stochastic case requires variational inference (next week)

- $$q_{\psi}\left(\mathbf{s}_{t}  \vert  \mathbf{o}_{t}\right)=\delta\left(\mathbf{s}_{t}=g_{\psi}\left(\mathbf{o}_{t}\right)\right) \Rightarrow \mathbf{s}_{t}=g_{\psi}\left(\mathbf{o}_{t}\right) \quad$$ **deterministic encoder**  ;

  delta这里可以看做0-1函数

$$\max _{\phi, \psi} \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \log p_{\phi}\left(g_{\psi}\left(\mathbf{o}_{t+1, i}\right) | g_{\psi}\left(\mathbf{o}_{t, i}\right), \mathbf{a}_{t, i}\right)+\log p_{\phi}\left(\mathbf{o}_{t, i} | g_{\psi}\left(\mathbf{o}_{t, i}\right)\right)$$

**Everything is differentiable, can train with backprop**


$$
\max _{\phi, \psi} \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \underbrace{ \log p_{\phi}\left(g_{\psi}\left(\mathbf{o}_{t+1, i}\right) | g_{\psi}\left(\mathbf{o}_{t, i}\right), \mathbf{a}_{t, i}\right)}_{\text{latent space dynamics}}  + \underbrace{ \log p_{\phi}\left(\mathbf{o}_{t, i} | g_{\psi}\left(\mathbf{o}_{t, i}\right)\right)}_{\text{image reconstruction}}+ \underbrace{ \log p_{\phi}\left(r_{t, i} | g_{\psi}\left(\mathbf{o}_{t, i}\right)\right)}_{\text{reward model}}
$$
Many practical methods use a **stochastic encoder** to model uncertainty



model-based reinforcement learning with latent state:
1. run base policy $$\pi_{0}\left(\mathbf{a}_{t}  \vert  \mathbf{o}_{t}\right)$$ (e.g., random policy) to collect $$\mathcal{D}=\left\{\left(\mathbf{o}, \mathbf{a}, \mathbf{o}^{\prime}\right)_{i}\right\}$$
2. learn $$p_{\phi}\left(\mathbf{s}_{t+1}  \vert  \mathbf{s}_{t}, \mathbf{a}_{t}\right), p_{\phi}\left(r_{t}  \vert  \mathbf{s}_{t}\right), p\left(\mathbf{o}_{t}  \vert  \mathbf{s}_{t}\right), g_{\psi}\left(\mathbf{o}_{t}\right)$$
3. plan through the model to choose actions
4. execute the first planned action, observe resulting o' $(\mathrm{MPC})$
5. append (o, a, o') to dataset $\mathcal{D}$  ;  ==> goto 3   or  { goto 2   every N steps } 





#### Examples

<img src="/img/CS285.assets/image-20200325022921517.png" alt="image-20200325022921517" style="zoom: 33%;" />



<img src="/img/CS285.assets/image-20200325023210340.png" alt="image-20200325023210340" style="zoom:33%;" />



##### Learn directly in observation space



<img src="/img/CS285.assets/image-20200325023355934.png" alt="image-20200325023355934" style="zoom:33%;" />



##### Use predictions to complete tasks

<img src="/img/CS285.assets/image-20200325023433937.png" alt="image-20200325023433937" style="zoom:33%;" />



##### Task execution

<img src="/img/CS285.assets/image-20200325023534488.png" alt="image-20200325023534488" style="zoom:33%;" />





