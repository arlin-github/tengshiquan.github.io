---
layout:     post
title:      Monte Carlo CFR
subtitle:   Monte Carlo Sampling for Regret Minimization in Extensive Games
date:       2020-04-01 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-poker.jpg"
catalog: true
tags:
    - AI
    - CFR
    - Imperfect Information
    - Alberta
    - Texas
    - Game Theory

---



## Monte Carlo Sampling for Regret Minimization in Extensive Games

2009   from  ualberta

论文的数学结论很重要. 先看了算法 ,  todo 没有把定理以及证明看完





### 3 Monte Carlo CFR

MCCFR的关键是, 避免在每次迭代时遍历整个博弈树，同时让及时CFR在预期中保持不变。一般来说，我们希望每次迭代时限制终端历史的范围。令 $$\mathcal{Q}=\left\{Q_{1}, \ldots, Q_{r}\right\}$$, 是$Z$ 的子集的集合,  覆盖 $Z$. 我们将把这些子集中的一个称为 **块block**。在每次迭代时，我们将采样出一个块，并只考虑该块中的终端历史。  
令 $q_{j}>0$ ,  在当前迭代中选择 block $Q_{j}$ 的概率 ( $\sum_{j=1}^{r} q_{j}=1$ ).

The **sampled counterfactual value** when updating block $j$ is:  采样的虚拟收益,  有点类似于**重要性采样**

$$
\tilde{v}_{i}(\sigma, I | j)=\sum_{z \in Q_{j} \cap Z_{I}} \frac{1}{q(z)} u_{i}(z) \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z)  \tag{6}
$$

通过采样 $\mathcal{Q}$  定义了 sample-based CFR 算法。不是完整的游戏树遍历，而是采样其中一个block，然后只检查该block中的终端历史。

假设我们选择 $Q=\{Z\}$ , 即，一个块包含所有的终端历史, 并且 $q_{1}=1 .$ 在这种情况下，采样的CFR等于CFR，**vanilla CFR**.     
假设我们choose each block to include all terminal histories with the same sequence of chance outcomes, 因此，$q_j$ 是 sample chance 结果的概率,product of the probabilities  in the sampled sequence of chance outcomes (which cancels with these same probabilities in the definition of counterfactual value) ，我们就有了Zinkevich的**chance-sampled  CFR**。  

Sampled counterfactual value被设计成与预期的counterfactual value相匹配。我们在这里证明了这一点，然后在下一节中用这个事实证明算法的平均总regret的概率约束。



##### Lemma 1

$$
E_{j \sim q_{j}}[\tilde{v_{i}}(\sigma, I | j)]=v_{i}(\sigma, I)
$$

Proof:

$$
\begin{aligned}
E_{j \sim q_{j}}\left[\tilde{v}_{i}(\sigma, I | j)\right] &=\sum_{j} q_{j} \tilde{v}_{i}(\sigma, I | j)=\sum_{j} \sum_{z \in Q_{j} \cap Z_{i}} \frac{q_{j}}{q(z)} \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z) u_{i}(z) \\
&=\sum_{z \in Z_{I}} \frac{\sum_{j: z \in Q_{j}} q_{j}}{q(z)} \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z) u_{i}(z) \\
&=\sum_{z \in Z_{I}} \pi_{-i}^{\sigma}(z[I]) \pi^{\sigma}(z[I], z) u_{i}(z)=v_{i}(\sigma, I)
\end{aligned}
$$

得到下面的MCCFR算法。  
sample a block and for each information set that contains a prefix of a terminal history in the block ，  计算出每个动作的 **sampled immediate counterfactual regrets**，$$\tilde{r}(I, a)=\tilde{v}_{i}\left(\sigma_{(I \rightarrow a)}^{t}, I\right)-\tilde{v}_{i}\left(\sigma^{t}, I\right)$$ . player's strategy on the next iteration applies the regret-matching algorithm to the accumulated regrets

现在我们介绍一下这个家族中的两个具体成员，详细介绍一下如何有效地更新遗憾值的具体方法



<img src="/img/2020-04-21-MCCFR.assets/image-20200519235016760.png" alt="image-20200519235016760" style="zoom:50%;" />

图来自 Targeted CFR

看图很简单,  文字描述也还行,  看数学公式表述很累..



##### Outcome-Sampling MCCFR

each block contains a single terminal history, i.e., $\forall Q \in \mathcal{Q},\vert Q \vert =1$ .  每个block只含一个结局.  每次挑一个结局, 但怎么挑路径走到这个结局是有讲究的.



这段描述比较清楚

In Outcome Sampling, a single action is sampled at every information set. 对每个信息集sample一个action.   

- At an information set where **chance** acts, we sample according to the **fixed chance distribution**. 
- At an information set where the **opponent** acts, we sample according to the **opponent’s current strategy**. 
- At an information set where the **target player** acts, we sample *approximately* according to the **target player’s current strategy**. 

The complication is that we require some “**exploration**” to ensure that the reachability requirement is satisfied. For example, with some small probability ε we may sample an action according to the uniform distribution.

A single iteration of Outcome Sampling involves following a single trajectory from the root of the game tree to a terminal history $z$. 



在每次迭代中，我们对一个终端历史进行采样，并且只更新沿该历史的每个信息集。采样概率 $q_{j}$ 是在终端历史上的分布。我们使用sampling profile  $\sigma^{\prime}$来指定这个分布,  $q(z)=\pi^{\sigma^{\prime}}(z) $  .  注意，任何抽样策略的选择都会导致block概率$q(z)$ 的特定分布。As long as $\sigma_{i}^{\prime}(a \vert I)>\epsilon$, then there exists a $\delta>0$ such that $q(z)>\delta$, 保证公式6是正定的well-defined.

该算法的工作原理是通过使用策略$\sigma^{\prime}$来采样$z$，存储$\pi^{\sigma^{\prime}}(z)$.  然后向前遍历单个历史（计算每个玩家的概率， playing to reach each prefix of the history, $\pi_{i}^{\sigma}(h)$）和向后遍历（计算每个玩家的概率，playing the remaining actions of the history, $\pi_{i}^{\sigma}(h, z)$）。在后向遍历过程中，计算出每个访问过的信息集上的sampled counterfactual regrets（并加入到total regret中）。

$$
\tilde{r}(I, a)=\left\{\begin{array}{cl}
w_{I} \cdot(1-\sigma(a | z[I])) & \text { if }(z[I] a) \sqsubseteq z \\
-w_{I} \cdot \sigma(a | z[I]) & \text { otherwise }
\end{array}, \text { where } w_{I}=\frac{u_{i}(z) \pi_{-i}^{\sigma}(z) \pi_{i}^{\sigma}(z[I] a, z)}{\pi^{\sigma^{\prime}}(z)}\right. \tag{10}
$$




outcome-sampling MCCFR的一个优点是，如果我们的终端历史是根据对手的策略进行采样的，$\sigma_{-i}^{\prime}=\sigma_{-i}$ ,  那么更新就不再需要$\sigma_{-i}$的显式的知识，因为它抵消了 $\sigma_{-i}^{\prime}$ .  所以 $w_{I}$ 变成 $u_{i}(z) \pi_{i}^{\sigma}(z[I], z) / \pi_{i}^{\sigma^{\prime}}(z)$  . 因此，我们可以使用outcomesampling MCCFR进行在线 **online** regret minimizatio。我们必须选择我们自己的行动，以便使 $\sigma_{i}^{\prime} \approx \sigma_{i}^{t}$，但要保证**探索性exploration** $q_{j} \geq \delta>0$ 。通过平衡探索引起的遗憾和$\delta$ 的遗憾, （参见第4节），我们可以对平均总遗憾进行约束，只要事先知道游戏次数T。这有效地模仿了regret minimization in normal-form games[9]。建议采用方程10的另一种形式来实现 [10] . 



##### External-Sampling MCCFR

sample only the actions of the opponent and chance (those choices external to the player).  只sample外部因素. 相当于, 面对一次随机的外部的纯策略.

External Sampling is like Outcome Sampling at information sets where **chance** acts or where the **opponent** acts; i.e., we **sample** a single action according to the current strategy profile. External Sampling differs at information sets where the **target player** acts. At these information sets we **evaluate all actions**. Since we evaluate all of the target player actions we **meet the reachability requirement**.





对每个对手跟chance的纯策略都有一个对应的block $$Q_{\tau} \in \mathcal{Q}$$  , 即对应每个deterministic mapping $$\tau$$ from $$I \in \mathcal{I}_{c} \cup   \tilde{\mathcal{I}}_{N \backslash\{i\}}$$ to $$\vec{A}(I) .$$    block probabilities  $$f_{c}$$ and $$\sigma_{-i}$$ 的分布来分配的. 所以 $$q_{\tau}=\prod_{I \in \mathcal{I}_{c}} f_{c}(\tau(I) | I) \prod_{I \in \mathcal{I}_{N} \backslash\{i\}} \sigma_{-i}(\tau(I) | I)$$ .  那么这个块$$Q_{\tau}$$包含了所有终端历史$$ z$$ consistent with $$\tau$$ , 
就是说, if $$h a$$ is a prefix of $$z$$ with $$h \in I$$ for some $$I \in \mathcal{I}_{-i}$$ then $$\tau(I)=a$$. 

在实践中，我们实际上不会对$\tau$ 进行抽样，而是只根据需要对构成$\tau$ 的单个动作进行抽样。关键的启示是，这些块概率导致$q(z)=\pi_{-i}^{\sigma}(z)$ . 该算法对$i \in N$ 进行迭代，每次做一次深度优先的后序遍历post-order depth-first traversa，在每一个历史h处采样操作，其中$P(h) \neq i$（存储这些选择，因此在同一信息集中的所有h处都会采样相同的操作）。由于完美recall，在这个遍历过程中，它永远不能从同一信息集中访问一个以上的历史。对于每一个这样的访问过的信息集， sampled counterfactual regrets 都会计算出（并加到总遗憾值中）。


$$
\tilde{r}(I, a)=(1-\sigma(a | I)) \sum_{z \in Q \cap Z_{i}} u_{i}(z) \pi_{i}^{\sigma}(z[I] a, z)
$$

请注意，在遍历过程中，通过维护 weighted sum of the utilities of all terminal histories rooted at the current history，可以很容易地计算出总和。







### 4 Theoretical Analysis



We now present regret bounds for members of the MCCFR family, starting with an improved bound for vanilla CFR that depends more explicitly on the exact structure of the extensive game. Let $$\vec{a}_{i}$$ be a subsequence of a history such that it contains only player $$i$$ 's actions in that history, and let $$\vec{A}_{i}$$ be the set of all such player $$i$$ action subsequences. Let $$\mathcal{I}_{i}\left(\vec{a}_{i}\right)$$ be the set of all information sets where player $$i$$ 's action sequence up to that information set is $$\vec{a}_{i}$$. Define the $$M$$ -value for player $$i$$ of the game to be $$M_{i}=\sum_{\vec{a}_{i} \in \vec{A}_{i}} \sqrt{\left \vert \mathcal{I}_{i}(\vec{a})\right \vert } .$$ Note that $$\sqrt{\left \vert \mathcal{I}_{i}\right \vert } \leq M_{i} \leq\left \vert \mathcal{I}_{i}\right \vert $$ with both sides of this bound
being realized by some game. We can strengthen vanilla CFR's regret bound using this constant. which also appears in the bounds for the MCCFR variants.




##### Theorem 3

When using vanilla CFR for player i, $R_{i}^{T} \leq \Delta_{u, i} M_{i} \sqrt{\left \vert A_{i}\right \vert } / \sqrt{T}$

We now turn our attention to the MCCFR family of algorithms, for which we can provide probabilistic regret bounds. We begin with the most exciting result: showing that **external-sampling requires only a constant factor more iterations than vanilla CFR** (where the constant depends on the desired confidence in the bound)



##### Theorem 4 

For any $p \in(0,1]$, when using external-sampling MCCFR, with probability at least $1-p,$ average overall regret is bounded by, $R_{i}^{T} \leq\left(1+\frac{\sqrt{2}}{\sqrt{p}}\right) \Delta_{u, i} M_{i} \sqrt{\left \vert A_{i}\right \vert } / \sqrt{T}$

Although requiring the same order of iterations, note that external-sampling need only traverse a fraction of the tree on each iteration. For balanced games where players make roughly equal numbers of decisions, the iteration cost of external-sampling is $O(\sqrt{ \vert H \vert }),$ while vanilla CFR is $O( \vert H \vert )$ meaning **external-sampling MCCFR requires asymptotically less time to compute an approximate equilibrium than vanilla CFR** (and consequently chance-sampling CFR, which is identical to vanilla CFR in the absence of chance nodes.



##### Theorem 5

For any $p \in(0,1]$, when using outcome-sampling $M C C F R$ where $\forall z \in Z$ either $\pi_{-i}^{\sigma}(z)=0$ or $q(z) \geq \delta>0$ at every timestep, with probability $1-p,$ average overall regret is bounded by $R_{i}^{T} \leq\left(1+\frac{\sqrt{2}}{\sqrt{p}}\right)\left(\frac{1}{\delta}\right) \Delta_{u, i} M_{i} \sqrt{\left \vert A_{i}\right \vert } / \sqrt{T}$








## Reference

Monte Carlo Sampling for Regret Minimization in Extensive Games

Targeted CFR











