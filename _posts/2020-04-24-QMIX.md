---
layout:     post
title:      QMIX
subtitle:   QMIX, Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning
date:       2020-04-24 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-dice.jpg"
catalog: true
tags:
    - AI

---



## QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning



### Abstract

Learning **joint action-values** conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear.  学习联合Q函数, 但如何分解出个体的最佳策略还不清楚.   
提出解决方案: Qmix ,  train decentralised policies in a centralised end-to-end fashion. QMIX采用了一个mixing网络，将 joint action-values 估计为每个代理值的单调组合。我们通过在混合网络中使用非负权重，在结构上强制执行联合行动值在每个代理值中是单调的，这保证了集中式策略和分散式策略之间的一致性。

 提出一个benchmark:  StarCraft Multi-Agent Challenge (SMAC) 



### Introduction

由于部分可观察性和/或通信限制，需要学习非中心化策略 decentralised policies，只依赖于每个代理的本地行动-观察历史。分散式策略也自然地缓解了联合行动空间随着代理数量的增加而呈指数级增长的问题，这往往使传统的单代理RL方法的应用变得不切实际。

去中心化的策略通常可以在仿真或实验室环境中以集中的方式学习 ，并且消除了代理之间的通信限制。**centralised training with decentralised execution**

这通常可以获得额外的状态信息，否则就会被代理隐藏起来，并且消除了代理之间的通信限制。

挑战是怎么对待 联合Q函数。该Q值基于全局状态和联合动作, 怎么去学习, 学到了, 怎么拆解为个体的策略.  

最简单的方法是,  Independent Q-learning (IQL) , 各学各的, 这样外部环境是非稳定态的.   这种方法不能明确地表示代理之间的交互，也可能不收敛，因为每个代理的学习都被其它代理的学习和探索干扰。

另一个极端, 学习一个完全的联合Q函数,   用actor-critic框架来指导去中心化政策的优化.  **counterfactual multi-agent (COMA)** policy gradients 的做法。然而，这需要on-policy学习，采样低效;   agent超过一定数量, 也不可行. 

两个极端之间，可以学习一个 打折的联合Q ，这是价值分解网络 **value decomposition networks（VDN）**所采取的一种方法。通过将$Q_{tot}$表示为单个agent $Q_a$的总和，个体策略就是基于个体$Q_a$的贪婪算法。然而，VDN这种联合Q的表示太简单了，并且忽略了训练过程中的任何额外状态信息。因为每个agent都只看自己的state, 所以全局的state信息没有利用上. 

QMIX，和VDN一样，介于IQL和COMA的两个极端之间，只是基于个体Q的函数更加丰富.方法的关键是，提取个体策略不需要VDN的完全因子化full factorisation。相反，我们只需要确保在$Q_{tot}$上执行的全局argmax操作与在每个$Q_a$上执行的一组单独的argmax操作产生相同的结果。	 只要方向一致就行 ,单调的. 这种非线性可以用NN来拟合.


$$
\frac{\partial Q_{t o t}}{\partial Q_{a}} \geq 0, \forall a
$$
QMIX 是各个$Q_a$网络组合的混合网络. 但不是简单相加, 是一种复杂非线性, 但要保证 中心策略和 个体策略的一致性consistency。同时，加了个限制, 混合网络的参数只能是正数. (这里假设在现实中应该是不成立的, 特别agent有不同的类型的, 有可能最好的收益是需要某些个体做出牺牲的)  使用hypernetworks condition the weights of the mixing network on the state。  
因此，QMIX 可以用 个体的Q 来描述 复杂的中心策略, 在代理数量上有很好的扩展性，并允许通过单个argmax操作来轻松地提取个体的策略。



### Related Work

focus on the **fully-cooperative setting** in which all agents must maximise a joint reward signal in the paradigm of **centralised training and decentralised execution**.



#### Independent Learners

1. IQL : Q-learning   -> MAS
2. DQN  -> MAS ,  instability arising from the non-stationarity of the environment induced by simultaneously learning and exploring agents
3. addresses the issue of non-stationarity when using an experience replay with independent learners to some extent
4. Lauer and Riedmiller (2000) 证明了 tabular 以及确定性 env 下, 忽略其他agent的更新导致的Q估值下降, Q-learning 可以收敛到唯一 optimal joint-action, 如果存在的话.
5. ~~Matignon et al. (2007) introduce Hysteretic Q-learning , 对Q估值降低的时候使用较小的学习率, 对随机env 以及 多个optimal 更加robust.~~ 
6. ~~Hysteretic Q-learning  -> MAS,  Deep Decentralized Multi-task Multi-Agent RL under Partial Observability~~
7.  Panait（2008）引入了**Leniency**，它以一定概率忽略了在训练过程中Q估值下降的更新。
8. Wei和Luke（2016）表明，Leniency在合作随机博弈中优于Hysteretic Q-learning
9. Palmer（2018）将Leniency 扩展到深度RL设置中，在有两个代理, 确定的和随机的 gridworlds上，显示出优于Hysteretic和 IQL 的优势。
10. Palmer等人（2019）维护了一个学习区间，其下限约为所有代理协作时获得的最小累积奖励。他们在递减Q值估计时使用这个区间来区分 代理之间的不协作 和 环境的随机性，而Hysteretic和Lenient学习者并不区分这两者。
11. Lu和Amato（2019）利用分布式 Distributional RL（Bellemare等人，2017年）与Hysteretic Q-learning结合使用，也为了区分不协调和随机性，并认为他们的方法比上述方法更稳定，对超参数更稳健。

所有这些方法都没有利用集中化训练机制中可用的额外状态信息，也没有尝试学习联合动作值函数。



#### Centralised Execution

集中式学习联合Q函数自动地解决了协作问题，并避免了非稳定性问题。 但联合动作空间随agent数量指数增长. 












## Reference

QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning



















