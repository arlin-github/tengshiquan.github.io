---
layout:     post
title:      Learning Without State Estimation in POMDPs
subtitle:   Note on "Learning Without State Estimation in Partially Observable Markovian Decision Processes"
date:       2019-12-31 12:00:00
author:     "tengshiquan"
header-img: ""
catalog: true
tags:
    - ai
    - Q-learning
    - POMDP
---



# Note on "Learning Without State Estimation in POMDPs"

老论文笔记

部分可观察的MDP.  通过一个case解释了 POMDP的一些特性. 

有Q-learning 收敛的证明



#### INTRO

**POMDPs** :  partially observable MDPs,  是一类很有用的 **non-Markovian** decision processes.

all of the elegant theory of RL is limited to MDPs.

The word **state** is used here as in control theory to mean <u>all the information necessary to make the prediction of the future states of the environment dependent only on the current state and the future actions and independent of the past states</u>.

**N-MDPs** : non-Markovian decision processes , has **hidden state**.

将MDP的RL算法应用到N-MDP问题 , 有些问题可以运行的很好, 但没有相关理论, 也不知道这些问题的共同特征. 

论文介绍

1. 为什么扩展RL到N-MDP很困难. 
2. TD, Q-learing 可以扩展到一类 N-MDPs
3. 提出一种新框架, 无需 state-estimation,  通过V(X)替换V(s)



#### PREVIOUS APPROACHES

以前解决N-MDP的方法,都是结合 state-estimation 与learning control. 



policies, that can be called "**memoryless**" since action decisions are made solely on this basis of the agent's current sensation.



#### PROBLEM FORMULATION

A **stationary** policy is a mapping from states to actions , that is independent of time.

For every finite MDP there exists a stationary deterministic policy $\pi^*$ that is **optimal**. 

Non-observable MDP is in state s,the *sensor reading* or **observation** is $X$ with fixed probability $P(X\vert s)$.



下面,会用一个 POMDP的子类, 用来当证明的反例. 该类POMDP特性是, 一个observation x可能对应多个state,但每个s只有一个x. 图示中会用椭圆把同属于一个observation的s都圈起来. 



##### STOCHASTIC POLICIES

<img src="/img/2019-12-31-POMDP.assets/image-20191231031236961.png" alt="image-20191231031236961" style="zoom: 33%;" />

图1, 只有一个state, 两个action, 两个observation.



**Fact 1**: 在POMDP中, 将MDP的两个状态合并为一个observation , 可能导致return减少很多. 

对图1, 显然, 如果是MDP, 则最优策略return是 $\frac{R}{1-\gamma}$.   对POMDP, 只能有两种 deterministic 策略, action要么一直选A, 要么B, return都是 $R  - \frac{\gamma R}{1-\gamma} = R - \gamma R - \gamma^2R - \dots$ , 所以相比MDP,减少了很多



**Fact 2**: 在POMDP中, 最好的stationary stochastic 策略比 最好的stationary deterministic策略 要好.

对图1 , 一个随机策略, 两个动作A,B各0.5几率, 则每步的期望都是0 , 显然比上面的要好.



**Fact3**:  POMDP最好的stationary stochastic策略 比 MDP的最优策略 差. 

图1, 最优MDPreturn是 $\frac{R}{1-\gamma}$



**Fact 4**: POMDP的最优策略可能是 non-stationary. 

图1, 一个交替选择A,B的non-stationary策略, 可能第一步得-R, 然后就一直是R, 明显比 POMDP的任何 stationary策略好. 

但寻找non-stationary策略非常的费事.  此外, non-stationary策略需要过去的memory,不是 memory-less的.  如果可以利用过去的memory, 即trajactory, 则memory-based的各种 state-estimation技术可以利用,  这时non-stationary策略不一定还有优势. 



下面用符号$\Pi$表示 POMDP的stochastic策略的空间.  该空间与 MDP的stochastic策略空间通常不一样.



**Assumption 1**: 论文下面假定所有的POMDP都有一个特性: 对应的MDP都是**ergodic**的, 即每个状态都是可以遍历到的. 

对图1的case, 可以加一个$\epsilon >0$的状态转移概率 对action A从1a到1b, B从1b到1a, 就可以变成 ergodic.  当$\epsilon$足够小, 对return的影响可以忽略不计.



#### EVALUATING A FIXED POLICY

$$
V^\pi(X) = \sum_{s \in \mathcal S} P^\pi (s \vert X) V^\pi(s) \tag{2}
$$

$P^\pi(s \vert X)$ : asymptotic **occupancy** probability distribution. 当POMDP的observation是X时,对应的MDP的state是s的几率. 



##### WHAT TD(0) LEARN

TD(0) : iterative stochastic approximation algorithm to **evaluate** a policy, not require knowledge of the MDPs transition probabilities.

$$
V_{k+1}(s_k) = (1-\alpha(s_k))V_k(s_k) + \alpha(s_k)(R_k + \gamma V_k(s_{k+1}))
$$

对Markov问题,  $\alpha$符合一定条件, TD(0)会收敛到 $V^\pi(s)$. 

对non-Markov问题, 

$$
V_{k+1}(X_k) = (1-\alpha(X_k))V_k(X_k) + \alpha(X_k)(R_k + \gamma V_k(X_{k+1}))
$$

**Theorem 1**:  对上面定义的POMDP,  TD(0) 最终会收敛到下面的公式:

$$
V(X) = \sum_{s \in \mathcal S} P^\pi(s \vert X)\left [R^\pi(s) + \gamma\sum_{X' \in \mathcal X}P^\pi(s, X')V(X') \right]  \tag{3}
$$

where $P^\pi(s, X') = \sum_{s'}(P^\pi(s,s')P(X' \vert s'))$

**Proof**:  考虑一个 semi-batch (batch固定大小) TD(0) , 收集了M步 value fauntion的改变,然后还没把这些改变更新到参数上面去. 如果让M足够大, 对于MDP里面的每个state都可以被sample到, with a frequency that matches $P^\pi(s \vert X)$ to within $\epsilon$ with probability $1-\epsilon$ .

证明见最后

semi-batch 的结论可以扩展到 online.





<img src="/img/2019-12-31-POMDP.assets/image-20191231155406012.png" alt="image-20191231155406012" style="zoom:50%;" />

通常, TD(0)学到的公式3 与上面定义的真值V 公式2 是不一样的.

图2, 有6个state, 5个observation,  对1-step Markov 算法,  o2, o3 都指向 o4, 又因为该步r=0, TD学到两个o的V是一样的. 但实际上的真值  V(o2) > V(o3)



#### OPTIMAL CONTROL

##### WHAT Q-LEARNING LEARN

One of the big advantages of Q-learning is that it separates **exploration** from **control**. In short , the control policy followed during learning has no impact on asymptotic convergence as long as every action gets executed in every state infinitely often.  Qlearning 将探索与control隔离, 即, 只要每个动作在每个状态下执行的次数足够多, 学习过程中所遵循的控制策略对渐近收敛性没有影响. 

POMPDs,没有这个优点, 因为在学习的过程中, control policy会影响occupancy probabilities. 

为了分析, 考虑一个特例, 应用Q-learning结合一个 fixed stationary **persistent excitation** learning policy, 即一个策略, 每个state下每个action都是非零概率, 然后对应的Markov chain 是 ergodic的. 



**Theorem 2**:  对上面定义的POMDP,  如果在学习中,遵循一个persistent excitation 策略,  Q-learning 会收敛到下面公式:

$$
Q(X,a) = \sum_{s \in \mathcal S} P^\pi(s \vert X, a)\left [R^\pi(s) + \gamma\sum_{X' \in \mathcal X}P^\pi(s, X') \max_{a' \in \mathcal A} Q(X', a) \right]  \tag{4}
$$

where $P^\pi(s, X') $ 是asymptotic probability. 

公式4 跟公式3有一样的问题, 就是 Q-learning 也是基于 1-step Markov 假设的. 



##### WHAT IS AN OPTIMAL POLICY

在discounted MDP里面, 只要使所有的state的value同时最大, 就是最优策略;  但在discounted POMDP里面, 不适用. 



**Fact 5**: 对之前定义的POMDP的特殊子类,  不一定存在一个 stationary 策略, 能够同时对每个observation都取到最大值.



<img src="/img/2019-12-31-POMDP.assets/image-20191231200538315.png" alt="image-20191231200538315" style="zoom:50%;" /> 

图3 显示, 通过在o1处增加A的几率,来增加 o1的value, 必然会降低o2的value.  因为有内在的相关性.



**Fact 6**: 对之前定义的POMDP的特殊子类,  不一定存在一个 stationary 策略, 能够满足对应的MDP对每个state都取到最大值.



<img src="/img/2019-12-31-POMDP.assets/image-20191231201209370.png" alt="image-20191231201209370" style="zoom:50%;" />

图4 显示,  在o2没有任何办法, 可以使得 2a,2b都取得最大值



对discounted POMDPs, 定义最优策略的困难可以用公式2来解释.  改变策略不仅仅改变对应MDP的state value, 对每个observation, 还改变了state的 occupancy distribution. 这两个效果使得, 用一个observation的value去还换取另外一个observation的value是可能的.



##### DEFINING AN OPTIMAL POLICY

**Discounted Payoff POMDPs:**  $ \pi^* = \arg\max_{\pi \in \Pi}\sum_{X \in \mathcal X} P^\pi(X) V^\pi(X)$  

**Average Payoff POMDPs:**  平均回报 : $\Lambda^\pi = \lim_{N \to \infty} E^\pi \left[ \frac{\sum_{t=0}^N R_t}{N}  \right ]$

对所有 ergodic MDP的 stationary 策略, 该值对于不依赖于起始状态.  

平均回报是个有界标量, 如果一个策略能使得该值最大, 则是最优策略.  $\pi^* = \arg\max_{\pi \in \Pi} \Lambda^\pi$

在average payoff MDP中,   **relative** value funtion 定义为: $V^\pi(s) = \sum_{a \in \mathcal A}Pr(a\vert \pi, s) \left[ (R^a(s) - \Lambda^\pi) + \sum_{s' \in \mathcal S}P^a(s,s')V^\pi(s') \right]$



**Fact 7:**  对POMDP, 有 $\sum_{X \in \mathcal X} P^\pi(X) V^\pi(X) = \frac{\Lambda^\pi}{1 - \gamma}$ ,  最大化 Discounted Payoff 等价于 最大化每一步的average payoff. 





#### DISCUSSION

In this paper, developed a new framework for learn ing without state estimation in POMDPs by including stochastic policies in the search space and by defining the value of an observation under a given policy. 





#### Convergence of semi-batch Q-learning

Let $M_{k}(X, a)$ be the number of times action $a$ was executed in observation $X$ within the $k^{t h}$ batch of size $M$,  $n_{k}(s \vert X, a)$ be the number of times the actual underlying state was $s$ when the observation-action pair was $(X, a),$ and $n\left(X, X^{\prime} \vert a\right)$ be the number of times a transition took place from observation $X$ to observation $X^{\prime}$ given that action $a$ was executed. The persistent excitation policy followed by Q-learning during learning is denoted $\pi .$ Then the Q-value of $(X, a)$ after the $k^{t h}$ batch is given by:


$$
\begin{aligned}
Q_{k+1}(X, a)=& \left(1-M_{k}(X, a) \alpha_{k}(X, a)\right) Q_{k}(X, a) \\
&+M_{k}(X, a) \alpha_{k}(X, a)  \left[\sum_{s} \frac{n(s | X, a)}{M_{k}(X, a)} r_{k}^{a}(s) +\gamma \sum_{X^{\prime}} \frac{n\left(X, X^{\prime} | a\right)}{M_{k}(X, a)} \max _{a^{\prime}} Q_{k}\left(X^{\prime}, a^{\prime}\right)\right]
\end{aligned}
$$

where $r_{k}^{a}(s)$ is the sample average of the actual payoffs received on executing action a in state $s$ in the $k^{t h}$ batch. Assume $Q(X, a)$ is the solution to Equation 4 
Let

$$
\begin{aligned}
F_{k}(X, a)=& \sum_{s} \frac{n(s | X, a)}{M_{k}(X, a)} r_{k}^{a}(s) \\
&+\gamma \sum_{X^{\prime}} \frac{n\left(X, X^{\prime} | a\right)}{M_{k}(X, a)} \max _{a^{\prime}} Q_{k}\left(X^{\prime}, a^{\prime}\right) \\
&-\bar{Q}(X, a)
\end{aligned}
$$

then, if $V_{k}(X)=\max _{a} Q_{k}(X, a)$ and $\bar{V}(X)=\max _{a} \bar{Q}(X, a)$

$$
\begin{aligned}
F_{k}(X, a)=& \gamma \sum_{X^{\prime}} \frac{n\left(X, X^{\prime} | a\right)}{M_{k}(X, a)}\left[V_{k}\left(X^{\prime}\right)-\bar{V}\left(X^{\prime}\right)\right] \\
&+\sum_{s}\left(\frac{n(s | X, a)}{M_{k}(X, a)} r_{k}^{a}(s)-P^{\pi}(s | X, a) R^{a}(s)\right) \\
&+\gamma \sum_{X^{\prime}}\left(\left(\frac{n\left(X, X^{\prime} | a\right)}{M_{k}(X, a)}-P^{a}\left(X, X^{\prime} | \pi\right)\right) \bar{V}\left(X^{\prime}\right)\right)
\end{aligned}
$$

where

$$
P^{a}\left(X, X^{\prime} | \pi\right)=\sum_{s} P^{\pi}(s | X, a)\left[\sum_{s^{\prime}}\left(P^{a}\left(s, s^{\prime}\right) P\left(X^{\prime} | s^{\prime}\right)\right)\right]
$$

The expected value of $F_{k}(X, a)$ can be bounded by

$$
\begin{aligned}
\left\|E\left\{F_{k}(X, a)\right\}\right\|  \leq &\gamma\left\|V_{k}-\bar{V}\right\| \\
&+\left\|E\left\{\sum_{s}\left(\frac{n(s | X, a)}{M_{k}(X, a)}-P^{\pi}(s | X, a)\right) R^{a}(s)\right\}\right\| \\ 
&\left. + \gamma \| \sum_{X^{\prime}} E\left\{\left(\frac{n\left(X, X^{\prime} | a\right)}{M_{k}(X, a)}-P^{a}\left(X, X^{\prime} | \pi\right)\right) \bar{V}\left(X^{\prime}\right)\right)\right\} \|  \\
& \leq \gamma\left\|V_{k}-\bar{V}\right\|+C \epsilon_{k}^{M}
\end{aligned}
$$

where $\epsilon_{k}^{M}$ is the larger of

$$
\begin{aligned}
&\max _{(s, X, a)}\left|E\left\{\frac{n(s | X, a)}{M_{k}(X, a)}\right\}-P^{\pi}(s | X, a)\right|, \text { and }\\
&\max _{\left(X, X^{\prime}, a\right)}\left|E\left\{\left(\frac{n\left(X, X^{\prime} | a\right)}{M_{k}(X, a)}\right\}-P^{a}\left(X, X^{\prime} | \pi\right)\right)\right|
\end{aligned}
$$

For any $\epsilon>0, \exists M_{\epsilon}$ such that $\epsilon_{k}^{M_{\epsilon}}<\epsilon$ (because the sample probabilities converge with probability one). The variance of $F_{k}(X)$ can also be shown to be bounded because the variance of the sample probabilities is bounded (everything else is similar to standard Q-learning for MDPs). Therefore by Theorem 1 of Jaakkola et al. $(1994),$ for any $\epsilon>0,$ with probability $1-\epsilon$, $\quad Q_{k}(X, a) \rightarrow Q_{\infty}(X, a), \quad$ where
$\left\vert Q_{\infty}(X, a)-\bar{Q}(X, a) \right \vert \leq \bar{C} \epsilon$ .Therefore, semi-batch Q-learning converges with probability one.



Convergence of semi-batch $\mathbf{T D}(0)$ 类似

