---
layout:     post
title:      CS 285. Model-Based Policy Learning
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

 

## Model-Based Policy Learning

之前都是 model-based 没用policy .

### Lecture

Last time: model-based reinforcement learning **without** policies

Today: model-based reinforcement learning of policies

- Learning global policies
- Learning local policies  在一个region可行

Combining local policies into global policies  

- Guided policy search
- Policy distillation

Goals:

- Understand how and why we should **use models to learn policies**
- Understand global and local policy learning

- Understand how **local policies can be merged via supervised learning into a global policy**





下面开始利用 model train policy.

#### Backpropagate directly into the policy?

<img src="/img/CS285.assets/image-20200325142230062.png" alt="image-20200325142230062" style="zoom:33%;" />



model-based reinforcement learning version 2.0:
1. run base policy $$\pi_{0}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)$$ (e.g., random policy) to collect $$\mathcal{D}=\left\{\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)_{i}\right\}$$
2. learn dynamics model $$f(\mathbf{s}, \mathbf{a})$$ to minimize $$\sum_{i}\left  \Vert f\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)-\mathbf{s}_{i}^{\prime}\right \Vert ^{2}$$
3. backpropagate through $$f(\mathbf{s}, \mathbf{a})$$ into the policy to optimize $$\pi_{\theta}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)$$  这里不再plan
4. run $$\pi_{\theta}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right),$$ appending the visited tuples $$\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}\right)$$ to $$\mathcal{D}$$

但这个策略有 梯度爆炸或者消失的问题.  因为步数太多. 

- Similar parameter sensitivity problems as shooting methods
  - But no longer have convenient second order LQR-like method, because policy parameters couple all the time steps, so no dynamic programming

- Similar problems to training long RNNs with BPTT
  - **Vanishing and exploding gradients**
  - Unlike LSTM, we can’t just “choose” a simple dynamics, dynamics are chosen by nature



##### What’s the solution?

- Use **derivative-free (“model-free”)** RL algorithms, with the model used to generate synthetic samples
  - Seems weirdly backwards  走回头路
  - Actually works very well  

  - Essentially “model-based acceleration” for model-free RL

- Use simpler policies than neural nets
  - LQR with learned models (LQR-FLM – **F**itted **L**ocal **M**odels) 
  - Train **local** policies to solve simple tasks
  - Combine them into **global** policies via supervised learning



#### Model-free optimization with a model

- Policy gradient:   不需要链式法则, 不需要BP through time

  $$
  \quad \nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right) \hat{Q}_{i, t}^{\pi}
  $$
  
- Backprop (pathwise) gradient:  poor numerical conditioning  

  $$
  \quad \nabla_{\theta} J(\theta)=\sum_{t=1}^{T} \frac{d r_{t}}{d \mathbf{s}_{t}} \prod_{t^{\prime}=2}^{t} \frac{d \mathbf{s}_{t^{\prime}}}{d \mathbf{a}_{t^{\prime}-1}} \frac{d \mathbf{a}_{t^{\prime}-1}}{d \mathbf{s}_{t^{\prime}-1}}
  $$
  



- Policy gradient might be more *stable* (if enough samples are used) because it does **not** require **multiplying many Jacobians** 
  - 代价是 高方差, 但sample多可以多少解决
- See a recent analysis here:
  - Parmas et al. ‘18: PIPP: Flexible Model-Based Policy Search Robust to the Curse of Chaos



##### Dyna

online Q-learning algorithm that performs model-free RL with a model

Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming.



1. given state $$s,$$ pick action $$a$$ using exploration policy
2. observe $$s^{\prime}$$ and $$r,$$ to get transition $$\left(s, a, s^{\prime}, r\right)$$
3. update model $$\hat{p}\left(s^{\prime} \vert s, a\right)$$ and $$\hat{r}(s, a)$$ using $$\left(s, a, s^{\prime}\right)$$  学习两个model
4. Q-update: $$Q(s, a) \leftarrow Q(s, a)+\alpha E_{s^{\prime}, r}\left[r+\max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right]$$
5. repeat $$K$$ times:
   6. sample $$(s, a) \sim \mathcal{B}$$ from buffer of past states and actions
   2. Q-update: $$Q(s, a) \leftarrow Q(s, a)+\alpha E_{s^{\prime}, r}\left[r+\max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)-Q(s, a)\right]$$  这个时候使用model来得到 s', r

随着model的越来越准, Q值拟合的也越来越准.  也相当于使用model拟合来降低方差.



##### General “Dyna-style” model-based RL recipe

- **+ only requires short (as few as one step) rollouts from model**
- **+ still sees diverse states**



<img src="/img/CS285.assets/image-20200326005903126.png" alt="image-20200326005903126" style="zoom:33%;" />

1. collect some data, consisting of transitions $$\left(s, a, s^{\prime}, r\right)$$
2. learn model $$\hat{p}\left(s^{\prime} \vert s, a\right)$$ (and optionally, $$\hat{r}(s, a)$$ )
3. repeat K times:
   4. sample $$s \sim \mathcal{B}$$ from buffer
   2. choose action $$a$$ (from $$\mathcal{B},$$ from $$\pi,$$ or random)   
   3. simulate $$\left.s^{\prime} \sim \hat{p}\left(s^{\prime} \vert s, a\right) \text { (and } r=\hat{r}(s, a)\right)$$ from model
   7. train on $$\left(s, a, s^{\prime}, r\right)$$ with model-free RL
   8. (optional) take $$N$$ more model-based steps



- Model-Based Acceleration (MBA) 

- Model-Based Value Expansion (MVE) 

- Model-Based Policy Optimization (MBPO)

<img src="/img/CS285.assets/image-20200326010057296.png" alt="image-20200326010057296" style="zoom:33%;" />

1. take some action a and observe $$\left(\mathbf{s}_{i}, \mathbf{a}_{i}, \mathbf{s}_{i}^{\prime}, r_{i}\right),$$ add it to $$\mathcal{B}$$
2. sample mini-batch $$\left\{\mathbf{s}_{j}, \mathbf{a}_{j}, \mathbf{s}_{j}^{\prime}, r_{j}\right\}$$ from $$\mathcal{B}$$ uniformly
3. use $$\left\{\mathbf{s}_{j}, \mathbf{a}_{j}, \mathbf{s}_{j}^{\prime}\right\}$$ to update model $$\hat{p}\left(\mathbf{s}^{\prime} \vert \mathbf{s}, \mathbf{a}\right)$$
4. sample $$\left\{\mathbf{s}_{j}\right\}$$ from $$\mathcal{B}$$
5. for each $$\mathbf{s}_{j},$$ perform model-based rollout with $$\mathbf{a}=\pi(\mathbf{s})$$
6. use all transitions $$\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}, r\right)$$ along rollout to update Q-function



- Gu et al. Continuous deep Q-learning with model-based acceleration. ‘16 
- Feinberg et al. Model-based value expansion. ’18
-  Janner et al. When to trust your model: model-based policy optimization. ‘19



#### The trouble with global models

来自 cs284

- Planner will seek out regions where the model is erroneously optimistic 因为model不准, 去搜索无效区域
- Need to find a very good model in most of the state space to converge on a good solution
- In some tasks, the model is much more complex than the policy


### Local models

相较于训练很好的全局模型，有一些我们可以做的事情，譬如训练**局部模型** (local models)。在我们之前所述的方法中，做轨迹优化通常需要的仅仅是模型在某点的导数，而这正是局部信息。因此，我们某种程度上也可以认为不见得一定要训练好一个全局模型才能做好优化问题，而训练一些能对梯度等局部信息有表征的模型也可以：事实上这个就简单很多了。

$$
\min_{\mathbf{u}_1,\ldots,\mathbf{u}_T}\sum_{t=1}^Tc(\mathbf{x}_t,\mathbf{u}_t)~\text{s.t.}~\mathbf{x}_t=f(\mathbf{x}_{t-1},\mathbf{u}_{t-1})
$$

$$
\min_{\mathbf{u}_1,\ldots,\mathbf{u}_T}c(\mathbf{x}_1,\mathbf{u}_1)+c(f(\mathbf{x}_1,\mathbf{u}_1),\mathbf{u}_2)+\ldots+c(f(\ldots),\mathbf{u}_T)
$$

- usual story: differentiate via backpropagation and optimize! 
-  need $$\frac{d f}{d \mathbf{x}_{t}}, \frac{d f}{d \mathbf{u}_{t}}, \frac{d c}{d \mathbf{x}_{t}}, \frac{d c}{d \mathbf{u}_{t}}$$  

对标准强化学习来说, 缺少 df/dx,  df/du;  如果能学到这些, 那么就可以用LQR 来得到local policies. 



Idea:  just  fit $$\frac{d f}{d \mathbf{x}_{t}}, \frac{d f}{d \mathbf{u}_{t}}$$  around current trajectory or policy!  线性回归 

去拟合关于当前轨迹或者策略的 $$\frac{d f}{d \mathbf{x}_{t}}, \frac{d f}{d \mathbf{u}_{t}}$$，譬如使用线性回归的方法。注意到LQR很好的特性是它是线性的，而且这个策略是可以在真实环境中运行的。


<img src="/img/CS285.assets/image-20200326015333937.png" alt="image-20200326015333937" style="zoom:33%;" />


##### Outline

<img src="/img/CS285.assets/image-20200326021357621.png" alt="image-20200326021357621" style="zoom:33%;" />

$$
\begin{aligned}
&p\left(\mathbf{x}_{t+1} | \mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathcal{N}\left(f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right), \Sigma\right)\\
&f\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right) \approx \mathbf{A}_{t} \mathbf{x}_{t}+\mathbf{B}_{t} \mathbf{u}_{t}\\
&\mathbf{A}_{t}=\frac{d f}{d \mathbf{x}_{t}} \quad \mathbf{B}_{t}=\frac{d f}{d \mathbf{u}_{t}}
\end{aligned}
$$


如果我们能拟合$(\mathbf{A}_t,\mathbf{B}_t)$，那么我们就能得到这两个微分结果。用这个来使用iLQR方法改进我们的策略函数$\pi(\mathbf{u}_t \vert \mathbf{x}_t)$，然后重新下一个循环。



#### What controller to execute?

improve controller :

iLQR produces: $$\hat{\mathbf{x}}_{t}, \hat{\mathbf{u}}_{t}, \mathbf{K}_{t}, \mathbf{k}_{t}$$ $$\mathbf{u}_{t}=\mathbf{K}_{t}\left(\mathbf{x}_{t}-\hat{\mathbf{x}}_{t}\right)+\mathbf{k}_{t}+\hat{\mathbf{u}}_{t}$$  

- Version $$0.5: p\left(\mathbf{u}_{t} \vert \mathbf{x}_{t}\right)=\delta\left(\mathbf{u}_{t}=\hat{\mathbf{u}}_{t}\right)$$
  - Doesn't correct deviations or drift
- Version $$1.0: p\left(\mathbf{u}_{t} \vert \mathbf{x}_{t}\right)=\delta\left(\mathbf{u}_{t}=\mathbf{K}_{t}\left(\mathbf{x}_{t}-\hat{\mathbf{x}}_{t}\right)+\mathbf{k}_{t}+\hat{\mathbf{u}}_{t}\right)$$
  - Better, but maybe a little too good?
- Version $$2.0: p\left(\mathbf{u}_{t} \vert \mathbf{x}_{t}\right)=\mathcal{N}\left(\mathbf{K}_{t}\left(\mathbf{x}_{t}-\hat{\mathbf{x}}_{t}\right)+\mathbf{k}_{t}+\hat{\mathbf{u}}_{t}, \Sigma_{t}\right)$$
  - Add noise so that all samples don't look the same!



#### How to fit the dynamics?

$$\left\{\left(\mathbf{x}_{t}, \mathbf{u}_{t}, \mathbf{x}_{t+1}\right)_{i}\right\}$$

Version 1.0: fit $$p\left(\mathbf{x}_{t+1} \vert \mathbf{x}_{t}, \mathbf{u}_{t}\right)$$ at each time step using linear regression
$$
p\left(\mathbf{x}_{t+1} | \mathbf{x}_{t}, \mathbf{u}_{t}\right)=\mathcal{N}\left(\mathbf{A}_{t} \mathbf{x}_{t}+\mathbf{B}_{t} \mathbf{u}_{t}+\mathbf{c}, \mathbf{N}_{t}\right) \quad \mathbf{A}_{t} \approx \frac{d f}{d \mathbf{x}_{t}} \quad \mathbf{B}_{t} \approx \frac{d f}{d \mathbf{u}_{t}}
$$



Can we do better?

Version 2.0: fit $$p\left(\mathbf{x}_{t+1}  \vert  \mathbf{x}_{t}, \mathbf{u}_{t}\right)$$ using Bayesian linear regression

Use your favorite global model as prior (GP, deep net, GMM)



#### How to stay close to old controller?

<img src="/img/CS285.assets/image-20200326030002812.png" alt="image-20200326030002812" style="zoom:33%;" />
$$
\begin{aligned}
&p\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right)=\mathcal{N}\left(\mathbf{K}_{t}\left(\mathbf{x}_{t}-\hat{\mathbf{x}}_{t}\right)+\mathbf{k}_{t}+\hat{\mathbf{u}}_{t}, \Sigma_{t}\right)\\
&p(\tau)=p\left(\mathbf{x}_{1}\right) \prod_{t=1}^{T} p\left(\mathbf{u}_{t} | \mathbf{x}_{t}\right) p\left(\mathbf{x}_{t+1} | \mathbf{x}_{t}, \mathbf{u}_{t}\right)
\end{aligned}
$$

What if the new $$p(\tau)$$ is "close" to the old one $$\bar{p}(\tau) ?$$

If trajectory distribution is close, then dynamics will be close too! What does "close" mean?      $$D_{\mathrm{KL}}(p(\tau) \Vert \bar{p}(\tau)) \leq \epsilon$$

This is easy to do if $$\bar{p}(\tau)$$ also came from linear controller!





最后, 怎么把 local policy 合成全局的 policy.  这个思路感觉很靠谱. 

##### Guided policy search: high-level idea

<img src="/img/CS285.assets/image-20200326024421459.png" alt="image-20200326024421459" style="zoom:33%;" />



##### Guided policy search: algorithm sketch

1. optimize each local policy $$\pi_{\mathrm{LQR}, i}\left(\mathbf{u}_{t} \vert \mathbf{x}_{t}\right)$$ on initial state $$\mathbf{x}_{0, i}$$ w.r.t. $$\tilde{c}_{k, i}\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)$$
2. use samples from step (1) to $$\operatorname{train} \pi_{\theta}\left(\mathbf{u}_{t} \vert \mathbf{x}_{t}\right)$$ to mimic each $$\pi_{\mathrm{LQR}, i}\left(\mathbf{u}_{t} \vert \mathbf{x}_{t}\right)$$
3. update cost function $$\tilde{c}_{k+1, i}\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)=c\left(\mathbf{x}_{t}, \mathbf{u}_{t}\right)+\lambda_{k+1, i} \log \pi_{\theta}\left(\mathbf{u}_{t} \vert \mathbf{x}_{t}\right)$$



##### Underlying principle: distillation

- **Ensemble models:** single models are often not the most robust – instead train many models and average their predictions
  - this is how most ML competitions (e.g., Kaggle) are won this is very expensive at test time

- **Can we make a single model that is as good as an ensemble?**

- **Distillation:** train on the ensemble’s predictions as “soft” targets

- **Intuition:** more knowledge in soft targets than hard labels!





#### Readings: guided policy search & distillation

- L.*, Finn*, et al. End-to-End Training of Deep Visuomotor Policies. 2015.
- Rusu et al. Policy Distillation. 2015.
- Parisotto et al. Actor-Mimic: Deep Multitask and Transfer Reinforcement Learning. 2015.
- Ghosh et al. Divide-and-Conquer Reinforcement Learning. 2017.
- Teh et al. Distral: Robust Multitask Reinforcement Learning. 2017.