---
layout:     post
title:      UCL Course on RL 1
subtitle:   David Silver 的课程笔记
date:       2020-03-02 12:00:00
author:     "tengshiquan"
header-img: "img/post-atari.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning
    - DavidSilver
    - UCL
---



# UCL Course on RL

David Silver 主讲的强化学习课程：<http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html>

自己的笔记汇总



## Introduction to Reinforcement Learning

基本概念和定义

#### Characteristics of Reinforcement Learning

- There is no supervisor, only a reward signal
- Feedback is delayed, not instantaneous
- Time really matters (**sequential, non i.i.d data**) 
- Agent’s actions affect the subsequent data it receives  时间以及策略都会造成data非i.i.d



以下符号,  $S_t$ 是变量,  $s$ 是该变量具体取值， 其他变量同理. 



### The RL Problem

#### Reward

- A reward $R_t$ is a **scalar** feedback signal Indicates how well agent is doing at step t , 
- The agent’s job is to **maximise cumulative reward**  目标, 最大化累积reward
- Reinforcement learning is based on the **reward hypothesis** :  
  All goals can be described by the maximisation of **expected** cumulative reward



#### Sequential Decision Making

- Goal: select actions to maximise total future reward 
- Actions may have **long term consequences**
- Reward may be **delayed** 
- It may be better to sacrifice immediate reward to gain more **long-term reward**



#### Environments

##### Agent and Environment

**observation**  $O$



#### State

##### History and State

- **history** is the sequence of observations, actions, rewards 
- $H_{t} = O_{1}, R_{1}, A_{1},..., O_{t-1}, R_{t-1}, A_{t-1}, O_{t}, R_{t}, A_{t}$ 
- **State** is the information used to determine what happens next
- Formally, state is a function of the history: $S_{t} = f(H_{t})$



##### Environment State 

- **environment state** $S_t^e$ is the environment’s **private** representation,  对agent的不可见

##### Agent State

- **agent state** $S_t^a$ is the agent’s internal representation ,  agent使用的 information



##### Information State

-  **information state** (a.k.a. **Markov state**) contains all useful information from the history.



##### Markov 马尔可夫性

- **Definition**: A state $S_t$ is **Markov** if and only if  $\mathbb P[S_{t+1} \vert S_{t}] =\mathbb  P[S_{t+1} \vert S_{1}, S_{2},..., S_{t}]$
- “The future is **independent** of the past given the present”  
- $H_{1:t} → S_t → H_{t+1:∞}  $ 
- Once the state is known, the history may be thrown away 
- i.e. The state is a sufficient statistic of the future
- The **environment state** $S_t^e$ is **Markov**
- The history $H_t$ is Markov 



##### Fully Observable Environments

- **Full observability**: agent **directly** observes environment state
-  $O_t = S_t^a = S_t^e$   
- Agent state = environment state = information state 
- **Markov decision process (MDP)**  	



##### Partially Observable Environments 部分可观测

- **Partial observability**: agent **indirectly** observes environment

- **partially observable Markov decision process (POMDP)**

- Agent must construct its own state **representation**:    这里, representation是个问题
  - **Complete history**: $S_t^a = H_t$     ; 这里的history是agent自己观测的state sequence.

  - **Beliefs** of environment state:  $S_t^a = (\mathbb P[S_t^e = s^1],...,\mathbb P[S_t^e = s^n])$    
    agent 根据当前的观测, 处于某个状态的置信度. 

  - **RNN**: $S_a = σ(S^a_{t-1} W_s + O_tW_o )$     
    根据上一时刻的状态和 现在时刻的观测 作为输入 通过RNN 预测出 现在的状态



### Inside An RL Agent

##### Major Components

- **Policy**: agent’s behaviour function 
- **Value** function: how good is each state and/or action 
- **Model**: agent’s **representation** of the environment 



##### Policy

- **polic** is the agent’s behaviour ,  map from state to action 
- It is a map from state to action, e.g.
- 确定性策略 **Deterministic** policy : $a = π(s)$  
- 随机策略 **Stochastic** policy: $π(a \vert s) = \mathbb P[A_t = a \vert S_t = s]$    

 

##### Value Function

- 值函数: 从当前state开始, 按照当前策略执行, 所能得到的预期累积reward.
- **prediction** of future reward
- Used to evaluate the goodness/badness of states

$$
v_{\pi}(s)=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right]
$$



##### Model

- model **predicts** what the environment will do next   
- model是对env的建模, **Reward** 和 状态转移概率, **Dynamics** 
- $\mathcal P$ predicts the next state   状态转移概率矩阵 	$\mathcal P^a_{ss'} =\mathbb P[S_{t+1} =s′  \ \vert \ S_t =s,A_t =a]$
- $\mathcal R$ predicts the **next (immediate) reward**   $\mathcal R_s^a =\mathbb  E[R_{t+1} \vert S_t =s,A_t =a]$ 



- Agent may have an internal model of the environment 

- **Dynamics: how actions change the state** 

- Rewards: how much reward from each state 
- The model may be **imperfect**



##### Categorizing RL agents

- Value Based 
  - No Policy (Implicit) 

  - Value Function 

- Policy Based 
  - Policy 

  - No Value Function 

- Actor Critic 
  - Policy
  - Value Function 

按model来分: 

- Model Free
  
- Model Based



### Problems within RL

#### Learning and Planning

Two fundamental problems in **sequential decision making**  序列决策的两个问题  
RL 与 Planning 的区别, 这里强调  RL 一开始不知道env的model

- **Reinforcement Learning**:    默认对环境未知，要先学习环境，然后与环境交互来改进策略 
  - The environment is initially unknown 

  - The agent interacts with the environment 
  - The agent improves its policy 

- **Planning**:    默认基于 model 
  - A model of the environment is known   环境如何工作对于个体是已知或近似已知的 

  - The agent performs computations with its model (without any external interaction) 不与外界环境直接交互

  - The agent improves its policy



#### Atari Example

##### RL

<img src="/img/2020-03-02-Silver.assets/image-20200630165352905.png" alt="image-20200630165352905" style="zoom: 25%;" />

- Rules of the game are unknown
- Learn directly from interactive game-play
- Pick actions on joystick, see pixels and scores

##### Planning

<img src="/img/2020-03-02-Silver.assets/image-20200630165442091.png" alt="image-20200630165442091" style="zoom: 33%;" />

- Rules of the game are known 
- Can query emulator, perfect model inside agent’s brain 
- If I take action a from state s:
  - what would the next state be? 
  - what would the score be?

- Plan ahead to find optimal policy.    e.g. **tree search**





##### Exploration and Exploitation

- Reinforcement learning is like <u>trial-and-error</u> learning
- agent 应该边探索边利用现有的经验, 在过程中不能丢失太多的reward
- **Exploration** finds more information about the environment
- **Exploitation** exploits known information to maximise rewards





### Prediction and Control

- **Prediction**: 	**evaluate** the future,    **given a policy**,       求 $V_\pi$

- **Control**: 		**optimise** the future,    find the **best policy**,    优化策略，求$\pi^*$





## Markov Decision Processes

- MDP formally describe an **environment** for RL

- Where the environment is **fully observable** 
- Almost all RL problems can be formalised as MDPs,  e.g.
  - **Optimal control** primarily deals with **continuous MDPs** 
  - **Partially observable** problems can be converted into MDPs 
  - **Bandits** are MDPs with one state



### Markov Processes

#### Markov Property

##### State Transition Matrix

- state transition probability :  $\mathcal P_{ss'} = \mathbb P[S_{t+1} = s' \vert S_t =s]$    

- **State transition matrix 状态转移矩阵** , 每行的和为1  
$$
\mathcal P  = {\begin{bmatrix} \mathcal P_{11} & \cdots & \mathcal P_{1n} \\ \vdots &\\\mathcal P_{n1} & \cdots  &\mathcal P_{nn}\end{bmatrix}}
$$

##### Markov Process

A Markov process is a **memoryless** **random process**  无记忆随机过程

A Markov Process (or **Markov Chain**) is a tuple $⟨\mathcal S,\mathcal P⟩$



### Markov Reward Process

##### Markov Reward Process   MRP  

MRP是某个策略的产出 <s,a,p,r> , 先不看action的部分

- A Markov reward process is a Markov chain with values.

- A Markov Reward Process is a tuple $⟨\mathcal S, \mathcal P, \mathcal R, γ⟩$

  - $\mathcal S$ is a finite set of states 

  - $\mathcal P$ is a state transition probability matrix, $\mathcal P_{ss′} =\mathbb P[S_{t+1}=s′ \vert S_t=s] $

  - $\mathcal R$ is a reward function, $\mathcal R_s = \mathbb E[R_{t+1} \vert  S_t = s]$ 
  
  - $γ$ is a discount factor, $γ ∈ [0, 1]$ 
  
    

#### Return

Return = Gain;  奖励是针对状态的，一次sample的回报是针对episode的

$$
G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

> 对于一个episode， 不一定非要走到最后，才能得到值或者精确值； 如果最后几步都是0，那么没结束就能得到比较准确的值； 对于一些棋类，不到最后，无法得知胜负，则需要走到最后

discount values immediate reward above delayed reward. 

- γ close to 0 leads to ”myopic” evaluation
- γ close to 1 leads to ”far-sighted” evaluation



##### Why discount?

- Mathematically convenient to discount rewards ,  数学上表示方便

- Avoids infinite returns in cyclic Markov processes  无限循环的情况

- Uncertainty about the future may not be fully represented   对未来不确定

- If the reward is financial, immediate rewards may earn more interest than delayed rewards 

- Animal/human behaviour shows preference for immediate reward 

- It is sometimes possible to use **undiscounted** Markov reward processes (i.e. $γ = 1$), e.g. if all sequences terminate. 



#### Value Function

- The value function v(s) gives the **long-term** value of state s  定义

- The state value function v(s) of an MRP is the **expected return** starting from state s

$$
v(s) = \mathbb E [ G_{t} | S_{t} = s ]
$$

<img src="/img/2020-03-02-Silver.assets/image-20200630181929829.png" alt="image-20200630181929829" style="zoom:33%;" />



### Bellman Equation

##### Bellman Equation for  MRPs

two parts: 

- **immediate reward** $R_{t+1}$ 

- discounted value of successor state $γv(S_{t+1})$ 

$$
v(s) = \mathbb E[R_{t+1} + \gamma v(S_{t+1})\ |\ S_t = s]
\\= \mathcal R_s + \gamma \sum_{s' \in \mathcal S} \mathcal P_{ss'} v(s')
$$

上面的公式是基于MRP，还没有考虑action的问题

$$
v_\pi(s) \doteq \mathbb E_\pi[G_t|S_t =s] = \sum_a \pi(a|s) \sum_{s',r}p(s',r|s,a) \Big[ r+\gamma v_\pi(s')  \Big]
$$

##### Bellman Equation in Matrix Form

下面就是求线性方程的 不动点 $v$
$$
v = \mathcal R + \gamma \mathcal P v
$$

$$
\begin{bmatrix}
 v(1)\\
\vdots\\
v(n)\\
\end{bmatrix}  = 
\begin{bmatrix}
\mathcal R_1\\
\vdots\\
\mathcal R_n\\
\end{bmatrix}  +
\gamma
\begin{bmatrix}
\mathcal P_{11} & \dots & \mathcal P_{1n}\\
\vdots\\
\mathcal P_{11} & \dots & \mathcal P_{nn}\\
\end{bmatrix} 
\begin{bmatrix}
 v(1)\\
\vdots\\
v(n)\\
\end{bmatrix}
$$

- 直接求解，只对小规模问题 ，求出来就是收敛值； 

$$
v = \mathcal R + \gamma \mathcal P v
\\ \to  v = (I - \gamma \mathcal P)^{-1} \mathcal R
$$

- Computational complexity is $O(n^3)$ for n states
- There are many **iterative** methods for large MRPs, e.g.
  - Dynamic programming 
  - Monte-Carlo evaluation 
  - Temporal-Difference learning



### Markov Decision Process

- MDP is a tuple $\mathcal{M} = \langle\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma\rangle$
- $\mathcal P^a_{ss′} = \mathbb P[S_{t+1}=s′ \vert S_t=s,A_t=a]$ 
- $\mathcal R^a_s = \mathbb  E[R_{t+1} \vert S_t = s,A_t = a]$



#### Policies

- Definition: **policy** $π$ is a distribution over actions given states

- Policies are  **stationary** (time-independent)  : $A_t \sim \pi( \cdot |S_t ), \forall t > 0$    
  do not change over time，不会随时间的流逝，对同一个状态会有不同的对应动作

  

$$
\begin{aligned}
\mathcal{P}_{s, s^{\prime}}^{\pi} &=\sum_{a \in \mathcal{A}} \pi(a \mid s) \mathcal{P}_{s s^{\prime}}^{a} \\
\mathcal{R}_{s}^{\pi} &=\sum_{a \in \mathcal{A}} \pi(a \mid s) \mathcal{R}_{s}^{a}
\end{aligned}
$$


##### action-value function

$q_\pi(s,a) = \mathbb E_\pi[G_t \vert S_t = s,A_t =a]$



#### Bellman Expectation Equation
 
<img src="/img/2020-03-02-Silver.assets/image-20190215100151774.png"  style="zoom:33%;" />
 
<img src="/img/2020-03-02-Silver.assets/image-20190215100731976.png"  style="zoom:33%;" />

<img src="/img/2020-03-02-Silver.assets/image-20190215100850966.png"   style="zoom:33%;" />

$$
v_\pi(s) = \mathbb E_\pi[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t = s ] = \sum_{a \in \mathcal A} \pi(a|s)q_\pi(s,a)
\\ q_\pi(s,a) = \mathbb E_\pi[R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1}) | S_t = s,A_t =a]
= \mathcal R_s^a + \gamma \sum_{s' \in \mathcal S} \mathcal P^a_{ss'}v_\pi(s')
$$

**Matrix Form direct solution** 


$$
v_\pi= \mathcal R^\pi + \gamma \mathcal P^\pi v_\pi
\\ \to v_\pi = (I - \gamma \mathcal P^\pi)^{-1} \mathcal R^\pi 
$$

### Optimal Value Function


$$
v_*(s) = \max_\pi v_\pi(s) 
\\ q_*(s,a) = \max_\pi q_\pi(s,a)
\\ v_*(s) = \max_a q_*(s,a)
\\ q_*(s,a) = \mathcal R_s^a + \gamma \sum_{s'\in \mathcal S} \mathcal P_{ss'}^a v_*(s')
\\ v_*(s) = \max_a \mathcal R_s^a + \gamma \sum_{s'\in \mathcal S} \mathcal P_{ss'}^a v_*(s')  \quad   
\\ q_*(s,a) = \mathcal R_s^a + \gamma \sum_{s'\in \mathcal S} \mathcal P_{ss'}^a \max_{a'} q_*(s',a')
$$



##### Optimal Policy

Define a **partial order**ing over policies ,  定义 偏序

$$
\pi \geq \pi' \quad  if \quad v_\pi(s) \geq v_{\pi'}(s) , \forall s
$$

$$
\pi_*\left( a|s \right) =\left\{              \begin{array}{lr}              1\quad {\text {if}}\ a=\ \operatorname*{argmax}\limits_{a\in {\cal A}}\ q_*\left( s,a \right)\\              0\quad {\text {otherwise}}              \end{array} \right.
$$



- Bellman Optimality Equation is **non-linear** 
- No closed form solution (in general)   无法直接求解
- Many **iterative** solution methods : 
  - Value Iteration 
  - Policy Iteration 
  - Q-learning 
  - Sarsa 



### Extensions to MDPs

- Infinite and **continuous MDPs**
- Partially observable MDPs	 **POMDP**
- Undiscounted, **average reward** MDPs



#### Infinite MDPs

The following extensions are all possible: 

- Countably infinite state and/or action spaces  可数无穷
  - Straightforward 
- Continuous state and/or action spaces 连续状态，动作空间，这个可以离散化
  - Closed form for **linear quadratic model (LQR)** 
- Continuous time  连续时间
  - Requires partial differential equations  偏微分方程式
  - Hamilton-Jacobi-Bellman (HJB) equation
  - Limiting case of Bellman equation as time-step → 0   时间间隔趋向0的极限情况



### Partially Observable MDPs

- POMDP is an MDP with hidden states.  a hidden Markov model with actions.
- POMDP is a tuple $\langle\mathcal{S}, \mathcal{A}, \mathcal{O}, \mathcal{P}, \mathcal{R}, \mathcal{Z}, \gamma\rangle$ 
- $\mathcal Z$ is an observation function,  $ \mathcal Z_{s′o}^a = \mathbb P[O_{t+1} = o \vert S_{t+1} = s' ,A_t = a]  $

#### Belief States

- **history** :  $H_t$ , sequence of actions, observations and rewards, $H_t = A_0,O_1,R_1,...,A_{t−1},O_t,R_t$
- **belief state** : $b(h)$ , probability distribution over states, conditioned on the history $h$

$$
b(h)=\left(\mathbb{P}\left[S_{t}=s^{1} \mid H_{t}=h\right], \ldots, \mathbb{P}\left[S_{t}=s^{n} \mid H_{t}=h\right]\right)
$$



#### Reductions of POMDPs

- The **history** $H_t$ satisfies the Markov property
- The **belief state** $b(H_t)$ satisfies the Markov property

![image-20190215143057150](/img/2020-03-02-Silver.assets/image-20190215143057150.png)

- A POMDP can be reduced to an (infinite) history tree
- A POMDP can be reduced to an (infinite) belief state tree









