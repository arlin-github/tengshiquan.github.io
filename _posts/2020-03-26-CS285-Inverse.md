---
layout:     post
title:      CS 285. Inverse Reinforcement Learning
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---

 

## Inverse Reinforcement Learning

不知道reward 函数情况下, 先去学习 reward 函数.  在游戏领域用处很窄. 



### Lecture

1. So far: manually design reward function to define a task
2. What if we want to *learn* the reward function from observing an expert, and then use reinforcement learning?
3. Apply approximate optimality model from last week, but now learn the reward!

##### Goals:

- Understand the inverse reinforcement learning problem definition
- Understand how probabilistic models of behavior can be used to derive inverse reinforcement learning algorithms
- Understand a few practical inverse reinforcement learning algorithms we can use



### Why should we worry about learning rewards?

#### The imitation learning perspective

##### Standard imitation learning:

- copy the *actions* performed by the expert   算法学习 action
- no reasoning about outcomes of actions

##### Human imitation learning:

- copy the *intent* of the expert  人学习意图
- might take very different actions!



#### The reinforcement learning perspective

Computer Games

Real World Scenarios  现实很难量化

<img src="/img/CS285.assets/image-20200326224935291.png" alt="image-20200326224935291" style="zoom:33%;" />



### Inverse reinforcement learning

Infer **reward functions** from **demonstrations**

by itself, this is an **underspecified** problem

many reward functions can explain the **same** behavior



#### A bit more formally

##### “forward” reinforcement learning

- given:
  1. states $$\mathbf{s} \in \mathcal{S},$$ actions $$\mathbf{a} \in \mathcal{A}$$
  2. (sometimes) transitions $$p\left(\mathbf{s}^{\prime}  \vert  \mathbf{s}, \mathbf{a}\right)$$
  3. reward function $$r(\mathbf{s}, \mathbf{a})$$ 

- learn $$\pi^{\star}(\mathbf{a}  \vert  \mathbf{s})$$



##### inverse reinforcement learning

- given:
  1. states $$\mathbf{s} \in \mathcal{S},$$ actions $$\mathbf{a} \in \mathcal{A}$$
  2. (sometimes) transitions $$p\left(\mathbf{s}^{\prime}  \vert  \mathbf{s}, \mathbf{a}\right)$$
  3. samples $$\left\{\tau_{i}\right\}$$ sampled from $$\pi^{\star}(\tau)$$

- learn $$r_{\psi}(\mathbf{s}, \mathbf{a})$$ and then use it to learn $$\pi^{\star}(\mathbf{a}  \vert  \mathbf{s})$$

 



#### Feature matching IRL

svm的解法这个, 了解

  先考虑简单情况, 用线性函数来近似. 

- linear reward function: $$r_{\psi}(\mathbf{s}, \mathbf{a})=\sum_{i} \psi_{i} f_{i}(\mathbf{s}, \mathbf{a})=\psi^{T} \mathbf{f}(\mathbf{s}, \mathbf{a})$$ 

- if features $$\mathbf{f}$$ are important, what if we match their expectations?

- let $$\pi^{r_{\psi}}$$ be the optimal policy for $$r_{\psi}$$

- pick $$\psi$$ such that $$E_{\pi^{r_ \psi}}[\mathbf{f}(\mathbf{s}, \mathbf{a})]=E_{\pi^{\star}}[\mathbf{f}(\mathbf{s}, \mathbf{a})]$$

  - 等号前面state-action marginal under $$\pi^{r_{\psi}} \quad$$ 
  - 等号后面 unknown optimal policy approximate using expert samples

最优策略下特征的期望是要用专家给出的样本来算的。事实上，参数并不是唯一的，可以从支持向量机 (SVM) 中借鉴最大间隔 (maximum margin) 原理来得到一个比较靠谱的解

maximum margin principle:

$$
\max _{\psi, m} m \quad  \text{such that } \psi^{T} E_{\pi^{\star}}[\mathbf{f}(\mathbf{s}, \mathbf{a})] \geq \max _{\pi \in \Pi} \psi^{T} E_{\pi}[\mathbf{f}(\mathbf{s}, \mathbf{a})]+m
$$

need to somehow "weight" by similarity between $$\pi^*$$ and $$\pi$$ .

也就是找一个分割超平面把最优解下的期望收益和策略簇$\Pi$内其他所有策略的期望收益相区分开，并且使得间隔 m 最大。这个问题与原问题不同，只是尝试去这么做。然而不难发现，如果$\pi^*\in\Pi$ ，那么最优解落在分割超平面上，这个m总是0，这个间隔就不起效果了。因此，可能这样一刀切的间隔是不好的，我们有必要去体现策略不同下期望收益和专家策略有差异（专家策略就应该间距为0），使得和专家策略相差得越多，策略越糟糕。



使用一个SVM的trick , 上述问题可以等价于下面
$$
\min_\psi\frac{1}{2}\Vert\psi\Vert^2\text{ s.t. }\psi^\top\mathbf{E}_{\pi^*}[\mathbf{f}(\mathbf{s},\mathbf{a})]\geq\max_{\pi\in\Pi}\psi^\top\mathbf{E}_\pi[\mathbf{f}(\mathbf{s},\mathbf{a})]+D(\pi,\pi^*)
$$

距离可以定义为两个策略的期望特征的差异。

Issues:

- Maximizing the margin is a bit arbitrary
- No clear model of expert suboptimality (can add slack variables...)
- Messy constrained optimization problem – not great for deep learning!

Further reading:

- Abbeel & Ng: Apprenticeship learning via inverse reinforcement learning
- Ratliff et al: Maximum margin planning





### A probabilistic graphical model of decision making

下面利用概率图模型来解决逆增强学习



#### Learning the optimality variable

$$p(\mathcal{O}_t \vert \mathbf{s}_t,\mathbf{a}_t,\psi)= \exp(r_\psi(\mathbf{s}_t,\mathbf{a}_t))$$ .   代入 reward parameters

$$
p(\tau \vert \mathcal{O}_{1:T},\psi)\propto p(\tau)\exp\left(\sum_tr_\psi(\mathbf{s}_t,\mathbf{a}_t)\right) \tag{1}
$$

given:  samples $$\{ \tau_i\}$$  sampled from $$\pi^*(\tau)$$ .



将上面公式1代入,求解下面最大化问题 , $$p(\tau)$$ 与$${\psi}$$无关, 可以忽略. 

maximum likelihood learning: 

$$
\quad \max _{\psi} \frac{1}{N} \sum_{i=1}^{N} \log p\left(\tau_{i}  \vert  \mathcal{O}_{1: T}, \psi\right)=\max _{\psi} \frac{1}{N} \sum_{i=1}^{N} r_{\psi}\left(\tau_{i}\right)-\log Z
$$

因为公式1 sum起来=1, 所以还有一个余项, 这个logZ 很难算, 因为有超级多的轨迹的可能性. logZ 称为 partition function , 



##### The IRL partition function

1. $$\max _{\psi} \frac{1}{N} \sum_{i=1}^{N} r_{\psi}\left(\tau_{i}\right)-\log Z$$ . 
2. $$Z=\int p(\tau)\exp(r_\psi(\tau))\mathrm{d}\tau$$.

代入, 求梯度

$$
\nabla_\psi\mathcal{L}=\frac{1}{N}\sum_{i=1}^N\nabla_\psi r_\psi(\tau_i)-\int \frac{1}{Z}p(\tau)\exp(r_\psi(\tau))\nabla_\psi r_\psi(\tau)\mathrm{d}\tau
$$

$$
\nabla_\psi\mathcal{L}=\mathbf{E}_{\tau\sim\pi^*(\tau)}[\nabla_\psi r_\psi(\tau)]-\mathbf{E}_{\tau\sim p(\tau \vert \mathcal{O}_{1:T},\psi)}[\nabla_\psi r_\psi(\tau)]
$$

1. 第一个期望是 estimate with expert samples 
2. 第二个期望是soft optimal policy under current reward



##### Estimating the expectation

讨论第二块期望怎么估计。把第二块期望的轨迹收益按照时间拆开

$$
\mathbf{E}_{\tau\sim p(\tau \vert \mathcal{O}_{1:T},\psi)}\left[\nabla_\psi\sum_{t=1}^Tr_\psi(\mathbf{s}_t,\mathbf{a}_t)\right]=\sum_{t=1}^T \mathbf{E}_{(\mathbf{s}_t,\mathbf{a}_t)\sim p(\mathbf{s}_t,\mathbf{a}_t \vert \mathcal{O}_{1:T},\psi)}[\nabla_\psi r_\psi(\mathbf{s}_t,\mathbf{a}_t)]
$$

上节课有,  $$p(\mathbf{s}_t,\mathbf{a}_t \vert \mathcal{O}_{1:T},\psi)=p(\mathbf{a}_t \vert \mathbf{s}_t,\mathcal{O}_{1:T},\psi)p(\mathbf{s}_t \vert \mathcal{O}_{1:T},\psi)$$ 

$$p(\mathbf{a}_t \vert \mathbf{s}_t,\mathcal{O}_{1:T},\psi)=\frac{\beta(\mathbf{s}_t,\mathbf{a}_t)}{\beta(\mathbf{s}_t)}$$ ,  $$p(\mathbf{s}_t \vert \mathcal{O}_{1:T},\psi)\propto\alpha(\mathbf{s}_t)\beta(\mathbf{s}_t)$$, 

则, $$p(\mathbf{s}_t,\mathbf{a}_t \vert \mathcal{O}_{1:T},\psi)\propto\beta(\mathbf{s}_t,\mathbf{a}_t)\alpha(\mathbf{s}_t)$$ . 

第二项写成二重积分

let $$\mu_t(\mathbf{s}_t,\mathbf{a}_t)\propto\beta_t(\mathbf{s}_t,\mathbf{a}_t)\alpha_t(\mathbf{s}_t)$$ ,  作为在t时刻状态行动访问$$(\mathbf{s}_t,\mathbf{a}_t)$$概率

$$
\sum_{t=1}^T\int\int \mu_t(\mathbf{s}_t,\mathbf{a}_t)\nabla_\psi r_\psi(\mathbf{s}_t,\mathbf{a}_t)\mathrm{d}\mathbf{s}_t\mathrm{d}\mathbf{a}_t 
\\ =  \sum_{t=1}^T \vec{\mu}_t^\top\nabla_\psi\vec{r}_\psi
$$

#### The MaxEnt IRL algorithm

**最大熵逆增强学习**

1. Given $$\psi,$$ compute backward message $$\beta\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)$$  
2. Given $$\psi,$$ compute forward message $$\alpha\left(\mathbf{s}_{t}\right)$$  
3. Compute $$\mu_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \propto \beta\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \alpha\left(\mathbf{s}_{t}\right)$$
4. Evaluate $$\nabla_{\psi} \mathcal{L}=\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\psi} r_{\psi}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)-\sum_{t=1}^{T} \iint \mu_{t}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \nabla_{\psi} r_{\psi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) d \mathbf{s}_{t} d \mathbf{a}_{t}$$
5. $$\psi \leftarrow \psi+\eta \nabla_{\psi} \mathcal{L}$$ .



##### Why MaxEnt?

为什么叫最大熵

Ziebart et al. 2008: **Maximum Entropy Inverse Reinforcement Learning**

in the case where $$r_\psi(\mathbf{s}_t,\mathbf{a}_t)=\psi^\top\mathbf{f}(\mathbf{s}_t,\mathbf{a}_t)$$, we can show that it optimizes 

$$
\max_\psi\mathcal{H}(\pi^{r_\psi})\text{ s.t. }\mathbf{E}_{\pi^{r_\psi}}[\mathbf{f}]=\mathbf{E}_{\pi^*}[\mathbf{f}]
$$

- 等号前面 орtіmаl mах-еnt роlісу undеr  $$\pi^{r_{\psi}} \quad$$ 
- 后面 unknown expert policy estimated with samples

finds a policy as random as possible while matching features .  没其他限制条件, 所以其他的应该尽可能的random.



#### exmples

<img src="/img/CS285.assets/image-20200327020344987.png" alt="image-20200327020344987" style="zoom:33%;" />



#### What’s missing so far?

- MaxEnt IRL so far requires...
  - Solving for (soft) optimal policy in the inner loop
  - Enumerating all state-action tuples for visitation frequency and gradient

- To apply this in practical problem settings, we need to handle... 
  - Large and continuous state and action spaces
  - States obtained via sampling only
  - Unknown dynamics



#### Unknown dynamics & large state/action spaces

Assume we don’t know the dynamics, but we can sample, like in standard RL

$$
\nabla_\psi\mathcal{L}=\mathbf{E}_{\tau\sim\pi^*(\tau)}[\nabla_\psi r_\psi(\tau)]-\mathbf{E}_{\tau\sim p(\tau \vert \mathcal{O}_{1:T},\psi)}[\nabla_\psi r_\psi(\tau)]
$$

idea : learn $$p(\mathbf{a}_t \vert \mathbf{s}_t,\mathcal{O}_{1:T},\psi)$$ using any max-ent RL algorithm then run this policy to sample   $$\{ \tau_i\}$$ . 

$$
\nabla_\psi\mathcal{L}\approx\frac{1}{N}\sum_{i=1}^N\nabla_\psi r_\psi(\tau_i)-\frac{1}{M}\sum_{j=1}^M\nabla_\psi r_\psi(\tau_j)
$$

##### More efficient sample-based updates

可以不用完全地把对应的最优策略学出来，而只是**每次把策略改进一点点**，然后用这个不准确的策略去近似估计梯度。然而现在多出来一个问题，由于我们使用的策略是不正确的（不是最优的策略），因此我们的估计量将不再无偏。

- looks expensive! what if we use "lazy" policy optimization?
- problem: estimator is now biased! wrong distribution!

solution 1: use importance sampling

$$
\nabla_\psi\mathcal{L}\approx\frac{1}{N}\sum_{i=1}^N\nabla_\psi r_\psi(\tau_i)-\frac{1}{\sum_jw_j}\sum_{j=1}^Mw_j\nabla_\psi r_\psi(\tau_j)
$$

$$
w_j=\frac{p(\tau)\exp(r_\psi(\tau_j))}{\pi(\tau_j)}
\\ =\frac{p(\mathbf{s}_1)\prod_tp(\mathbf{s}_{t+1} \vert \mathbf{s}_t,\mathbf{a}_t)\exp(r_\psi(\mathbf{s}_t,\mathbf{a}_t))}{p(\mathbf{s}_1)\prod_tp(\mathbf{s}_{t+1} \vert \mathbf{s}_t,\mathbf{a}_t)\pi(\mathbf{a}_t \vert \mathbf{s}_t)}
\\=\frac{\exp(\sum_tr_\psi(\mathbf{s}_t,\mathbf{a}_t))}{\prod_t\pi(\mathbf{a}_t \vert \mathbf{s}_t)}
$$

each policy update w.r.t. $$r_\psi$$ brings us closer to the target distribution! 每一步策略迭代都使我们更接近最优分布，因此事实上是在逐步改进的。



##### guided cost learning algorithm

(Finn et al. ICML ’16)

<img src="/img/CS285.assets/image-20200327030030990.png" alt="image-20200327030030990" style="zoom:33%;" />


#### It looks a bit like a game...

1. initial policy $$\pi$$  ==>  samples from $$\pi_\theta(\tau)$$

2. human demonstrations  ==>  samples from $$\pi^*(\tau)$$

3. $$\nabla_\psi\mathcal{L}\approx\frac{1}{N}\sum_{i=1}^N\nabla_\psi r_\psi(\tau_i)-\frac{1}{\sum_jw_j}\sum_{j=1}^Mw_j\nabla_\psi r_\psi(\tau_j)$$ .  

    demos are made more likely , samples less likely

4. $$\nabla_{\theta} \mathcal{L} \approx \frac{1}{M} \sum_{j=1}^{M} \nabla_{\theta} \log \pi_{\theta}\left(\tau_{j}\right) r_{\psi}\left(\tau_{j}\right)$$
   policy changed to make it harder to distinguish from demos



#### Generative Adversarial Networks

Goodfellow et al. ‘14

<img src="/img/CS285.assets/image-20200327032113334.png" alt="image-20200327032113334" style="zoom: 50%;" />

which discriminator is best?

$$
D^{\star}(\mathbf{x})=\frac{p^{\star}(\mathbf{x})}{p_{\theta}(\mathbf{x})+p^{\star}(\mathbf{x})}
$$

这里, 这两个p都是未知的, 一般generator 输出带噪声的图片, 计算图片 log probability . 

#### Inverse RL as a GAN

Finn*, Christiano* et al. “A Connection Between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models.”


- Inverse RL     					GANs
- trajectory $$\tau$$ 		$$\longleftrightarrow$$ 	sample $$x$$
- policy $$\pi \sim q(\tau)$$ 	$$\longleftrightarrow$$ 	generator $$G$$
- reward $$r$$ 			$$\longrightarrow$$ 		discriminator $$D$$



令GAN的判别器取决于收益的方式来完成类似的目标。假设一个轨迹在专家（数据）分布下的概率是$$p(\tau)$$，当前策略下的概率是$$q(\tau)$$ ，最优判别器应该为$$D^*(\tau)=\frac{p(\tau)}{p(\tau)+q(\tau)}$$

for IRL, optimal policy approaches $$\pi_{\theta}(\tau) \propto p(\tau) \exp \left(r_{\psi}(\tau)\right)$$ 

choose this parameterization for discriminator: $$\quad$$ 

$$
D_{\psi}(\tau)=\frac{p(\tau) \frac{1}{Z} \exp (r(\tau))}{p_{\theta}(\tau)+p(\tau) \frac{1}{Z} \exp (r(\tau))}= \frac{  \frac{1}{Z} \exp (r(\tau))}{\prod_{t} \pi_{\theta}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)+\frac{1}{Z} \exp (r(\tau))}
$$

IRL这里,  action的prob 我们是知道的.

optimize $$Z$$   w.r.t. same objective as $$\psi$$  !  

$$
\psi \leftarrow \arg \max _{\psi} E_{\tau \sim p^{\star}}\left[\log D_{\psi}(\tau)\right]+E_{\tau \sim \pi_{\theta}}\left[\log \left(1-D_{\psi}(\tau)\right)\right]
$$

we don't need importance weights anymore - they are subsumed into $$Z$$



1. generator/policy $$\pi_\theta$$  ==>  samples from $$\pi_\theta(\tau)$$

2. data/demonstrations  ==>  samples from $$p^*(\tau)$$

3. $$\psi \leftarrow \arg \max _{\psi} E_{\tau \sim p^{\star}}\left[\log D_{\psi}(\tau)\right]+E_{\tau \sim \pi_{\theta}}\left[\log \left(1-D_{\psi}(\tau)\right)\right]$$ .  

    $$D_{\psi}(\tau)=  \frac{  \frac{1}{Z} \exp (r(\tau))}{\prod_{t} \pi_{\theta}\left(\mathbf{a}_{t}  \vert  \mathbf{s}_{t}\right)+\frac{1}{Z} \exp (r(\tau))}$$. 

4. $$\nabla_{\theta} \mathcal{L} \approx \frac{1}{M} \sum_{j=1}^{M} \nabla_{\theta} \log \pi_{\theta}\left(\tau_{j}\right) r_{\psi}\left(\tau_{j}\right)$$
   policy changed to make it harder to distinguish from demos



##### Generalization via inverse RL

<img src="/img/CS285.assets/image-20200327042238292.png" alt="image-20200327042238292" style="zoom:50%;" />

Fu et al. Learning Robust Rewards with Adversarial Inverse Reinforcement Learning



##### Can we just use a regular discriminator?

Ho & Ermon. Generative adversarial imitation learning.

discriminator D : standard binary neural net classifier

**Pros & cons:**

\+ often simpler to set up optimization, fewer moving parts

\- discriminator knows *nothing* at convergence

\- generally cannot reoptimize the “reward”



#### IRL as adversarial optimization

<img src="/img/CS285.assets/image-20200327042813334.png" alt="image-20200327042813334" style="zoom:50%;" />



Ho and Ermon (2016) 发表在NIPS上的 "Generative adversarial imitation learning" 一文将GAN和模仿学习联系得更直接，就认为机器人的动作是负样本，人类示范动作是正样本，  $$D(\tau)$$是一个二分类器来表示轨迹是一个正样本的概率，并使用$$\log D(\tau)$$ 作为收益函数。事实上它和GCL是差不多的，只是GCL的D是一个给定的函数形式，而这边D是一个二分类器（因此该算法不是IRL，但是非常像），总体来说两个算法都是GAN的变种。



#### Review

- IRL: infer unknown reward from expert demonstrations
- MaxEnt IRL: infer reward by learning under the control-as-inference framework
- MaxEnt IRL with dynamic programming: simple and efficient, but requires small state space and known dynamics
- Differential MaxEnt IRL: good for large, continuous spaces, but requires known dynamics and is local
- Sampling-based MaxEnt IRL: generate samples to estimate the partition function
  - Guided cost learning algorithm
  - Connection to generative adversarial networks
  - Generative adversarial imitation learning (not IRL per se, but very similar)



总体来说，IRL是从专家示范中推断出未知收益函数的手段， 一类比较好用的IRL算法是最大熵IRL，相对类似超平面分割的方法来说可以消除歧义，也解决了人类示范可能不是最优这种情况。这类算法可以用表格动态规划来实现，比较简单有效，但是只有在状态行动空间较小，动态已知的情况下才能应用。有一类微分最大熵IRL这边没有涉及，它适合于大而连续的空间，但需要知道系统动态。我们这里讲的深度IRL使用的是基于样本的最大熵IRL，可以用于连续空间，可以不假设有模型存在，较广泛。它的实现可以用GCL算法，该算法与GAN也有很深的渊源，和它紧密相关的还有生成对抗模仿学习算法（但不是IRL，不推测收益函数）。



#### Suggested Reading on Inverse RL

**Classic Papers**:

- Abbeel & Ng ICML ’04. *Apprenticeship Learning via Inverse Reinforcement Learning.* Good introduction to inverse reinforcement learning
- Ziebart et al. AAAI ’08. *Maximum Entropy Inverse Reinforcement Learning.* Introduction to probabilistic method for inverse reinforcement learning

**Modern Papers**:

- Finn et al. ICML ’16. *Guided Cost Learning.* Sampling based method for MaxEnt IRL that handles unknown dynamics and deep reward functions 
- Wulfmeier et al. arXiv ’16. *Deep Maximum Entropy Inverse Reinforcement Learning.* MaxEnt inverse RL using deep reward functions

- Ho & Ermon NIPS ’16. *Generative Adversarial Imitation Learning.* Inverse RL method using generative adversarial networks
- Fu, Luo, Levine ICLR ‘18. Learning Robust Rewards with Adversarial Inverse Reinforcement Learning