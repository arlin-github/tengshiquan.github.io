---
layout:     post
title:      Policy Gradient and Cross Entropy
subtitle:   Policy Gradient 与 cross-entropy 的关系
date:       2019-12-05 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-cartpole.jpg"
catalog: true
tags:
    - Policy Gradient
    - Cross Entropy
    - Reinforcement Learning

---



# Policy Gradient and Cross Entropy

有一些课程以及代码在实现Policy Gradient的时候, 使用了交叉熵损失函数这个trick,下面就两者关系整理一下.

##### cross-entropy

对使用交叉熵损失函数的分类问题, 有 :

$$
Loss = - Y \cdot \log \hat Y
$$

交叉熵有个很好的性质:

$$
\frac{\partial Loss}{\partial z_i} =  \hat y_i - y_i
$$

$z_i$ 是网络的最后一层节点的输出, 但不经过softmax, 即 y=softmax(z)



##### Policy Gradient

下面的Policy网络,采用的是 action-out 的方式. 即输入只有(state),

由于Policy网络即策略函数输出的是action的概率分布,要加起来为1, 一般最后都是要走softmax层. 

Policy Gradient 目标函数

$$
J(\theta)  = E_{\tau \sim \pi_\theta(\tau)}\underbrace{[r(\tau)]}_{\sum_{t=1}^Tr(s_t,a_t)} = \int \pi_\theta(\tau)r(\tau)\mathrm{d}\tau
$$

$$
\nabla_\theta J(\theta)=\int \nabla_\theta \pi_\theta(\tau)r(\tau)\mathrm{d}\tau
 = \int \pi_\theta(\tau)\nabla_\theta \log \pi_\theta(\tau)r(\tau)\mathrm{d}\tau
\\ =\mathbf{E}_{\tau\sim \pi_\theta(\tau)}\left[ \nabla_\theta \log \pi_\theta(\tau)r(\tau) \right]
$$

以基于蒙特卡洛的REINFORCE 算法为例, 采样再求期望, G代表Gain收益,标量,  是(s,a)之后所有reward的sum

强化学习中, 与env交互得到的sample, 是[s, a, r, s'] ,   假设sample实际采样到了第$act_{th}$个action, 可以把该sample看成 $G \cdot onehot(act_{th}) = [0,0,...G,...]$. 

把policy网络看成一个分类网络, 按照分类问题的梯度公式, 代入 $\pi \to \hat Y$,    $G \cdot onehot(act) \to Y$, 设计loss函数 $Loss = -G \cdot onehot(act)\cdot \log \pi$  ,  则按照这个Loss梯度下降会跟按照上面 $\nabla_\theta J(\theta)$的梯度上升效果一样.  其中, $onehot(act)\cdot \log \pi$  的结果是一个标量, 过滤掉了没采样到的action的prob值.

所以,可以把sample $G \cdot onehot(act)$看成监督学习的标签. 

同样也可以利用上面交叉熵的性质:  

$$
\frac{\partial J}{\partial z_i} = G \cdot (onehot(act)_i - \pi_i)
$$

该公式有很直观的意义: 突然采样到了一个$\pi_i$很小的动作, 然后如果G很大的话,则那个输出节点会有$G(1-\pi_i)$ 的梯度,进而对策略有很大的改动; 如果该动作G很小,则偶尔采样到也不会对策略造成较大的影响; 对没有被采样的动作, 同样也会有一个$G(0-\pi_i)$ 的梯度, 会影响后面策略对该动作的选择的几率



##### 两者的关系总结

对Policy Gradient, 不同于监督学习, 是没有训练的标签的, 只有sample [s, a, r, s'] 

对分类问题, 目标函数是要下降误差, Policy Gradient则是要提升目标函数总收益. 

Policy Gradient可以使用交叉熵的梯度下降算法, 主要把sample $G \cdot onehot(act)$看成标签 ,  这里与分类问题不同的一点是,  分类问题的onehot就是0和1, 而对Policy Gradient, 相当于有了一个加权, Gain可以提到最前面.



下面是基于keras实现, 利用了cross-entropy loss 的Policy gradient的一个例子:

```python
import sys
import gym
import pylab
import numpy as np
from keras.layers import Dense
from keras.models import Sequential
from keras.optimizers import Adam

EPISODES = 1000


# This is Policy Gradient agent for the Cartpole
# In this example, we use REINFORCE algorithm which uses monte-carlo update rule
class REINFORCEAgent:
    def __init__(self, state_size, action_size):
        # if you want to see Cartpole learning, then change to True
        self.render = False
        self.load_model = False
        # get size of state and action
        self.state_size = state_size
        self.action_size = action_size

        # These are hyper parameters for the Policy Gradient
        self.discount_factor = 0.99
        self.learning_rate = 0.001
        self.hidden1, self.hidden2 = 24, 24

        # create model for policy network
        self.model = self.build_model()

        # lists for the states, actions and rewards
        self.states, self.actions, self.rewards = [], [], []

        if self.load_model:
            self.model.load_weights("./save_model/cartpole_reinforce.h5")

    # approximate policy using Neural Network
    # state is input and probability of each action is output of network
    def build_model(self):
        model = Sequential()
        model.add(Dense(self.hidden1, input_dim=self.state_size, activation='relu', kernel_initializer='glorot_uniform'))
        model.add(Dense(self.hidden2, activation='relu', kernel_initializer='glorot_uniform'))
        model.add(Dense(self.action_size, activation='softmax', kernel_initializer='glorot_uniform'))
        model.summary()
        # Using categorical crossentropy as a loss is a trick to easily
        # implement the policy gradient. Categorical cross entropy is defined
        # H(p, q) = sum(p_i * log(q_i)). For the action taken, a, you set 
        # p_a = advantage. q_a is the output of the policy network, which is
        # the probability of taking the action a, i.e. policy(s, a). 
        # All other p_i are zero, thus we have H(p, q) = A * log(policy(s, a))
        model.compile(loss="categorical_crossentropy", optimizer=Adam(lr=self.learning_rate))
        return model

    # using the output of policy network, pick action stochastically
    def get_action(self, state):
        policy = self.model.predict(state, batch_size=1).flatten()
        return np.random.choice(self.action_size, 1, p=policy)[0]

    # In Policy Gradient, Q function is not available.
    # Instead agent uses sample returns for evaluating policy
    def discount_rewards(self, rewards):
        discounted_rewards = np.zeros_like(rewards)
        running_add = 0
        for t in reversed(range(0, len(rewards))):
            running_add = running_add * self.discount_factor + rewards[t]
            discounted_rewards[t] = running_add
        return discounted_rewards

    # save <s, a ,r> of each step
    def append_sample(self, state, action, reward):
        self.states.append(state)
        self.rewards.append(reward)
        self.actions.append(action)

    # update policy network every episode
    def train_model(self):
        episode_length = len(self.states)

        discounted_rewards = self.discount_rewards(self.rewards)
        discounted_rewards -= np.mean(discounted_rewards)
        discounted_rewards /= np.std(discounted_rewards)

        update_inputs = np.zeros((episode_length, self.state_size))
        advantages = np.zeros((episode_length, self.action_size))

        for i in range(episode_length):
            update_inputs[i] = self.states[i]
            advantages[i][self.actions[i]] = discounted_rewards[i]

        self.model.fit(update_inputs, advantages, epochs=1, verbose=0)
        self.states, self.actions, self.rewards = [], [], []

if __name__ == "__main__":
    # In case of CartPole-v1, you can play until 500 time step
    env = gym.make('CartPole-v1')
    # get size of state and action from environment
    state_size = env.observation_space.shape[0]
    action_size = env.action_space.n

    # make REINFORCE agent
    agent = REINFORCEAgent(state_size, action_size)

    scores, episodes = [], []

    for e in range(EPISODES):
        done = False
        score = 0
        state = env.reset()
        state = np.reshape(state, [1, state_size])

        while not done:
            if agent.render:
                env.render()

            # get action for the current state and go one step in environment
            action = agent.get_action(state)
            next_state, reward, done, info = env.step(action)
            next_state = np.reshape(next_state, [1, state_size])
            reward = reward if not done or score == 499 else -100

            # save the sample <s, a, r> to the memory
            agent.append_sample(state, action, reward)

            score += reward
            state = next_state

            if done:
                # every episode, agent learns from sample returns
                agent.train_model()

                # every episode, plot the play time
                score = score if score == 500 else score + 100
                scores.append(score)
                episodes.append(e)
                pylab.plot(episodes, scores, 'b')
                pylab.savefig("./save_graph/cartpole_reinforce.png")
                print("episode:", e, "  score:", score)

                # if the mean of scores of last 10 episode is bigger than 490
                # stop training
                if np.mean(scores[-min(10, len(scores)):]) > 490:
                    sys.exit()

        # save the model
        if e % 50 == 0:
            agent.model.save_weights("./save_model/cartpole_reinforce.h5")
```