---
layout:     post
title:      CS285 Notes
subtitle:   
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-rl-sutton.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---



# CS285

2019 新版课程重头撸一遍 



## Intro

Deep 自动提取特征

<img src="/img/CS285.assets/image-20200316113451117.png" alt="image-20200316113451117" style="zoom:50%;" />



![image-20200316114541450](/img/CS285.assets/image-20200316114541450.png)

#### Beyond learning from reward

##### advanced topics

- Learning **reward functions** from example (**inverse reinforcement learning**)
- Transferring knowledge between domains (**transfer learning, meta-learning**) 
- Learning to predict and using prediction to act



#### Are there other forms of supervision?

- Learning from demonstrations  **Imitation Learning**
  - Directly copying observed behavior
  - Inferring rewards from observed behavior (**inverse reinforcement learning**)

- Learning from observing the world
  - Learning to predict
  - Unsupervised learning
- Learning from other tasks
  - **Transfer learning**
  - **Meta-learning**: learning to learn



- More than imitation: inferring intentions   推断意图
- Inverse RL 



##### Prediction 预测动作行为后的结果



#### Why deep reinforcement learning?

- Deep = can process complex sensory input
  -  ...and also compute really complex functions
- Reinforcement learning = can choose complex actions





## Supervised Learning of Behaviors

#### Imitation Learning

##### behavioral cloning = supervised learning

<img src="/img/CS285.assets/image-20200316150244300.png" alt="image-20200316150244300" style="zoom:50%;" />

stability : 如果训练数据只是单独一条轨迹, 则实际运行的时候, 可能造成很大的偏差. 如果有很多训练数据, 就是训练的路径宽了很多, 覆盖更多的区域, 这样实际运行的时候, 大多数都见过, 稳定性就好很多. 

stable controller 产生这些训练数据, 如果遇到扰动,可以自己纠正. 可以sample很多trajectory, 都有一点小错误, 然后纠正这些错误.  按照这个思路,可以启发式的, taking data and relabelling with fake actions. 也可以影响采集数据, 故意使用有点小错误的suboptimal demonstrator ,加入很多噪声. 或者不直接从人学习, 从stable controller 监督学习稳健的策略.



<img src="/img/CS285.assets/image-20200316143445136.png" alt="image-20200316143445136" style="zoom: 50%;" />

监督学习的一个问题是, 从一个分布中采样数据训练一个model, 这个model在另外一个分布的数据集上的表现是不可预测的. 这个就是模仿学习不work的根源. 



##### DAgger: Dataset Aggregation

如果强制 training data distribution 与 policy running observation distribution 一样. 则通过监督学习来的policy应该表现不错.   

- make $$p_{\text {data }}\left(\mathbf{o}_{t}\right)=p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right) ?$$
- idea: instead of being clever about $$p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right),$$ be clever about $$p_{\text {data }}\left(\mathbf{o}_{t}\right) !$$
- goal: collect training data from $$p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right)$$ instead of $$p_{\text {data }}\left(\mathbf{o}_{t}\right)$$ 
- how? just run $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ but need labels $a_t$ !   比如, 自动驾驶的时候, 人也按照车的策略来开, 然后给出正确的action.

1. train $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ from human data $$\mathcal{D}=\left\{\mathbf{o}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{o}_{N}, \mathbf{a}_{N}\right\}$$
2. run $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ to get dataset $$\mathcal{D}_{\pi}=\left\{\mathbf{o}_{1}, \ldots, \mathbf{o}_{M}\right\}$$
3. Ask human to label $$\mathcal{D}_{\pi}$$ with actions  $a_{t}$
4. Aggregate: $\mathcal{D} \leftarrow \mathcal{D} \cup \mathcal{D}_{\pi}$



















