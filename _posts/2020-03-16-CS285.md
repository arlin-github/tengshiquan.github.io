---
layout:     post
title:      CS 285.  OverView,  Imitation Learning,  Introduction
subtitle:   CS 285. Deep Reinforcement Learning, Decision Making, and Control
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---



# CS 285. Deep Reinforcement Learning, Decision Making, and Control

Sergey Levine 2019 新版课程重新过一遍 

http://rail.eecs.berkeley.edu/deeprlcourse/



CS294 的笔记, 很详细 https://www.zhihu.com/people/xie-tian-55-77/posts



## Overview

Deep 自动提取特征

<img src="/img/CS285.assets/image-20200316113451117.png" alt="image-20200316113451117" style="zoom:50%;" />



![image-20200316114541450](/img/CS285.assets/image-20200316114541450.png)

#### Beyond learning from reward

##### advanced topics

- Learning **reward functions** from example (**inverse reinforcement learning**)
- Transferring knowledge between domains (**transfer learning, meta-learning**) 
- Learning to predict and using prediction to act



#### Are there other forms of supervision?

- Learning from demonstrations  **Imitation Learning**
  - Directly copying observed behavior
  - Inferring rewards from observed behavior (**inverse reinforcement learning**)

- Learning from observing the world
  - Learning to predict
  - Unsupervised learning
- Learning from other tasks
  - **Transfer learning**
  - **Meta-learning**: learning to learn



- More than imitation: inferring intentions   推断意图
- Inverse RL 



##### Prediction 预测动作行为后的结果



#### Why deep reinforcement learning?

- Deep = can process complex sensory input
  -  ...and also compute really complex functions
- Reinforcement learning = can choose complex actions





## Supervised Learning of Behaviors

#### Terminology & notation

- $$\pi_\theta(\mathbf{a}_t \vert \mathbf{o}_t)$$   policy
- $$\pi_\theta(\mathbf{a}_t \vert \mathbf{s}_t)$$   policy (fully observed)
- $$(\mathbf{s}, \mathbf{a})$$ RL ;  $$(\mathbf{x}, \mathbf{u})$$ Control
- $$r(\mathbf{s}, \mathbf{a}) = -c(\mathbf{x}, \mathbf{u})$$.



#### Imitation Learning

**behavioral cloning = supervised learning**

<img src="/img/CS285.assets/image-20200316150244300.png" alt="image-20200316150244300" style="zoom:50%;" />

stability : 如果训练数据只是单独一条轨迹, 则实际运行的时候, 可能造成很大的偏差. 如果有很多训练数据, 就是训练的路径宽了很多, 覆盖更多的区域, 这样实际运行的时候, 大多数都见过, 稳定性就好很多. 

stable controller 产生这些训练数据, 如果遇到扰动,可以自己纠正. 可以sample很多trajectory, 都有一点小错误, 然后纠正这些错误.  按照这个思路,可以启发式的, taking data and relabelling with fake actions. 也可以影响采集数据, 故意使用有点小错误的suboptimal demonstrator ,加入很多噪声. 或者不直接从人学习, 从stable controller 监督学习稳健的策略.



<img src="/img/CS285.assets/image-20200316143445136.png" alt="image-20200316143445136" style="zoom: 50%;" />

**域转移 (domain shift)** 监督学习的一个问题是, 从一个分布中采样数据训练一个model, 这个model在另外一个分布的数据集上的表现是不可预测的. 这个就是模仿学习不work的根源. 



#### DAgger: Dataset Aggregation 数据集聚合

如果强制 training data distribution 与 policy running observation distribution 一样. 则通过监督学习来的policy应该表现不错.   

- make $$p_{\text {data }}\left(\mathbf{o}_{t}\right)=p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right) ?$$
- idea: instead of being clever about $$p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right),$$ be clever about $$p_{\text {data }}\left(\mathbf{o}_{t}\right) !$$
- goal: collect training data from $$p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right)$$ instead of $$p_{\text {data }}\left(\mathbf{o}_{t}\right)$$ 
- how? just run $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ but need labels $a_t$ !   需要人工打标策略自动sample到的data. 比如, 自动驾驶的时候, 人也按照车的策略来开, 然后标记出正确的action.

1. train $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ from human data $$\mathcal{D}=\left\{\mathbf{o}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{o}_{N}, \mathbf{a}_{N}\right\}$$
2. run $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ to get dataset $$\mathcal{D}_{\pi}=\left\{\mathbf{o}_{1}, \ldots, \mathbf{o}_{M}\right\}$$
3. Ask human to label $$\mathcal{D}_{\pi}$$ with actions  $a_{t}$
4. **Aggregate**: $\mathcal{D} \leftarrow \mathcal{D} \cup \mathcal{D}_{\pi}$ , goto 1.



#### Why might we fail to fit the expert?

1. Non-Markovian behavior
2. Multimodal behavior



##### use the whole history 

1. 过去几帧直接拼在一起
2. RNN , LSTM



##### causal confusion 因果混乱

<img src="/img/CS285.assets/image-20200316165335772.png" alt="image-20200316165335772" style="zoom:50%;" />

为什么越复杂的网络, 特别是用了history的复杂model, 表现不好,  这里举了一例子,  就是刹车的时候, 刹车灯会亮,  如果用了历史, 则model会学到最简单关联, 刹车灯亮了就是踩刹车. 

DAgger 无法搞定这个问题. 



##### Multimodal

例子, 左右都可以, 模型如果只用一个高斯分布, 平均下来却会选择中间.

<img src="/img/CS285.assets/image-20200316165616269.png" alt="image-20200316165616269" style="zoom: 33%;" />

解决方案: 

1. Output **mixture of Gaussians**  $\pi(\mathbf{a} \vert \mathbf{o})=\sum_{i} w_{i} \mathcal{N}\left(\mu_{i}, \Sigma_{i}\right)$ ,  实现最简单,但N是写死的

2. **Latent variable models**  **隐变量模型**   现在我们输入的不仅仅是一个观测图像本身，同样也输入一个噪音进去，譬如给定维数的多元高斯噪音，然后得到输出。这一模型可以学习任何的非线性函数，可以把单峰的噪音变成多峰的输出。 理论好, 但很难实现, hard to train
  
    <img src="/img/CS285.assets/image-20200322010600078.png" alt="image-20200322010600078" style="zoom:33%;" />
   
   1. Conditional variational autoencoder
   2. Normalizing flow/realNVP
   3. Stein variational gradient descent
   
3. **Autoregressive discretization** 自回归离散化  兼顾理论以及实现 

   - 如果是连续的动作空间，一个可行的方法是将其离散化；但是如果维度大了，离散化后的联合分布将**维度灾难**。一个小技巧是避免联合离散化所有维度。假设我们有三个维度，首先我们离散化维度1，通过诸如Softmax的方法得到维度1的几个离散分类的分布$p(d_1)$。然后我们从这个分布里面进行抽样，得到维度1的值（其实是某个分类），然后把这个值输送给另一个神经网络（顺便还有图像或者某些隐藏层数据），这个神经网络给出离散化后维度2的分布，再如此得到维度3的分布。这样做的一个好处是，维度2的分布是以维度1的样本为条件的，即$p(d_2\vert d_1)$。这样就可以表示出任何的联合分布，但是在一个时段只需要离散化一个维度。当你训练这样的模型时，只需要给每个维度的正确值就可以了，做一个标准的监督学习过程。在测试中，需要依此采样然后馈入后续网络之中。

<img src="/img/CS285.assets/image-20200322010725882.png" alt="image-20200322010725882" style="zoom:33%;" />



#### Cases

##### Case study 1: trail following as classification

<img src="/img/CS285.assets/image-20200322010809923.png" alt="image-20200322010809923" style="zoom:33%;" />

使用了左右摄像头进行补偿.  遇到的问题比NVIDIA的自动驾驶更为困难：因为森林小道通常是非常凌乱的，很难进行清晰的感知，有些时候给一个图应该往哪儿飞都得仔细分析一会儿。首先，他们将动作的输出离散为向前、向左、向右三个；然后训练深度卷积神经网络来从图片得到动作。用人头上绑三个GoPro摄像机来收集数据，并认为左中右三个相机拍摄到的图像应该被分别标为右前左。事实上，这样做的效果非常好：

##### Case study 2: DAgger & domain adaptation  

老的里面ppt里面, 新的没介绍这个

<img src="/img/CS285.assets/image-20200322010943759.png" alt="image-20200322010943759" style="zoom:33%;" />

##### Case study 3: Imitation with LSTMs

<img src="/img/CS285.assets/image-20200322011017784.png" alt="image-20200322011017784" style="zoom:33%;" />

<img src="/img/CS285.assets/image-20200322011044391.png" alt="image-20200322011044391" style="zoom: 50%;" />

使用游戏手柄和模拟器训练机械臂的运作，把一个盒子（标志物）夹起来放到某个地方去。使用LSTM来处理非Markov问题，使用混合高斯分布来处理多峰问题。

##### case 4: Follow-up: adding vision

<img src="/img/CS285.assets/image-20200322011113053.png" alt="image-20200322011113053" style="zoom: 33%;" />

#### Other topics in imitation learning

老的课程里面的

- Structured prediction RNN
- Interaction & active learning
- **Inverse reinforcement learning**
  - Instead of copying the demonstration, figure out the *goal* 



#### Imitation learning: what’s the problem?

- Humans need to provide data, which is typically finite
  - Deep learning works best when data is plentiful

- Humans are not good at providing some kinds of actions

- Humans can learn autonomously; can our machines do the same?
  - Unlimited data from own experience
  - Continuous self-improvement



#### A cost function for imitation  代价函数的设计

模仿学习也可以写出奖励函数，如$r(\mathbf{s},\mathbf{a})=\log p(\mathbf{a}=\pi^*(\mathbf{s})\vert \mathbf{s})$，也就是我们策略的行动应该尽量与专家的指导意见一致。 另外一个更简单, 0-1loss 统计与专家意见不同的次数. 

 



#### Some analysis

 <img src="/img/CS285.assets/image-20200322011336505.png" alt="image-20200322011336505" style="zoom:33%;" />

How bad is it?  现在要衡量, 如果没有dagger , 两个trajectory离得越来越远, 那么这个问题多严重? 

这里用 0-1 loss

$$
c(\mathbf{s}, \mathbf{a})=\left\{\begin{array}{l}0 \text { if } \mathbf{a}=\pi^{\star}(\mathbf{s}) \\ 1 \text { otherwise }\end{array}\right.
$$

下面说明, 用这个做模仿学习,对某些问题可能出很大的问题

assume: $\pi_{\theta}\left(\mathbf{a} \neq \pi^{\star}(\mathbf{s}) \vert \mathbf{s}\right) \leq \epsilon$  for all $\mathbf{s} \in \mathcal{D}_{\text {train }}$    假设model的表现在训练集上还不错, 有一定小的几率犯错 ;  但是对于没见过的数据, 则不成立. 就会带来很多问题.

就如走钢丝, 一直很稳的话没问题, 如果遇到一个意外,就无法应对, 也无法恢复.  如果用0-1loss,则这个机器人最担心的是与专家的意见是不是一致, 而不是专心于走钢丝.  这个问题主要在于训练集没有很多意外情况的data ,但很多问题的意外有无穷多种, 比如自动驾驶, 所以这就是问题.

下面, 分析下走钢丝问题, 总损失的期望值,  第一步犯错的几率是 $\epsilon$ , 但如果犯错,就掉下去, 都是错误. 然后看第二步犯错的几率.. 右边有T项, 每项都有数量级为T的cost 

$$
E\left[\sum_{t} c\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \leq \underbrace{\epsilon T+ (1-\epsilon)(\epsilon(T-1) \dots )       }_{ \text{ T  terms, each } O(\epsilon T)}
$$

所以总的cost数量级就是 $O(\epsilon T^2)$  , 所以只要T很大, cost就非常大.



#### More general analysis

对于一般的问题.  已经知道, 监督学习勉强能work. . 监督学习, 只要data足够多, 就可以泛化 , 泛化那些未见过的data, 来自同一个sample分布的.

下面假定, model 不光在训练集上不错, 泛化能力也不错. 

assume: $\pi_{\theta}\left(\mathbf{a} \neq \pi^{\star}(\mathbf{s}) \vert \mathbf{s}\right) \leq \epsilon$  for all $\mathbf{s} \sim p_{\text {train }}(\mathbf s)$   

With DAgger,  分布不匹配就会消失,  with DAgger, $$p_{\text {train }}(\mathbf{s}) \rightarrow p_{\theta}(\mathbf{s})$$.

$$E\left[\sum_{t} c\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right] \leq \epsilon T$$  cost 上限的数量级是 T , 因为有了DAgger数据, 所以就算犯错之后, 也有应对的action. 

当没有 DAgger 的时候, 可以跟上面走钢丝的问题差不多, 推导  结论, 就是cost仍然是二次 T^2

if  $p_{\text {train }}(\mathbf{s}) \neq p_{\theta}(\mathbf{s})$:

$$p_{\theta}\left(\mathbf{s}_{t}\right)= {(1-\epsilon)^{t}} p_{\text {train }}\left(\mathbf{s}_{t}\right)+\left(1-(1-\epsilon)^{t}\right)  p_{\text {mistake }}\left(\mathbf{s}_{t}\right)$$

$$\left \vert p_{\theta}\left(\mathbf{s}_{t}\right)-p_{\text {train }}\left(\mathbf{s}_{t}\right)\right \vert =\left(1-(1-\epsilon)^{t}\right)\left \vert p_{\text {mistake }}\left(\mathbf{s}_{t}\right)-p_{\text {train }}\left(\mathbf{s}_{t}\right)\right \vert  \leq 2\left(1-(1-\epsilon)^{t}\right) \leq 2 \epsilon t$$

useful identity: $$(1-\epsilon)^{t} \geq 1-\epsilon t$$  ,  for $$\epsilon \in[0,1] \quad $$
$$
\sum_{t} E_{p_{\theta}\left(\mathbf{s}_{t}\right)}\left[c_{t}\right]=\sum_{t} \sum_{\mathbf{s}_{t}} p_{\theta}\left(\mathbf{s}_{t}\right) c_{t}\left(\mathbf{s}_{t}\right) \leq \sum_{t} \sum_{\mathbf{s}_{t}} p_{\text {train }}\left(\mathbf{s}_{t}\right) c_{t}\left(\mathbf{s}_{t}\right)+\left|p_{\theta}\left(\mathbf{s}_{t}\right)-p_{\text {train }}\left(\mathbf{s}_{t}\right)\right| c_{\max } \\
\leq \sum_{t} \epsilon+2 \epsilon t \leq \epsilon T + 2\epsilon T^2  \quad \quad  O(\epsilon T^2)
$$


For more analysis, see Ross et al. “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning” 





#### Cost/reward functions in theory and practice

在实践中，奖励函数有很多种形式。譬如让一个机械手抓住一个小球并放到某个指定地点，当然我们可以选择$r(\mathbf{s},\mathbf{a})=\delta(小球在目标位置)$ 这样简单的函数，但这样的函数通常很难帮我们解决增强学习问题：直到你把小球移动到目标位置之前，你真的不知道你应该这样做。所以通常解决实践问题，我们会设计一些更循序渐进的奖励函数.  如离目标的距离.





#### 模仿学习总结:

- Often (but not always) insufficient by itself : **Distribution mismatch problem**

- Sometimes works well
  - Hacks (e.g. left/right images)
  - Samples from a stable trajectory distribution 
  - Add more **on-policy** data, e.g. using Dagger
  - Better models that fit more accurately





## Introduction to Reinforcement Learning

##### Markov Chain 

 $\mathcal{M}=\{\mathcal{S},\mathcal{T}\}$ , 

$\mathcal{T}$  transition operator 状态概率转移算子.    $p(s_{t+1} \vert s_t)$

let $$\mu_{t,i}=p(s_t=i)$$ ,  $$ \mathcal{T}_{i,j}=p(s_{t+1}=i\vert s_t=j)$$   , then $$\mu_{t+1}=\mathcal{T}\mu_t$$



##### Markov Decision Process

$\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{T},r\}$

$\mathcal{T}$  a tensor!

Let  $\mu_{t,j}=p(s_t = j)$  , $\xi_{t,k}=p(a_t=k)$  ,  $$\mathcal{T}_{i,j,k}=p(s_{t+1}=i\vert s_t=j,a_t=k)$$ 

$$\mu_{t+1,i}=\sum_{j,k}\mathcal{T}_{i,j,k}\mu_{t,j}\xi_{t,k}$$  



##### Partially Observed Markov Decision Process, POMDP

$\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{O},\mathcal{T},\mathcal{E},r\}$

$\mathcal{E}$ : emission probability ,  $p(o_t \vert s_t)$



#### The goal of reinforcement learning

$$
\underbrace{p_\theta(\mathbf{s}_1,\mathbf{a}_1,\ldots,\mathbf{s}_T,\mathbf{a}_T)}_{p_\theta(\tau)}=\underbrace{p(\mathbf{s}_1)\prod_{t=1}^T\pi_\theta(\mathbf{a}_t\vert\mathbf{s}_t)p(\mathbf{s}_{t+1}\vert\mathbf{s}_t,\mathbf{a}_t)}_{\text{Markov chain on }(\mathbf s, \mathbf a)}
$$

$$
\theta^*=\arg\max_\theta\mathbf{E}_{\tau\sim p_\theta(\tau)}\left[\sum_tr(\mathbf{s}_t,\mathbf{a}_t)\right]
$$



##### Finite horizon case: state-action marginal

因为上面公式是按照每个$\tau$的维度来求期望, 期望是线性相加的. 可以把sum拿到外面来, 下面就可以 按时间维度来统计边际分布

$$
\theta^*=\arg\max_\theta\sum_{t=1}^T\mathbf{E}_{(\mathbf{s}_t,\mathbf{a}_t)\sim p_\theta(\mathbf{s}_t,\mathbf{a}_t)}r(\mathbf{s}_t,\mathbf{a}_t) 
$$

$p_\theta(\mathbf{s}_t,\mathbf{a}_t)$ : **state-action marginal 边际分布**  ,  在时刻t, 各种(s,a)的几率



##### Infinite horizon case: stationary distribution

$\mathcal{T}$  state-action transition operator   ，是个线性变换

$$
\left(\begin{array}{l}\mathbf{s}_{t+1}\\\mathbf{a}_{t+1}\end{array}\right)=\mathcal{T}\left(\begin{array}{l}\mathbf{s}_t\\\mathbf{a}_t\end{array}\right) ,  \left(\begin{array}{l}\mathbf{s}_{t+k}\\\mathbf{a}_{t+k}\end{array}\right)=\mathcal{T}^k\left(\begin{array}{l}\mathbf{s}_t\\\mathbf{a}_t\end{array}\right)
$$



渐收敛到一个   $\mu=p_\theta(\mathbf{s},\mathbf{a})$   **平稳分布 (stationary distribution)**：之所以说平稳分布，是因为经过一次状态转移后，分布不发生变化   

$\mu=\mathcal{T}\mu$  => $(\mathcal{T}-\mathbf{I})\mu=0$ ;  $\mu$是 $\mathcal{T}$ 特征值为1的特征向量 

对目标函数进行平均，可以看出完全由平稳分布下的情形所控制。去掉了sum
$$
\theta^*=\arg\max_\theta\frac{1}{T}\sum_{t=1}^T\mathbf{E}_{(\mathbf{s}_t,\mathbf{a}_t)\sim p_\theta(\mathbf{s}_t,\mathbf{a}_t)}r(\mathbf{s}_t,\mathbf{a}_t)\rightarrow \mathbf{E}_{(\mathbf{s},\mathbf{a})\sim p_\theta(\mathbf{s},\mathbf{a})}r(\mathbf{s},\mathbf{a})
$$


##### Expectations and stochastic systems

- In RL, we almost always care about *expectations* .   增强学习中，我们几乎只关心期望，而不是个别的值，这是因为这给予了我们很好的数学性质。
- $r(\mathbf X)$ - not smooth 譬如说在盘山公路上开一辆车，如果正在运行那么收益函数为+1，如果掉下山崖则收益函数为-1。此时，我们的收益函数是不光滑的。
- $\pi_\theta(\mathbf a = fall) = \theta$  假如说我们从系统中提取出了一个概率，作为掉下的概率。
- $\mathbf E_{\pi_\theta} [r(\mathbf x)]$ - smooth in $\theta$.  此时如果我们关注期望的话，平稳分布下的收益函数的期望，则是关于 $\theta$ 光滑的！
- 这一点非常重要，允许我们使用诸如基于梯度的算法来优化非光滑的目标（可能是非光滑的转移，或者非光滑的收益函数等等导致）



#### The anatomy of a reinforcement learning algorithm

<img src="/img/CS285.assets/image-20200317013208886.png" alt="image-20200317013208886" style="zoom: 33%;" />



##### Review

- Definitions
  - Markov chain
  - Markov decision process

- RL objective
  - Expected reward
  - How to evaluate expected reward?

- Structure of RL algorithms
  1. Sample generation
  2. Fitting a model/estimating return
  3. Policy Improvement



##### How do we deal with all these expectations?

$$E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$$

$$E_{\mathrm{s}_{1} \sim p\left(\mathrm{s}_{1}\right)}$$  $$ + \underbrace{     \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad              }_{}$$

​				$$Q\left(\mathbf{s}_{1}, \mathbf{a}_{1}\right)=r\left(\mathbf{s}_{1}, \mathbf{a}_{1}\right)+E_{\mathbf{s}_{2} \sim p\left(\mathbf{s}_{2} \vert \mathbf{s}_{1}, \mathbf{a}_{1}\right)}\left[E_{\mathbf{a}_{2} \sim \pi\left(\mathbf{a}_{2} \vert \mathbf{s}_{2}\right)}\left[r\left(\mathbf{s}_{2}, \mathbf{a}_{2}\right)+\ldots \vert \mathbf{s}_{2}\right] \vert \mathbf{s}_{1}, \mathbf{a}_{1}\right]$$

$$E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]=E_{\mathbf{s}_{1} \sim p\left(\mathbf{s}_{1}\right)}\left[E_{\mathbf{a}_{1} \sim \pi\left(\mathbf{a}_{1} \vert \mathbf{s}_{1}\right)}\left[Q\left(\mathbf{s}_{1}, \mathbf{a}_{1}\right) \vert \mathbf{s}_{1}\right]\right]$$

如果知道了Qfuntion, 则很容易improve 策略.



##### Q-funtion , value funtion

$$
Q^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_t,\mathbf{a}_t]
\\
V^\pi(\mathbf{s}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_t]
\\
V^\pi(\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim\pi(\mathbf{a}_t|\mathbf{s}_t)}[Q^\pi(\mathbf{s}_t,\mathbf{a}_t)]
$$



$\mathbf{E}_{\mathbf{s}_1\sim p(\mathbf{s}_1)}[V^\pi(\mathbf{s}_1)]$ is the RL objective !



##### Using Q-functions and value functions

- Idea 1: if we have policy $\pi,$ and we know $Q^{\pi}(\mathbf{s}, \mathbf{a}),$ then we can improve $\pi:$
  set $\pi^{\prime}(\mathbf{a} | \mathbf{s})=1$ if $\mathbf{a}=\arg \max _{\mathbf{a}} Q^{\pi}(\mathbf{s}, \mathbf{a})$
- Idea 2: compute gradient to increase probability of good actions a:
  if $Q^{\pi}(\mathbf{s}, \mathbf{a})>V^{\pi}(\mathbf{s}),$ then $\mathbf{a}$ is better than average  



#### Types of RL algorithms

- **Policy gradients**: directly differentiate the above objective
- **Value-based**: estimate value function or Q-function of the optimal policy  (no explicit policy) 

- **Actor-critic**: estimate value function or Q-function of the current policy, use it to improve policy 
- **Model-based** RL: **estimate the transition model**, and then...
  - Use it for planning (no explicit policy) 
  - Use it to improve a policy ...



##### Model-based RL algorithms

<img src="/img/CS285.assets/image-20200317031143346.png" alt="image-20200317031143346" style="zoom: 33%;" />

核心是学习 状态转移概率的model,  可以使用各种方法, 神经网络之类.

- Just use the model to plan (no policy)
  - **Trajectory optimization**/**optimal control** (primarily in continuous spaces) – essentially backpropagation to optimize over actions
  - **Discrete planning** in discrete action spaces – e.g., **Monte Carlo tree search** 
- Backpropagate gradients into the policy    
  - Requires some tricks to make it work, unstable
- Use the model to learn a value function
  - **Dynamic programming**
  - Generate simulated experience for model-free learner (**Dyna**) 



##### Value function based algorithms

- fit $V(s)$ or $Q(s,a)$
- set $\pi(s) = \arg\max_a Q(s,a)$

##### Direct policy gradients

- evaluate returns : $R_\tau=\sum_tr(\mathbf{s}_t,\mathbf{a}_t)$
- $\theta\leftarrow\theta+\alpha\nabla_\theta\mathbf{E}\sum_tr(\mathbf{s}_t,\mathbf{a}_t)$

##### Actor-critic: value functions + policy gradients

- fit $V(s)$ or $Q(s,a)$
- $\theta\leftarrow\theta+\alpha\nabla_\theta\mathbf{E}\sum_tr(\mathbf{s}_t,\mathbf{a}_t)$



#### Tradeoffs

- Different tradeoffs
  - Sample efficiency
  - Stability & ease of use

- Different assumptions
  - Stochastic or deterministic?
  - Continuous or discrete?
  - Episodic or infinite horizon?	

- Different things are easy or hard in different settings
  - Easier to represent the policy? 
  - Easier to represent the model?



##### sample efficiency  采样效率

- Off policy: able to improve the policy without generating new samples from that policy
- On policy: each time the policy is changed, even a little bit, we need to generate new samples



<img src="/img/CS285.assets/image-20200317033227158.png" alt="image-20200317033227158" style="zoom:50%;" />



##### stability and ease of use

converge  收敛性

- Supervised learning: almost *always* gradient descent

- Reinforcement learning: often *not* gradient descent 强化学习不是真正意义上的GD based的优化
  - Q-learning: fixed point iteration  没用到梯度,  对Function Approximation不收敛
  - Model-based RL: model is not optimized for expected reward, model收敛但未必得到预期的reward
  - Policy gradient: *is* gradient descent, but also often the least efficient! 低效



- Value function fitting
  - At best, minimizes error of fit (“Bellman error”)
    - Not the same as expected reward
  - At worst, doesn’t optimize anything
    - Many popular deep RL value fitting algorithms are not guaranteed to converge to *anything* in the nonlinear case

- Model-based RL
  - Model minimizes error of fit
    - This will converge
  - No guarantee that better model = better policy

- Policy gradient
  - The only one that actually performs gradient descent (ascent) on the true objective



##### assumptions

- full observability
- episodic learning
- continuity or smoothness



#### Examples of specific algorithms 

-  Value function fitting methods 
   -  Q-learning, DQN
   -  Temporal difference learning
   -  Fitted value iteration   
-  Policy gradient methods
   - REINFORCE 
   - Natural policy gradient
   - Trust region policy optimization 
-  Actor-critic algorithms
   - Asynchronous advantage actor critic (A3C) 
-  Model-based RL algorithms
   - Dyna 
   - Guided policy search 



#### Example

##### Atari games with Q-functions

- Playing Atari with deep reinforcement learning, Mnih et al. ‘13
- Q-learning with convolutional neural networks

##### robots and model-based RL

- End-to-end training of deep visuomotor policies, L.* , Finn* ’16
- Guided policy search (model-based RL) for image-based robotic manipulation

##### walking with policy gradients

- High-dimensional continuous control with generalized advantage estimation, Schulman et al. ‘16
- Trust region policy optimization with value function approximation

##### robotic grasping with Q-functions

- QT-Opt, Kalashnikov et al. ‘18

- Q-learning from images for real-world robotic grasping

​	

 

 



