---
layout:     post
title:      CS285 Notes
subtitle:   
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-rl-sutton.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---



# CS285

2019 新版课程重头撸一遍 



## Overview

Deep 自动提取特征

<img src="/img/CS285.assets/image-20200316113451117.png" alt="image-20200316113451117" style="zoom:50%;" />



![image-20200316114541450](/img/CS285.assets/image-20200316114541450.png)

#### Beyond learning from reward

##### advanced topics

- Learning **reward functions** from example (**inverse reinforcement learning**)
- Transferring knowledge between domains (**transfer learning, meta-learning**) 
- Learning to predict and using prediction to act



#### Are there other forms of supervision?

- Learning from demonstrations  **Imitation Learning**
  - Directly copying observed behavior
  - Inferring rewards from observed behavior (**inverse reinforcement learning**)

- Learning from observing the world
  - Learning to predict
  - Unsupervised learning
- Learning from other tasks
  - **Transfer learning**
  - **Meta-learning**: learning to learn



- More than imitation: inferring intentions   推断意图
- Inverse RL 



##### Prediction 预测动作行为后的结果



#### Why deep reinforcement learning?

- Deep = can process complex sensory input
  -  ...and also compute really complex functions
- Reinforcement learning = can choose complex actions





## Supervised Learning of Behaviors

#### Imitation Learning

**behavioral cloning = supervised learning**

<img src="/img/CS285.assets/image-20200316150244300.png" alt="image-20200316150244300" style="zoom:50%;" />

stability : 如果训练数据只是单独一条轨迹, 则实际运行的时候, 可能造成很大的偏差. 如果有很多训练数据, 就是训练的路径宽了很多, 覆盖更多的区域, 这样实际运行的时候, 大多数都见过, 稳定性就好很多. 

stable controller 产生这些训练数据, 如果遇到扰动,可以自己纠正. 可以sample很多trajectory, 都有一点小错误, 然后纠正这些错误.  按照这个思路,可以启发式的, taking data and relabelling with fake actions. 也可以影响采集数据, 故意使用有点小错误的suboptimal demonstrator ,加入很多噪声. 或者不直接从人学习, 从stable controller 监督学习稳健的策略.



<img src="/img/CS285.assets/image-20200316143445136.png" alt="image-20200316143445136" style="zoom: 50%;" />

**域转移 (domain shift)** 监督学习的一个问题是, 从一个分布中采样数据训练一个model, 这个model在另外一个分布的数据集上的表现是不可预测的. 这个就是模仿学习不work的根源. 



#### DAgger: Dataset Aggregation

如果强制 training data distribution 与 policy running observation distribution 一样. 则通过监督学习来的policy应该表现不错.   

- make $$p_{\text {data }}\left(\mathbf{o}_{t}\right)=p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right) ?$$
- idea: instead of being clever about $$p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right),$$ be clever about $$p_{\text {data }}\left(\mathbf{o}_{t}\right) !$$
- goal: collect training data from $$p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right)$$ instead of $$p_{\text {data }}\left(\mathbf{o}_{t}\right)$$ 
- how? just run $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ but need labels $a_t$ !   需要人工打标. 比如, 自动驾驶的时候, 人也按照车的策略来开, 然后给出正确的action.

1. train $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ from human data $$\mathcal{D}=\left\{\mathbf{o}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{o}_{N}, \mathbf{a}_{N}\right\}$$
2. run $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ to get dataset $$\mathcal{D}_{\pi}=\left\{\mathbf{o}_{1}, \ldots, \mathbf{o}_{M}\right\}$$
3. Ask human to label $$\mathcal{D}_{\pi}$$ with actions  $a_{t}$
4. Aggregate: $\mathcal{D} \leftarrow \mathcal{D} \cup \mathcal{D}_{\pi}$ , goto 1.



#### Why might we fail to fit the expert?

1. Non-Markovian behavior
2. Multimodal behavior



##### use the whole history 

1. 过去几帧直接拼在一起
2. RNN , LSTM



##### causal confusion 因果混乱

<img src="/img/CS285.assets/image-20200316165335772.png" alt="image-20200316165335772" style="zoom:50%;" />

为什么越复杂的网络, 特别是用了history的复杂model, 表现不好,  这里举了一例子,  就是刹车的时候, 刹车灯会亮,  如果用了历史, 则model会学到最简单关联, 刹车灯亮了就是踩刹车. 

DAgger 无法搞定这个问题. 



##### Multimodal

例子, 左右都可以, 模型如果用一个高斯分布, 却会选择中间.

<img src="/img/CS285.assets/image-20200316165616269.png" alt="image-20200316165616269" style="zoom: 33%;" />

解决方案: 

1. Output mixture of Gaussians  $\pi(\mathbf{a} \vert \mathbf{o})=\sum_{i} w_{i} \mathcal{N}\left(\mu_{i}, \Sigma_{i}\right)$  实现最简单,但N是写死的
2. **Latent variable models**  **隐变量模型** 理论好, 但很难实现, hard to train
   1. Conditional variational autoencoder
   2. Normalizing flow/realNVP
   3. Stein variational gradient descent
3. Autoregressive discretization 自回归离散化  兼顾理论以及实现 
   - 有连续的动作，一个可行的方法是将其离散化；但是如果维度大了，离散化后的联合分布将维度灾难。一个小技巧是避免联合离散化所有维度。



#### A cost function for imitation

在实践中，奖励函数有很多种形式。譬如让一个机械手抓住一个小球并放到某个指定地点，当然我们可以选择$r(\mathbf{s},\mathbf{a})=\delta(小球在目标位置)$ 这样简单的函数，但这样的函数通常很难帮我们解决增强学习问题：直到你把小球移动到目标位置之前，你真的不知道你应该这样做。就像走钢丝.  所以通常解决实践问题，我们会设计一些更循序渐进的奖励函数.  如离目标的距离. 



#### 模仿学习总结:

- Often (but not always) insufficient by itself : **Distribution mismatch problem**

- Sometimes works well
  - Hacks (e.g. left/right images)
  - Samples from a stable trajectory distribution 
  - Add more **on-policy** data, e.g. using Dagger
  - Better models that fit more accurately





## Introduction to Reinforcement Learning

##### Markov Chain 

 $\mathcal{M}=\{\mathcal{S},\mathcal{T}\}$ , 

$\mathcal{T}$  transition operator 状态概率转移算子.    $p(s_{t+1} \vert s_t)$

let $$\mu_{t,i}=p(s_t=i)$$ ,  $$ \mathcal{T}_{i,j}=p(s_{t+1}=i\vert s_t=j)$$   , then $$\mu_{t+1}=\mathcal{T}\mu_t$$



##### Markov Decision Process

$\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{T},r\}$

$\mathcal{T}$  a tensor!

Let  $\mu_{t,j}=p(s_t = j)$  , $\xi_{t,k}=p(a_t=k)$  ,  $$\mathcal{T}_{i,j,k}=p(s_{t+1}=i\vert s_t=j,a_t=k)$$ 

$$\mu_{t+1,i}=\sum_{j,k}\mathcal{T}_{i,j,k}\mu_{t,j}\xi_{t,k}$$  



##### Partially Observed Markov Decision Process, POMDP

$\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{O},\mathcal{T},\mathcal{E},r\}$

$\mathcal{E}$ : emission probability ,  $p(o_t \vert s_t)$



#### The goal of reinforcement learning

$$
\underbrace{p_\theta(\mathbf{s}_1,\mathbf{a}_1,\ldots,\mathbf{s}_T,\mathbf{a}_T)}_{p_\theta(\tau)}=\underbrace{p(\mathbf{s}_1)\prod_{t=1}^T\pi_\theta(\mathbf{a}_t\vert\mathbf{s}_t)p(\mathbf{s}_{t+1}\vert\mathbf{s}_t,\mathbf{a}_t)}_{\text{Markov chain on }(\mathbf s, \mathbf a)}
$$

$$
\theta^*=\arg\max_\theta\mathbf{E}_{\tau\sim p_\theta(\tau)}\left[\sum_tr(\mathbf{s}_t,\mathbf{a}_t)\right]
$$



##### Finite horizon case: state-action marginal

因为期望是线性相加的，所以可以把sum拿到外面来， 下面就可以 按时间维度来切分

$$
\theta^*=\arg\max_\theta\sum_{t=1}^T\mathbf{E}_{(\mathbf{s}_t,\mathbf{a}_t)\sim p_\theta(\mathbf{s}_t,\mathbf{a}_t)}r(\mathbf{s}_t,\mathbf{a}_t) 
$$

$p_\theta(\mathbf{s}_t,\mathbf{a}_t)$ : state-action marginal 边际分布



##### Infinite horizon case: stationary distribution

$\mathcal{T}$  state-action transition operator   ，是个线性变换

$$
\left(\begin{array}{l}\mathbf{s}_{t+1}\\\mathbf{a}_{t+1}\end{array}\right)=\mathcal{T}\left(\begin{array}{l}\mathbf{s}_t\\\mathbf{a}_t\end{array}\right) ,  \left(\begin{array}{l}\mathbf{s}_{t+k}\\\mathbf{a}_{t+k}\end{array}\right)=\mathcal{T}^k\left(\begin{array}{l}\mathbf{s}_t\\\mathbf{a}_t\end{array}\right)
$$



渐收敛到一个   $\mu=p_\theta(\mathbf{s},\mathbf{a})$   **平稳分布 (stationary distribution)**：之所以说平稳分布，是因为经过一次状态转移后，分布不发生变化   

$\mu=\mathcal{T}\mu$  => $(\mathcal{T}-\mathbf{I})\mu=0$ ;  $\mu$是 $\mathcal{T}$ 特征值为1的特征向量 

对目标函数进行平均，可以看出完全由平稳分布下的情形所控制。
$$
\theta^*=\arg\max_\theta\frac{1}{T}\sum_{t=1}^T\mathbf{E}_{(\mathbf{s}_t,\mathbf{a}_t)\sim p_\theta(\mathbf{s}_t,\mathbf{a}_t)}r(\mathbf{s}_t,\mathbf{a}_t)\rightarrow \mathbf{E}_{(\mathbf{s},\mathbf{a})\sim p_\theta(\mathbf{s},\mathbf{a})}r(\mathbf{s},\mathbf{a})
$$


##### Expectations and stochastic systems

- In RL, we almost always care about *expectations* .   增强学习中，我们几乎只关心期望，而不是个别的值，这是因为这给予了我们很好的数学性质。
- $r(\mathbf X)$ - not smooth 譬如说在盘山公路上开一辆车，如果正在运行那么收益函数为+1，如果掉下山崖则收益函数为-1。此时，我们的收益函数是不光滑的。
- $\pi_\theta(\mathbf a = fall) = \theta$  假如说我们从系统中提取出了一个概率，作为掉下的概率。
- $\mathbf E_{\pi_\theta} [r(\mathbf x)]$ - smooth in $\theta$.  此时如果我们关注期望的话，平稳分布下的收益函数的期望，则是关于 $\theta$ 光滑的！
- 这一点非常重要，允许我们使用诸如基于梯度的算法来优化非光滑的目标（可能是非光滑的转移，或者非光滑的收益函数等等导致）



#### The anatomy of a reinforcement learning algorithm

<img src="/img/CS285.assets/image-20200317013208886.png" alt="image-20200317013208886" style="zoom: 33%;" />



##### How do we deal with all these expectations?

$$E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$$

$$E_{\mathrm{s}_{\mathrm{s}} \sim p\left(\mathrm{s}_{1}\right)}$$  $$ + \underbrace{     \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad              }_{}$$

​				$$Q\left(\mathbf{s}_{1}, \mathbf{a}_{1}\right)=r\left(\mathbf{s}_{1}, \mathbf{a}_{1}\right)+E_{\mathbf{s}_{2} \sim p\left(\mathbf{s}_{2} \vert \mathbf{s}_{1}, \mathbf{a}_{1}\right)}\left[E_{\mathbf{a}_{2} \sim \pi\left(\mathbf{a}_{2} \vert \mathbf{s}_{2}\right)}\left[r\left(\mathbf{s}_{2}, \mathbf{a}_{2}\right)+\ldots \vert \mathbf{s}_{2}\right] \vert \mathbf{s}_{1}, \mathbf{a}_{1}\right]$$

$$E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]=E_{\mathbf{s}_{1} \sim p\left(\mathbf{s}_{1}\right)}\left[E_{\mathbf{a}_{1} \sim \pi\left(\mathbf{a}_{1} \vert \mathbf{s}_{1}\right)}\left[Q\left(\mathbf{s}_{1}, \mathbf{a}_{1}\right) \vert \mathbf{s}_{1}\right]\right]$$



##### Q-funtion , value funtion

$$
Q^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_t,\mathbf{a}_t]
\\
V^\pi(\mathbf{s}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_t]
\\
V^\pi(\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim\pi(\mathbf{a}_t|\mathbf{s}_t)}[Q^\pi(\mathbf{s}_t,\mathbf{a}_t)]
$$



$\mathbf{E}_{\mathbf{s}_1\sim p(\mathbf{s}_1)}[V^\pi(\mathbf{s}_1)]$ is the RL objective !



##### Using Q-functions and value functions

- Idea 1: if we have policy $\pi,$ and we know $Q^{\pi}(\mathbf{s}, \mathbf{a}),$ then we can improve $\pi:$
  set $\pi^{\prime}(\mathbf{a} | \mathbf{s})=1$ if $\mathbf{a}=\arg \max _{\mathbf{a}} Q^{\pi}(\mathbf{s}, \mathbf{a})$
- Idea 2: compute gradient to increase probability of good actions a:
  if $Q^{\pi}(\mathbf{s}, \mathbf{a})>V^{\pi}(\mathbf{s}),$ then $\mathbf{a}$ is better than average  



#### Types of RL algorithms

- **Policy gradients**: directly differentiate the above objective
- **Value-based**: estimate value function or Q-function of the optimal policy  (no explicit policy) 

- **Actor-critic**: estimate value function or Q-function of the current policy, use it to improve policy 
- **Model-based** RL: **estimate the transition model**, and then...
  - Use it for planning (no explicit policy) 
  - Use it to improve a policy ...



##### Model-based RL algorithms

<img src="/img/CS285.assets/image-20200317031143346.png" alt="image-20200317031143346" style="zoom: 33%;" />

核心是学习 状态转移概率的model,  可以使用各种方法, 神经网络之类.

- Just use the model to plan (no policy)
  - **Trajectory optimization**/**optimal control** (primarily in continuous spaces) – essentially backpropagation to optimize over actions
  - **Discrete planning** in discrete action spaces – e.g., **Monte Carlo tree search** 
- Backpropagate gradients into the policy    
  - Requires some tricks to make it work, unstable
- Use the model to learn a value function
  - **Dynamic programming**
  - Generate simulated experience for model-free learner (**Dyna**) 



##### Value function based algorithms

- fit $V(s)$ or $Q(s,a)$
- set $\pi(s) = \arg\max_a Q(s,a)$

##### Direct policy gradients

- evaluate returns : $R_\tau=\sum_tr(\mathbf{s}_t,\mathbf{a}_t)$
- $\theta\leftarrow\theta+\alpha\nabla_\theta\mathbf{E}\sum_tr(\mathbf{s}_t,\mathbf{a}_t)$

##### Actor-critic: value functions + policy gradients

- fit $V(s)$ or $Q(s,a)$
- $\theta\leftarrow\theta+\alpha\nabla_\theta\mathbf{E}\sum_tr(\mathbf{s}_t,\mathbf{a}_t)$



#### Tradeoffs

- Different tradeoffs
  - Sample efficiency
  - Stability & ease of use

- Different assumptions
  - Stochastic or deterministic?
  - Continuous or discrete?
  - Episodic or infinite horizon?	

- Different things are easy or hard in different settings
  - Easier to represent the policy? 
  - Easier to represent the model?



##### sample efficiency

- Off policy: able to improve the policy without generating new samples from that policy
- On policy: each time the policy is changed, even a little bit, we need to generate new samples



<img src="/img/CS285.assets/image-20200317033227158.png" alt="image-20200317033227158" style="zoom:50%;" />



##### stability and ease of use

converge

- Supervised learning: almost *always* gradient descent

- Reinforcement learning: often *not* gradient descent 强化学习不是真正意义上的GD based的优化
  - Q-learning: fixed point iteration  没用到梯度,  对Function Approximation不收敛
  - Model-based RL: model is not optimized for expected reward, model收敛但未必得到预期的reward
  - Policy gradient: *is* gradient descent, but also often the least efficient! 低效



- Value function fitting
  - At best, minimizes error of fit (“Bellman error”)
    - Not the same as expected reward
  - At worst, doesn’t optimize anything
    - Many popular deep RL value fitting algorithms are not guaranteed to converge to *anything* in the nonlinear case

- Model-based RL
  - Model minimizes error of fit
    - This will converge
  - No guarantee that better model = better policy

- Policy gradient
  - The only one that actually performs gradient descent (ascent) on the true objective



##### assumptions

- full observability
- episodic learning
- continuity or smoothness



#### Examples of specific algorithms 

-  Value function fitting methods 
   -  Q-learning, DQN
   -  Temporal difference learning
   -  Fitted value iteration   
-  Policy gradient methods
   - REINFORCE 
   - Natural policy gradient
   - Trust region policy optimization 
-  Actor-critic algorithms
   - Asynchronous advantage actor critic (A3C) 
-  Model-based RL algorithms
   - Dyna 
   - Guided policy search 



