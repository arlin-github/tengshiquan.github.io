---
layout:     post
title:      CS 285. Deep Reinforcement Learning, Decision Making, and Control
subtitle:   
date:       2020-03-16 12:00:00
author:     "tengshiquan"
header-img: "img/post-bg-berkeley.jpg"
catalog: true
tags:
    - AI
    - Reinforcement Learning

---



# CS 285. Deep Reinforcement Learning, Decision Making, and Control

2019 新版课程重头撸一遍 



## Overview

Deep 自动提取特征

<img src="/img/CS285.assets/image-20200316113451117.png" alt="image-20200316113451117" style="zoom:50%;" />



![image-20200316114541450](/img/CS285.assets/image-20200316114541450.png)

#### Beyond learning from reward

##### advanced topics

- Learning **reward functions** from example (**inverse reinforcement learning**)
- Transferring knowledge between domains (**transfer learning, meta-learning**) 
- Learning to predict and using prediction to act



#### Are there other forms of supervision?

- Learning from demonstrations  **Imitation Learning**
  - Directly copying observed behavior
  - Inferring rewards from observed behavior (**inverse reinforcement learning**)

- Learning from observing the world
  - Learning to predict
  - Unsupervised learning
- Learning from other tasks
  - **Transfer learning**
  - **Meta-learning**: learning to learn



- More than imitation: inferring intentions   推断意图
- Inverse RL 



##### Prediction 预测动作行为后的结果



#### Why deep reinforcement learning?

- Deep = can process complex sensory input
  -  ...and also compute really complex functions
- Reinforcement learning = can choose complex actions





## Supervised Learning of Behaviors

#### Terminology & notation

- $\pi_\theta(\mathbf{a}_t|\mathbf{o}_t)$   policy
- $\pi_\theta(\mathbf{a}_t|\mathbf{s}_t)$   policy (fully observed)
- $(\mathbf{s}, \mathbf{a})$ RL ;  $(\mathbf{x}, \mathbf{u})$ Control
- $r(\mathbf{s}, \mathbf{a}) = -c(\mathbf{x}, \mathbf{u})$



#### Imitation Learning

**behavioral cloning = supervised learning**

<img src="/img/CS285.assets/image-20200316150244300.png" alt="image-20200316150244300" style="zoom:50%;" />

stability : 如果训练数据只是单独一条轨迹, 则实际运行的时候, 可能造成很大的偏差. 如果有很多训练数据, 就是训练的路径宽了很多, 覆盖更多的区域, 这样实际运行的时候, 大多数都见过, 稳定性就好很多. 

stable controller 产生这些训练数据, 如果遇到扰动,可以自己纠正. 可以sample很多trajectory, 都有一点小错误, 然后纠正这些错误.  按照这个思路,可以启发式的, taking data and relabelling with fake actions. 也可以影响采集数据, 故意使用有点小错误的suboptimal demonstrator ,加入很多噪声. 或者不直接从人学习, 从stable controller 监督学习稳健的策略.



<img src="/img/CS285.assets/image-20200316143445136.png" alt="image-20200316143445136" style="zoom: 50%;" />

**域转移 (domain shift)** 监督学习的一个问题是, 从一个分布中采样数据训练一个model, 这个model在另外一个分布的数据集上的表现是不可预测的. 这个就是模仿学习不work的根源. 



#### DAgger: Dataset Aggregation

如果强制 training data distribution 与 policy running observation distribution 一样. 则通过监督学习来的policy应该表现不错.   

- make $$p_{\text {data }}\left(\mathbf{o}_{t}\right)=p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right) ?$$
- idea: instead of being clever about $$p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right),$$ be clever about $$p_{\text {data }}\left(\mathbf{o}_{t}\right) !$$
- goal: collect training data from $$p_{\pi_{\theta}}\left(\mathbf{o}_{t}\right)$$ instead of $$p_{\text {data }}\left(\mathbf{o}_{t}\right)$$ 
- how? just run $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ but need labels $a_t$ !   需要人工打标. 比如, 自动驾驶的时候, 人也按照车的策略来开, 然后给出正确的action.

1. train $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ from human data $$\mathcal{D}=\left\{\mathbf{o}_{1}, \mathbf{a}_{1}, \ldots, \mathbf{o}_{N}, \mathbf{a}_{N}\right\}$$
2. run $$\pi_{\theta}\left(\mathbf{a}_{t} \vert \mathbf{o}_{t}\right)$$ to get dataset $$\mathcal{D}_{\pi}=\left\{\mathbf{o}_{1}, \ldots, \mathbf{o}_{M}\right\}$$
3. Ask human to label $$\mathcal{D}_{\pi}$$ with actions  $a_{t}$
4. Aggregate: $\mathcal{D} \leftarrow \mathcal{D} \cup \mathcal{D}_{\pi}$ , goto 1.



#### Why might we fail to fit the expert?

1. Non-Markovian behavior
2. Multimodal behavior



##### use the whole history 

1. 过去几帧直接拼在一起
2. RNN , LSTM



##### causal confusion 因果混乱

<img src="/img/CS285.assets/image-20200316165335772.png" alt="image-20200316165335772" style="zoom:50%;" />

为什么越复杂的网络, 特别是用了history的复杂model, 表现不好,  这里举了一例子,  就是刹车的时候, 刹车灯会亮,  如果用了历史, 则model会学到最简单关联, 刹车灯亮了就是踩刹车. 

DAgger 无法搞定这个问题. 



##### Multimodal

例子, 左右都可以, 模型如果用一个高斯分布, 却会选择中间.

<img src="/img/CS285.assets/image-20200316165616269.png" alt="image-20200316165616269" style="zoom: 33%;" />

解决方案: 

1. Output mixture of Gaussians  $\pi(\mathbf{a} \vert \mathbf{o})=\sum_{i} w_{i} \mathcal{N}\left(\mu_{i}, \Sigma_{i}\right)$  实现最简单,但N是写死的
2. **Latent variable models**  **隐变量模型** 理论好, 但很难实现, hard to train
   1. Conditional variational autoencoder
   2. Normalizing flow/realNVP
   3. Stein variational gradient descent
3. Autoregressive discretization 自回归离散化  兼顾理论以及实现 
   - 有连续的动作，一个可行的方法是将其离散化；但是如果维度大了，离散化后的联合分布将维度灾难。一个小技巧是避免联合离散化所有维度。



#### A cost function for imitation

在实践中，奖励函数有很多种形式。譬如让一个机械手抓住一个小球并放到某个指定地点，当然我们可以选择$r(\mathbf{s},\mathbf{a})=\delta(小球在目标位置)$ 这样简单的函数，但这样的函数通常很难帮我们解决增强学习问题：直到你把小球移动到目标位置之前，你真的不知道你应该这样做。就像走钢丝.  所以通常解决实践问题，我们会设计一些更循序渐进的奖励函数.  如离目标的距离. 



#### 模仿学习总结:

- Often (but not always) insufficient by itself : **Distribution mismatch problem**

- Sometimes works well
  - Hacks (e.g. left/right images)
  - Samples from a stable trajectory distribution 
  - Add more **on-policy** data, e.g. using Dagger
  - Better models that fit more accurately





## Introduction to Reinforcement Learning

##### Markov Chain 

 $\mathcal{M}=\{\mathcal{S},\mathcal{T}\}$ , 

$\mathcal{T}$  transition operator 状态概率转移算子.    $p(s_{t+1} \vert s_t)$

let $$\mu_{t,i}=p(s_t=i)$$ ,  $$ \mathcal{T}_{i,j}=p(s_{t+1}=i\vert s_t=j)$$   , then $$\mu_{t+1}=\mathcal{T}\mu_t$$



##### Markov Decision Process

$\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{T},r\}$

$\mathcal{T}$  a tensor!

Let  $\mu_{t,j}=p(s_t = j)$  , $\xi_{t,k}=p(a_t=k)$  ,  $$\mathcal{T}_{i,j,k}=p(s_{t+1}=i\vert s_t=j,a_t=k)$$ 

$$\mu_{t+1,i}=\sum_{j,k}\mathcal{T}_{i,j,k}\mu_{t,j}\xi_{t,k}$$  



##### Partially Observed Markov Decision Process, POMDP

$\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{O},\mathcal{T},\mathcal{E},r\}$

$\mathcal{E}$ : emission probability ,  $p(o_t \vert s_t)$



#### The goal of reinforcement learning

$$
\underbrace{p_\theta(\mathbf{s}_1,\mathbf{a}_1,\ldots,\mathbf{s}_T,\mathbf{a}_T)}_{p_\theta(\tau)}=\underbrace{p(\mathbf{s}_1)\prod_{t=1}^T\pi_\theta(\mathbf{a}_t\vert\mathbf{s}_t)p(\mathbf{s}_{t+1}\vert\mathbf{s}_t,\mathbf{a}_t)}_{\text{Markov chain on }(\mathbf s, \mathbf a)}
$$

$$
\theta^*=\arg\max_\theta\mathbf{E}_{\tau\sim p_\theta(\tau)}\left[\sum_tr(\mathbf{s}_t,\mathbf{a}_t)\right]
$$



##### Finite horizon case: state-action marginal

因为上面公式是按照每个$\tau$的维度来求期望, 期望是线性相加的. 可以把sum拿到外面来, 下面就可以 按时间维度来统计边际分布

$$
\theta^*=\arg\max_\theta\sum_{t=1}^T\mathbf{E}_{(\mathbf{s}_t,\mathbf{a}_t)\sim p_\theta(\mathbf{s}_t,\mathbf{a}_t)}r(\mathbf{s}_t,\mathbf{a}_t) 
$$

$p_\theta(\mathbf{s}_t,\mathbf{a}_t)$ : **state-action marginal 边际分布**  ,  在时刻t, 各种(s,a)的几率



##### Infinite horizon case: stationary distribution

$\mathcal{T}$  state-action transition operator   ，是个线性变换

$$
\left(\begin{array}{l}\mathbf{s}_{t+1}\\\mathbf{a}_{t+1}\end{array}\right)=\mathcal{T}\left(\begin{array}{l}\mathbf{s}_t\\\mathbf{a}_t\end{array}\right) ,  \left(\begin{array}{l}\mathbf{s}_{t+k}\\\mathbf{a}_{t+k}\end{array}\right)=\mathcal{T}^k\left(\begin{array}{l}\mathbf{s}_t\\\mathbf{a}_t\end{array}\right)
$$



渐收敛到一个   $\mu=p_\theta(\mathbf{s},\mathbf{a})$   **平稳分布 (stationary distribution)**：之所以说平稳分布，是因为经过一次状态转移后，分布不发生变化   

$\mu=\mathcal{T}\mu$  => $(\mathcal{T}-\mathbf{I})\mu=0$ ;  $\mu$是 $\mathcal{T}$ 特征值为1的特征向量 

对目标函数进行平均，可以看出完全由平稳分布下的情形所控制。去掉了sum
$$
\theta^*=\arg\max_\theta\frac{1}{T}\sum_{t=1}^T\mathbf{E}_{(\mathbf{s}_t,\mathbf{a}_t)\sim p_\theta(\mathbf{s}_t,\mathbf{a}_t)}r(\mathbf{s}_t,\mathbf{a}_t)\rightarrow \mathbf{E}_{(\mathbf{s},\mathbf{a})\sim p_\theta(\mathbf{s},\mathbf{a})}r(\mathbf{s},\mathbf{a})
$$


##### Expectations and stochastic systems

- In RL, we almost always care about *expectations* .   增强学习中，我们几乎只关心期望，而不是个别的值，这是因为这给予了我们很好的数学性质。
- $r(\mathbf X)$ - not smooth 譬如说在盘山公路上开一辆车，如果正在运行那么收益函数为+1，如果掉下山崖则收益函数为-1。此时，我们的收益函数是不光滑的。
- $\pi_\theta(\mathbf a = fall) = \theta$  假如说我们从系统中提取出了一个概率，作为掉下的概率。
- $\mathbf E_{\pi_\theta} [r(\mathbf x)]$ - smooth in $\theta$.  此时如果我们关注期望的话，平稳分布下的收益函数的期望，则是关于 $\theta$ 光滑的！
- 这一点非常重要，允许我们使用诸如基于梯度的算法来优化非光滑的目标（可能是非光滑的转移，或者非光滑的收益函数等等导致）



#### The anatomy of a reinforcement learning algorithm

<img src="/img/CS285.assets/image-20200317013208886.png" alt="image-20200317013208886" style="zoom: 33%;" />



##### How do we deal with all these expectations?

$$E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]$$

$$E_{\mathrm{s}_{1} \sim p\left(\mathrm{s}_{1}\right)}$$  $$ + \underbrace{     \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad              }_{}$$

​				$$Q\left(\mathbf{s}_{1}, \mathbf{a}_{1}\right)=r\left(\mathbf{s}_{1}, \mathbf{a}_{1}\right)+E_{\mathbf{s}_{2} \sim p\left(\mathbf{s}_{2} \vert \mathbf{s}_{1}, \mathbf{a}_{1}\right)}\left[E_{\mathbf{a}_{2} \sim \pi\left(\mathbf{a}_{2} \vert \mathbf{s}_{2}\right)}\left[r\left(\mathbf{s}_{2}, \mathbf{a}_{2}\right)+\ldots \vert \mathbf{s}_{2}\right] \vert \mathbf{s}_{1}, \mathbf{a}_{1}\right]$$

$$E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right]=E_{\mathbf{s}_{1} \sim p\left(\mathbf{s}_{1}\right)}\left[E_{\mathbf{a}_{1} \sim \pi\left(\mathbf{a}_{1} \vert \mathbf{s}_{1}\right)}\left[Q\left(\mathbf{s}_{1}, \mathbf{a}_{1}\right) \vert \mathbf{s}_{1}\right]\right]$$

如果知道了Qfuntion, 则很容易improve 策略.



##### Q-funtion , value funtion

$$
Q^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_t,\mathbf{a}_t]
\\
V^\pi(\mathbf{s}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_t]
\\
V^\pi(\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim\pi(\mathbf{a}_t|\mathbf{s}_t)}[Q^\pi(\mathbf{s}_t,\mathbf{a}_t)]
$$



$\mathbf{E}_{\mathbf{s}_1\sim p(\mathbf{s}_1)}[V^\pi(\mathbf{s}_1)]$ is the RL objective !



##### Using Q-functions and value functions

- Idea 1: if we have policy $\pi,$ and we know $Q^{\pi}(\mathbf{s}, \mathbf{a}),$ then we can improve $\pi:$
  set $\pi^{\prime}(\mathbf{a} | \mathbf{s})=1$ if $\mathbf{a}=\arg \max _{\mathbf{a}} Q^{\pi}(\mathbf{s}, \mathbf{a})$
- Idea 2: compute gradient to increase probability of good actions a:
  if $Q^{\pi}(\mathbf{s}, \mathbf{a})>V^{\pi}(\mathbf{s}),$ then $\mathbf{a}$ is better than average  



#### Types of RL algorithms

- **Policy gradients**: directly differentiate the above objective
- **Value-based**: estimate value function or Q-function of the optimal policy  (no explicit policy) 

- **Actor-critic**: estimate value function or Q-function of the current policy, use it to improve policy 
- **Model-based** RL: **estimate the transition model**, and then...
  - Use it for planning (no explicit policy) 
  - Use it to improve a policy ...



##### Model-based RL algorithms

<img src="/img/CS285.assets/image-20200317031143346.png" alt="image-20200317031143346" style="zoom: 33%;" />

核心是学习 状态转移概率的model,  可以使用各种方法, 神经网络之类.

- Just use the model to plan (no policy)
  - **Trajectory optimization**/**optimal control** (primarily in continuous spaces) – essentially backpropagation to optimize over actions
  - **Discrete planning** in discrete action spaces – e.g., **Monte Carlo tree search** 
- Backpropagate gradients into the policy    
  - Requires some tricks to make it work, unstable
- Use the model to learn a value function
  - **Dynamic programming**
  - Generate simulated experience for model-free learner (**Dyna**) 



##### Value function based algorithms

- fit $V(s)$ or $Q(s,a)$
- set $\pi(s) = \arg\max_a Q(s,a)$

##### Direct policy gradients

- evaluate returns : $R_\tau=\sum_tr(\mathbf{s}_t,\mathbf{a}_t)$
- $\theta\leftarrow\theta+\alpha\nabla_\theta\mathbf{E}\sum_tr(\mathbf{s}_t,\mathbf{a}_t)$

##### Actor-critic: value functions + policy gradients

- fit $V(s)$ or $Q(s,a)$
- $\theta\leftarrow\theta+\alpha\nabla_\theta\mathbf{E}\sum_tr(\mathbf{s}_t,\mathbf{a}_t)$



#### Tradeoffs

- Different tradeoffs
  - Sample efficiency
  - Stability & ease of use

- Different assumptions
  - Stochastic or deterministic?
  - Continuous or discrete?
  - Episodic or infinite horizon?	

- Different things are easy or hard in different settings
  - Easier to represent the policy? 
  - Easier to represent the model?



##### sample efficiency  采样效率

- Off policy: able to improve the policy without generating new samples from that policy
- On policy: each time the policy is changed, even a little bit, we need to generate new samples



<img src="/img/CS285.assets/image-20200317033227158.png" alt="image-20200317033227158" style="zoom:50%;" />



##### stability and ease of use

converge  收敛性

- Supervised learning: almost *always* gradient descent

- Reinforcement learning: often *not* gradient descent 强化学习不是真正意义上的GD based的优化
  - Q-learning: fixed point iteration  没用到梯度,  对Function Approximation不收敛
  - Model-based RL: model is not optimized for expected reward, model收敛但未必得到预期的reward
  - Policy gradient: *is* gradient descent, but also often the least efficient! 低效



- Value function fitting
  - At best, minimizes error of fit (“Bellman error”)
    - Not the same as expected reward
  - At worst, doesn’t optimize anything
    - Many popular deep RL value fitting algorithms are not guaranteed to converge to *anything* in the nonlinear case

- Model-based RL
  - Model minimizes error of fit
    - This will converge
  - No guarantee that better model = better policy

- Policy gradient
  - The only one that actually performs gradient descent (ascent) on the true objective



##### assumptions

- full observability
- episodic learning
- continuity or smoothness



#### Examples of specific algorithms 

-  Value function fitting methods 
   -  Q-learning, DQN
   -  Temporal difference learning
   -  Fitted value iteration   
-  Policy gradient methods
   - REINFORCE 
   - Natural policy gradient
   - Trust region policy optimization 
-  Actor-critic algorithms
   - Asynchronous advantage actor critic (A3C) 
-  Model-based RL algorithms
   - Dyna 
   - Guided policy search 





## Policy Gradients
##### Evaluating the objective

$$
\theta^*=\arg\max_\theta  \underbrace{\mathbf{E}_{\tau\sim p_\theta(\tau)}\left[\sum_tr(\mathbf{s}_t,\mathbf{a}_t)\right]}_{J(\theta)}
$$

$$
J(\theta)=\mathbf{E}_{\tau\sim p_\theta(\tau)}\left[\sum_tr(\mathbf{s}_t,\mathbf{a}_t)\right] 
\approx \frac{1}{N}\sum_i\sum_tr(\mathbf{s}_{i,t},\mathbf{a}_{i,t})
$$

约等于是因为 蒙特卡洛方法抽样近似,  i : 1~N



##### Direct policy differentiation

- 这里的积分用期望公式的角度去看待 , 按照某个策略走得到的$\tau$分布, 再求期望.
- $r(\tau)$ 是与策略无关的, 因为具体的$\tau$ 已经生成, 只与env有关, 然后 状态转移概率也只与env相关.

$$
J(\theta)  = E_{\tau \sim \pi_\theta(\tau)}\underbrace{[r(\tau)]}_{\sum_{t=1}^Tr(s_t,a_t)} = \int \pi_\theta(\tau)r(\tau)\mathrm{d}\tau
$$

$$
\nabla_\theta J(\theta)=\int \nabla_\theta \pi_\theta(\tau)r(\tau)\mathrm{d}\tau = \int \pi_\theta(\tau)\nabla_\theta \log \pi_\theta(\tau)r(\tau)\mathrm{d}\tau\\ =\mathbf{E}_{\tau\sim p_\theta(\tau)}\left[ \nabla_\theta \log \pi_\theta(\tau)r(\tau) \right]
$$

- 上面推导利用了一个 log-gradient trick:

$$
\pi_\theta(\tau)\nabla_\theta \log \pi_\theta(\tau)= \pi_\theta(\tau)\frac{\nabla_\theta \pi_\theta(\tau)}{\pi_\theta(\tau)}=\nabla_\theta \pi_\theta(\tau)
$$

- 推导: 

$$
\underbrace{\pi_\theta(\mathbf{s}_1,\mathbf{a}_1,\ldots,\mathbf{s}_T,\mathbf{a}_T)}_{\pi_\theta(\tau)}= {p(\mathbf{s}_1)\prod_{t=1}^T\pi_\theta(\mathbf{a}_t\vert\mathbf{s}_t)p(\mathbf{s}_{t+1}\vert\mathbf{s}_t,\mathbf{a}_t)}  \\
\text{log of both sides} \\
\Rightarrow \log \pi_\theta(\tau)=\log p(\mathbf{s}_1)+\sum_{t=1}^T[\log \pi_\theta(\mathbf{a}_t|\mathbf{s}_t)+\log p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)]
$$

$$
\begin{aligned}
\nabla_\theta J(\theta) &=\mathbf{E}_{\tau\sim \pi_\theta(\tau)}[\nabla_\theta \log \pi_\theta(\tau)r(\tau)] \quad \text{代入上面的 log, 这里} r(\tau) 看成常量 \\ 
&= \mathbf{E}_{\tau\sim \pi_\theta(\tau)} \left[\nabla_\theta \left( {\color{red}{ \underline{\log p(\mathbf{s}_1)}}}+\sum_{t=1}^T[\log \pi_\theta(\mathbf{a}_t|\mathbf{s}_t)+ {\color{red}{ \underline{\log p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)}}} ] \right)  r(\tau) \right] \quad \text{红色部分梯度为0}\\  
&=   \mathbf{E}_{\tau\sim p_\theta(\tau)}\left[\left(\sum_{t=1}^T\nabla_\theta\log \pi_\theta(\mathbf{a}_t|\mathbf{s}_t)\right)\left(\sum_{t=1}^Tr(\mathbf{s}_t,\mathbf{a}_t)\right)\right]
\end{aligned}
$$

- 初始分布和转移概率本身都与参数$\theta$ 并不相关;  $r(\tau)$ 也只与env相关, 将$r(\tau)$展开, 这部分已经完全在梯度之外了, 可以用因果律



##### Evaluating the policy gradient  评估策略梯度

使用蒙特卡洛估计法来评估 , 就得到  REINFORCE 算法

$$
\begin{aligned}
J(\theta)&=E_{\tau \sim p_{\theta}(\tau)}\left[\sum_{t} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \right] \approx  \frac{1}{N} \sum_{i} \sum_{t} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right) \\
\nabla_{\theta} J(\theta)&=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right] \\
\nabla_{\theta} J(\theta) &\approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)
 
\end{aligned}
$$


最简单的策略梯度法REINFORCE (Williams, 1992):

1. 运行策略$\pi_\theta(\mathbf{a} \vert \mathbf{s})$，抽取样本$$\{\tau^i\}$$
2. 估计梯度$$\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t} \vert \mathbf{s}_{i,t})\right)\left(\sum_{t=1}^Tr(\mathbf{s}_{i,t},\mathbf{a}_{i,t})\right)\right]$$
3.  $\theta\leftarrow\theta+\alpha\nabla_\theta J$



#### Comparison to maximum likelihood

<img src="/img/CS285.assets/image-20200317155857540.png" alt="image-20200317155857540" style="zoom: 33%;" />

- policy gradient:

$$
\nabla_{\theta} J(\theta)  \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)\right)
$$

- maximum likelihood:

$$
\nabla_\theta J_\mathrm{ML}(\theta)\approx\frac{1}{N}\sum_{i=1}^N\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right) 
$$

- “trial and error” : maximum likelihood 是提升所有的样本中出现的action的可能性, 模仿学习中, 只要出现了(s,a),就说明人类在s下选择了a,都是正向的. 这时loss函数只考虑策略与样本的概率距离, 当然也可以设定没选的其他a都是-1;  PG的每一步算上了总体的回报, 好的action提升的可能性更高. 

- 对模仿学习中的监督学习,  两者的一部分很相似:   **Score Function**   $\nabla_{\theta} \log \pi_{\theta}\left(\mathbf{s} , \mathbf{a} \right) $

- 一个trick, 可以用交叉熵来实现PG.



补充: **Gaussian Policy**: Continuous Action

- In continuous action spaces, a Gaussian policy is natural 
- Mean is a linear combination of state features   $\mu(s) = \phi(s)^T \theta$ 
- Nonlinear extensions: replace $\phi(s)$ with a deep neural network with trainable weights w
- Variance may be fixed $\sigma^2$, or can also parameterized 
- Policy is Gaussian $a \sim \mathcal{N}\left(\mu(s), \sigma^{2}\right)$ 
- The score function is

$$
\nabla_{\theta} \log \pi_{\theta}(s, a)=\frac{(a-\mu(s)) \phi(s)}{\sigma^{2}}
$$

#### Partial observability 

- 如果观测不完全，即只有部分观测信息
- 在策略梯度法中, 没有利用Markov性,  所以PG算法可以直接使用
- 任何系统都可以转化为Markovian System, 只要定义过去所有的观察序列作为state

$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{o}_{i,t})\right)\left(\sum_{t=1}^Tr(\mathbf{s}_{i,t},\mathbf{a}_{i,t})\right)\right]
$$

- Env 会给出observation 以及reward value, not reward  function,  这些value depends on states. 如果value depends on observation, 则可以等效的假定reward是随机的. 因为observation是state的随机consequence. 



#### What is wrong with the policy gradient?

PG的一个重要问题就是整体reward的取值影响收敛速度.  即reward函数的设计非常关键.

例子, 下面假设一个env, 是一维的, 越靠左边的trajectory越不好, 实线的曲线代表当前策略,  黄色的3个sample的reward都是正的, 将当前策略移到了中间虚线, 绿色的3个sample,一负两正, 将当前策略优化到了右边的虚线位置.  更差的情况, 如果一个好的action得到的总回报是0, 则这个sample不会对策略进行任何的改进.

<img src="/img/CS285.assets/image-20200317224936146.png" alt="image-20200317224936146" style="zoom:50%;" />

**High variance 高方差**: 从不同的sample里面得到非常不同的gradient. 说明gradient非常noisy. 如果用modest数量的sample, 不会straight to optimum, 会得到一个zigzag的路径, 如果是较大的学习率, 可能永远达不到optimum. 实践中最大的问题.



##### Reducing variance

- 对于一个变量的方差, 如果这个变量本身小的话, 则显然方差也会小. 所以要尽量减少这个变量变化的幅度. 

- **causality**  利用因果性来减少取值.  policy at time t' cannot affect reward at time t when t< t' . 

- 之前是看总体reward, 一个$\tau$ 的每个action都乘以 $r(\tau)$ , 是一个sum

$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\left[\left(\sum_{t=1}^T\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{o}_{i,t})\right)\left(\sum_{t=1}^Tr(\mathbf{s}_{i,t},\mathbf{a}_{i,t})\right)\right]
$$

- 然后把这个sum拿到每步里面去. 

$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t})\left(\sum_{t'=1}^Tr(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)\right]
$$

- 再利用因果性, 每个action只是在产生作用后才有影响, 所以内部的sum从t开始, 这里减小了reward的波动幅度, 值跟之前的总体reward不相等, 

$$
\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T \nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}|\mathbf{s}_{i,t}) \underbrace{\left(\sum_{\color{red}{t'= t}}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)}_{\text{reward to go: } \hat Q_{i,t}}
$$

- 这个sum这时可以替换为Q,   而这个公式也是一般PG代码中的实现, 这个trick是标配. only helps.



##### Baselines

- 这个trick 也是 never hurts, only helps. 减去一个baseline, 可以减少方差

- 一个极端反例: 好的action  reward 1000000+1, 坏的action reward 1000000-1, 这样好的坏的都有类似幅度的提升, 如果一开始坏的随机到的多, 基本上没法再学到好的了

- 核心思想:   让 better than average more likely ,  worse than average less likely. 

- 直觉上就是用所有回报的平均值作为baseline.   not best, but pretty good

  $$
  b=\frac{1}{N}\sum_{i=1}^Nr(\tau_i) \\
  \nabla_\theta J(\theta)\approx \frac{1}{N} \sum_{i=t}^N \Big[\nabla_\theta \log \pi_\theta(\tau) \big(r(\tau)-b \big)\Big]
  $$
  
-  证明,  subtracting a baseline is *unbiased* in expectation!   只要这个baseline是const,  有没有baseline的期望与之前一样.   能提取到最前面的前提是, baseline需要对batch中的每个$\tau$ 取值都一样即可.

  $$
  E\left[\nabla_{\theta} \log \pi_{\theta}(\tau) b\right]=\int \pi_{\theta}(\tau) \nabla_{\theta} \log \pi_{\theta}(\tau) b d \tau=\int \nabla_{\theta} \pi_{\theta}(\tau) b d \tau=b \nabla_{\theta} \int \pi_{\theta}(\tau) d \tau=b \nabla_{\theta} 1=0
  $$
  

##### Analyzing variance

下面分析为什么baseline会减少方差. 以及求出 理论上最优的baseline.

- 方差定义:  $$\operatorname{Var}[x]=E\left[x^{2}\right]-E[x]^{2} $$

- $$\nabla_{\theta} J(\theta)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau)(r(\tau)-b)\right]$$ 

- $$
  \operatorname{Var}=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\nabla_{\theta} \log \pi_{\theta}(\tau)(r(\tau)-b)\right)^{2}\right]- \underbrace{ E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau)(r(\tau)-b)\right]^{2} }_{\text { this bit is just } E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau)\right]   }
  $$

-  let $$g(\tau):=\nabla_\theta \log \pi_\theta(\tau)$$

- $$
  \frac{d \operatorname{Var}}{d b}=\frac{d}{d b} E\left[g(\tau)^{2}(r(\tau)-b)^{2}\right]=\frac{d}{d b}\left(E\left[g(\tau)^{2} r(\tau)^{2}\right]-2 E\left[g(\tau)^{2} r(\tau) b\right]+b^{2} E\left[g(\tau)^{2}\right]\right)\\
   
  =-2 E\left[g(\tau)^{2} r(\tau)\right]+2 b E\left[g(\tau)^{2}\right]=0
  $$

- 得出最优b , just expected reward, but weighted by gradient magnitudes!
  $$
  b=\frac{E\left[g(\tau)^{2} r(\tau)\right]}{E\left[g(\tau)^{2}\right]}
  $$
  
- 实践中, 用平均回报即可





#### Policy gradient is on-policy 

- PG是on-policy ,  求期望的时候必须是从当前的分布上采样才能有无偏性，REINFORCE算法必须在当前策略上采样很多,才评估当前策略, 这就要求每次梯度更新之后就根据新分布全部重新采样
- 在DRL中, 神经网络每次BP, 在每个gradient step可能只更新一点点，但也要求把之前的样本全都扔了然后重新采样，对数据的利用率是非常低的。



#### Off-policy learning & importance sampling

- 还是要求去估计最新的期望，但是可以考虑用其他分布去估计它。提高sample利用率.

- IS 是 somewhat  off-policy , 一般被认为 off-policy

- **重要性抽样** (importance sampling):

  $$
  \mathbf{E}_{x\sim p(x)}[f(x)]=\int p(x)f(x)\mathrm{d}x=\int q(x)\frac{p(x)}{q(x)}f(x)\mathrm{d}x=\mathbf{E}_{x\sim q(x)}\left[\frac{p(x)}{q(x)}f(x)\right]
  $$
  
- 将 IS 代入 
  $$
  J(\theta)=\mathbf{E}_{\tau\sim \bar{\pi}(\tau)}\left[\frac{\pi_\theta(\tau)}{\bar{\pi}(\tau)}r(\tau)\right]
  $$

- $$
  \frac{\pi_\theta(\tau)}{\bar{\pi}(\tau)}=\frac{p(\mathbf{s}_1)\prod_{t=1}^T\pi_\theta(\mathbf{a}_t|\mathbf{s}_t)p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)}{p(\mathbf{s}_1)\prod_{t=1}^T\bar{\pi}(\mathbf{a}_t|\mathbf{s}_t)p(\mathbf{s}_{t+1}|\mathbf{s}_t,\mathbf{a}_t)}=\frac{\prod_{t=1}^T\pi_\theta(\mathbf{a}_t|\mathbf{s}_t)}{\prod_{t=1}^T\bar{\pi}(\mathbf{a}_t|\mathbf{s}_t)}
  $$
  
- estimate the value of some new parameters $\theta^{\prime}$

- $$
  J\left(\theta^{\prime}\right)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\pi_{\theta^{\prime}}(\tau)}{\pi_{\theta}(\tau)} r(\tau)\right]
  $$

  只有分子依赖参数$\theta^{\prime}$

- $$
  \nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\nabla_{\theta^{\prime}} \pi_{\theta^{\prime}}(\tau)}{\pi_{\theta}(\tau)} r(\tau)\right]=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\pi_{\theta^{\prime}}(\tau)}{\pi_{\theta} (\tau)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\tau) r(\tau)\right]
  $$

- 上面公式是 off-policy,  如果要该公式用作on-policy, 则 estimate locally , at $ \theta=\theta^{\prime}$: 则回到了之前的PG公式. 也是一个推导的角度. on-policy是off-policy的特例.
  $$
   \nabla_{\theta} J(\theta)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta} \log \pi_{\theta}(\tau) r(\tau)\right]
  $$

下面重点看 off-policy PG


$$
\begin{aligned}
\nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) &=E_{\tau \sim \pi_{\theta}(\tau)}\left[\frac{\pi_{\theta^{\prime}}(\tau)}{\pi_{\theta}(\tau)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}(\tau) r(\tau)\right] \quad \text { when } \theta \neq \theta^{\prime} \\
&=E_{\tau \sim \pi_{\theta}(\tau)}\left[\left(\prod_{t=1}^{T} \frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}{\pi_{\theta}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)}\right)\left(\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\right)\left(\sum_{t=1}^{T} r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\right] \text { what about causality? } \\
&=E_{\tau \sim \pi_{\theta}(\tau)} \left [\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\left(\prod_{t^{\prime}=1}^{t} \frac{\pi_{\theta^{\prime}} (\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}} )}{\pi_{\theta} (\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}} )}\right) \left(\sum_{t^{\prime}=t}^{T} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)\left(\color{red}{  \prod_{t^{\prime \prime} = t}^{t'} \frac{\pi_{\theta^{\prime}} (\mathbf{a}_{t^{\prime\prime}} | \mathbf{s}_{t^{\prime \prime}} )}{\pi_{\theta}\left(\mathbf{a}_{t^{''}} | \mathbf{s}_{t^{''}}\right)} } \right)\right)\right]
\end{aligned}
$$

- 一个问题是, 都是很多概率相乘, T个,指数级,  会得到超级大或者小的数, 会有精度问题, 并且造成方差很大.
- 可以引入因果性, 一定程度上缓解这个问题. 
- 做一个近似, 不再是原来的PG算法, 前提是, off-policy采样的策略是old $\theta$,而不是其他不相干的策略. 将上式最后一部分近似成1, 可以得到  policy iteration algorithm,  然后仍然能improve策略.



##### A first-order approximation for IS

$$
\nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right)=E_{\tau \sim \pi_{\theta}(\tau)}\left[\sum_{t=1}^{T} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{t} | \mathbf{s}_{t}\right)\left( \underline{ \prod_{t^{\prime}=1}^{t} \frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}\right)}{\pi_{\theta}\left(\mathbf{a}_{t^{\prime}} | \mathbf{s}_{t^{\prime}}\right)} } \right) \left(\sum_{t^{\prime}=t}^{T} r\left(\mathbf{s}_{t^{\prime}}, \mathbf{a}_{t^{\prime}}\right)\right)\right]
$$

- 画线部分仍然是指数级 exponential in T ,  改写成下面两种情况

- on-policy policy gradient:  $$\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} \vert \mathbf{s}_{i, t}\right) \hat{Q}_{i, t}$$

- off-policy policy gradient:   连乘拿到log外, 求导只是对$log\pi$的 , 但$\pi(s,a)$仍然是麻烦的,是个边际分布, 无法确切知道每个t所有的(s,a)的分布, 所以再改成条件概率

  $$
  \nabla_{\theta^{\prime}} J\left(\theta^{\prime}\right) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \frac{\pi_{\theta^{\prime}}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)}{\pi_{\theta}\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right) \hat{Q}_{i, t} \\
  =\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \color{red}{ \frac{\pi_{\theta^{\prime}}\left(\mathbf{s}_{i, t}\right)}{\pi_{\theta}\left(\mathbf{s}_{i, t}\right)} }\frac{\pi_{\theta^{\prime}}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)}{\pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)} \nabla_{\theta^{\prime}} \log \pi_{\theta^{\prime}}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right) \hat{Q}_{i, t}
  $$
  
  当两个策略很接近的时候, 可以忽略掉红色部分. 前提, old $\theta$ 不能太old, 两个策略必须很接近. 



####  Policy gradient with automatic differentiation

具体实现部分. 利用交叉熵*Q



#### Policy gradient in practice

Practical considerations: batch size, learning rates, optimizers

- Remember that the gradient has **high variance**
  - This isn’t the same as supervised learning!
  - Gradients will be really noisy!
- Consider using much **larger batches**
- Tweaking learning rates is very hard
  - Adaptive step size rules like **ADAM** can be OK-ish
  - We’ll learn about policy gradient-specific learning rate adjustment methods later!





## Actor-Critic Algorithms

#### Improving the policy gradient

- PG 利用causality,  将 total reward 换成了 Q

- $\hat Q_{i,t}$ : estimate of expected reward   taken $a_{i,t}$ in $s_{i,t}$ ,  是个统计值, 因为采样的随机性, 方差很大, 很可能不准

- $$Q(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'}) \vert \mathbf{s}_t,\mathbf{a}_t]$$  :  true  expected  reward-to-go , Q的真值, 代入梯度公式效果肯定更好,  是个 **lower variance PG**  算法

- 再考虑 baseline,  从 average reward  =>average Q, $$b_t=\frac{1}{N}\sum_iQ^\pi(\mathbf{s}_{i,t},\mathbf{a}_{i,t})$$ 

- 而  $$V (\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim\pi_\theta(\mathbf{a}_t\vert \mathbf{s}_t)}[Q (\mathbf{s}_t,\mathbf{a}_t)]$$ ,   直观解释, 比平均好的action有更多的可能性. 

- 这里,有个homework, 是证明baseline 是可以 depend on  state. 

- 代入, 得到Advantage.  下面公式里面都带上标$\pi$

- $$Q^\pi(\mathbf{s}_t,\mathbf{a}_t)=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})\vert\mathbf{s}_t,\mathbf{a}_t]$$

- $$V^\pi(\mathbf{s}_t)=\mathbf{E}_{\mathbf{a}_t\sim\pi_\theta(\mathbf{a}_t\vert\mathbf{s}_t)}[Q^\pi(\mathbf{s}_t,\mathbf{a}_t)]$$

- $A^\pi(\mathbf{s}_t,\mathbf{a}_t)=Q^\pi(\mathbf{s}_t,\mathbf{a}_t)-V^\pi(\mathbf{s}_t)$  how much better $\mathcal a_t$ is 

- $$\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}\vert\mathbf{s}_{i,t})A^\pi(\mathbf{s}_t,\mathbf{a}_t)\right]$$ ,   A越准确, 方差越低

- 然而 Advantage  function 也是未知的, 所以只能approximation

- $$\nabla_\theta J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t=1}^T\left[\nabla_\theta\log \pi_\theta(\mathbf{a}_{i,t}\vert\mathbf{s}_{i,t}) \bigg(\sum_{t'=1}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'}) -b \bigg) \right]$$  , 之前的baseline公式, 是一种Advantage,但不是best.  括号中的部分 unbiased, but high variance single-sample estimate 

- 显然 从一个个sample中直接平均求 V, Q, A 效果不好, 方差大

- 下面寻找更好的 estimate方法 ,  fit  what  to  what ,   V, Q, A 三个选哪个作为拟合目标

- 一般AC算法都是拟合V ,   off-policy的情况会拟合Q

- $$
  \begin{aligned}
  Q^\pi(\mathbf{s}_t,\mathbf{a}_t)&=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})\vert\mathbf{s}_t,\mathbf{a}_t] \\
  
  & 拆分Q, 当前的r(s_t,a_t)不依赖于任何\theta, 是个准确的值, 再加上未来V的期望 \\ 
  
  &=r(\mathbf{s}_t,\mathbf{a}_t)+\sum_{t'=t+1}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_t,\mathbf{a}_t] \\ 
  
  
  &=r(\mathbf{s}_t,\mathbf{a}_t)+ V^\pi(\mathbf{s}_{t+1})\quad 因为具体的s_{t+1} 是什么状态并不知道, 所以还是要用期望 \\
  
  &=r(\mathbf{s}_t,\mathbf{a}_t)+\mathbf{E}_{\mathbf{s}_{t+1}\sim p(\mathbf{s_{t+1}}|\mathbf{s}_t,\mathbf{a}_t)}[V^\pi(\mathbf{s}_{t+1})] \\
  
  &如果 V^\pi 的估值比较准确,那可能做个近似是ok的,利用一个无偏估计的近似,就是只用single\  sample的下个状态的V \\
  & \approx  r(\mathbf{s}_t,\mathbf{a}_t)+ V^\pi(\mathbf{s}_{t+1})\\ 
  &因为s_{t+1}有各种可能,这里只取一个具体sample.但这是个decent\  approximation ,损失一些精度, 引入了一些方差, 但是很方便使用\\
  \end{aligned}
  $$

  

- 下面 A 可以方便的用V 表示
  $$
  A^\pi(\mathbf{s}_t,\mathbf{a}_t)\approx r(\mathbf{s}_t,\mathbf{a}_t)+V^\pi(\mathbf{s}_{t+1})-V^\pi(\mathbf{s}_t)
  $$



#### Policy evaluation

- 下面就考虑怎么准确的 fit  V, 也就是 评估当前策略. 

- 方法之一: Monte Carlo policy evaluation  (PG也是蒙特卡洛)

- $$V^\pi(\mathbf{s}_t)\approx\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$  single sample estimation

- $$V^\pi(\mathbf{s}_t)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$  需要在$s_t$这个状态很多采样, 需要在终点后回到$s_t$, reset the simulator, 很多env无法做到

  <img src="/img/CS285.assets/image-20200318235808468.png" alt="image-20200318235808468" style="zoom: 33%;" />

- 下面假设env无法reset,  只能一个一个trajectory 的sample.  这里是从时间t维度看的, 而不是具体的s. 所以下面图中的多个trajectory, 在t时刻, 可能遇到差不多类似的$s_t$(欧拉距离非常接近).  

- 感觉这种的考虑的方式适合连续情况, 比如自动驾驶, 如果用图像做state, 基本无法重现同一个state, 或者state本身不包含太多逻辑性在里面的env;  对于围棋之类的, 两个state,只差一个落子, 结果天壤之别的,不是很适用. 还一个是情况比如是迷宫, $s_t$ 中, t 与 s 的关联程度可能很低.

- 感觉RL想用统一的框架去解决所有问题, 忽视了问题本身的特性, 目前来说还是不靠谱.

- value  function的value 会changes drastically as time step changes,  即v(last t) = last step reward,   v(firtst  step) = sum T 个reward

- $$V^\pi(\mathbf{s}_t)\approx\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$  ,  not as good  as  $$V^\pi(\mathbf{s}_t)\approx\frac{1}{N}\sum_{i=1}^N\sum_{t'=t}^Tr(\mathbf{s}_{t'},\mathbf{a}_{t'})$$ ,  but  pretty good.  是不如同一个$s_t$出发, 但还不错, 但如果有很多类似的$s_t$, 也会减小方差 ; 神经网络也有泛化能力,会去平均比较相似的一类state的reward

- PG的方差指的是variance of the entire gradient, not the variance in a particular state. 整个的方差才是需要care的, 下面学习的时候BP, 应用到网络参数中. 从初始状态开始生成很多trajectory是ok的.

- <img src="/img/CS285.assets/image-20200319000233746.png" alt="image-20200319000233746" style="zoom:33%;" />

- training  data: $$\left\{\left(\mathbf{s}_{i,t},y_{i,t}:=\sum_{t'=t}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})\right)\right\}$$

- Supervised regression:   神经网络监督学习最小化loss  $\mathcal{L}(\phi)=\frac{1}{2}\sum_i\left\Vert\hat{V}_\phi^\pi(\mathbf{s}_i)-y_i\right\Vert^2$

##### Can we do better?

- 引入 TD,  **bootstrap** estimate.  感觉 RL是基于统计的, 函数拟合的,  这个算法只是为了拟合value的时候尽可能的方差小, 拟合起来更容易. 不会去考虑value是怎么来的. 但对一定规模问题已经可以求解.

- ideal target :  这里有i, 代表一个sample;  跟之前 Q值的近似一样
  $$
  \begin{aligned}
  y_{i,t} &=\sum_{t'=t}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_{i,t}] \\
  &\approx r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+ \sum_{t'={t+1}}^T\mathbf{E}_{\pi_\theta}[r(\mathbf{s}_{t'},\mathbf{a}_{t'})|\mathbf{s}_{i,{t+1}}]  \\
  &\approx r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+V^\pi(\mathbf{s}_{i,t+1}) \quad 这个真值不知道 \\
  &\approx r(\mathbf{s}_{i,t},\mathbf{a}_{i,t})+\hat{V}_\phi^\pi(\mathbf{s}_{i,t+1}) \quad 这个目标是有偏的, 但是方差小, 因为更稳
  \end{aligned}
  $$

- variance 与 biases  两者的关系类似于信噪比 signal-to-noise ratio , 两个信号, 一个方差很高, 淹没了真值,  一个方差很低, 稍微有点偏差, 样本数量比较少的时候, 当然后者好.  不过这个也看具体问题, 需要 tradeoff.

- 之前的 Monte Carlo target :  $$y_{i,t} = \sum_{t'=t}^T r(\mathbf{s}_{i,t'},\mathbf{a}_{i,t'})$$

- training  data: $$\left\{\left(\mathbf{s}_{i, t}, r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)\right)\right\}$$   biased, low variance

- 一个点是, 如果用 bootstrap,  一开始的网络的输出都是垃圾值,  所以网络的参数初始化方面可以做点文章, 比如用蒙特卡洛的return来初始化.



##### Policy evaluation examples

从大框架说, TD-Gammon  与 AlphaGo 差不多.

<img src="/img/CS285.assets/image-20200319024553029.png" alt="image-20200319024553029" style="zoom:50%;" />





#### Actor-critic algorithm

batch actor-critic algorithm:
1. sample $$\left\{\mathbf{s}_{i}, \mathbf{a}_{i}\right\}$$ from $$\pi_{\theta}(\mathbf{a} \vert \mathbf{s})$$ (run it on the robot)
2. fit $\hat{V}_{\phi}^{\pi}$ (s) to sampled reward sums
3. evaluate $$\hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}^{\prime}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)$$
4.    $$\nabla_{\theta} J(\theta) \approx \sum_{i} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i} \vert \mathbf{s}_{i}\right) \hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)$$
5.    $\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$

显然这里有两个网络要去train. 



#### Discount factors

1. episodic task
2. continuous/cyclical tasks

- what if $T$ (episode length) is $\infty ?$ $\hat{V}_{\phi}^{\pi}$ can get infinitely large in many cases

- simple trick: **better to get rewards sooner than later**
- ​    $$y_{i, t} \approx r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)$$
- discount factor $\gamma \in[0,1]$ (0.99 works well)



<img src="/img/CS285.assets/image-20200319033806252.png" alt="image-20200319033806252" style="zoom: 33%;" />

 为了更好的理解 $\gamma$, 可以构建一个不含discount的MDP, 但是该MDP的RL公式是带discount的.  在状态集合中新增一个死亡状态 (death state)，本质上是一个吸收状态，且此后收益一直为0（或者理解为游戏立即结束). 



##### discount factors for policy gradients

$$
\begin{aligned}
&y_{i, t} \approx r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)\\
&\mathcal{L}(\phi)=\frac{1}{2} \sum_{i}\left\|\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)-y_{i}\right\|^{2} \quad \begin{array}{l}
\text { with critic: } \\
\nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)(\overbrace{r\left(\mathbf{s}_{i, t}, \mathbf{a}_{i, t}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t+1}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i, t}\right)}^{\hat A^\pi(\mathbf s_{i,t}, \mathbf a_{i,t})})
\end{array}
\end{aligned}
$$



what about (Monte Carlo) policy gradients? 这里比较重要, 关于$\gamma$ 的解释

- option 1:  对$s_t$后面每个r discount 一下
  $$
  \nabla_{\theta} J(\theta) \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\sum_{t^{\prime}=t}^{T} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right)
  $$

- option 2:  对所有r 都discount 
  $$
  \begin{aligned}
  \nabla_{\theta} J(\theta) & \approx \frac{1}{N} \sum_{i=1}^{N}\left(\sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\right)\left(\sum_{t=1}^{T} \gamma^{t-1} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right) \\
  & \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\sum_{t'=t}^{T} \gamma^{t^{\prime}-1} r \left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right) \\
  & \approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \gamma^{t-1} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i, t} | \mathbf{s}_{i, t}\right)\left(\sum_{t'=t}^{T} \gamma^{t^{\prime}-t} r\left(\mathbf{s}_{i, t^{\prime}}, \mathbf{a}_{i, t^{\prime}}\right)\right)
  \end{aligned}
  $$

- Option2  看起来更像 short-term 策略, 因为更关注最近的r , 后期的r因为权重原因, 不太重要了. 对于上面那个自己构建的MDP, 有死亡状态的,  有几率直接结束游戏的, 所以必须更加关注前面步骤的优化 , 显然option2更合适.  
- Option1 是实践中经常用到的,    主要是解决 continuous/cyclical tasks,  比如想让机器人走路尽可能时间长.  用option2会在前期过度优化.  option1 实际上并不是准确的policy gradient. option1 可以被解释为approximation to the gradient of average reward without a discount. 参考 Philip Thomas 论文.
- Option 1的直觉解释, 数列的收敛性.  $\gamma \to 1$ , 可以得到 total average reward, 让其收敛.  但未来的reward是高度不确定的, 越往后方差越大, 后期超级不稳定, 基本会让任务失败.  所以通过$\gamma$ 来切断很遥远未来的reward, 将他们弄的很小, 减小方差的手段.  然后关注于当下时刻的new   reward, 因为这些都是更准确的reward, 可以作为 biased但方差更小的average reward.
- 所以还是看具体问题来选择参数.  游戏类的不太关注这个,  搞机器人control的讲究无限长周期的平均回报



#### Actor-critic algorithms (with discount)

batch actor-critic algorithm:
1. sample $$\left\{\mathbf{s}_{i}, \mathbf{a}_{i}\right\}$$ from $$\pi_{\theta}(\mathbf{a} \vert \mathbf{s})$$ (run it on the robot)
2. fit $$\hat{V}_{\phi}^{\pi}(\mathbf{s})$$ to sampled reward sums
3. evaluate $$\hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)=r\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}^{\prime}\right)-\hat{V}_{\phi}^{\pi}\left(\mathbf{s}_{i}\right)$$
4.   $$\nabla_{\theta} J(\theta) \approx \sum_{i} \nabla_{\theta} \log \pi_{\theta}\left(\mathbf{a}_{i} \vert \mathbf{s}_{i}\right) \hat{A}^{\pi}\left(\mathbf{s}_{i}, \mathbf{a}_{i}\right)$$
5.    $$\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$$

online actor-critic algorithm:
1. take action a $$\sim \pi_{\theta}(\mathbf{a} \vert \mathbf{s}),$$ get $$\left(\mathbf{s}, \mathbf{a}, \mathbf{s}^{\prime}, r\right)$$
2. update $$\hat{V}_{\phi}^{\pi}$$ using target $$r+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}^{\prime}\right)$$
3. evaluate $$\hat{A}^{\pi}(\mathbf{s}, \mathbf{a})=r(\mathbf{s}, \mathbf{a})+\gamma \hat{V}_{\phi}^{\pi}\left(\mathbf{s}^{\prime}\right)-\hat{V}_{\phi}^{\pi}(\mathbf{s})$$
4. ​    $$\nabla_{\theta} J(\theta) \approx \nabla_{\theta} \log \pi_{\theta}(\mathbf{a} \vert \mathbf{s}) \hat{A}^{\pi}(\mathbf{s}, \mathbf{a})$$
5. ​    $$\theta \leftarrow \theta+\alpha \nabla_{\theta} J(\theta)$$







